
principal component analysis pca is popular technique for analyzing large datasets containing high number of dimensions features per observation increasing the interpretability of data while preserving the maximum amount of information and enabling the visualization of multidimensional data 
formally pca is statistical technique for reducing the dimensionality of dataset 
this is accomplished by linearly transforming the data into new coordinate system where most of the variation in the data can be described with fewer dimensions than the initial data 
many studies use the first two principal components in order to plot the data in two dimensions and to visually identify clusters of closely related data points 
principal component analysis has applications in many fields such as population genetics microbiome studies and atmospheric science 
the principal components of collection of points in real coordinate space are sequence of unit vectors where the th vector is the direction of line that best fits the data while being orthogonal to the first vectors 
here best fitting line is defined as one that minimizes the average squared perpendicular distance from the points to the line 
these directions constitute an orthonormal basis in which different individual dimensions of the data are linearly uncorrelated 
principal component analysis pca is the process of computing the principal components and using them to perform change of basis on the data sometimes using only the first few principal components and ignoring the rest 
in data analysis the first principal component of set of variables presumed to be jointly normally distributed is the derived variable formed as linear combination of the original variables that explains the most variance 
the second principal component explains the most variance in what is left once the effect of the first component is removed and we may proceed through iterations until all the variance is explained 
pca is most commonly used when many of the variables are highly correlated with each other and it is desirable to reduce their number to an independent set 
pca is used in exploratory data analysis and for making predictive models 
it is commonly used for dimensionality reduction by projecting each data point onto only the first few principal components to obtain lower dimensional data while preserving as much of the data variation as possible 
the first principal component can equivalently be defined as direction that maximizes the variance of the projected data 
the th principal component can be taken as direction orthogonal to the first principal components that maximizes the variance of the projected data 
for either objective it can be shown that the principal components are eigenvectors of the data covariance matrix 
thus the principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix 
pca is the simplest of the true eigenvector based multivariate analyses and is closely related to factor analysis 
factor analysis typically incorporates more domain specific assumptions about the underlying structure and solves eigenvectors of slightly different matrix 
pca is also related to canonical correlation analysis cca 
cca defines coordinate systems that optimally describe the cross covariance between two datasets while pca defines new orthogonal coordinate system that optimally describes variance in single dataset 
robust and norm based variants of standard pca have also been proposed 
history pca was invented in by karl pearson as an analogue of the principal axis theorem in mechanics it was later independently developed and named by harold hotelling in the 
depending on the field of application it is also named the discrete karhunen lo ve transform klt in signal processing the hotelling transform in multivariate quality control proper orthogonal decomposition pod in mechanical engineering singular value decomposition svd of invented in the last quarter of the th century eigenvalue decomposition evd of xtx in linear algebra factor analysis for discussion of the differences between pca and factor analysis see ch 
of jolliffe principal component analysis eckart young theorem harman or empirical orthogonal functions eof in meteorological science empirical eigenfunction decomposition sirovich empirical component analysis lorenz quasiharmonic modes brooks et al spectral decomposition in noise and vibration and empirical modal analysis in structural dynamics 
intuition pca can be thought of as fitting dimensional ellipsoid to the data where each axis of the ellipsoid represents principal component 
if some axis of the ellipsoid is small then the variance along that axis is also small 
to find the axes of the ellipsoid we must first center the values of each variable in the dataset on by subtracting the mean of the variable observed values from each of those values 
these transformed values are used instead of the original observed values for each of the variables 
then we compute the covariance matrix of the data and calculate the eigenvalues and corresponding eigenvectors of this covariance matrix 
then we must normalize each of the orthogonal eigenvectors to turn them into unit vectors 
once this is done each of the mutually orthogonal unit eigenvectors can be interpreted as an axis of the ellipsoid fitted to the data 
this choice of basis will transform the covariance matrix into diagonalized form in which the diagonal elements represent the variance of each axis 
the proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues 
biplots and scree plots degree of explained variance are used to explain findings of the pca 
details pca is defined as an orthogonal linear transformation that transforms the data to new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate called the first principal component the second greatest variance on the second coordinate and so on consider an data matrix with column wise zero empirical mean the sample mean of each column has been shifted to zero where each of the rows represents different repetition of the experiment and each of the columns gives particular kind of feature say the results from particular sensor 
mathematically the transformation is defined by set of size of dimensional vectors of weights or coefficients that map each row vector of to new vector of principal component scores given by in such way that the individual variables of considered over the data set successively inherit the maximum possible variance from with each coefficient vector constrained to be unit vector where is usually selected to be strictly less than to reduce dimensionality 
first component in order to maximize variance the first weight vector thus has to satisfy arg max arg max equivalently writing this in matrix form gives arg max arg max since has been defined to be unit vector it equivalently also satisfies arg max the quantity to be maximised can be recognised as rayleigh quotient 
standard result for positive semidefinite matrix such as xtx is that the quotient maximum possible value is the largest eigenvalue of the matrix which occurs when is the corresponding eigenvector 
with found the first principal component of data vector can then be given as score in the transformed co ordinates or as the corresponding vector in the original variables 
further components the th component can be found by subtracting the first principal components from and then finding the weight vector which extracts the maximum variance from this new data matrix arg max it turns out that this gives the remaining eigenvectors of xtx with the maximum values for the quantity in brackets given by their corresponding eigenvalues 
thus the weight vectors are eigenvectors of xtx 
the th principal component of data vector can therefore be given as score tk in the transformed coordinates or as the corresponding vector in the space of the original variables where is the kth eigenvector of xtx 
the full principal components decomposition of can therefore be given as where is by matrix of weights whose columns are the eigenvectors of xtx 
the transpose of is sometimes called the whitening or sphering transformation 
columns of multiplied by the square root of corresponding eigenvalues that is eigenvectors scaled up by the variances are called loadings in pca or in factor analysis 
covariances xtx itself can be recognized as proportional to the empirical sample covariance matrix of the dataset xt 
the sample covariance between two of the different principal components over the dataset is given by where the eigenvalue property of has been used to move from line to line however eigenvectors and corresponding to eigenvalues of symmetric matrix are orthogonal if the eigenvalues are different or can be orthogonalised if the vectors happen to share an equal repeated value 
the product in the final line is therefore zero there is no sample covariance between different principal components over the dataset 
another way to characterise the principal components transformation is therefore as the transformation to coordinates which diagonalise the empirical sample covariance matrix 
in matrix form the empirical covariance matrix for the original variables can be written the empirical covariance matrix between the principal components becomes where is the diagonal matrix of eigenvalues of xtx 
is equal to the sum of the squares over the dataset associated with each component that is tk 
dimensionality reduction the transformation maps data vector from an original space of variables to new space of variables which are uncorrelated over the dataset 
however not all the principal components need to be kept 
keeping only the first principal components produced by using only the first eigenvectors gives the truncated transformation where the matrix tl now has rows but only columns 
in other words pca learns linear transformation where the columns of matrix form an orthogonal basis for the features the components of representation that are decorrelated 
by construction of all the transformed data matrices with only columns this score matrix maximises the variance in the original data that has been preserved while minimising the total squared reconstruction error or such dimensionality reduction can be very useful step for visualising and processing high dimensional datasets while still retaining as much of the variance in the dataset as possible 
for example selecting and keeping only the first two principal components finds the two dimensional plane through the high dimensional dataset in which the data is most spread out so if the data contains clusters these too may be most spread out and therefore most visible to be plotted out in two dimensional diagram whereas if two directions through the data or two of the original variables are chosen at random the clusters may be much less spread apart from each other and may in fact be much more likely to substantially overlay each other making them indistinguishable 
similarly in regression analysis the larger the number of explanatory variables allowed the greater is the chance of overfitting the model producing conclusions that fail to generalise to other datasets 
one approach especially when there are strong correlations between different possible explanatory variables is to reduce them to few principal components and then run the regression against them method called principal component regression 
dimensionality reduction may also be appropriate when the variables in dataset are noisy 
if each column of the dataset contains independent identically distributed gaussian noise then the columns of will also contain similarly identically distributed gaussian noise such distribution is invariant under the effects of the matrix which can be thought of as high dimensional rotation of the co ordinate axes 
however with more of the total variance concentrated in the first few principal components compared to the same noise variance the proportionate effect of the noise is less the first few components achieve higher signal to noise ratio 
pca thus can have the effect of concentrating much of the signal into the first few principal components which can usefully be captured by dimensionality reduction while the later principal components may be dominated by noise and so disposed of without great loss 
if the dataset is not too large the significance of the principal components can be tested using parametric bootstrap as an aid in determining how many principal components to retain 
singular value decomposition the principal components transformation can also be associated with another matrix factorization the singular value decomposition svd of here is an by rectangular diagonal matrix of positive numbers called the singular values of is an by matrix the columns of which are orthogonal unit vectors of length called the left singular vectors of and is by whose columns are orthogonal unit vectors of length and called the right singular vectors of in terms of this factorization the matrix xtx can be written where is the square diagonal matrix with the singular values of and the excess zeros chopped off that satisfies comparison with the eigenvector factorization of xtx establishes that the right singular vectors of are equivalent to the eigenvectors of xtx while the singular values of are equal to the square root of the eigenvalues of xtx 
using the singular value decomposition the score matrix can be written so each column of is given by one of the left singular vectors of multiplied by the corresponding singular value 
this form is also the polar decomposition of efficient algorithms exist to calculate the svd of without having to form the matrix xtx so computing the svd is now the standard way to calculate principal components analysis from data matrix unless only handful of components are required 
as with the eigen decomposition truncated score matrix tl can be obtained by considering only the first largest singular values and their singular vectors the truncation of matrix or using truncated singular value decomposition in this way produces truncated matrix that is the nearest possible matrix of rank to the original matrix in the sense of the difference between the two having the smallest possible frobenius norm result known as the eckart young theorem 
further considerations the singular values in are the square roots of the eigenvalues of the matrix xtx 
each eigenvalue is proportional to the portion of the variance more correctly of the sum of the squared distances of the points from their multidimensional mean that is associated with each eigenvector 
the sum of all the eigenvalues is equal to the sum of the squared distances of the points from their multidimensional mean 
pca essentially rotates the set of points around their mean in order to align with the principal components 
this moves as much of the variance as possible using an orthogonal transformation into the first few dimensions 
the values in the remaining dimensions therefore tend to be small and may be dropped with minimal loss of information see below 
pca is often used in this manner for dimensionality reduction 
pca has the distinction of being the optimal orthogonal transformation for keeping the subspace that has largest variance as defined above 
this advantage however comes at the price of greater computational requirements if compared for example and when applicable to the discrete cosine transform and in particular to the dct ii which is simply known as the dct 
nonlinear dimensionality reduction techniques tend to be more computationally demanding than pca 
pca is sensitive to the scaling of the variables 
if we have just two variables and they have the same sample variance and are completely correlated then the pca will entail rotation by and the weights they are the cosines of rotation for the two variables with respect to the principal component will be equal 
but if we multiply all values of the first variable by then the first principal component will be almost the same as that variable with small contribution from the other variable whereas the second component will be almost aligned with the second original variable 
this means that whenever the different variables have different units like temperature and mass pca is somewhat arbitrary method of analysis 
different results would be obtained if one used fahrenheit rather than celsius for example 
pearson original paper was entitled on lines and planes of closest fit to systems of points in space in space implies physical euclidean space where such concerns do not arise 
one way of making the pca less arbitrary is to use variables scaled so as to have unit variance by standardizing the data and hence use the autocorrelation matrix instead of the autocovariance matrix as basis for pca 
however this compresses or expands the fluctuations in all dimensions of the signal space to unit variance 
mean centering is necessary for performing classical pca to ensure that the first principal component describes the direction of maximum variance 
if mean subtraction is not performed the first principal component might instead correspond more or less to the mean of the data 
mean of zero is needed for finding basis that minimizes the mean square error of the approximation of the data mean centering is unnecessary if performing principal components analysis on correlation matrix as the data are already centered after calculating correlations 
correlations are derived from the cross product of two standard scores scores or statistical moments hence the name pearson product moment correlation 
also see the article by kromrey foster johnson on mean centering in moderated regression much ado about nothing 
since covariances are correlations of normalized variables or standard scores pca based on the correlation matrix of is equal to pca based on the covariance matrix of the standardized version of pca is popular primary technique in pattern recognition 
it is not however optimized for class separability 
however it has been used to quantify the distance between two or more classes by calculating center of mass for each class in principal component space and reporting euclidean distance between center of mass of two or more classes 
the linear discriminant analysis is an alternative which is optimized for class separability 
table of symbols and abbreviations properties and limitations of pca properties some properties of pca include property for any integer consider the orthogonal linear transformation where is element vector and is matrix and let be the variance covariance matrix for then the trace of denoted tr is maximized by taking where consists of the first columns of is the transpose of property consider again the orthonormal transformation with and defined as before 
then tr is minimized by taking where consists of the last columns of the statistical implication of this property is that the last few pcs are not simply unstructured left overs after removing the important pcs 
because these last pcs have variances as small as possible they are useful in their own right 
they can help to detect unsuspected near constant linear relationships between the elements of and they may also be useful in regression in selecting subset of variables from and in outlier detection 
property spectral decomposition of before we look at its usage we first look at diagonal elements var then perhaps the main statistical implication of the result is that not only can we decompose the combined variances of all the elements of into decreasing contributions due to each pc but we can also decompose the whole covariance matrix into contributions from each pc 
although not strictly decreasing the elements of will tend to become smaller as increases as is nonincreasing for increasing whereas the elements of tend to stay about the same size because of the normalization constraints limitations as noted above the results of pca depend on the scaling of the variables 
this can be cured by scaling each feature by its standard deviation so that one ends up with dimensionless features with unital variance the applicability of pca as described above is limited by certain tacit assumptions made in its derivation 
in particular pca can capture linear correlations between the features but fails when this assumption is violated see figure in the reference 
in some cases coordinate transformations can restore the linearity assumption and pca can then be applied see kernel pca 
another limitation is the mean removal process before constructing the covariance matrix for pca 
in fields such as astronomy all the signals are non negative and the mean removal process will force the mean of some astrophysical exposures to be zero which consequently creates unphysical negative fluxes and forward modeling has to be performed to recover the true magnitude of the signals 
as an alternative method non negative matrix factorization focusing only on the non negative elements in the matrices which is well suited for astrophysical observations 
see more at relation between pca and non negative matrix factorization 
pca is at disadvantage if the data has not been standardized before applying the algorithm to it 
pca transforms original data into data that is relevant to the principal components of that data which means that the new data variables cannot be interpreted in the same ways that the originals were 
they are linear interpretations of the original variables 
also if pca is not performed properly there is high likelihood of information loss pca relies on linear model 
if dataset has pattern hidden inside it that is nonlinear then pca can actually steer the analysis in the complete opposite direction of progress 
researchers at kansas state university discovered that the sampling error in their experiments impacted the bias of pca results 
if the number of subjects or blocks is smaller than and or the researcher is interested in pc beyond the first it may be better to first correct for the serial correlation before pca is conducted 
the researchers at kansas state also found that pca could be seriously biased if the autocorrelation structure of the data is not correctly handled 
pca and information theory dimensionality reduction results in loss of information in general 
pca based dimensionality reduction tends to minimize that information loss under certain signal and noise models 
under the assumption that that is that the data vector is the sum of the desired information bearing signal and noise signal one can show that pca can be optimal for dimensionality reduction from an information theoretic point of view 
in particular linsker showed that if is gaussian and is gaussian noise with covariance matrix proportional to the identity matrix the pca maximizes the mutual information between the desired information and the dimensionality reduced output if the noise is still gaussian and has covariance matrix proportional to the identity matrix that is the components of the vector are iid but the information bearing signal is non gaussian which is common scenario pca at least minimizes an upper bound on the information loss which is defined as 
the optimality of pca is also preserved if the noise is iid and at least more gaussian in terms of the kullback leibler divergence than the information bearing signal in general even if the above signal model holds pca loses its information theoretic optimality as soon as the noise becomes dependent 
computing pca using the covariance method the following is detailed description of pca using the covariance method see also here as opposed to the correlation method the goal is to transform given data set of dimension to an alternative data set of smaller dimension equivalently we are seeking to find the matrix where is the karhunen lo ve transform klt of matrix organize the data setsuppose you have data comprising set of observations of variables and you want to reduce the data so that each observation can be described with only variables suppose further that the data are arranged as set of data vectors with each representing single grouped observation of the variables 
write as row vectors each with elements 
place the row vectors into single matrix of dimensions calculate the empirical meanfind the empirical mean along each column place the calculated mean values into an empirical mean vector of dimensions calculate the deviations from the meanmean subtraction is an integral part of the solution towards finding principal component basis that minimizes the mean square error of approximating the data 
hence we proceed by centering the data as follows subtract the empirical mean vector from each row of the data matrix store mean subtracted data in the matrix where is an column vector of all for in some applications each variable column of may also be scaled to have variance equal to see score 
this step affects the calculated principal components but makes them independent of the units used to measure the different variables 
find the covariance matrixfind the empirical covariance matrix from matrix where is the conjugate transpose operator 
if consists entirely of real numbers which is the case in many applications the conjugate transpose is the same as the regular transpose 
the reasoning behind using instead of to calculate the covariance is bessel correction find the eigenvectors and eigenvalues of the covariance matrixcompute the matrix of eigenvectors which diagonalizes the covariance matrix where is the diagonal matrix of eigenvalues of this step will typically involve the use of computer based algorithm for computing eigenvectors and eigenvalues 
these algorithms are readily available as sub components of most matrix algebra systems such as sas matlab mathematica scipy idl interactive data language or gnu octave as well as opencv 
matrix will take the form of an diagonal matrix where is the jth eigenvalue of the covariance matrix and matrix also of dimension contains column vectors each of length which represent the eigenvectors of the covariance matrix the eigenvalues and eigenvectors are ordered and paired 
the jth eigenvalue corresponds to the jth eigenvector 
matrix denotes the matrix of right eigenvectors as opposed to left eigenvectors 
in general the matrix of right eigenvectors need not be the conjugate transpose of the matrix of left eigenvectors rearrange the eigenvectors and eigenvaluessort the columns of the eigenvector matrix and eigenvalue matrix in order of decreasing eigenvalue 
make sure to maintain the correct pairings between the columns in each matrix compute the cumulative energy content for each eigenvectorthe eigenvalues represent the distribution of the source data energy among each of the eigenvectors where the eigenvectors form basis for the data 
the cumulative energy content for the jth eigenvector is the sum of the energy content across all of the eigenvalues from through for select subset of the eigenvectors as basis vectorssave the first columns of as the matrix where use the vector as guide in choosing an appropriate value for the goal is to choose value of as small as possible while achieving reasonably high value of on percentage basis 
for example you may want to choose so that the cumulative energy is above certain threshold like percent 
in this case choose the smallest value of such that project the data onto the new basisthe projected data points are the rows of the matrix that is the first column of is the projection of the data points onto the first principal component the second column is the projection onto the second principal component etc 
derivation of pca using the covariance method let be dimensional random vector expressed as column vector 
without loss of generality assume has zero mean 
we want to find orthonormal transformation matrix so that px has diagonal covariance matrix that is px is random vector with all its distinct components pairwise uncorrelated 
quick computation assuming were unitary yields cov cov hence holds if and only if cov were diagonalisable by this is very constructive as cov is guaranteed to be non negative definite matrix and thus is guaranteed to be diagonalisable by some unitary matrix 
covariance free computation in practical implementations especially with high dimensional data large the naive covariance method is rarely used because it is not efficient due to high computational and memory costs of explicitly determining the covariance matrix 
the covariance free approach avoids the np operations of explicitly calculating and storing the covariance matrix xtx instead utilizing one of matrix free methods for example based on the function evaluating the product xt at the cost of np operations 
iterative computation one way to compute the first principal component efficiently is shown in the following pseudo code for data matrix with zero mean without ever computing its covariance matrix 
random vector of length norm do times vector of length for each row in rts is the eigenvalue error norm exit if error tolerance return this power iteration algorithm simply calculates the vector xt normalizes and places the result back in the eigenvalue is approximated by rt xtx which is the rayleigh quotient on the unit vector for the covariance matrix xtx 
if the largest singular value is well separated from the next largest one the vector gets close to the first principal component of within the number of iterations which is small relative to at the total cost cnp 
the power iteration convergence can be accelerated without noticeably sacrificing the small cost per iteration using more advanced matrix free methods such as the lanczos algorithm or the locally optimal block preconditioned conjugate gradient lobpcg method 
subsequent principal components can be computed one by one via deflation or simultaneously as block 
in the former approach imprecisions in already computed approximate principal components additively affect the accuracy of the subsequently computed principal components thus increasing the error with every new computation 
the latter approach in the block power method replaces single vectors and with block vectors matrices and every column of approximates one of the leading principal components while all columns are iterated simultaneously 
the main calculation is evaluation of the product xt 
implemented for example in lobpcg efficient blocking eliminates the accumulation of the errors allows using high level blas matrix matrix product functions and typically leads to faster convergence compared to the single vector one by one technique 
the nipals method non linear iterative partial least squares nipals is variant the classical power iteration with matrix deflation by subtraction implemented for computing the first few components in principal component or partial least squares analysis 
for very high dimensional datasets such as those generated in the omics sciences for example genomics metabolomics it is usually only necessary to compute the first few pcs 
the non linear iterative partial least squares nipals algorithm updates iterative approximations to the leading scores and loadings and by the power iteration multiplying on every iteration by on the left and on the right that is calculation of the covariance matrix is avoided just as in the matrix free implementation of the power iterations to xtx based on the function evaluating the product xt tx the matrix deflation by subtraction is performed by subtracting the outer product from leaving the deflated residual matrix used to calculate the subsequent leading pcs 
for large data matrices or matrices that have high degree of column collinearity nipals suffers from loss of orthogonality of pcs due to machine precision round off errors accumulated in each iteration and matrix deflation by subtraction 
gram schmidt re orthogonalization algorithm is applied to both the scores and the loadings at each iteration step to eliminate this loss of orthogonality 
nipals reliance on single vector multiplications cannot take advantage of high level blas and results in slow convergence for clustered leading singular values both these deficiencies are resolved in more sophisticated matrix free block solvers such as the locally optimal block preconditioned conjugate gradient lobpcg method 
online sequential estimation in an online or streaming situation with data arriving piece by piece rather than being stored in single batch it is useful to make an estimate of the pca projection that can be updated sequentially 
this can be done efficiently but requires different algorithms 
pca and qualitative variables in pca it is common that we want to introduce qualitative variables as supplementary elements 
for example many quantitative variables have been measured on plants 
for these plants some qualitative variables are available as for example the species to which the plant belongs 
these data were subjected to pca for quantitative variables 
when analyzing the results it is natural to connect the principal components to the qualitative variable species 
for this the following results are produced 
identification on the factorial planes of the different species for example using different colors 
representation on the factorial planes of the centers of gravity of plants belonging to the same species 
for each center of gravity and each axis value to judge the significance of the difference between the center of gravity and origin these results are what is called introducing qualitative variable as supplementary element 
this procedure is detailed in and husson pag and pag few software offer this option in an automatic way 
this is the case of spad that historically following the work of ludovic lebart was the first to propose this option and the package factominer 
applications intelligence the earliest application of factor analysis was in locating and measuring components of human intelligence 
it was believed that intelligence had various uncorrelated components such as spatial intelligence verbal intelligence induction deduction etc and that scores on these could be adduced by factor analysis from results on various tests to give single index known as the intelligence quotient iq 
the pioneering statistical psychologist spearman actually developed factor analysis in for his two factor theory of intelligence adding formal technique to the science of psychometrics 
in thurstone looked for factors of intelligence developing the notion of mental age 
standard iq tests today are based on this early work 
residential differentiation in shevky and williams introduced the theory of factorial ecology which dominated studies of residential differentiation from the to the 
neighbourhoods in city were recognizable or could be distinguished from one another by various characteristics which could be reduced to three by factor analysis 
these were known as social rank an index of occupational status familism or family size and ethnicity cluster analysis could then be applied to divide the city into clusters or precincts according to values of the three key factor variables 
an extensive literature developed around factorial ecology in urban geography but the approach went out of fashion after as being methodologically primitive and having little place in postmodern geographical paradigms 
one of the problems with factor analysis has always been finding convincing names for the various artificial factors 
in flood revived the factorial ecology approach to show that principal components analysis actually gave meaningful answers directly without resorting to factor rotation 
the principal components were actually dual variables or shadow prices of forces pushing people together or apart in cities 
the first component was accessibility the classic trade off between demand for travel and demand for space around which classical urban economics is based 
the next two components were disadvantage which keeps people of similar status in separate neighbourhoods mediated by planning and ethnicity where people of similar ethnic backgrounds try to co locate about the same time the australian bureau of statistics defined distinct indexes of advantage and disadvantage taking the first principal component of sets of key variables that were thought to be important 
these seifa indexes are regularly published for various jurisdictions and are used frequently in spatial analysis 
development indexes pca has been the only formal method available for the development of indexes which are otherwise hit or miss ad hoc undertaking 
the city development index was developed by pca from about indicators of city outcomes in survey of global cities 
the first principal component was subject to iterative regression adding the original variables singly until about of its variation was accounted for 
the index ultimately used about indicators but was good predictor of many more variables 
its comparative value agreed very well with subjective assessment of the condition of each city 
the coefficients on items of infrastructure were roughly proportional to the average costs of providing the underlying services suggesting the index was actually measure of effective physical and social investment in the city 
the country level human development index hdi from undp which has been published since and is very extensively used in development studies has very similar coefficients on similar indicators strongly suggesting it was originally constructed using pca 
population genetics in cavalli sforza and others pioneered the use of principal components analysis pca to summarise data on variation in human gene frequencies across regions 
the components showed distinctive patterns including gradients and sinusoidal waves 
they interpreted these patterns as resulting from specific ancient migration events 
since then pca has been ubiquitous in population genetics with thousands of papers using pca as display mechanism 
genetics varies largely according to proximity so the first two principal components actually show spatial distribution and may be used to map the relative geographical location of different population groups thereby showing individuals who have wandered from their original locations pca in genetics has been technically controversial in that the technique has been performed on discrete non normal variables and often on binary allele markers 
the lack of any measures of standard error in pca are also an impediment to more consistent usage 
in august the molecular biologist eran elhaik published theoretical paper in scientific reports analyzing pca applications 
he concluded that it was easy to manipulate the method which in his view generated results that were erroneous contradictory and absurd 
specifically he argued the results achieved in population genetics were characterized by cherry picking and circular reasoning 
market research and indexes of attitude market research has been an extensive user of pca 
it is used to develop customer satisfaction or customer loyalty scores for products and with clustering to develop market segments that may be targeted with advertising campaigns in much the same way as factorial ecology will locate geographical areas with similar characteristics pca rapidly transforms large amounts of data into smaller easier to digest variables that can be more rapidly and readily analyzed 
in any consumer questionnaire there are series of questions designed to elicit consumer attitudes and principal components seek out latent variables underlying these attitudes 
for example the oxford internet survey in asked people about their attitudes and beliefs and from these analysts extracted four principal component dimensions which they identified as escape social networking efficiency and problem creating another example from joe flood in extracted an attitudinal index toward housing from attitude questions in national survey of households in australia 
the first principal component represented general attitude toward property and home ownership 
the index or the attitude questions it embodied could be fed into general linear model of tenure choice 
the strongest determinant of private renting by far was the attitude index rather than income marital status or household type 
quantitative finance in quantitative finance principal component analysis can be directly applied to the risk management of interest rate derivative portfolios 
trading multiple swap instruments which are usually function of other market quotable swap instruments is sought to be reduced to usually or principal components representing the path of interest rates on macro basis 
converting risks to be represented as those to factor loadings or multipliers provides assessments and understanding beyond that available to simply collectively viewing risks to individual buckets 
pca has also been applied to equity portfolios in similar fashion both to portfolio risk and to risk return 
one application is to reduce portfolio risk where allocation strategies are applied to the principal portfolios instead of the underlying stocks 
second is to enhance portfolio return using the principal components to select stocks with upside potential 
neuroscience variant of principal components analysis is used in neuroscience to identify the specific properties of stimulus that increases neuron probability of generating an action potential 
this technique is known as spike triggered covariance analysis 
in typical application an experimenter presents white noise process as stimulus usually either as sensory input to test subject or as current injected directly into the neuron and records train of action potentials or spikes produced by the neuron as result 
presumably certain features of the stimulus make the neuron more likely to spike 
in order to extract these features the experimenter calculates the covariance matrix of the spike triggered ensemble the set of all stimuli defined and discretized over finite time window typically on the order of ms that immediately preceded spike 
the eigenvectors of the difference between the spike triggered covariance matrix and the covariance matrix of the prior stimulus ensemble the set of all stimuli defined over the same length time window then indicate the directions in the space of stimuli along which the variance of the spike triggered ensemble differed the most from that of the prior stimulus ensemble 
specifically the eigenvectors with the largest positive eigenvalues correspond to the directions along which the variance of the spike triggered ensemble showed the largest positive change compared to the varince of the prior 
since these were the directions in which varying the stimulus led to spike they are often good approximations of the sought after relevant stimulus features 
in neuroscience pca is also used to discern the identity of neuron from the shape of its action potential 
spike sorting is an important procedure because extracellular recording techniques often pick up signals from more than one neuron 
in spike sorting one first uses pca to reduce the dimensionality of the space of action potential waveforms and then performs clustering analysis to associate specific action potentials with individual neurons 
pca as dimension reduction technique is particularly suited to detect coordinated activities of large neuronal ensembles 
it has been used in determining collective variables that is order parameters during phase transitions in the brain 
relation with other methods correspondence analysis correspondence analysis ca was developed by jean paul benz cri and is conceptually similar to pca but scales the data which should be non negative so that rows and columns are treated equivalently 
it is traditionally applied to contingency tables 
ca decomposes the chi squared statistic associated to this table into orthogonal factors 
because ca is descriptive technique it can be applied to tables for which the chi squared statistic is appropriate or not 
several variants of ca are available including detrended correspondence analysis and canonical correspondence analysis 
one special extension is multiple correspondence analysis which may be seen as the counterpart of principal component analysis for categorical data 
factor analysis principal component analysis creates variables that are linear combinations of the original variables 
the new variables have the property that the variables are all orthogonal 
the pca transformation can be helpful as pre processing step before clustering 
pca is variance focused approach seeking to reproduce the total variable variance in which components reflect both common and unique variance of the variable 
pca is generally preferred for purposes of data reduction that is translating variable space into optimal factor space but not when the goal is to detect the latent construct or factors 
factor analysis is similar to principal component analysis in that factor analysis also involves linear combinations of variables 
different from pca factor analysis is correlation focused approach seeking to reproduce the inter correlations among variables in which the factors represent the common variance of variables excluding unique variance 
in terms of the correlation matrix this corresponds with focusing on explaining the off diagonal terms that is shared co variance while pca focuses on explaining the terms that sit on the diagonal 
however as side result when trying to reproduce the on diagonal terms pca also tends to fit relatively well the off diagonal correlations 
results given by pca and factor analysis are very similar in most situations but this is not always the case and there are some problems where the results are significantly different 
factor analysis is generally used when the research purpose is detecting data structure that is latent constructs or factors or causal modeling 
if the factor model is incorrectly formulated or the assumptions are not met then factor analysis will give erroneous results 
means clustering it has been asserted that the relaxed solution of means clustering specified by the cluster indicators is given by the principal components and the pca subspace spanned by the principal directions is identical to the cluster centroid subspace 
however that pca is useful relaxation of means clustering was not new result and it is straightforward to uncover counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions 
non negative matrix factorization non negative matrix factorization nmf is dimension reduction method where only non negative elements in the matrices are used which is therefore promising method in astronomy in the sense that astrophysical signals are non negative 
the pca components are orthogonal to each other while the nmf components are all non negative and therefore constructs non orthogonal basis 
in pca the contribution of each component is ranked based on the magnitude of its corresponding eigenvalue which is equivalent to the fractional residual variance frv in analyzing empirical data 
for nmf its components are ranked based only on the empirical frv curves 
the residual fractional eigenvalue plots that is as function of component number given total of components for pca has flat plateau where no data is captured to remove the quasi static noise then the curves dropped quickly as an indication of over fitting and captures random noise 
the frv curves for nmf is decreasing continuously when the nmf components are constructed sequentially indicating the continuous capturing of quasi static noise then converge to higher levels than pca indicating the less over fitting property of nmf 
iconography of correlations it is often difficult to interpret the principal components when the data include many variables of various origins or when some variables are qualitative 
this leads the pca user to delicate elimination of several variables 
if observations or variables have an excessive impact on the direction of the axes they should be removed and then projected as supplementary elements 
in addition it is necessary to avoid interpreting the proximities between the points close to the center of the factorial plane 
the iconography of correlations on the contrary which is not projection on system of axes does not have these drawbacks 
we can therefore keep all the variables 
the principle of the diagram is to underline the remarkable correlations of the correlation matrix by solid line positive correlation or dotted line negative correlation 
strong correlation is not remarkable if it is not direct but caused by the effect of third variable 
conversely weak correlations can be remarkable 
for example if variable depends on several independent variables the correlations of with each of them are weak and yet remarkable 
generalizations sparse pca particular disadvantage of pca is that the principal components are usually linear combinations of all input variables 
sparse pca overcomes this disadvantage by finding linear combinations that contain just few input variables 
it extends the classic method of principal component analysis pca for the reduction of dimensionality of data by adding sparsity constraint on the input variables 
several approaches have been proposed including regression framework convex relaxation semidefinite programming framework generalized power method framework an alternating maximization framework forward backward greedy search and exact methods using branch and bound techniques bayesian formulation framework the methodological and theoretical developments of sparse pca as well as its applications in scientific studies were recently reviewed in survey paper 
nonlinear pca most of the modern methods for nonlinear dimensionality reduction find their theoretical and algorithmic roots in pca or means 
pearson original idea was to take straight line or plane which will be the best fit to set of data points 
trevor hastie expanded on this concept by proposing principal curves as the natural extension for the geometric interpretation of pca which explicitly constructs manifold for data approximation followed by projecting the points onto it as is illustrated by fig 
see also the elastic map algorithm and principal geodesic analysis 
another popular generalization is kernel pca which corresponds to pca performed in reproducing kernel hilbert space associated with positive definite kernel 
in multilinear subspace learning pca is generalized to multilinear pca mpca that extracts features directly from tensor representations 
mpca is solved by performing pca in each mode of the tensor iteratively 
mpca has been applied to face recognition gait recognition etc 
mpca is further extended to uncorrelated mpca non negative mpca and robust mpca 
way principal component analysis may be performed with models such as tucker decomposition parafac multiple factor analysis co inertia analysis statis and distatis 
robust pca while pca finds the mathematically optimal method as in minimizing the squared error it is still sensitive to outliers in the data that produce large errors something that the method tries to avoid in the first place 
it is therefore common practice to remove outliers before computing pca 
however in some contexts outliers can be difficult to identify 
for example in data mining algorithms like correlation clustering the assignment of points to clusters and outliers is not known beforehand 
recently proposed generalization of pca based on weighted pca increases robustness by assigning different weights to data objects based on their estimated relevancy 
outlier resistant variants of pca have also been proposed based on norm formulations pca robust principal component analysis rpca via decomposition in low rank and sparse matrices is modification of pca that works well with respect to grossly corrupted observations 
similar techniques independent component analysis independent component analysis ica is directed to similar problems as principal component analysis but finds additively separable components rather than successive approximations 
network component analysis given matrix it tries to decompose it into two matrices such that key difference from techniques such as pca and ica is that some of the entries of are constrained to be here is termed the regulatory layer 
while in general such decomposition can have multiple solutions they prove that if the following conditions are satisfied has full column rank each column of must have at least zeroes where is the number of columns of or alternatively the number of rows of 
the justification for this criterion is that if node is removed from the regulatory layer along with all the output nodes connected to it the result must still be characterized by connectivity matrix with full column rank 
must have full row rank then the decomposition is unique up to multiplication by scalar 
discriminant analysis of principal components discriminant analysis of principal components dapc is multivariate method used to identify and describe clusters of genetically related individuals 
genetic variation is partitioned into two components variation between groups and within groups and it maximizes the former 
linear discriminants are linear combinations of alleles which best separate the clusters 
alleles that most contribute to this discrimination are therefore those that are the most markedly different across groups 
the contributions of alleles to the groupings identified by dapc can allow identifying regions of the genome driving the genetic divergence among groups in dapc data is first transformed using principal components analysis pca and subsequently clusters are identified using discriminant analysis da 
dapc can be realized on using the package adegenet 
more info adegenet on the web software source code alglib and library that implements pca and truncated pca analytica the built in eigendecomp function computes principal components 
elki includes pca for projection including robust variants of pca as well as pca based clustering algorithms 
gretl principal component analysis can be performed either via the pca command or via the princomp function 
julia supports pca with the pca function in the multivariatestats package knime java based nodal arranging software for analysis in this the nodes called pca pca compute pca apply pca inverse make it easily 
mathematica implements principal component analysis with the principalcomponents command using both covariance and correlation methods 
mathphp php mathematics library with support for pca 
matlab the svd function is part of the basic system 
in the statistics toolbox the functions princomp and pca give the principal components while the function pcares gives the residuals and reconstructed matrix for low rank pca approximation 
matplotlib python library have pca package in the mlab module 
mlpack provides an implementation of principal component analysis in 
nag library principal components analysis is implemented via the aa routine available in both the fortran versions of the library 
nmath proprietary numerical library containing pca for the net framework 
gnu octave free software computational environment mostly compatible with matlab the function princomp gives the principal component 
opencv oracle database implemented via dbms data mining svds scoring mode by specifying setting value svds scoring pca orange software integrates pca in its visual programming environment 
pca displays scree plot degree of explained variance where user can interactively select the number of principal components 
origin contains pca in its pro version 
qlucore commercial software for analyzing multivariate data with instant response using pca 
free statistical package the functions princomp and prcomp can be used for principal component analysis prcomp uses singular value decomposition which generally gives better numerical accuracy 
some packages that implement pca in include but are not limited to ade vegan exposition dimred and factominer 
sas proprietary software for example see scikit learn python library for machine learning which contains pca probabilistic pca kernel pca sparse pca and other techniques in the decomposition module 
spss proprietary software most commonly used by social scientists for pca factor analysis and associated cluster analysis 
weka java library for machine learning which contains modules for computing principal components 
see also references further reading jackson 
user guide to principal components wiley 
springer series in statistics 
springer series in statistics 
new york springer verlag 
isbn husson fran ois bastien pag me 
exploratory multivariate analysis by example using chapman hall crc the series london 
multiple factor analysis by example using chapman hall crc the series london external links university of copenhagen video by rasmus bro on youtube stanford university video by andrew ng on youtube tutorial on principal component analysis layman introduction to principal component analysis on youtube video of less than seconds 
statquest principal component analysis pca clearly explained on youtube see also the list of software implementations
in statistics the logistic model or logit model is statistical model that models the probability of an event taking place by having the log odds for the event be linear combination of one or more independent variables 
in regression analysis logistic regression or logit regression is estimating the parameters of logistic model the coefficients in the linear combination 
formally in binary logistic regression there is single binary dependent variable coded by an indicator variable where the two values are labeled and while the independent variables can each be binary variable two classes coded by an indicator variable or continuous variable any real value 
the corresponding probability of the value labeled can vary between certainly the value and certainly the value hence the labeling the function that converts log odds to probability is the logistic function hence the name 
the unit of measurement for the log odds scale is called logit from logistic unit hence the alternative names 
see background and definition for formal mathematics and example for worked example 
binary variables are widely used in statistics to model the probability of certain class or event taking place such as the probability of team winning of patient being healthy etc 
see applications and the logistic model has been the most commonly used model for binary regression since about binary variables can be generalized to categorical variables when there are more than two possible values 
whether an image is of cat dog lion etc 
and the binary logistic regression generalized to multinomial logistic regression 
if the multiple categories are ordered one can use the ordinal logistic regression for example the proportional odds ordinal logistic model 
see extensions for further extensions 
the logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification it is not classifier though it can be used to make classifier for instance by choosing cutoff value and classifying inputs with probability greater than the cutoff as one class below the cutoff as the other this is common way to make binary classifier 
analogous linear models for binary variables with different sigmoid function instead of the logistic function to convert the linear combination to probability can also be used most notably the probit model see alternatives 
the defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at constant rate with each independent variable having its own parameter for binary dependent variable this generalizes the odds ratio 
more abstractly the logistic function is the natural parameter for the bernoulli distribution and in this sense is the simplest way to convert real number to probability 
in particular it maximizes entropy minimizes added information and in this sense makes the fewest assumptions of the data being modeled see maximum entropy 
the parameters of logistic regression are most commonly estimated by maximum likelihood estimation mle 
this does not have closed form expression unlike linear least squares see model fitting 
logistic regression by mle plays similarly basic role for binary or categorical responses as linear regression by ordinary least squares ols plays for scalar responses it is simple well analyzed baseline model see comparison with linear regression for discussion 
the logistic regression as general statistical model was originally developed and popularized primarily by joseph berkson beginning in berkson where he coined logit see history 
applications logistic regression is used in various fields including machine learning most medical fields and social sciences 
for example the trauma and injury severity score triss which is widely used to predict mortality in injured patients was originally developed by boyd et al 
many other medical scales used to assess severity of patient have been developed using logistic regression 
logistic regression may be used to predict the risk of developing given disease 
diabetes coronary heart disease based on observed characteristics of the patient age sex body mass index results of various blood tests etc 
another example might be to predict whether nepalese voter will vote nepali congress or communist party of nepal or any other party based on age income sex race state of residence votes in previous elections etc 
the technique can also be used in engineering especially for predicting the probability of failure of given process system or product 
it is also used in marketing applications such as prediction of customer propensity to purchase product or halt subscription etc 
in economics it can be used to predict the likelihood of person ending up in the labor force and business application would be to predict the likelihood of homeowner defaulting on mortgage 
conditional random fields an extension of logistic regression to sequential data are used in natural language processing 
example problem as simple example we can use logistic regression with one explanatory variable and two categories to answer the following question group of students spends between and hours studying for an exam 
how does the number of hours spent studying affect the probability of the student passing the exam 
the reason for using logistic regression for this problem is that the values of the dependent variable pass and fail while represented by and are not cardinal numbers 
if the problem was changed so that pass fail was replaced with the grade cardinal numbers then simple regression analysis could be used 
the table shows the number of hours each student spent studying and whether they passed or failed 
we wish to fit logistic function to the data consisting of the hours studied xk and the outcome of the test yk for pass for fail 
the data points are indexed by the subscript which runs from to the variable is called the explanatory variable and the variable is called the categorical variable consisting of two categories pass or fail corresponding to the categorical values and respectively 
model the logistic function is of the form where is location parameter the midpoint of the curve where and is scale parameter 
this expression may be rewritten as where and is known as the intercept it is the vertical intercept or intercept of the line and inverse scale parameter or rate parameter these are the intercept and slope of the log odds as function of conversely and 
fit the usual measure of goodness of fit for logistic regression uses logistic loss or log loss the negative log likelihood 
for given xk and yk write 
the are the probabilities that the corresponding will be unity and are the probabilities that they will be zero see bernoulli distribution 
we wish to find the values of and which give the best fit to the data 
in the case of linear regression the sum of the squared deviations of the fit from the data points yk the squared error loss is taken as measure of the goodness of fit and the best fit is obtained when that function is minimized 
the log loss for the th point is ln if ln if the log loss can be interpreted as the surprisal of the actual outcome relative to the prediction and is measure of information content 
note that log loss is always greater than or equal to equals only in case of perfect prediction when and or and and approaches infinity as the prediction gets worse when and or and meaning the actual outcome is more surprising 
since the value of the logistic function is always strictly between zero and one the log loss is always greater than zero and less than infinity 
note that unlike in linear regression where the model can have zero loss at point by passing through data point and zero loss overall if all points are on line in logistic regression it is not possible to have zero loss at any points since is either or but these can be combined into single expression ln ln 
this expression is more formally known as the cross entropy of the predicted distribution from the actual distribution as probability distributions on the two element space of pass fail 
the sum of these the total loss is the overall negative log likelihood and the best fit is obtained for those choices of and for which is minimized 
alternatively instead of minimizing the loss one can maximize its inverse the positive log likelihood ln ln ln ln or equivalently maximize the likelihood function itself which is the probability that the given data set is produced by particular logistic function this method is known as maximum likelihood estimation 
parameter estimation since is nonlinear in and determining their optimum values will require numerical methods 
note that one method of maximizing is to require the derivatives of with respect to and to be zero and the maximization procedure can be accomplished by solving the above two equations for and which again will generally require the use of numerical methods 
the values of and which maximize and using the above data are found to be which yields value for and of predictions the and coefficients may be entered into the logistic regression equation to estimate the probability of passing the exam 
for example for student who studies hours entering the value into the equation gives the estimated probability of passing the exam of probability of passing exam similarly for student who studies hours the estimated probability of passing the exam is probability of passing exam this table shows the estimated probability of passing the exam for several values of hours studying 
model evaluation the logistic regression analysis gives the following output 
by the wald test the output indicates that hours studying is significantly associated with the probability of passing the exam 
rather than the wald method the recommended method to calculate the value for logistic regression is the likelihood ratio test lrt which for this data gives see error and significance of the fit below 
generalizations this simple model is an example of binary logistic regression and has one explanatory variable and binary categorical variable which can assume one of two categorical values 
multinomial logistic regression is the generalization of binary logistic regression to include any number of explanatory variables and any number of categories 
background definition of the logistic function an explanation of logistic regression can begin with an explanation of the standard logistic function 
the logistic function is sigmoid function which takes any real input and outputs value between zero and one 
for the logit this is interpreted as taking input log odds and having output probability 
the standard logistic function is defined as follows graph of the logistic function on the interval is shown in figure let us assume that is linear function of single explanatory variable the case where is linear combination of multiple explanatory variables is treated similarly 
we can then express as follows and the general logistic function can now be written as in the logistic model is interpreted as the probability of the dependent variable equaling success case rather than failure non case 
it clear that the response variables are not identically distributed differs from one data point to another though they are independent given design matrix and shared parameters 
definition of the inverse of the logistic function we can now define the logit log odds function as the inverse of the standard logistic function 
it is easy to see that it satisfies logit ln and equivalently after exponentiating both sides we have the odds 
interpretation of these terms in the above equations the terms are as follows is the logit function 
the equation for illustrates that the logit log odds or natural logarithm of the odds is equivalent to the linear regression expression 
ln denotes the natural logarithm 
is the probability that the dependent variable equals case given some linear combination of the predictors 
the formula for illustrates that the probability of the dependent variable equaling case is equal to the value of the logistic function of the linear regression expression 
this is important in that it shows that the value of the linear regression expression can vary from negative to positive infinity and yet after transformation the resulting expression for the probability ranges between and is the intercept from the linear regression equation the value of the criterion when the predictor is equal to zero 
is the regression coefficient multiplied by some value of the predictor 
base denotes the exponential function 
definition of the odds the odds of the dependent variable equaling case given some linear combination of the predictors is equivalent to the exponential function of the linear regression expression 
this illustrates how the logit serves as link function between the probability and the linear regression expression 
given that the logit ranges between negative and positive infinity it provides an adequate criterion upon which to conduct linear regression and the logit is easily converted back into the odds so we define odds of the dependent variable equaling case given some linear combination of the predictors as follows odds 
the odds ratio for continuous independent variable the odds ratio can be defined as odds odds this exponential relationship provides an interpretation for the odds multiply by for every unit increase in for binary independent variable the odds ratio is defined as where and are cells in contingency table 
multiple explanatory variables if there are multiple explanatory variables the above expression can be revised to then when this is used in the equation relating the log odds of success to the values of the predictors the linear regression will be multiple regression with explanators the parameters for all are all estimated 
again the more traditional equations are log and where usually definition the basic setup of logistic regression is as follows 
we are given dataset containing points 
each point consists of set of input variables xm also called independent variables explanatory variables predictor variables features or attributes and binary outcome variable yi also known as dependent variable response variable output variable or class 
it can assume only the two possible values often meaning no or failure or often meaning yes or success 
the goal of logistic regression is to use the dataset to create predictive model of the outcome variable 
as in linear regression the outcome variables yi are assumed to depend on the explanatory variables xm 
explanatory variablesthe explanatory variables may be of any type real valued binary categorical etc 
the main distinction is between continuous variables and discrete variables 
discrete variables referring to more than two possible choices are typically coded using dummy variables or indicator variables that is separate explanatory variables taking the value or are created for each possible value of the discrete variable with meaning variable does have the given value and meaning variable does not have that value 
outcome variablesformally the outcomes yi are described as being bernoulli distributed data where each outcome is determined by an unobserved probability pi that is specific to the outcome at hand but related to the explanatory variables 
this can be expressed in any of the following equivalent forms bernoulli pr if if pr the meanings of these four lines are the first line expresses the probability distribution of each yi conditioned on the explanatory variables it follows bernoulli distribution with parameters pi the probability of the outcome of for trial as noted above each separate trial has its own probability of success just as each trial has its own explanatory variables 
the probability of success pi is not observed only the outcome of an individual bernoulli trial using that probability 
the second line expresses the fact that the expected value of each yi is equal to the probability of success pi which is general property of the bernoulli distribution 
in other words if we run large number of bernoulli trials using the same probability of success pi then take the average of all the and outcomes then the result would be close to pi 
this is because doing an average this way simply computes the proportion of successes seen which we expect to converge to the underlying probability of success 
the third line writes out the probability mass function of the bernoulli distribution specifying the probability of seeing each of the two possible outcomes 
the fourth line is another way of writing the probability mass function which avoids having to write separate cases and is more convenient for certain types of calculations 
this relies on the fact that yi can take only the value or in each case one of the exponents will be choosing the value under it while the other is canceling out the value under it 
hence the outcome is either pi or pi as in the previous line linear predictor functionthe basic idea of logistic regression is to use the mechanism already developed for linear regression by modeling the probability pi using linear predictor function 
linear combination of the explanatory variables and set of regression coefficients that are specific to the model at hand but the same for all trials 
the linear predictor function for particular data point is written as where are regression coefficients indicating the relative effect of particular explanatory variable on the outcome 
the model is usually put into more compact form as follows the regression coefficients are grouped into single vector of size for each data point an additional explanatory pseudo variable is added with fixed value of corresponding to the intercept coefficient 
the resulting explanatory variables xm are then grouped into single vector xi of size this makes it possible to write the linear predictor function as follows using the notation for dot product between two vectors 
many explanatory variables two categories the above example of binary logistic regression on one explanatory variable can be generalized to binary logistic regression on any number of explanatory variables and any number of categorical values 
to begin with we may consider logistic model with explanatory variables xm and as in the example above two categorical values and 
for the simple binary logistic regression model we assumed linear relationship between the predictor variable and the log odds also called logit of the event that this linear relationship may be extended to the case of explanatory variables log where is the log odds and are parameters of the model 
an additional generalization has been introduced in which the base of the model is not restricted to the euler number in most applications the base of the logarithm is usually taken to be however in some cases it can be easier to communicate results by working in base or base for more compact notation we will specify the explanatory variables and the coefficients as dimensional vectors with an added explanatory variable 
the logit may now be written as solving for the probability that yields where is the sigmoid function with base the above formula shows that once the are fixed we can easily compute either the log odds that for given observation or the probability that for given observation 
the main use case of logistic model is to be given an observation and estimate the probability that the optimum beta coefficients may again be found by maximizing the log likelihood 
for measurements defining xk as the explanatory vector of the th measurement and yk as the categorical outcome of that measurement the log likelihood may be written in form very similar to the simple case above log log as in the simple example above finding the optimum parameters will require numerical methods 
one useful technique is to equate the derivatives of the log likelihood with respect to each of the parameters to zero yielding set of equations which will hold at the maximum of the log likelihood where xmk is the value of the xm explanatory variable from the th measurement 
consider an example with explanatory variables and coefficients and which have been determined by the above method 
to be concrete the model is log where is the probability of the event that this can be interpreted as follows is the intercept 
it is the log odds of the event that when the predictors by exponentiating we can see that when the odds of the event that are to or similarly the probability of the event that when can be computed as means that increasing by increases the log odds by so if increases by the odds that increase by factor of note that the probability of has also increased but it has not increased by as much as the odds have increased 
means that increasing by increases the log odds by so if increases by the odds that increase by factor of note how the effect of on the log odds is twice as great as the effect of but the effect on the odds is times greater 
but the effect on the probability of is not as much as times greater it only the effect on the odds that is times greater 
multinomial logistic regression many explanatory variables and many categories in the above cases of two categories binomial logistic regression the categories were indexed by and and we had two probability distributions the probability that the outcome was in category was given by and the probability that the outcome was in category was given by 
the sum of both probabilities is equal to unity as they must be 
in general if we have explanatory variables including and categories we will need separate probability distributions one for each category indexed by which describe the probability that the categorical outcome for explanatory vector will be in category 
it will also be required that the sum of these probabilities over all categories be equal to unity 
using the mathematically convenient base these probabilities are for each of the probabilities except will have their own set of regression coefficients it can be seen that as required the sum of the over all categories is unity 
note that the selection of to be defined in terms of the other probabilities is artificial 
any of the probabilities could have been selected to be so defined 
this special value of is termed the pivot index and the log odds tn are expressed in terms of the pivot probability and are again expressed as linear combination of the explanatory variables ln note also that for the simple case of the two category case is recovered with and 
the log likelihood that particular set of measurements or data points will be generated by the above probabilities can now be calculated 
indexing each measurement by let the th set of measured explanatory variables be denoted by and their categorical outcomes be denoted by which can be equal to any integer in 
the log likelihood is then ln where is an indicator function which is equal to unity if yk and zero otherwise 
in the case of two explanatory variables this indicator function was defined as yk when and yk when this was convenient but not necessary 
again the optimum beta coefficients may be found by maximizing the log likelihood function generally using numerical methods 
possible method of solution is to set the derivatives of the log likelihood with respect to each beta coefficient equal to zero and solve for the beta coefficients where is the th coefficient of the vector and is the th explanatory variable of the th measurement 
once the beta coefficients have been estimated from the data we will be able to estimate the probability that any subsequent set of explanatory variables will result in any of the possible outcome categories 
interpretations there are various equivalent specifications and interpretations of logistic regression which fit into different types of more general models and allow different generalizations 
as generalized linear model the particular model used by logistic regression which distinguishes it from standard linear regression and from other types of regression analysis used for binary valued outcomes is the way the probability of particular outcome is linked to the linear predictor function logit logit ln written using the more compact notation described above this is logit logit ln this formulation expresses logistic regression as type of generalized linear model which predicts variables with various types of probability distributions by fitting linear predictor function of the above form to some sort of arbitrary transformation of the expected value of the variable 
the intuition for transforming using the logit function the natural log of the odds was explained above 
it also has the practical effect of converting the probability which is bounded to be between and to variable that ranges over thereby matching the potential range of the linear prediction function on the right side of the equation 
note that both the probabilities pi and the regression coefficients are unobserved and the means of determining them is not part of the model itself 
they are typically determined by some sort of optimization procedure 
maximum likelihood estimation that finds values that best fit the observed data 
that give the most accurate predictions for the data already observed usually subject to regularization conditions that seek to exclude unlikely values 
extremely large values for any of the regression coefficients 
the use of regularization condition is equivalent to doing maximum posteriori map estimation an extension of maximum likelihood 
regularization is most commonly done using squared regularizing function which is equivalent to placing zero mean gaussian prior distribution on the coefficients but other regularizers are also possible 
whether or not regularization is used it is usually not possible to find closed form solution instead an iterative numerical method must be used such as iteratively reweighted least squares irls or more commonly these days quasi newton method such as the bfgs method the interpretation of the parameter estimates is as the additive effect on the log of the odds for unit change in the the explanatory variable 
in the case of dichotomous explanatory variable for instance gender is the estimate of the odds of having the outcome for say males compared with females 
an equivalent formula uses the inverse of the logit function which is the logistic function 
logit the formula can also be written as probability distribution specifically using probability mass function pr as latent variable model the logistic model has an equivalent formulation as latent variable model 
this formulation is common in the theory of discrete choice models and makes it easier to extend to certain more complicated models with multiple correlated choices as well as to compare logistic regression to the closely related probit model 
imagine that for each trial there is continuous latent variable yi 
an unobserved random variable that is distributed as follows where logistic 
the latent variable can be written directly in terms of the linear predictor function and an additive random error variable that is distributed according to standard logistic distribution 
then yi can be viewed as an indicator for whether this latent variable is positive if 
the choice of modeling the error variable specifically with standard logistic distribution rather than general logistic distribution with the location and scale set to arbitrary values seems restrictive but in fact it is not 
it must be kept in mind that we can choose the regression coefficients ourselves and very often can use them to offset changes in the parameters of the error variable distribution 
for example logistic error variable distribution with non zero location parameter which sets the mean is equivalent to distribution with zero location parameter where has been added to the intercept coefficient 
both situations produce the same value for yi regardless of settings of explanatory variables 
similarly an arbitrary scale parameter is equivalent to setting the scale parameter to and then dividing all regression coefficients by in the latter case the resulting value of yi will be smaller by factor of than in the former case for all sets of explanatory variables but critically it will always remain on the same side of and hence lead to the same yi choice 
note that this predicts that the irrelevancy of the scale parameter may not carry over into more complex models where more than two choices are available 
it turns out that this formulation is exactly equivalent to the preceding one phrased in terms of the generalized linear model and without any latent variables 
this can be shown as follows using the fact that the cumulative distribution function cdf of the standard logistic distribution is the logistic function which is the inverse of the logit function 
pr logit then pr pr pr pr pr because the logistic distribution is symmetric logit see above this formulation which is standard in discrete choice models makes clear the relationship between logistic regression the logit model and the probit model which uses an error variable distributed according to standard normal distribution instead of standard logistic distribution 
both the logistic and normal distributions are symmetric with basic unimodal bell curve shape 
the only difference is that the logistic distribution has somewhat heavier tails which means that it is less sensitive to outlying data and hence somewhat more robust to model mis specifications or erroneous data 
two way latent variable model yet another formulation uses two separate latent variables where ev ev where ev is standard type extreme value distribution 
pr pr then if otherwise 
this model has separate latent variable and separate set of regression coefficients for each possible outcome of the dependent variable 
the reason for this separation is that it makes it easy to extend logistic regression to multi outcome categorical variables as in the multinomial logit model 
in such model it is natural to model each possible outcome using different set of regression coefficients 
it is also possible to motivate each of the separate latent variables as the theoretical utility associated with making the associated choice and thus motivate logistic regression in terms of utility theory 
in terms of utility theory rational actor always chooses the choice with the greatest associated utility 
this is the approach taken by economists when formulating discrete choice models because it both provides theoretically strong foundation and facilitates intuitions about the model which in turn makes it easy to consider various sorts of extensions 
see the example below 
the choice of the type extreme value distribution seems fairly arbitrary but it makes the mathematics work out and it may be possible to justify its use through rational choice theory 
it turns out that this model is equivalent to the previous model although this seems non obvious since there are now two sets of regression coefficients and error variables and the error variables have different distribution 
in fact this model reduces directly to the previous one with the following substitutions an intuition for this comes from the fact that since we choose based on the maximum of two values only their difference matters not the exact values and this effectively removes one degree of freedom 
another critical fact is that the difference of two type extreme value distributed variables is logistic distribution 
we can demonstrate the equivalent as follows pr pr pr pr pr pr pr substitute as above pr substitute as above pr now same as above model pr logit example as an example consider province level election where the choice is between right of center party left of center party and secessionist party 
the parti qu cois which wants quebec to secede from canada 
we would then use three latent variables one for each choice 
then in accordance with utility theory we can then interpret the latent variables as expressing the utility that results from making each of the choices 
we can also interpret the regression coefficients as indicating the strength that the associated factor 
explanatory variable has in contributing to the utility or more correctly the amount by which unit change in an explanatory variable changes the utility of given choice 
voter might expect that the right of center party would lower taxes especially on rich people 
this would give low income people no benefit 
no change in utility since they usually don pay taxes would cause moderate benefit 
somewhat more money or moderate utility increase for middle incoming people would cause significant benefits for high income people 
on the other hand the left of center party might be expected to raise taxes and offset it with increased welfare and other assistance for the lower and middle classes 
this would cause significant positive benefit to low income people perhaps weak benefit to middle income people and significant negative benefit to high income people 
finally the secessionist party would take no direct actions on the economy but simply secede 
low income or middle income voter might expect basically no clear utility gain or loss from this but high income voter might expect negative utility since he she is likely to own companies which will have harder time doing business in such an environment and probably lose money 
these intuitions can be expressed as follows this clearly shows that separate sets of regression coefficients need to exist for each choice 
when phrased in terms of utility this can be seen very easily 
different choices have different effects on net utility furthermore the effects vary in complex ways that depend on the characteristics of each individual so there need to be separate sets of coefficients for each characteristic not simply single extra per choice characteristic 
even though income is continuous variable its effect on utility is too complex for it to be treated as single variable 
either it needs to be directly split up into ranges or higher powers of income need to be added so that polynomial regression on income is effectively done 
as log linear model yet another formulation combines the two way latent variable formulation above with the original formulation higher up without latent variables and in the process provides link to one of the standard formulations of the multinomial logit 
here instead of writing the logit of the probabilities pi as linear predictor we separate the linear predictor into two one for each of the two outcomes ln pr ln ln pr ln two separate sets of regression coefficients have been introduced just as in the two way latent variable model and the two equations appear form that writes the logarithm of the associated probability as linear predictor with an extra term ln at the end 
this term as it turns out serves as the normalizing factor ensuring that the result is distribution 
this can be seen by exponentiating both sides pr pr in this form it is clear that the purpose of is to ensure that the resulting distribution over yi is in fact probability distribution 
it sums to this means that is simply the sum of all un normalized probabilities and by dividing each probability by the probabilities become normalized 
that is and the resulting equations are pr pr or generally pr this shows clearly how to generalize this formulation to more than two outcomes as in multinomial logit 
note that this general formulation is exactly the softmax function as in pr softmax 
in order to prove that this is equivalent to the previous model note that the above model is overspecified in that pr and pr cannot be independently specified rather pr pr so knowing one automatically determines the other 
as result the model is nonidentifiable in that multiple combinations of and will produce the same probabilities for all possible explanatory variables 
in fact it can be seen that adding any constant vector to both of them will produce the same probabilities pr as result we can simplify matters and restore identifiability by picking an arbitrary value for one of the two vectors 
we choose to set then and so pr which shows that this formulation is indeed equivalent to the previous formulation 
as in the two way latent variable formulation any settings where will produce equivalent results 
note that most treatments of the multinomial logit model start out either by extending the log linear formulation presented here or the two way latent variable formulation presented above since both clearly show the way that the model could be extended to multi way outcomes 
in general the presentation with latent variables is more common in econometrics and political science where discrete choice models and utility theory reign while the log linear formulation here is more common in computer science 
machine learning and natural language processing 
as single layer perceptron the model has an equivalent formulation 
this functional form is commonly called single layer perceptron or single layer artificial neural network 
single layer neural network computes continuous output instead of step function 
the derivative of pi with respect to xk is computed from the general form where is an analytic function in with this choice the single layer neural network is identical to the logistic regression model 
this function has continuous derivative which allows it to be used in backpropagation 
this function is also preferred because its derivative is easily calculated 
in terms of binomial data closely related model assumes that each is associated not with single bernoulli trial but with ni independent identically distributed trials where the observation yi is the number of successes observed the sum of the individual bernoulli distributed random variables and hence follows binomial distribution bin for an example of this distribution is the fraction of seeds pi that germinate after ni are planted 
in terms of expected values this model is expressed as follows so that logit logit ln or equivalently pr this model can be fit using the same sorts of methods as the above more basic model 
model fitting maximum likelihood estimation mle the regression coefficients are usually estimated using maximum likelihood estimation 
unlike linear regression with normally distributed residuals it is not possible to find closed form expression for the coefficient values that maximize the likelihood function so that an iterative process must be used instead for example newton method 
this process begins with tentative solution revises it slightly to see if it can be improved and repeats this revision until no more improvement is made at which point the process is said to have converged in some instances the model may not reach convergence 
non convergence of model indicates that the coefficients are not meaningful because the iterative process was unable to find appropriate solutions 
failure to converge may occur for number of reasons having large ratio of predictors to cases multicollinearity sparseness or complete separation 
having large ratio of variables to cases results in an overly conservative wald statistic discussed below and can lead to non convergence 
regularized logistic regression is specifically intended to be used in this situation 
multicollinearity refers to unacceptably high correlations between predictors 
as multicollinearity increases coefficients remain unbiased but standard errors increase and the likelihood of model convergence decreases 
to detect multicollinearity amongst the predictors one can conduct linear regression analysis with the predictors of interest for the sole purpose of examining the tolerance statistic used to assess whether multicollinearity is unacceptably high 
sparseness in the data refers to having large proportion of empty cells cells with zero counts 
zero cell counts are particularly problematic with categorical predictors 
with continuous predictors the model can infer values for the zero cell counts but this is not the case with categorical predictors 
the model will not converge with zero cell counts for categorical predictors because the natural logarithm of zero is an undefined value so that the final solution to the model cannot be reached 
to remedy this problem researchers may collapse categories in theoretically meaningful way or add constant to all cells 
another numerical problem that may lead to lack of convergence is complete separation which refers to the instance in which the predictors perfectly predict the criterion all cases are accurately classified and the likelihood maximized with infinite coefficients 
in such instances one should re examine the data as there may be some kind of error 
one can also take semi parametric or non parametric approaches via local likelihood or nonparametric quasi likelihood methods which avoid assumptions of parametric form for the index function and is robust to the choice of the link function probit or logit 
iteratively reweighted least squares irls binary logistic regression or can for example be calculated using iteratively reweighted least squares irls which is equivalent to maximizing the log likelihood of bernoulli distributed process using newton method 
if the problem is written in vector matrix form with parameters explanatory variables and expected value of the bernoulli distribution the parameters can be found using the following iterative algorithm where diag is diagonal weighting matrix the vector of expected values the regressor matrix and the vector of response variables 
more details can be found in the literature 
bayesian in bayesian statistics context prior distributions are normally placed on the regression coefficients for example in the form of gaussian distributions 
there is no conjugate prior of the likelihood function in logistic regression 
when bayesian inference was performed analytically this made the posterior distribution difficult to calculate except in very low dimensions 
now though automatic software such as openbugs jags pymc stan or turing jl allows these posteriors to be computed using simulation so lack of conjugacy is not concern 
however when the sample size or the number of parameters is large full bayesian simulation can be slow and people often use approximate methods such as variational bayesian methods and expectation propagation 
rule of ten widely used rule of thumb the one in ten rule states that logistic regression models give stable values for the explanatory variables if based on minimum of about events per explanatory variable epv where event denotes the cases belonging to the less frequent category in the dependent variable 
thus study designed to use explanatory variables for an event 
myocardial infarction expected to occur in proportion of participants in the study will require total of participants 
however there is considerable debate about the reliability of this rule which is based on simulation studies and lacks secure theoretical underpinning 
according to some authors the rule is overly conservative in some circumstances with the authors stating if we somewhat subjectively regard confidence interval coverage less than percent type error greater than percent or relative bias greater than percent as problematic our results indicate that problems are fairly frequent with epv uncommon with epv and still observed with epv 
the worst instances of each problem were not severe with epv and usually comparable to those with epv others have found results that are not consistent with the above using different criteria 
useful criterion is whether the fitted model will be expected to achieve the same predictive discrimination in new sample as it appeared to achieve in the model development sample 
for that criterion events per candidate variable may be required 
also one can argue that observations are needed only to estimate the model intercept precisely enough that the margin of error in predicted probabilities is with confidence level 
error and significance of fit deviance and likelihood ratio test simple case in any fitting procedure the addition of another fitting parameter to model 
the beta parameters in logistic regression model will almost always improve the ability of the model to predict the measured outcomes 
this will be true even if the additional term has no predictive value since the model will simply be overfitting to the noise in the data 
the question arises as to whether the improvement gained by the addition of another fitting parameter is significant enough to recommend the inclusion of the additional term or whether the improvement is simply that which may be expected from overfitting 
in short for logistic regression statistic known as the deviance is defined which is measure of the error between the logistic model fit and the outcome data 
in the limit of large number of data points the deviance is chi squared distributed which allows chi squared test to be implemented in order to determine the significance of the explanatory variables 
linear regression and logistic regression have many similarities 
for example in simple linear regression set of data points xk yk are fitted to proposed model function of the form the fit is obtained by choosing the parameters which minimize the sum of the squares of the residuals the squared error term for each data point the minimum value which constitutes the fit will be denoted by the idea of null model may be introduced in which it is assumed that the variable is of no use in predicting the yk outcomes the data points are fitted to null model function of the form with squared error term the fitting process consists of choosing value of which minimizes of the fit to the null model denoted by where the subscript denotes the null model 
it is seen that the null model is optimized by where is the mean of the yk values and the optimized is which is proportional to the square of the uncorrected sample standard deviation of the yk data points 
we can imagine case where the yk data points are randomly assigned to the various xk and then fitted using the proposed model 
specifically we can consider the fits of the proposed model to every permutation of the yk outcomes 
it can be shown that the optimized error of any of these fits will never be less than the optimum error of the null model and that the difference between these minimum error will follow chi squared distribution distribution with degrees of freedom equal those of the proposed model minus those of the null model which in this case will be 
using the chi squared test we may then estimate how many of these permuted sets of yk will yield an minimum error less than or equal to the minimum error using the original yk and so we can estimate how significant an improvement is given by the inclusion of the variable in the proposed model 
for logistic regression the measure of goodness of fit is the likelihood function or its logarithm the log likelihood the likelihood function is analogous to the in the linear regression case except that the likelihood is maximized rather than minimized 
denote the maximized log likelihood of the proposed model by 
in the case of simple binary logistic regression the set of data points are fitted in probabilistic sense to function of the form where is the probability that the log odds are given by and the log likelihood is ln ln for the null model the probability that is given by the log odds for the null model are given by and the log likelihood is ln ln since we have at the maximum of the maximum log likelihood for the null model is ln ln the optimum is ln where is again the mean of the yk values 
again we can conceptually consider the fit of the proposed model to every permutation of the yk and it can be shown that the maximum log likelihood of these permutation fits will never be smaller than that of the null model also as an analog to the error of the linear regression case we may define the deviance of logistic regression fit as ln which will always be positive or zero 
the reason for this choice is that not only is the deviance good measure of the goodness of fit it is also approximately chi squared distributed with the approximation improving as the number of data points increases becoming exactly chi square distributed in the limit of an infinite number of data points 
as in the case of linear regression we may use this fact to estimate the probability that random set of data points will give better fit than the fit obtained by the proposed model and so have an estimate how significantly the model is improved by including the xk data points in the proposed model 
for the simple model of student test scores described above the maximum value of the log likelihood of the null model is the maximum value of the log likelihood for the simple model is so that the deviance is using the chi squared test of significance the integral of the chi squared distribution with one degree of freedom from to infinity is equal to this effectively means that about out of fits to random yk can be expected to have better fit smaller deviance than the given yk and so we can conclude that the inclusion of the variable and data in the proposed model is very significant improvement over the null model 
in other words we reject the null hypothesis with confidence 
goodness of fit summary goodness of fit in linear regression models is generally measured using 
since this has no direct analog in logistic regression various methods ch including the following can be used instead 
deviance and likelihood ratio tests in linear regression analysis one is concerned with partitioning variance via the sum of squares calculations variance in the criterion is essentially divided into variance accounted for by the predictors and residual variance 
in logistic regression analysis deviance is used in lieu of sum of squares calculations 
deviance is analogous to the sum of squares calculations in linear regression and is measure of the lack of fit to the data in logistic regression model 
when saturated model is available model with theoretically perfect fit deviance is calculated by comparing given model with the saturated model 
this computation gives the likelihood ratio test ln likelihood of the fitted model likelihood of the saturated model 
in the above equation represents the deviance and ln represents the natural logarithm 
the log of this likelihood ratio the ratio of the fitted model to the saturated model will produce negative value hence the need for negative sign 
can be shown to follow an approximate chi squared distribution 
smaller values indicate better fit as the fitted model deviates less from the saturated model 
when assessed upon chi square distribution nonsignificant chi square values indicate very little unexplained variance and thus good model fit 
conversely significant chi square value indicates that significant amount of the variance is unexplained 
when the saturated model is not available common case deviance is calculated simply as log likelihood of the fitted model and the reference to the saturated model log likelihood can be removed from all that follows without harm 
two measures of deviance are particularly important in logistic regression null deviance and model deviance 
the null deviance represents the difference between model with only the intercept which means no predictors and the saturated model 
the model deviance represents the difference between model with at least one predictor and the saturated model 
in this respect the null model provides baseline upon which to compare predictor models 
given that deviance is measure of the difference between given model and the saturated model smaller values indicate better fit 
thus to assess the contribution of predictor or set of predictors one can subtract the model deviance from the null deviance and assess the difference on chi square distribution with degrees of freedom equal to the difference in the number of parameters estimated 
let null ln likelihood of null model likelihood of the saturated model fitted ln likelihood of fitted model likelihood of the saturated model 
then the difference of both is null fitted ln likelihood of null model likelihood of the saturated model ln likelihood of fitted model likelihood of the saturated model ln likelihood of null model likelihood of the saturated model likelihood of fitted model likelihood of the saturated model ln likelihood of the null model likelihood of fitted model 
if the model deviance is significantly smaller than the null deviance then one can conclude that the predictor or set of predictors significantly improve the model fit 
this is analogous to the test used in linear regression analysis to assess the significance of prediction 
pseudo squared in linear regression the squared multiple correlation is used to assess goodness of fit as it represents the proportion of variance in the criterion that is explained by the predictors 
in logistic regression analysis there is no agreed upon analogous measure but there are several competing measures each with limitations four of the most commonly used indices and one less commonly used one are examined on this page likelihood ratio cox and snell cs nagelkerke mcfadden mcf tjur hosmer lemeshow test the hosmer lemeshow test uses test statistic that asymptotically follows distribution to assess whether or not the observed event rates match expected event rates in subgroups of the model population 
this test is considered to be obsolete by some statisticians because of its dependence on arbitrary binning of predicted probabilities and relative low power 
coefficient significance after fitting the model it is likely that researchers will want to examine the contribution of individual predictors 
to do so they will want to examine the regression coefficients 
in linear regression the regression coefficients represent the change in the criterion for each unit change in the predictor 
in logistic regression however the regression coefficients represent the change in the logit for each unit change in the predictor 
given that the logit is not intuitive researchers are likely to focus on predictor effect on the exponential function of the regression coefficient the odds ratio see definition 
in linear regression the significance of regression coefficient is assessed by computing test 
in logistic regression there are several different tests designed to assess the significance of an individual predictor most notably the likelihood ratio test and the wald statistic 
likelihood ratio test the likelihood ratio test discussed above to assess model fit is also the recommended procedure to assess the contribution of individual predictors to given model 
in the case of single predictor model one simply compares the deviance of the predictor model with that of the null model on chi square distribution with single degree of freedom 
if the predictor model has significantly smaller deviance 
chi square using the difference in degrees of freedom of the two models then one can conclude that there is significant association between the predictor and the outcome 
although some common statistical packages 
spss do provide likelihood ratio test statistics without this computationally intensive test it would be more difficult to assess the contribution of individual predictors in the multiple logistic regression case 
to assess the contribution of individual predictors one can enter the predictors hierarchically comparing each new model with the previous to determine the contribution of each predictor 
there is some debate among statisticians about the appropriateness of so called stepwise procedures 
the fear is that they may not preserve nominal statistical properties and may become misleading 
wald statistic alternatively when assessing the contribution of individual predictors in given model one may examine the significance of the wald statistic 
the wald statistic analogous to the test in linear regression is used to assess the significance of coefficients 
the wald statistic is the ratio of the square of the regression coefficient to the square of the standard error of the coefficient and is asymptotically distributed as chi square distribution 
although several statistical packages spss sas report the wald statistic to assess the contribution of individual predictors the wald statistic has limitations 
when the regression coefficient is large the standard error of the regression coefficient also tends to be larger increasing the probability of type ii error 
the wald statistic also tends to be biased when data are sparse 
case control sampling suppose cases are rare 
then we might wish to sample them more frequently than their prevalence in the population 
for example suppose there is disease that affects person in and to collect our data we need to do complete physical 
it may be too expensive to do thousands of physicals of healthy people in order to obtain data for only few diseased individuals 
thus we may evaluate more diseased individuals perhaps all of the rare outcomes 
this is also retrospective sampling or equivalently it is called unbalanced data 
as rule of thumb sampling controls at rate of five times the number of cases will produce sufficient control data logistic regression is unique in that it may be estimated on unbalanced data rather than randomly sampled data and still yield correct coefficient estimates of the effects of each independent variable on the outcome 
that is to say if we form logistic model from such data if the model is correct in the general population the parameters are all correct except for we can correct if we know the true prevalence as follows log log where is the true prevalence and is the prevalence in the sample 
discussion like other forms of regression analysis logistic regression makes use of one or more predictor variables that may be either continuous or categorical 
unlike ordinary linear regression however logistic regression is used for predicting dependent variables that take membership in one of limited number of categories treating the dependent variable in the binomial case as the outcome of bernoulli trial rather than continuous outcome 
given this difference the assumptions of linear regression are violated 
in particular the residuals cannot be normally distributed 
in addition linear regression may make nonsensical predictions for binary dependent variable 
what is needed is way to convert binary variable into continuous one that can take on any real value negative or positive 
to do that binomial logistic regression first calculates the odds of the event happening for different levels of each independent variable and then takes its logarithm to create continuous criterion as transformed version of the dependent variable 
the logarithm of the odds is the logit of the probability the logit is defined as follows although the dependent variable in logistic regression is bernoulli the logit is on an unrestricted scale 
the logit function is the link function in this kind of generalized linear model 
is the bernoulli distributed response variable and is the predictor variable the values are the linear parameters 
the logit of the probability of success is then fitted to the predictors 
the predicted value of the logit is converted back into predicted odds via the inverse of the natural logarithm the exponential function 
thus although the observed dependent variable in binary logistic regression is or variable the logistic regression estimates the odds as continuous variable that the dependent variable is success 
in some applications the odds are all that is needed 
in others specific yes or no prediction is needed for whether the dependent variable is or is not success this categorical prediction can be based on the computed odds of success with predicted odds above some chosen cutoff value being translated into prediction of success 
maximum entropy of all the functional forms used for estimating the probabilities of particular categorical outcome which optimize the fit by maximizing the likelihood function 
probit regression poisson regression etc 
the logistic regression solution is unique in that it is maximum entropy solution 
this is case of general property an exponential family of distributions maximizes entropy given an expected value 
in the case of the logistic model the logistic function is the natural parameter of the bernoulli distribution it is in canonical form and the logistic function is the canonical link function while other sigmoid functions are non canonical link functions this underlies its mathematical elegance and ease of optimization 
see exponential family maximum entropy derivation for details 
proof in order to show this we use the method of lagrange multipliers 
the lagrangian is equal to the entropy plus the sum of the products of lagrange multipliers times various constraint expressions 
the general multinomial case will be considered since the proof is not made that much simpler by considering simpler cases 
equating the derivative of the lagrangian with respect to the various probabilities to zero yields functional form for those probabilities which corresponds to those used in logistic regression as in the above section on multinomial logistic regression we will consider explanatory variables denoted and which include there will be total of data points indexed by and the data points are given by and the xmk will also be represented as an dimensional vector 
there will be possible values of the categorical variable ranging from to let pn be the probability given explanatory variable vector that the outcome will be define which is the probability that for the th measurement the categorical outcome is the lagrangian will be expressed as function of the probabilities pnk and will minimized by equating the derivatives of the lagrangian with respect to these probabilities to zero 
an important point is that the probabilities are treated equally and the fact that they sum to unity is part of the lagrangian formulation rather than being assumed from the beginning 
the first contribution to the lagrangian is the entropy ln the log likelihood is ln assuming the multinomial logistic function the derivative of the log likelihood with respect the beta coefficients was found to be very important point here is that this expression is remarkably not an explicit function of the beta coefficients 
it is only function of the probabilities pnk and the data 
rather than being specific to the assumed multinomial logistic case it is taken to be general statement of the condition at which the log likelihood is maximized and makes no reference to the functional form of pnk 
there are then fitting constraints and the fitting constraint term in the lagrangian is then where the nm are the appropriate lagrange multipliers 
there are normalization constraints which may be written so that the normalization term in the lagrangian is where the are the appropriate lagrange multipliers 
the lagrangian is then the sum of the above three terms setting the derivative of the lagrangian with respect to one of the probabilities to zero yields ln using the more condensed vector notation and dropping the primes on the and indices and then solving for yields where imposing the normalization constraint we can solve for the zk and write the probabilities as the are not all independent 
we can add any constant dimensional vector to each of the without changing the value of the probabilities so that there are only rather than independent in the multinomial logistic regression section above the was subtracted from each which set the exponential term involving to unity and the beta coefficients were given by 
other approaches in machine learning applications where logistic regression is used for binary classification the mle minimises the cross entropy loss function 
logistic regression is an important machine learning algorithm 
the goal is to model the probability of random variable being or given experimental data consider generalized linear model function parameterized by pr therefore pr and since we see that pr is given by pr 
we now calculate the likelihood function assuming that all the observations in the sample are independently bernoulli distributed pr pr typically the log likelihood is maximized log log pr which is maximized using optimization techniques such as gradient descent 
assuming the pairs are drawn uniformly from the underlying distribution then in the limit of large lim log pr pr log pr pr log pr pr log pr kl where is the conditional entropy and kl is the kullback leibler divergence 
this leads to the intuition that by maximizing the log likelihood of model you are minimizing the kl divergence of your model from the maximal entropy distribution 
intuitively searching for the model that makes the fewest assumptions in its parameters 
comparison with linear regression logistic regression can be seen as special case of the generalized linear model and thus analogous to linear regression 
the model of logistic regression however is based on quite different assumptions about the relationship between the dependent and independent variables from those of linear regression 
in particular the key differences between these two models can be seen in the following two features of logistic regression 
first the conditional distribution is bernoulli distribution rather than gaussian distribution because the dependent variable is binary 
second the predicted values are probabilities and are therefore restricted to through the logistic distribution function because logistic regression predicts the probability of particular outcomes rather than the outcomes themselves 
alternatives common alternative to the logistic model logit model is the probit model as the related names suggest 
from the perspective of generalized linear models these differ in the choice of link function the logistic model uses the logit function inverse logistic function while the probit model uses the probit function inverse error function 
equivalently in the latent variable interpretations of these two methods the first assumes standard logistic distribution of errors and the second standard normal distribution of errors 
other sigmoid functions or error distributions can be used instead 
logistic regression is an alternative to fisher method linear discriminant analysis 
if the assumptions of linear discriminant analysis hold the conditioning can be reversed to produce logistic regression 
the converse is not true however because logistic regression does not require the multivariate normal assumption of discriminant analysis the assumption of linear predictor effects can easily be relaxed using techniques such as spline functions 
history detailed history of the logistic regression is given in cramer 
the logistic function was developed as model of population growth and named logistic by pierre fran ois verhulst in the and under the guidance of adolphe quetelet see logistic function history for details 
in his earliest paper verhulst did not specify how he fit the curves to the data 
in his more detailed paper verhulst determined the three parameters of the model by making the curve pass through three observed points which yielded poor predictions the logistic function was independently developed in chemistry as model of autocatalysis wilhelm ostwald 
an autocatalytic reaction is one in which one of the products is itself catalyst for the same reaction while the supply of one of the reactants is fixed 
this naturally gives rise to the logistic equation for the same reason as population growth the reaction is self reinforcing but constrained 
the logistic function was independently rediscovered as model of population growth in by raymond pearl and lowell reed published as pearl reed which led to its use in modern statistics 
they were initially unaware of verhulst work and presumably learned about it from gustave du pasquier but they gave him little credit and did not adopt his terminology 
verhulst priority was acknowledged and the term logistic revived by udny yule in and has been followed since 
pearl and reed first applied the model to the population of the united states and also initially fitted the curve by making it pass through three points as with verhulst this again yielded poor results in the the probit model was developed and systematized by chester ittner bliss who coined the term probit in bliss and by john gaddum in gaddum and the model fit by maximum likelihood estimation by ronald fisher in fisher as an addendum to bliss work 
the probit model was principally used in bioassay and had been preceded by earlier work dating to see probit model history 
the probit model influenced the subsequent development of the logit model and these models competed with each other the logistic model was likely first used as an alternative to the probit model in bioassay by edwin bidwell wilson and his student jane worcester in wilson worcester 
however the development of the logistic model as general alternative to the probit model was principally due to the work of joseph berkson over many decades beginning in berkson where he coined logit by analogy with probit and continuing through berkson and following years 
the logit model was initially dismissed as inferior to the probit model but gradually achieved an equal footing with the logit particularly between and by the logit model achieved parity with the probit model in use in statistics journals and thereafter surpassed it 
this relative popularity was due to the adoption of the logit outside of bioassay rather than displacing the probit within bioassay and its informal use in practice the logit popularity is credited to the logit model computational simplicity mathematical properties and generality allowing its use in varied fields various refinements occurred during that time notably by david cox as in cox the multinomial logit model was introduced independently in cox and thiel which greatly increased the scope of application and the popularity of the logit model 
in daniel mcfadden linked the multinomial logit to the theory of discrete choice specifically luce choice axiom showing that the multinomial logit followed from the assumption of independence of irrelevant alternatives and interpreting odds of alternatives as relative preferences this gave theoretical foundation for the logistic regression 
extensions there are large numbers of extensions multinomial logistic regression or multinomial logit handles the case of multi way categorical dependent variable with unordered values also called classification 
note that the general case of having dependent variables with more than two values is termed polytomous regression 
ordered logistic regression or ordered logit handles ordinal dependent variables ordered values 
mixed logit is an extension of multinomial logit that allows for correlations among the choices of the dependent variable 
an extension of the logistic model to sets of interdependent variables is the conditional random field 
conditional logistic regression handles matched or stratified data when the strata are small 
it is mostly used in the analysis of observational studies 
software most statistical software can do binary logistic regression 
spss for basic logistic regression 
stata sas proc logistic for basic logistic regression 
proc catmod when all the variables are categorical 
proc glimmix for multilevel model logistic regression 
glm in the stats package using family binomial lrm in the rms package glmnet package for an efficient implementation regularized logistic regression lmer for mixed effects logistic regression rfast package command gm logistic for fast and heavy calculations involving large scale data 
arm package for bayesian logistic regression python logit in the statsmodels module 
logisticregression in the scikit learn module 
logisticregressor in the tensorflow module 
full example of logistic regression in the theano tutorial bayesian logistic regression with ard prior code tutorial variational bayes logistic regression with ard prior code tutorial bayesian logistic regression code tutorial ncss logistic regression in ncss matlab mnrfit in the statistics and machine learning toolbox with incorrect coded as instead of fminunc fmincon fitglm mnrfit fitclinear mle can all do logistic regression 
java jvm liblinear apache flink apache spark sparkml supports logistic regression fpga logistic regresesion ip core in hls for fpga notably microsoft excel statistics extension package does not include it 
see also logistic function discrete choice jarrow turnbull model limited dependent variable multinomial logit model ordered logit hosmer lemeshow test brier score mlpack contains implementation of logistic regression local case control sampling logistic model tree references further reading external links media related to logistic regression at wikimedia commons econometrics lecture topic logit model on youtube by mark thoma logistic regression tutorial mlelr software in for teaching purposes
the chromatic circle is clock diagram for displaying relationships among the equal tempered pitch classes making up the familiar chromatic scale on circle 
explanation if one starts on any equal tempered pitch and repeatedly ascends by the musical interval of semitone one will eventually land on pitch with the same pitch class as the initial one having passed through all the other equal tempered chromatic pitch classes in between 
since the space is circular it is also possible to descend by semitone 
the chromatic circle is useful because it represents melodic distance which is often correlated with physical distance on musical instruments 
for instance to move from any on piano keyboard to the nearest one must move up four semitones corresponding to four clockwise steps on the chromatic circle 
one can also move down by eight semitones corresponding to eight counterclockwise steps on the pitch class circle 
larger motions on the piano or in pitch space can be represented in pitch class space by paths that wrap around the chromatic circle one or more times 
one can represent the twelve equal tempered pitch classes by the cyclic group of order twelve or equivalently the residue classes modulo twelve 
the group has four generators which can be identified with the ascending and descending semitones and the ascending and descending perfect fifths 
the semitonal generator gives rise to the chromatic circle while the perfect fifth gives rise to the circle of fifths 
comparison with circle of fifths key difference between the chromatic circle and the circle of fifths is that the former is truly continuous space every point on the circle corresponds to conceivable pitch class and every conceivable pitch class corresponds to point on the circle 
by contrast the circle of fifths is fundamentally discrete structure and there is no obvious way to assign pitch classes to each of its points 
pitch constellation pitch constellation is graphical representation of pitches used to describe musical scales modes chords or other groupings of pitches within an octave range 
it consists of circle with markings along the circumference or lines from the center which indicate pitches 
most pitch constellations use of subset of pitches chosen from the twelve pitch chromatic scale 
in this case the points on the circle are spaced like the twelve hour markings on an analog clock where each tick mark represents semitone 
scales and modes the pitch constellation provides an easy way to identify certain patterns and similarities between harmonic structures 
major scale consists of circle with markings at or and clock 
minor scale consists of circle with markings at or and clock 
the diagrams above show the two scales marked with scale degrees 
it can be observed that the tonic second fourth and fifth are shared while the minor scale flattens the third sixth and seventh notes relative to the major scale 
another observation is that the minor scale constellation is the same as the major scale but rotated degrees 
in the following drawing all of the major minor scales are drawn 
note that the constellation for all the major scales or all the minor scales are identical 
the different scales are generated by rotating the note overlay 
the notes that need to be sharpened flattened can be easily identified 
moreover if we draw all seven diatonic modes we can see them all as rotations of the ionian mode 
note also the significance of the clock point 
this corresponds to tritone 
the modes including pitches tritone from the tonic locrian and lydian are least used 
the clock and clock pitches are also important points corresponding to perfect fourth and perfect fifth respectively 
the most used scales modes major ionian mode minor aeolian mode and mixolydian include these pitches 
symmetric scales have simple representations in this scheme 
more exotic scales such as the pentatonic blues and octatonic can also be drawn and related to the common scales 
more complete list of musical scales and modes other overlays in previous sections we saw how various overlays scale degrees semi tone numbering notes can be used to notate the circumference of the constellation 
various other overlays can be laid around the constellation 
pitch ratios ratios of pitch frequencies 
note that once pitch constellation has been determined any number of overlays notes solf ge intervals etc 
may be placed on top for analysis comparison 
often generating one harmonic relationship from another is simply matter of rotating the overlay or constellation or shifting one or two pitch locations 
chords similarities between chords can also be observed as well as the significance of augmented diminished notes for triads we have the following and for seventh chords circle of fifths beginning with pitch constellation of chromatic scale the notes of circle of fifths can be easily generated 
starting at and moving across the circle and then one tick clockwise line is drawn with an arrow indicating the direction moved 
continuing from that point across the circle and one tick clockwise all points are connected 
moving through this pattern the notes of the circle of fifths can be determined 
one can also depict non tempered intervals on chromatic circle which allows one to depict commas small intervals particularly comma pumps 
for example using sequence of twelve just fifths ratio does not quite return to the starting point the size of the gap is the pythagorean comma resulting in broken circle of fifths 
technical note the ratio of the frequencies between two pitches in the constellation can be determined as follows 
take the length of the arc measured clockwise between the two points and divide by the circumference of the circle 
the frequency ratio is two raised to this power 
for example for fifth which is located at clock relative to the tonic the frequency ratio is references further reading brower candace cognitive theory of musical meaning journal of music theory duke university press doi jstor ku inskas darius symmetry in creative work of mikalojus konstantinas iurlionis pdf menotyra olson harry music physics and engineering dover publications isbn external links notenscheibe web application pitch constellations of scales triads intervals and the circle of fifths with basic audio on line app illustrating pitch constellations scaletapper iphone app which utilizes pitch constellations 
pdf of musical scales
in cryptography optimal asymmetric encryption padding oaep is padding scheme often used together with rsa encryption
oaep was introduced by bellare and rogaway and subsequently standardized in pkcs and rfc the oaep algorithm is form of feistel network which uses pair of random oracles and to process the plaintext prior to asymmetric encryption
when combined with any secure trapdoor one way permutation this processing is proved in the random oracle model to result in combined scheme which is semantically secure under chosen plaintext attack ind cpa
when implemented with certain trapdoor permutations rsa oaep is also proven to be secure against chosen ciphertext attack
oaep can be used to build an all or nothing transform
oaep satisfies the following two goals add an element of randomness which can be used to convert deterministic encryption scheme traditional rsa into probabilistic scheme
prevent partial decryption of ciphertexts or other information leakage by ensuring that an adversary cannot recover any portion of the plaintext without being able to invert the trapdoor one way permutation the original version of oaep bellare rogaway showed form of plaintext awareness which they claimed implies security against chosen ciphertext attack in the random oracle model when oaep is used with any trapdoor permutation
subsequent results contradicted this claim showing that oaep was only ind cca secure
however the original scheme was proved in the random oracle model to be ind cca secure when oaep is used with the rsa permutation using standard encryption exponents as in the case of rsa oaep
an improved scheme called oaep that works with any trapdoor one way permutation was offered by victor shoup to solve this problem
more recent work has shown that in the standard model that is when hash functions are not modeled as random oracles it is impossible to prove the ind cca security of rsa oaep under the assumed hardness of the rsa problem
algorithm in the diagram mgf is the mask generating function usually mgf hash is the chosen hash function hlen is the length of the output of the hash function in bytes is the length of the rsa modulus in bytes is the message to be padded at most bytes is an optional label to be associated with the message the label is the empty string by default and can be used to authenticate data without requiring encryption ps is byte string of null bytes
is an xor operation
encoding rfc for pkcs specifies the oaep scheme as follows for encoding hash the label using the chosen hash function generate padding string ps consisting of bytes with the value
concatenate lhash ps the single byte and the message to form data block db this data block has length bytes
generate random seed of length hlen
use the mask generating function to generate mask of the appropriate length for the data block mask the data block with the generated mask use the mask generating function to generate mask of length hlen for the seed mask the seed with the generated mask the encoded padded message is the byte concatenated with the maskedseed and maskeddb decoding decoding works by reversing the steps taken in the encoding algorithm hash the label using the chosen hash function to reverse step split the encoded message em into the byte the maskedseed with length hlen and the maskeddb generate the seedmask which was used to mask the seed to reverse step recover the seed with the seedmask generate the dbmask which was used to mask the data block to reverse step recover the data block db to reverse step split the data block into its parts verify that lhash is equal to the computed lhash ps only consists of bytes ps and are separated by the byte and the first byte of em is the byte
if any of these conditions aren met then the padding is invalid usage in rsa the encoded message can then be encrypted with rsa
the deterministic property of rsa is now avoided by using the oaep encoding because the seed is randomly generated and influences the entire encoded message
security the all or nothing security is from the fact that to recover one must recover the entire maskeddb and the entire maskedseed maskeddb is required to recover the seed from the maskedseed and the seed is required to recover the data block db from maskeddb
since any changed bit of cryptographic hash completely changes the result the entire maskeddb and the entire maskedseed must both be completely recovered
implementation in the pkcs standard the random oracles are identical
the pkcs standard further requires that the random oracles be mgf with an appropriate hash function
see also key encapsulation references
the digital signature algorithm dsa is public key cryptosystem and federal information processing standard for digital signatures based on the mathematical concept of modular exponentiation and the discrete logarithm problem
dsa is variant of the schnorr and elgamal signature schemes
the national institute of standards and technology nist proposed dsa for use in their digital signature standard dss in and adopted it as fips in four revisions to the initial specification have been released
the newest specification is fips from july dsa is patented but nist has made this patent available worldwide royalty free
draft version of the specification fips indicates dsa will no longer be approved for digital signature generation but may be used to verify signatures generated prior to the implementation date of that standard
overview the dsa works in the framework of public key cryptosystems and is based on the algebraic properties of modular exponentiation together with the discrete logarithm problem which is considered to be computationally intractable
the algorithm uses key pair consisting of public key and private key
the private key is used to generate digital signature for message and such signature can be verified by using the signer corresponding public key
the digital signature provides message authentication the receiver can verify the origin of the message integrity the receiver can verify that the message has not been modified since it was signed and non repudiation the sender cannot falsely claim that they have not signed the message
history in the government solicited proposals for public key signature standard
in august the national institute of standards and technology nist proposed dsa for use in their digital signature standard dss
initially there was significant criticism especially from software companies that had already invested effort in developing digital signature software based on the rsa cryptosystem
nevertheless nist adopted dsa as federal standard fips in four revisions to the initial specification have been released fips in fips in fips in and fips in draft version of standard fips forbids signing with dsa while allowing verification of signatures generated prior to the implementation date of the standard as document
it is to be replaced by newer signature schemes such as eddsa dsa is covered by patent filed july and now expired and attributed to david kravitz former nsa employee
this patent was given to the united states of america as represented by the secretary of commerce washington and nist has made this patent available worldwide royalty free
claus schnorr claims that his patent also now expired covered dsa this claim is disputed
operation the dsa algorithm involves four operations key generation which creates the key pair key distribution signing and signature verification
key generation key generation has two phases
the first phase is choice of algorithm parameters which may be shared between different users of the system while the second phase computes single key pair for one user
parameter generation choose an approved cryptographic hash function with output length bits
in the original dss was always sha but the stronger sha hash functions are approved for use in the current dss
if is greater than the modulus length only the leftmost bits of the hash output are used
choose key length the original dss constrained to be multiple of between and inclusive
nist recommends lengths of or for keys with security lifetimes extending beyond or
choose the modulus length such that and
fips specifies and to have one of the values or
choose an bit prime choose an bit prime such that is multiple of choose an integer randomly from
compute mod in the rare case that try again with different commonly is used
this modular exponentiation can be computed efficiently even if the values are large the algorithm parameters are
these may be shared between different users of the system
per user keys given set of parameters the second phase computes the key pair for single user choose an integer randomly from
compute mod is the private key and is the public key
key distribution the signer should publish the public key that is they should send the key to the receiver via reliable but not necessarily secret mechanism
the signer should keep the private key secret
signing message is signed as follows choose an integer randomly from compute mod mod in the unlikely case that start again with different random compute mod in the unlikely case that start again with different random the signature is the calculation of and amounts to creating new per message key
the modular exponentiation in computing is the most computationally expensive part of the signing operation but it may be computed before the message is known
calculating the modular inverse mod is the second most expensive part and it may also be computed before the message is known
it may be computed using the extended euclidean algorithm or using fermat little theorem as mod
signature verification one can verify that signature is valid signature for message as follows verify that and compute mod compute mod compute mod compute mod mod the signature is valid if and only if correctness of the algorithm the signature scheme is correct in the sense that the verifier will always accept genuine signatures
this can be shown as follows first since mod it follows that mod by fermat little theorem
since and is prime must have order the signer computes mod thus mod since has order we have mod finally the correctness of dsa follows from mod mod mod mod sensitivity with dsa the entropy secrecy and uniqueness of the random signature value are critical
it is so critical that violating any one of those three requirements can reveal the entire private key to an attacker
using the same value twice even while keeping secret using predictable value or leaking even few bits of in each of several signatures is enough to reveal the private key this issue affects both dsa and elliptic curve digital signature algorithm ecdsa in december group calling itself fail verflow announced recovery of the ecdsa private key used by sony to sign software for the playstation game console
the attack was made possible because sony failed to generate new random for each signature this issue can be prevented by deriving deterministically from the private key and the message hash as described by rfc this ensures that is different for each and unpredictable for attackers who do not know the private key in addition malicious implementations of dsa and ecdsa can be created where is chosen in order to subliminally leak information via signatures
for example an offline private key could be leaked from perfect offline device that only released innocent looking signatures
implementations below is list of cryptographic libraries that provide support for dsa botan bouncy castle cryptlib crypto libgcrypt nettle openssl wolfcrypt gnutls see also modular arithmetic rsa cryptosystem ecdsa references external links fips pub digital signature standard dss the fourth and current revision of the official dsa specification
recommendation for key management part general nist special publication
in cryptography related key attack is any form of cryptanalysis where the attacker can observe the operation of cipher under several different keys whose values are initially unknown but where some mathematical relationship connecting the keys is known to the attacker
for example the attacker might know that the last bits of the keys are always the same even though they don know at first what the bits are
this appears at first glance to be an unrealistic model it would certainly be unlikely that an attacker could persuade human cryptographer to encrypt plaintexts under numerous secret keys related in some way
kasumi kasumi is an eight round bit block cipher with bit key
it is based upon misty and was designed to form the basis of the confidentiality and integrity algorithms
mark blunden and adrian escott described differential related key attacks on five and six rounds of kasumi
differential attacks were introduced by biham and shamir
related key attacks were first introduced by biham
differential related key attacks are discussed in kelsey et al
wep an important example of cryptographic protocol that failed because of related key attack is wired equivalent privacy wep used in wi fi wireless networks
each client wi fi network adapter and wireless access point in wep protected network shares the same wep key
encryption uses the rc algorithm stream cipher
it is essential that the same key never be used twice with stream cipher
to prevent this from happening wep includes bit initialization vector iv in each message packet
the rc key for that packet is the iv concatenated with the wep key
wep keys have to be changed manually and this typically happens infrequently
an attacker therefore can assume that all the keys used to encrypt packets share single wep key
this fact opened up wep to series of attacks which proved devastating
the simplest to understand uses the fact that the bit iv only allows little under million possibilities
because of the birthday paradox it is likely that for every packets two will share the same iv and hence the same rc key allowing the packets to be attacked
more devastating attacks take advantage of certain weak keys in rc and eventually allow the wep key itself to be recovered
in agents from the federal bureau of investigation publicly demonstrated the ability to do this with widely available software tools in about three minutes
preventing related key attacks one approach to preventing related key attacks is to design protocols and applications so that encryption keys will never have simple relationship with each other
for example each encryption key can be generated from the underlying key material using key derivation function
for example replacement for wep wi fi protected access wpa uses three levels of keys master key working key and rc key
the master wpa key is shared with each client and access point and is used in protocol called temporal key integrity protocol tkip to create new working keys frequently enough to thwart known attack methods
the working keys are then combined with longer bit iv to form the rc key for each packet
this design mimics the wep approach enough to allow wpa to be used with first generation wi fi network cards some of which implemented portions of wep in hardware
however not all first generation access points can run wpa
another more conservative approach is to employ cipher designed to prevent related key attacks altogether usually by incorporating strong key schedule
newer version of wi fi protected access wpa uses the aes block cipher instead of rc in part for this reason
there are related key attacks against aes but unlike those against rc they re far from practical to implement and wpa key generation functions may provide some security against them
many older network cards cannot run wpa
in western music the adjectives major and minor may describe chord scale or key 
as such composition movement section or phrase may be referred to by its key including whether that key is major or minor 
intervals some intervals may be referred to as major and minor 
major interval is one semitone larger than minor interval 
the words perfect diminished and augmented are also used to describe the quality of an interval 
only the intervals of second third sixth and seventh and the compound intervals based on them may be major or minor or rarely diminished or augmented 
unisons fourths fifths and octaves and their compound interval must be perfect or rarely diminished or augmented 
in western music minor chord sounds darker than major chord 
scales and chords the other uses of major and minor generally refer to scales and chords that contain major third or minor third respectively 
major scale is scale in which the third scale degree the mediant is major third above the tonic note 
in minor scale the third degree is minor third above the tonic 
similarly in major triad or major seventh chord the third is major third above the chord root 
in minor triad or minor seventh chord the third is minor third above the root 
keys the hallmark that distinguishes major keys from minor is whether the third scale degree is major or minor 
as musicologist roger kamien explains the crucial difference is that in the minor scale there is only half step between nd and rd note and between th and th note as compared to the major scales where the difference between rd and th note and between th and th note is half step 
this alteration in the third degree greatly changes the mood of the music and music based on minor scales tends to be considered to sound serious or melancholic minor keys are sometimes said to have more interesting possibly darker sound than plain major scales 
harry partch considers minor as the immutable faculty of ratios which in turn represent an immutable faculty of the human ear 
the minor key and scale are also considered less justifiable than the major with paul hindemith calling it clouding of major and moritz hauptmann calling it falsehood of the major changes of mode which involve the alteration of the third and mode mixture are often analyzed as minor changes unless structurally supported because the root and overall key and tonality remain unchanged 
this is in contrast with for instance transposition 
transposition is done by moving all intervals up or down certain constant interval and does change the key but not the mode which requires the alteration of intervals 
the use of triads only available in the minor mode such as the use of major in major is relatively decorative chromaticism considered to add color and weaken the sense of key without entirely destroying or losing it 
intonation and tuning musical tuning of intervals is expressed by the ratio between the pitches frequencies 
simple fractions can sound more harmonious than complex fractions for instance an octave is simple ratio and fifth is the relatively simple ratio 
the table below gives approximations of scale to ratios that are rounded to be as simple as possible 
in just intonation minor chord is often but not exclusively tuned in the frequency ratio play 
in tone equal temperament tet which is now the most common tuning system in the west minor chord has semitones between the root and third between the third and fifth and between the root and fifth 
in tet the perfect fifth cents is only about two cents narrower than the just tuned perfect fifth cents but the minor third cents is noticeably about cents narrower than the just minor third cents 
moreover the minor third cents more closely approximates the limit limit music minor third play cents the nineteenth harmonic with about two cents error alexander ellis proposes that the conflict between mathematicians and physicists on one hand and practicing musicians on the other regarding the supposed inferiority of the minor chord and scale to the major may be explained due to physicists comparison of just minor and major triads in which case minor comes out the loser versus the musicians comparison of the equal tempered triads in which case minor comes out the winner since the et major third is about cents sharp from the just major third cents but just about four cents narrower than the limit major third cents while the et minor third closely approximates the minor third which many find pleasing 
advanced theory in the neo riemannian theory the minor mode is considered the inverse of the major mode an upside down major scale based on theoretical undertones rather than actual overtones harmonics see also utonality 
the root of the minor triad is thus considered the top of the fifth which in the united states is called the fifth 
so in minor the tonic is actually and the leading tone is half step rather than in major the root being and the leading tone half step 
also since all chords are analyzed as having tonic subdominant or dominant function with for instance in minor being considered the tonic parallel us relative tp the use of minor mode root chord progressions in major such as major major major is analyzed as sp dp the minor subdominant parallel see parallel chord the minor dominant parallel and the major tonic 
see also gypsy scale list of major minor compositions music written in all major and or minor keys otonality and utonality references
searchable symmetric encryption sse is form of encryption that allows one to efficiently search over collection of encrypted documents or files without the ability to decrypt them
sse can be used to outsource files to an untrusted cloud storage server without ever revealing the files in the clear but while preserving the server ability to search over them
description searchable symmetric encryption scheme is symmetric key encryption scheme that encrypts collection of documents where each document is viewed as set of keywords from keyword space given the encryption key and keyword one can generate search token with which the encrypted data collection can be searched for the result of the search is the subset of encrypted documents that contain the keyword static sse static sse scheme consists of three algorithms that work as follows takes as input security parameter and document collection and outputs symmetric key and an encrypted document collection takes as input the secret key and keyword and outputs search token takes as input the encrypted document collection and search token and outputs set of encrypted documents static sse scheme is used by client and an untrusted server as follows
the client encrypts its data collection using the algorithm which returns secret key and an encrypted document collection the client keeps secret and sends to the untrusted server
to search for keyword the client runs the algorithm on and to generate search token which it sends to the server
the server runs search with and and returns the resulting encrypted documents back to the server
dynamic sse dynamic sse scheme supports in addition to search the insertion and deletion of documents
dynamic sse scheme consists of seven algorithms where and are as in the static case and the remaining algorithms work as follows takes as input the secret key and new document and outputs an insert token takes as input the encrypted document collection edc and an insert token and outputs an updated encrypted document collection takes as input the secret key and document identifier and outputs delete token takes as input the encrypted data collection and delete token and outputs an updated encrypted data collection to add new document the client runs on and to generate an insert token which it sends to the server
the server runs with and and stores the updated encrypted document collection
to delete document with identifier the client runs the algorithm with and to generate delete token which it sends to the server
the server runs with and and stores the updated encrypted document collection
an sse scheme that does not support and is called semi dynamic
history of searchable symmetric encryption the problem of searching on encrypted data was considered by song wagner and perrig though previous work on oblivious ram by goldreich and ostrovsky could be used in theory to address the problem
this work proposed an sse scheme with search algorithm that runs in time where
goh and chang and mitzenmacher gave new sse constructions with search algorithms that run in time where is the number of documents
curtmola garay kamara and ostrovsky later proposed two static constructions with search time where is the number of documents that contain which is optimal
this work also proposed semi dynamic construction with log search time where is the number of updates
an optimal dynamic sse construction was later proposed by kamara papamanthou and roeder goh and chang and mitzenmacher proposed security definitions for sse
these were strengthened and extended by curtmola garay kamara and ostrovsky who proposed the notion of adaptive security for sse
this work also was the first to observe leakage in sse and to formally capture it as part of the security definition
leakage was further formalized and generalized by chase and kamara
islam kuzu and kantarcioglu described the first leakage attack all the previously mentioned constructions support single keyword search
cash jarecki jutla krawczyk rosu and steiner proposed an sse scheme that supports conjunctive search in sub linear time in the construction can also be extended to support disjunctive and boolean searches that can be expressed in searchable normal form snf in sub linear time
at the same time pappas krell vo kolesnikov malkin choi george keromytis and bellovin described construction that supports conjunctive and all disjunctive and boolean searches in sub linear time
security sse schemes are designed to guarantee that the untrusted server cannot learn any partial information about the documents or the search queries beyond some well defined and reasonable leakage
the leakage of scheme is formally described using leakage profile which itself can consists of several leakage patterns
sse constructions attempt to minimize leakage while achieving the best possible search efficiency
sse security can be analyzed in several adversarial models but the most common are the persistent model where an adversary is given the encrypted data collection and transcript of all the operations executed on the collection the snapshot model where an adversary is only given the encrypted data collection but possibly after each operation
security in the persistent model in the persistent model there are sse schemes that achieve wide variety of leakage profiles
the most common leakage profile for static schemes that achieve single keyword search in optimal time is which reveals the number of documents in the collection the size of each document in the collection if and when query was repeated and which encrypted documents match the search query
it is known however how to construct schemes that leak considerably less at an additional cost in search time and storage when considering dynamic sse schemes the state of the art constructions with optimal time search have leakage profiles that guarantee forward privacy which means that inserts cannot be correlated with past search queries
security in the snapshot model in the snapshot model efficient dynamic sse schemes with no leakage beyond the number of documents and the size of the collection can be constructed
when using an sse construction that is secure in the snapshot model one has to carefully consider how the scheme will be deployed because some systems might cache previous search queries
cryptanalysis leakage profile only describes the leakage of an sse scheme but it says nothing about whether that leakage can be exploited or not
cryptanalysis is therefore used to better understand the real world security of leakage profile
there is wide variety of attacks working in different adversarial models based on variety of assumptions and attacking different leakage profiles
see also homomorphic encryption oblivious ram structured encryption deterministic encryption references
aids is caused by human immunodeficiency virus hiv which originated in non human primates in central and west africa 
while various sub groups of the virus acquired human infectivity at different times the present pandemic had its origins in the emergence of one specific strain hiv subgroup in opoldville in the belgian congo now kinshasa in the democratic republic of the congo in the there are two types of hiv hiv and hiv 
hiv is more virulent easily transmitted and is the cause of the vast majority of hiv infections globally 
the pandemic strain of hiv is closely related to virus found in chimpanzees of the subspecies pan troglodytes troglodytes which live in the forests of the central african nations of cameroon equatorial guinea gabon the republic of the congo and the central african republic 
hiv is less transmittable and is largely confined to west africa along with its closest relative virus of the sooty mangabey cercocebus atys atys an old world monkey inhabiting southern senegal guinea bissau guinea sierra leone liberia and western ivory coast 
transmission from non humans to humans research in this area is conducted using molecular phylogenetics comparing viral genomic sequences to determine relatedness 
hiv from chimpanzees and gorillas to humans scientists generally accept that the known strains or groups of hiv are most closely related to the simian immunodeficiency viruses sivs endemic in wild ape populations of west central african forests 
in particular each of the known hiv strains is either closely related to the siv that infects the chimpanzee subspecies pan troglodytes troglodytes sivcpz or closely related to the siv that infects western lowland gorillas gorilla gorilla gorilla called sivgor 
the pandemic hiv strain group or main and rare strain found only in few cameroonian people group are clearly derived from sivcpz strains endemic in pan troglodytes troglodytes chimpanzee populations living in cameroon 
another very rare hiv strain group is clearly derived from sivgor strains of cameroon 
finally the primate ancestor of hiv group strain infecting people mostly from cameroon but also from neighbouring countries was confirmed in to be sivgor 
the pandemic hiv group is most closely related to the sivcpz collected from the southeastern rain forests of cameroon modern east province near the sangha river 
thus this region is presumably where the virus was first transmitted from chimpanzees to humans 
however reviews of the epidemiological evidence of early hiv infection in stored blood samples and of old cases of aids in central africa have led many scientists to believe that hiv group early human centre was probably not in cameroon but rather further south in the democratic republic of the congo then the belgian congo more probably in its capital city kinshasa formerly opoldville using hiv sequences preserved in human biological samples along with estimates of viral mutation rates scientists calculate that the jump from chimpanzee to human probably happened during the late th or early th century time of rapid urbanisation and colonisation in equatorial africa 
exactly when the zoonosis occurred is not known 
some molecular dating studies suggest that hiv group had its most recent common ancestor mrca that is started to spread in the human population in the early th century probably between and study published in analyzing viral sequences recovered from biopsy made in kinshasa in along with previously known sequences suggested common ancestor between and with central estimates varying between and 
genetic recombination had earlier been thought to seriously confound such phylogenetic analysis but later work has suggested that recombination is not likely to systematically bias results although recombination is expected to increase variance 
the results of phylogenetics study support the later work and indicate that hiv evolves fairly reliably 
further research was hindered due to the primates being critically endangered 
sample analyses resulted in little data due to the rarity of experimental material 
the researchers however were able to hypothesize phylogeny from the gathered data 
they were also able to use the molecular clock of specific strain of hiv to determine the initial date of transmission which is estimated to be around 
hiv from sooty mangabeys to humans similar research has been undertaken with siv strains collected from several wild sooty mangabey cercocebus atys atys sivsmm populations of the west african nations of sierra leone liberia and ivory coast 
the resulting phylogenetic analyses show that the viruses most closely related to the two strains of hiv that spread considerably in humans hiv groups and are the sivsmm found in the sooty mangabeys of the tai forest in western ivory coast there are six additional known hiv groups each having been found in just one person 
they all seem to derive from independent transmissions from sooty mangabeys to humans 
groups and have been found in two people from liberia groups and have been discovered in two people from sierra leone and groups and have been detected in two people from the ivory coast 
these hiv strains are probably dead end infections and each of them is most closely related to sivsmm strains from sooty mangabeys living in the same country where the human infection was found molecular dating studies suggest that both the epidemic groups and started to spread among humans between and with the central estimates varying between and 
bushmeat practice according to the natural transfer theory also called hunter theory or bushmeat theory in the simplest and most plausible explanation for the cross species transmission of siv or hiv post mutation the virus was transmitted from an ape or monkey to human when hunter or bushmeat vendor handler was bitten or cut while hunting or butchering the animal 
the resulting exposure to blood or other bodily fluids of the animal can result in siv infection 
prior to wwii some sub saharan africans were forced out of the rural areas because of the european demand for resources 
since rural africans were not keen to pursue agricultural practices in the jungle they turned to non domesticated animals as their primary source of meat 
this over exposure to bushmeat and malpractice of butchery increased blood to blood contact which then increased the probability of transmission 
recent serological survey showed that human infections by siv are not rare in central africa the percentage of people showing seroreactivity to antigens evidence of current or past siv infection was among the general population of cameroon in villages where bushmeat is hunted or used and in the most exposed people of these villages 
how the siv virus would have transformed into hiv after infection of the hunter or bushmeat handler from the ape monkey is still matter of debate although natural selection would favour any viruses capable of adjusting so that they could infect and reproduce in the cells of human host 
emergence unresolved questions about hiv origins and emergence the discovery of the main hiv siv phylogenetic relationships permits explaining broad hiv biogeography the early centres of the hiv groups were in central africa where the primate reservoirs of the related sivcpz and sivgor viruses chimpanzees and gorillas exist similarly the hiv groups had their centres in west africa where sooty mangabeys which harbour the related sivsmm virus exist 
however these relationships do not explain more detailed patterns of biogeography such as why epidemic hiv groups and only evolved in the ivory coast which is one of only six countries harbouring the sooty mangabey 
it is also unclear why the sivcpz endemic in the chimpanzee subspecies pan troglodytes schweinfurthii inhabiting the democratic republic of congo central african republic rwanda burundi uganda and tanzania did not spawn an epidemic hiv strain to humans while the democratic republic of congo was the main centre of hiv group virus descended from sivcpz strains of subspecies pan troglodytes troglodytes that does not exist in this country 
it is clear that the several hiv and hiv strains descend from sivcpz sivgor and sivsmm viruses and that bushmeat practice provides the most plausible cause of cross species transfer to humans 
however some loose ends remain 
it is not yet explained why only four hiv groups hiv groups and and hiv groups and spread considerably in human populations despite bushmeat practices being widespread in central and west africa and the resulting human siv infections being common it also remains unexplained why all epidemic hiv groups emerged in humans nearly simultaneously and only in the th century despite very old human exposure to siv phylogenetic study demonstrated that siv is at least tens of thousands of years old 
origin and epidemic emergence several of the theories of hiv origin accept the established knowledge of the hiv siv phylogenetic relationships and also accept that bushmeat practice was the most likely cause of the initial transfer to humans 
all of them propose that the simultaneous epidemic emergences of four hiv groups in the late th early th century and the lack of previous known emergences are explained by new factor that appeared in the relevant african regions in that timeframe 
these new factor would have acted either to increase human exposures to siv to help it to adapt to the human organism by mutation thus enhancing its between humans transmissibility or to cause an initial burst of transmissions crossing an epidemiological threshold and therefore increasing the probability of continued spread 
genetic studies of the virus suggested in that the most recent common ancestor of the hiv group dates back to the belgian congo city of opoldville modern kinshasa circa proponents of this dating link the hiv epidemic with the emergence of colonialism and growth of large colonial african cities leading to social changes including higher degree of non monogamous sexual activity the spread of prostitution and the concomitant high frequency of genital ulcer diseases such as syphilis in nascent colonial cities in study conducted by scientists from the university of oxford and the university of leuven in belgium revealed that because approximately one million people every year would flow through the prominent city of kinshasa which served as the origin of the first known hiv cases in the passengers riding on the region belgian railway trains were able to spread the virus to larger areas 
the study also identified roaring sex trade rapid population growth and unsterilised needles used in health clinics as other factors which contributed to the emergence of the africa hiv epidemic 
social changes and urbanization beatrice hahn paul sharp and their colleagues proposed that the epidemic emergence of hiv most likely reflects changes in population structure and behaviour in africa during the th century and perhaps medical interventions that provided the opportunity for rapid human to human spread of the virus 
after the scramble for africa started in the european colonial powers established cities towns and other colonial stations 
largely masculine labor force was hastily recruited to work in fluvial and sea ports railways other infrastructures and in plantations 
this disrupted traditional tribal values and favored casual sexual activity with an increased number of partners 
in the nascent cities women felt relatively liberated from rural tribal rules and many remained unmarried or divorced during long periods this being rare in african traditional societies 
this was accompanied by unprecedented increase in people movements 
michael worobey and colleagues observed that the growth of cities probably played role in the epidemic emergence of hiv since the phylogenetic dating of the two older strains of hiv groups and suggest that these viruses started to spread soon after the main central african colonial cities were founded 
colonialism in africa amit chitnis diana rawls and jim moore proposed that hiv may have emerged epidemically as result of harsh conditions forced labor displacement and unsafe injection and vaccination practices associated with colonialism particularly in french equatorial africa 
the workers in plantations construction projects and other colonial enterprises were supplied with bushmeat which would have contributed to an increase in hunting and it follows higher incidence of human exposure to siv 
several historical sources support the view that bushmeat hunting indeed increased both because of the necessity to supply workers and because firearms became more widely available the colonial authorities also gave many vaccinations against smallpox and injections of which many would be made without sterilising the equipment between uses 
proposed that both these parenteral risks and the prostitution associated with forced labor camps could have caused serial transmission or serial passage of siv between humans see discussion of this in the next section 
in addition they proposed that the conditions of extreme stress associated with forced labor could depress the immune system of workers therefore prolonging the primary acute infection period of someone newly infected by siv thus increasing the odds of both adaptation of the virus to humans and of further transmissions the authors proposed that hiv originated in the area of french equatorial africa in the early th century when the colonial abuses and forced labor were at their peak 
later research established that these theories were mostly correct hiv groups and started to spread in humans in late th early th century 
in addition all groups of hiv descend from either sivcpz or sivgor from apes living to the west of the ubangi river either in countries that belonged to the french equatorial africa federation of colonies in equatorial guinea then spanish colony or in cameroon which was german colony between and and then fell to allied forces in world war and had most of its area administered by france in close association with french equatorial africa 
this theory was later dubbed heart of darkness by jim moore alluding to the book of the same title written by joseph conrad the main focus of which is colonial abuses in equatorial africa 
unsterile injections in several articles published since preston marx philip alcabes and ernest drucker proposed that hiv emerged because of rapid serial human to human transmission of siv after bushmeat hunter or handler became siv infected through unsafe or unsterile injections 
although both chitnis et al 
and sharp et al 
also suggested that this may have been one of the major risk factors at play in hiv emergence see above marx et al 
enunciated the underlying mechanisms in greater detail and wrote the first review of the injection campaigns made in colonial africa central to the marx et al 
argument is the concept of adaptation by serial passage or serial transmission an adventitious virus or other pathogen can increase its biological adaptation to new host species if it is rapidly transmitted between hosts while each host is still in the acute infection period 
this process favors the accumulation of adaptive mutations more rapidly therefore increasing the odds that better adapted viral variant will appear in the host before the immune system suppresses the virus 
such better adapted variants could then survive in the human host for longer than the short acute infection period in high numbers high viral load which would grant it more possibilities of epidemic spread 
reported experiments of cross species transfer of siv in captive monkeys some of which made by themselves in which the use of serial passage helped to adapt siv to the new monkey species after passage by three or four animals in agreement with this model is also the fact that while both hiv and hiv attain substantial viral loads in the human organism adventitious siv infecting humans seldom does so people with siv antibodies often have very low or even undetectable siv viral load 
this suggests that both hiv and hiv are adapted to humans and serial passage could have been the process responsible for it 
proposed that unsterile injections that is injections where the needle or syringe is reused without sterilization or cleaning between uses which were likely very prevalent in africa during both the colonial period and afterwards provided the mechanism of serial passage that permitted hiv to adapt to humans therefore explaining why it emerged epidemically only in the th century 
massive injections of the antibiotic era marx et al 
emphasize the massive number of injections administered in africa after antibiotics were introduced around as being the most likely implicated in the origin of hiv because by these times roughly in the period to injection intensity in africa was maximal 
they argued that serial passage chain of or transmissions between humans is an unlikely event the probability of transmission after needle reuse is something between and and only few people have an acute siv infection at any time and so hiv emergence may have required the very high frequency of injections of the antibiotic era the molecular dating studies place the initial spread of the epidemic hiv groups before that time see above 
according to marx et al these studies could have overestimated the age of the hiv groups because they depend on molecular clock assumption may not have accounted for the effects of natural selection in the viruses and the serial passage process alone would be associated with strong natural selection 
injection campaigns against sleeping sickness david gisselquist proposed that the mass injection campaigns to treat trypanosomiasis sleeping sickness in central africa were responsible for the emergence of hiv 
unlike marx et al gisselquist argued that the millions of unsafe injections administered during these campaigns were sufficient to spread rare hiv infections into an epidemic and that evolution of hiv through serial passage was not essential to the emergence of the hiv epidemic in the th century this theory focuses on injection campaigns that peaked in the period that is around the time the hiv groups started to spread 
it also focuses on the fact that many of the injections in these campaigns were intravenous which are more likely to transmit siv hiv than subcutaneous or intramuscular injections and many of the patients received many often more than injections per year therefore increasing the odds of siv serial passage 
other early injection campaigns jacques pin and annie claude labb reviewed the colonial health reports of cameroon and french equatorial africa for the period calculating the incidences of the diseases requiring intravenous injections 
they concluded that trypanosomiasis leprosy yaws and syphilis were responsible for most intravenous injections 
schistosomiasis tuberculosis and vaccinations against smallpox represented lower parenteral risks schistosomiasis cases were relatively few tuberculosis patients only became numerous after mid century and there were few smallpox vaccinations in the lifetime of each person the authors suggested that the very high prevalence of the hepatitis virus in southern cameroon and forested areas of french equatorial africa around can be better explained by the unsterile injections used to treat yaws because this disease was much more prevalent than syphilis trypanosomiasis and leprosy in these areas 
they suggested that all these parenteral risks caused not only the massive spread of hepatitis but also the spread of other pathogens and the emergence of hiv the same procedures could have exponentially amplified hiv from single hunter cook occupationally infected with sivcpz to several thousand patients treated with arsenicals or other drugs threshold beyond which sexual transmission could prosper 
they do not suggest specifically serial passage as the mechanism of adaptation 
according to pin book the origins of aids the virus can be traced to central african bush hunter in with colonial medical campaigns using improperly sterilized syringe and needles playing key role in enabling future epidemic 
pin concludes that aids spread silently in africa for decades fueled by urbanization and prostitution since the initial cross species infection 
pin also claims that the virus was brought to the americas by haitian teacher returning home from zaire in the 
sex tourism and contaminated blood transfusion centers ultimately propelled aids to public consciousness in the and worldwide pandemic 
genital ulcer diseases and evolution of sexual activity jo dinis de sousa viktor ller philippe lemey and anne mieke vandamme proposed that hiv became epidemic through sexual serial transmission in nascent colonial cities helped by high frequency of genital ulcers caused by genital ulcer diseases gud 
gud are simply sexually transmitted diseases that cause genital ulcers examples are syphilis chancroid lymphogranuloma venereum and genital herpes 
these diseases increase the probability of hiv transmission dramatically from around to per heterosexual act because the genital ulcers provide portal of viral entry and contain many activated cells expressing the ccr co receptor the main cell targets of hiv 
probable time interval of cross species transfer sousa et al 
use molecular dating techniques to estimate the time when each hiv group split from its closest siv lineage 
each hiv group necessarily crossed to humans between this time and the time when it started to spread the time of the mrca because after the mrca certainly all lineages were already in humans and before the split with the closest simian strain the lineage was in simian 
hiv groups and split from their closest sivs around and respectively 
this information together with the datations of the hiv groups mrcas mean that all hiv groups likely crossed to humans in the early th century 
strong genital ulcer disease incidence in nascent colonial cities the authors reviewed colonial medical articles and archived medical reports of the countries at or near the ranges of chimpanzees gorillas and sooty mangabeys and found that genital ulcer diseases guds peaked in the colonial cities during their early growth period up to 
the colonial authorities recruited men to work in railways fluvial and sea ports and other infrastructure projects and most of these men did not bring their wives with them 
then the highly male biased sex ratio favoured prostitution which in its turn caused an explosion of gud especially syphilis and chancroid 
after the mid people movements were more tightly controlled and mass surveys and treatments of arsenicals and other drugs were organized and so the gud incidences started to decline 
they declined even further after world war ii because of the heavy use of antibiotics so that by the late opoldville which is the probable center of hiv group had very low gud incidence 
similar processes happened in the cities of cameroon and ivory coast where hiv group and hiv respectively evolved therefore the peak gud incidences in cities have good temporal coincidence with the period when all main hiv groups crossed to humans and started to spread 
in addition the authors gathered evidence that syphilis and the other guds were like injections absent from the densely forested areas of central and west africa before organized colonialism socially disrupted these areas starting in the 
thus this theory also potentially explains why hiv emerged only after the late th century 
female genital mutilation uli linke has argued that the practice of female genital mutilation either or both of clitoridectomy and infibulation is responsible for the high incidence of aids in africa since intercourse with female who has undergone clitoridectomy is conducive to exchange of blood 
male circumcision distribution and hiv origins male circumcision may reduce the probability of hiv acquisition by men 
leaving aside blood transfusions the highest hiv transmissibility ever measured was from female prostitutes with prevalence of hiv to uncircumcised men with gud cumulative seroconverted to hiv after single sexual exposure 
there was no seroconversion in the absence of male gud 
reasoned that the adaptation and epidemic emergence of each hiv group may have required such extreme conditions and thus reviewed the existing ethnographic literature for patterns of male circumcision and hunting of apes and monkeys for bushmeat focusing on the period and on most of the ethnic groups living in central and west africa 
they also collected censuses and other literature showing the ethnic composition of colonial cities in this period 
then they estimated the circumcision frequencies of the central african cities over time 
charts reveal that male circumcision frequencies were much lower in several cities of western and central africa in the early th century than they are currently 
the reason is that many ethnic groups not performing circumcision by that time gradually adopted it to imitate other ethnic groups and enhance the social acceptance of their boys colonialism produced massive intermixing between african ethnic groups 
about of men in opoldville and douala in the early th century should be uncircumcised and these cities were the probable centers of hiv groups and respectively the authors studied early circumcision frequencies in cities of central and west africa to test if this variable correlated with hiv emergence 
this correlation was strong for hiv among west african cities that could have received immigrants infected with sivsmm the two cities from the ivory coast studied abidjan and bouak had much higher frequency of uncircumcised men than the others and epidemic hiv groups emerged initially in this country only 
this correlation was less clear for hiv in central africa 
computer simulations of hiv emergence sousa et al 
then built computer simulations to test if an ill adapted siv meaning simian immunodeficiency virus already infecting human but incapable of transmission beyond the short acute infection period could spread in colonial cities 
the simulations used parameters of sexual transmission obtained from the current hiv literature 
they modelled people sexual links with different levels of sexual partner change among different categories of people prostitutes single women with several partners year married women and men according to data obtained from modern studies of sexual activity in african cities 
the simulations let the parameters city size proportion of people married gud frequency male circumcision frequency and transmission parameters vary and explored several scenarios 
each scenario was run times to test the probability of siv generating long chains of sexual transmission 
the authors postulated that such long chains of sexual transmission were necessary for the siv strain to adapt better to humans becoming an hiv capable of further epidemic emergence 
the main result was that genital ulcer frequency was by far the most decisive factor 
for the gud levels prevailing in opoldville in the early th century long chains of siv transmission had high probability 
for the lower gud levels existing in the same city in the late see above they were much less likely 
and without gud situation typical of villages in forested equatorial africa before colonialism siv could not spread at all 
city size was not an important factor 
the authors propose that these findings explain the temporal patterns of hiv emergence no hiv emerging in tens of thousands of years of human slaughtering of apes and monkeys several hiv groups emerging in the nascent gud riddled colonial cities and no epidemically successful hiv group emerging in mid th century when gud was more controlled and cities were much bigger 
male circumcision had little to moderate effect in their simulations but given the geographical correlation found the authors propose that it could have had an indirect role either by increasing genital ulcer disease itself it is known that syphilis chancroid and several other guds have higher incidences in uncircumcised men or by permitting further spread of the hiv strain after the first chains of sexual transmission permitted adaptation to the human organism 
one of the main advantages of this theory is stressed by the authors it the theory also offers conceptual simplicity because it proposes as causal factors for siv adaptation to humans and initial spread the very same factors that most promote the continued spread of hiv nowadays promiscuous sic sex particularly involving sex workers gud and possibly lack of circumcision 
iatrogenic and other theories iatrogenic theories propose that medical interventions were responsible for hiv origins 
by proposing factors that only appeared in central and west africa after the late th century they seek to explain why all hiv groups also started after that 
the theories centred on the role of parenteral risks such as unsterile injections transfusions or smallpox vaccinations are accepted as plausible by most scientists of the field 
discredited hiv aids origins theories include several iatrogenic theories such as the polio vaccine hypothesis which argues that the early oral polio vaccines were contaminated with chimpanzee virus leading to the central african outbreak 
pathogenicity of siv in non human primates in most non human primate species natural siv infection does not cause fatal disease but see below 
comparison of the gene sequence of siv with hiv should therefore provide information about the factors necessary to cause disease in humans 
the factors that determine the virulence of hiv as compared to most sivs are only now being elucidated 
non human sivs contain nef gene that down regulates cd cd and mhc class expression most non human sivs therefore do not induce immunodeficiency the hiv nef gene however has lost its ability to down regulate cd which results in the immune activation and apoptosis that is characteristic of chronic hiv infection in addition long term survey of chimpanzees naturally infected with sivcpz in gombe national park tanzania found that contrary to the previous paradigm chimpanzees with sivcpz infection do experience an increased mortality and also suffer from human aids like illness 
siv pathogenicity in wild animals could exist in other chimpanzee subspecies and other primate species as well and stay unrecognized by lack of relevant long term studies 
history of spread david carr david carr was an apprentice printer usually mistakenly referred to as sailor carr had served in the navy between and from manchester england who died on august and was for some time mistakenly reported to have died from aids defining opportunistic infections adois 
following the failure of his immune system he succumbed to pneumonia 
doctors baffled by what he had died from preserved of his tissue samples for inspection 
in the tissues were found to be hiv positive 
however in second test by aids researcher david ho found that the strain of hiv present in the tissues was similar to those found in rather than an earlier strain which would have mutated considerably over the course of years 
he concluded that the dna samples provided actually came from patient with aids in the 
upon retesting david carr tissues he found no sign of the virus 
congolese man one of the earliest documented hiv infections was discovered in preserved blood sample taken in from man from opoldville in the belgian congo 
however it is unknown whether this anonymous person ever developed aids and died of its complications 
congolese woman second early documented hiv infection was discovered in preserved lymph node biopsy sample taken in from woman from opoldville belgian congo 
congolese man strain with large amount of the genetic material present was dated to from sample from year old man 
robert rayford in may year old african american robert rayford died at the st louis city hospital from kaposi sarcoma 
in researchers at tulane university school of medicine detected virus closely related or identical to hiv in his preserved blood and tissues 
the doctors who worked on his case at the time suspected he was prostitute or the victim of sexual abuse though the patient did not discuss his sexual history with them in detail 
ugandan children from to researchers drew blood from children in uganda to serve as controls for study of burkitt lymphoma 
in retroactive testing of the frozen blood serum indicated that antibodies to virus related to hiv were present in of the children 
arvid noe in and norwegian sailor with the alias name arvid noe his wife and his seven year old daughter died of aids 
the sailor had first presented symptoms in eight years after he first spent time in ports along the west african coastline 
gonorrhea infection during his first african voyage shows he was sexually active at this time 
tissue samples from the sailor and his wife were tested in and found to contain hiv group 
grethe rask grethe rask was danish surgeon who traveled to za re in then again in to aid the sick 
she was likely directly exposed to blood from many congolese patients one of whom infected her 
she became unwell from then returned to denmark in with her colleagues baffled by her symptoms 
she died of pneumocystis pneumonia in december her tissues were examined and tested by her colleagues and found positive in 
spread to the western hemisphere hiv strains were once thought to have arrived in new york city from haiti around it spread from new york city to san francisco around hiv is believed to have arrived in haiti from central africa possibly from the democratic republic of the congo around the current consensus is that hiv was introduced to haiti by an unknown individual or individuals who contracted it while working in the democratic republic of the congo circa mini epidemic followed and circa yet another unknown individual took hiv from haiti to the united states 
the vast majority of cases of aids outside sub saharan africa can be traced back to that single patient 
later numerous unrelated incidents of aids among haitian immigrants to the were recorded in the early 
also as evidenced by the case of robert rayford isolated occurrences of this infection may have been emerging as early as the virus eventually entered gay male communities in large united states cities where combination of casual multi partner sexual activity with individuals reportedly averaging over unprotected sexual partners per year and relatively high transmission rates associated with anal intercourse allowed it to spread explosively enough to finally be noticed because of the long incubation period of hiv up to decade or longer before symptoms of aids appear and because of the initially low incidence hiv was not noticed at first 
by the time the first reported cases of aids were found in large united states cities the prevalence of hiv infection in some communities had passed 
worldwide hiv infection has spread from urban to rural areas and has appeared in regions such as china and india 
canadian flight attendant theory canadian airline steward named ga tan dugas was referred to as case and later patient with the alphabet letter standing for outside southern california in an early aids study by dr william darrow of the centers for disease control 
because of this many people had considered dugas to be responsible for taking hiv to north america 
however hiv reached new york city around while dugas did not start work at air canada until in randy shilts book and the band played on and the movie based on it dugas is referred to as aids patient zero instead of patient but neither the book nor the movie states that he had been the first to bring the virus to north america 
he was incorrectly called patient zero because at least of the people known to be infected by hiv in had had sex with him or with person who had sexual intercourse with dugas 
homeless people and intravenous drug users in new york volunteer social worker called betty williams quaker who worked with the homeless in new york from the seventies and early eighties onwards has talked about people at that time whose death would be labelled as junkie flu or the dwindles 
in an interview for the act up oral history project in she said of course the horror stories came mainly concerning women who were injection drug users who had pcp pneumonia pneumocystis pneumonia and were told that they just had bronchitis 
she continues actually believe that aids kind of existed among this group of people first because if you look back there was something called junkie pneumonia there was something called the dwindles that addicts got and think this was another early aids population way too helpless to ever do anything for themselves on their own behalf 
julia epstein writes in her book altered conditions disease medicine and storytelling that as we uncover more of the early history of hiv infection it becomes clear that by at least the the virus was already making major inroads into the immune systems of number of diverse populations in the united states the retrospectively diagnosed epidemic of junkie pneumonia in new york city in the late for example and had for some time been causing devastation in several countries in africa 
anecdotal evidence suggests that so called junkie pneumonia first began to afflict heroin addicts in new york in in her book engendering aids deconstructing sex text and epidemic tamsin wilton writes people had been sickening and dying of mysterious conditions since the early conditions that we can retrospectively diagnose as aids related 
there was for example phenomenon known as junkie pneumonia which spread among some populations of injecting street drug users in the and which is now believed to have been caused by hiv infection 
melinda cooper writes in her book family values between neoliberalism and the new social conservatism it is plausible that these cases of aids did not come to light in the for the same reason that junkie pneumonia was not recognized as the sign of an emerging infectious disease the people in question had such precarious access to health care that news of their death was never communicated to public health authorities 
an article by pattrice maurer in the newspaper agenda from april explores some of the issues surrounding junkie pneumonia 
it starts in the late while the epidemic known as disco fever swept through the an epidemic known as junkie pneumonia raged among injection drug users in new york city 
it continues few people were aware that large numbers of injections drug users were inexplicably dying of pneumonia 
those few who did notice these deaths did not feel compelled to investigate the public health puzzle they posed 
the author opinion is that if anyone had bothered to investigate these deaths they would have found an immune system disorder that is now called aids steven thrasher writes in the guardian indeed those of us who study aids have long known that long before common symptoms such as kaposi sarcoma and pneumonia were showing up among hemophiliacs and gay men they were likely affecting homeless people who lived off society radar people who used iv intravenous drugs and those who avoided medical treatment out of fear 
chapter in the proceedings of the world conference of therapeutic communities th san francisco california september gives details about serum samples that were tested for signs of hiv then called htlv iii lav antibodies 
quoting we have also conducted historical studies of the epidemic in new york city using serum samples that were originally collected for other purposes 
we have sera from iv drug users that go back to the middle 
the first indication of htlv iii lav antibody presence is in one of eleven samples from of samples in of samples from and of samples from the htlv iii lav virus appears to have been introduced among iv drug users in the late in new york city 
anna thompson writes on the website thebody com in an article dated autumn many women were dying in the late of pneumonia cervical cancer and other illnesses complicated by mysteriously suppressed immune systems 
yet it was not until that case of aids in woman was first reported by the centers for disease control cdc 
she continues the cdc refusal to address women issues led to the overall perception that women do not get aids 
in an article published in aids cultural analysis cultural activism author douglas crimp draws attention to anecdotal evidence about junkie pneumonia 
quoting even these statistics are based on cdc epidemiology that continues to see the beginning of the epidemic as in spite of widespread anecdotal reporting of high rate of deaths throughout the from what was known as junkie pneumonia and was likely pneumocystis pneumonia 
the statistics crimp writes about were taken from new york times article from october about nyc department of health study that showed that of aids sufferers were people who injected drugs more than percent higher than previously reported 
quoting city health officials estimated that half of the city intravenous drug users were infected with the virus that causes aids the study hiv infection among intravenous drug users in manhattan new york city from through published in february seeks to understand long term trends in the spread of hiv among intravenous drug users idus 
aids surveillance data and studies which detail the number of persons who tested hiv positive in manhattan are used to compile information deemed critical to realising the extent of the aids epidemic 
it starts by stating that up to september idu was the risk behaviour in or of the first cases of aids in the us 
cases among idus in new york city in the same period numbered approximately third of national idu cases 
the study continues to outline the methodology used in the compilation of data 
it says that while truly representative samples of idus within community are probably impossible to obtain samples of idus entering treatment provide good source for monitoring trends 
in the results section it states quoting the first evidence for hiv infection among iv drug users in new york is from three cases of aids in children born in these cases were later reported to the new york city department of health aids surveillance unit 
these children did not receive any known transfusions prior to developing aids and were born to mothers known to be iv drug users 
it continues to outline that the earliest known case of aids in an adult idu occurred in mixed risk and that known cases among idus increased rapidly from the cases in mixed risk to cases in to cases in and to cases in statistics on the incidence of positive tests for hiv mainly using archived samples are out of in out of in out of in out of between out of and out of in out of in and out of in in the comments section it states the three cases in of apparent perinatal transmission mother to child from iv drug using women strongly suggest that the introduction of hiv into the iv drug use group occurred around or or perhaps even earlier 
it says that without extensive samples from this period it is not possible to be certain about the spread of hiv among idus but the samples from idus with chronic liver disease suggest that the rates of infection were below for the first or years after its introduction hiv is thought to have entered the population of people using intravenous drugs in new york city in approximately in spring the government of new york city underwent fiscal crisis which led to the closing of many social services with people who used intravenous drugs living in hostile sociopolitical and legal environment 
this fiscal crisis led to many agencies with health responsibilities being particularly hard hit which in turn might have led to an increase in hiv aids and tuberculosis tb 
quoting from american journal of public health study between and the department of health doh budget in ny was cut by and by the department had lost staff members of its workforce 
to achieve these reductions the department closed of district health centers cut million from its methadone program terminated the employment of of health educators and closed of child health stations and of chest clinics the units responsible for tb screening and diagnosis 
study published in the journal of the american medical association in linked tb and hiv aids severe and unusual presentation of overwhelming tuberculosis in appropriate clinical circumstances may be considered an infection predictive of the presence of aids 
further study from stated there was link between the rise in tb aids and drug users within the united states aids thus compounds the risk of acquiring tuberculosis and in the united states most patients with aids and tuberculosis have been drug users 
newsletter from spring by the national coalition of gay std services featured an article titled tuberculosis and aids connecticut that suggested an association between tb and aids within that state 
from grid to aids the aids epidemic officially began on june when the centers for disease control and prevention in its morbidity and mortality weekly report newsletter reported unusual clusters of pneumocystis pneumonia pcp caused by form of pneumocystis carinii now recognized as distinct species pneumocystis jirovecii in five homosexual men in los angeles 
over the next months more pcp clusters were discovered among otherwise healthy men in cities throughout the country along with other opportunistic diseases such as kaposi sarcoma and persistent generalized lymphadenopathy common in immunosuppressed patients 
in june report of group of cases amongst gay men in southern california suggested that sexually transmitted infectious agent might be the etiological agent 
the syndrome was initially termed grid or gay related immune deficiency other less common gay specific terms included gay compromise syndrome gay lymph node syndrome gay cancer gay plague homosexual syndrome community acquired immunodeficiency caid and acquired community immunodeficiency syndrome acids 
health authorities soon realized however that nearly half of the people identified with the syndrome were not homosexual men 
the same opportunistic infections were also reported among hemophiliacs users of intravenous drugs such as heroin and haitian immigrants leading some researchers to call it the disease 
by august the disease was being referred to by its new cdc coined name acquired immune deficiency syndrome aids 
activism by aids patients and families in new york city nathan fain larry kramer larry mass paul popham paul rapoport and edmund white officially established the gay men health crisis gmhc in also in michael callen and richard berkowitz published how to have sex in an epidemic one approach 
in this short work they described ways gay men could be sexual and affectionate while dramatically reducing the risk of contracting or spreading hiv 
both authors were themselves gay men living with aids 
this booklet was one of the first times men were advised to use condoms when having sexual relations with other men at the beginning of the aids epidemic in the there was very little information about the disease 
because aids affected stigmatized groups such as lgbtq people people of low socioeconomic status sex workers and addicts there was also initially little mass media coverage when the epidemic started 
however with the rise of activist groups composed of people suffering from aids either directly or through loved one more public attention was brought to the epidemic 
identification of the virus may lav in may team of doctors at the pasteur institute in france including fran oise barr sinoussi and luc montagnier reported that they had isolated new retrovirus from lymphoid ganglions that they believed was the cause of aids 
the virus was later named lymphadenopathy associated virus lav and sample was sent to the centers for disease control which was later passed to the national cancer institute nci 
may htlv iii in may team led by robert gallo of the united states confirmed the discovery of the virus but they renamed it human lymphotropic virus type iii htlv iii 
august arv dr jay levy group at the university of california san francisco also played role in the discovery of hiv 
he independently isolated the aids virus in and named it the aids associated retrovirus arv publishing his findings in the journal science in 
january both found to be the same in january number of more detailed reports were published concerning lav and htlv iii and by march it was clear that the viruses were the same indeed it was later determined that the virus isolated by the gallo lab was from the lymph nodes of the patient studied in the original report by montagnier and was the etiological agent of aids 
may the name hiv in may the international committee on taxonomy of viruses ruled that both names should be dropped and new name hiv human immunodeficiency virus be used 
nobel whether barr sinoussi and montagnier deserve more credit than gallo for the discovery of the virus that causes aids has been matter of considerable controversy 
barr sinoussi and montagnier were awarded the nobel prize in physiology or medicine for their discovery of human immunodeficiency virus and harald zur hausen also shared the prize for his discovery that human papilloma virus leads to cervical cancer but gallo was left out 
gallo said that it was disappointment that he was not named co recipient 
montagnier said he was surprised gallo was not recognized by the nobel committee it was important to prove that hiv was the cause of aids and gallo had very important role in that 
very sorry for robert gallo 
dr levy contribution to the discovery of hiv was also cited in the nobel prize ceremony 
case definition for epidemiological surveillance since june many definitions have been developed for epidemiological surveillance such as the bangui definition and the expanded world health organization aids case definition 
genetic studies according to study published in the proceedings of the national academy of sciences in team led by robert shafer at stanford university school of medicine discovered that the gray mouse lemur has an endogenous lentivirus the genus to which hiv belongs in its genetic makeup 
this suggests that lentiviruses have existed for at least million years much longer than the currently known existence of hiv 
in addition the time frame falls in the period when madagascar was still connected to what is now the african continent the said lemurs later developed immunity to the virus strain and survived an era when the lentivirus was widespread among other mammals 
the study was hailed as crucial as it fills the blanks in the origin of the virus as well as in its evolution and could be important in the development of new antiviral drugs in researchers reported that siv had infected monkeys in bioko for at least years 
previous to this time it was thought that siv infection in monkeys had happened over the past few hundred years 
scientists estimated that it would take similar amount of time before humans adapted naturally to hiv infection in the way monkeys in africa have adapted to siv and not suffer any harm from the infection czech study of the genome of malayan flying lemurs an order of mammals parallel to primates and sharing an immediate common ancestor with them found endogenous lentiviruses that emerged an estimated million years ago based on rates of viral mutation versus modern lentiviruses 
debunked hiv aids conspiracy theories aids denialism aids denialists argue that aids does not exist or that aids is not caused by hiv some of its proponents believe that aids is caused by lifestyle including sexuality or drug use and not by hiv 
both forms of aids denialism contradict scientific consensus 
the evidence that hiv causes aids is generally considered conclusive among pathologists 
most arguments for denialism are based on misrepresentations of outdated data 
the belief that hiv was created by the us government as bioweapon an idea invented by soviet propaganda operation is held by disproportionately high number of africans and african americans 
influence on bolsonaro conspiracy theorists influence reached peak in with brazilian president jair bolsonaro claiming that covid vaccines can lead to aids 
the supreme federal court of brazil ordered an investigation into bolsonaro for falsely claiming that covid vaccines could increase the risk of contracting aids 
see also timeline of hiv aids references further reading shilts randy 
and the band played on politics people and the aids epidemic 
new york st martin press 
isbn oclc brier jennier 
infectious ideas political responses to the aids crisis 
chapel hill university of north carolina press
in mathematics the linear span also called the linear hull or just span of set of vectors from vector space denoted span is defined as the set of all linear combinations of the vectors in it can be characterized either as the intersection of all linear subspaces that contain or as the smallest subspace containing the linear span of set of vectors is therefore vector space itself
spans can be generalized to matroids and modules
to express that vector space is linear span of subset one commonly uses the following phrases either spans is spanning set of is spanned generated by or is generator or generator set of definition given vector space over field the span of set of vectors not necessarily infinite is defined to be the intersection of all subspaces of that contain is referred to as the subspace spanned by or by the vectors in conversely is called spanning set of and we say that spans alternatively the span of may be defined as the set of all finite linear combinations of elements vectors of which follows from the above definition
in the case of infinite infinite linear combinations
where combination may involve an infinite sum assuming that such sums are defined somehow as in say banach space are excluded by the definition generalization that allows these is not equivalent
examples the real vector space has as spanning set
this particular spanning set is also basis
if were replaced by it would also form the canonical basis of another spanning set for the same space is given by but this set is not basis because it is linearly dependent
the set is not spanning set of since its span is the space of all vectors in whose last component is zero
that space is also spanned by the set as is linear combination of and
it does however span
when interpreted as subset of
the empty set is spanning set of since the empty set is subset of all possible vector spaces in and is the intersection of all of these vector spaces
the set of functions xn where is non negative integer spans the space of polynomials
theorems equivalence of definitions the set of all linear combinations of subset of vector space over is the smallest linear subspace of containing proof
we first prove that span is subspace of since is subset of we only need to prove the existence of zero vector in span that span is closed under addition and that span is closed under scalar multiplication
letting it is trivial that the zero vector of exists in span since adding together two linear combinations of also produces linear combination of where all and multiplying linear combination of by scalar will produce another linear combination of thus is subspace of suppose that is linear subspace of containing it follows that span since every vi is linear combination of trivially
since is closed under addition and scalar multiplication then every linear combination must be contained in thus span is contained in every subspace of containing and the intersection of all such subspaces or the smallest such subspace is equal to the set of all linear combinations of size of spanning set is at least size of linearly independent set every spanning set of vector space must contain at least as many elements as any linearly independent set of vectors from proof
let be spanning set and be linearly independent set of vectors from we want to show that since spans then must also span and must be linear combination of thus is linearly dependent and we can remove one vector from that is linear combination of the other elements
this vector cannot be any of the wi since is linearly indepedent
the resulting set is which is spanning set of we repeat this step times where the resulting set after the pth step is the union of and vectors of it is ensured until the nth step that there will always be some to remove out of for every adjoint of and thus there are at least as many vi as there are wi
to verify this we assume by way of contradiction that then at the mth step we have the set and we can adjoin another vector but since is spanning set of is linear combination of
this is contradiction since is linearly independent
spanning set can be reduced to basis let be finite dimensional vector space
any set of vectors that spans can be reduced to basis for by discarding vectors if necessary
if there are linearly dependent vectors in the set
if the axiom of choice holds this is true without the assumption that has finite dimension
this also indicates that basis is minimal spanning set when is finite dimensional
generalizations generalizing the definition of the span of points in space subset of the ground set of matroid is called spanning set if the rank of equals the rank of the entire ground set
the vector space definition can also be generalized to modules
given an module and collection of elements an of the submodule of spanned by an is the sum of cyclic modules consisting of all linear combinations of the elements ai
as with the case of vector spaces the submodule of spanned by any subset of is the intersection of all submodules containing that subset
closed linear span functional analysis in functional analysis closed linear span of set of vectors is the minimal closed set which contains the linear span of that set
suppose that is normed vector space and let be any non empty subset of the closed linear span of denoted by sp or span is the intersection of all the closed linear subspaces of which contain one mathematical formulation of this is sp sp
the closed linear span of the set of functions xn on the interval where is non negative integer depends on the norm used
if the norm is used then the closed linear span is the hilbert space of square integrable functions on the interval
but if the maximum norm is used the closed linear span will be the space of continuous functions on the interval
in either case the closed linear span contains functions that are not polynomials and so are not in the linear span itself
however the cardinality of the set of functions in the closed linear span is the cardinality of the continuum which is the same cardinality as for the set of polynomials
notes the linear span of set is dense in the closed linear span
moreover as stated in the lemma below the closed linear span is indeed the closure of the linear span
closed linear spans are important when dealing with closed linear subspaces which are themselves highly important see riesz lemma
useful lemma let be normed space and let be any non empty subset of then so the usual way to find the closed linear span is to find the linear span first and then the closure of that linear span
see also affine hull conical combination convex hull citations sources textbooks axler sheldon jay
linear algebra done right rd ed
linear algebra th ed
isbn lane saunders mac birkhoff garrett
advanced linear algebra nd ed
isbn rynne brian youngson martin
isbn lay david linear algebra and its applications th edition
web lankham isaiah nachtergaele bruno schilling anne february
linear algebra as an introduction to abstract mathematics pdf
university of california davis
retrieved september weisstein eric wolfgang
retrieved feb cs maint url status link linear hull
april retrieved feb cs maint url status link external links linear combinations and span understanding linear combinations and spans of vectors khanacademy org
linear combinations span and basis vectors
essence of linear algebra
archived from the original on via youtube
in mathematics normed vector space or normed space is vector space over the real or complex numbers on which norm is defined 
norm is the formalization and the generalization to real vector spaces of the intuitive notion of length in the real physical world 
norm is real valued function defined on the vector space that is commonly denoted and has the following properties it is nonnegative meaning that for every vector it is positive on nonzero vectors that is for every vector and every scalar the triangle inequality holds that is for every vectors and norm induces distance called its norm induced metric by the formula which makes any normed vector space into metric space and topological vector space 
if this metric is complete then the normed space is banach space 
every normed vector space can be uniquely extended to banach space which makes normed spaces intimately related to banach spaces 
every banach space is normed space but converse is not true 
for example the set of the finite sequences of real numbers can be normed with the euclidean norm but it is not complete for this norm 
an inner product space is normed vector space whose norm is the square root of the inner product of vector and itself 
the euclidean norm of euclidean vector space is special case that allows defining euclidean distance by the formula the study of normed spaces and banach spaces is fundamental part of functional analysis which is major subfield of mathematics 
definition normed vector space is vector space equipped with norm 
seminormed vector space is vector space equipped with seminorm 
useful variation of the triangle inequality is for any vectors and this also shows that vector norm is uniformly continuous function 
property depends on choice of norm on the field of scalars 
when the scalar field is or more generally subset of this is usually taken to be the ordinary absolute value but other choices are possible 
for example for vector space over one could take to be the adic absolute value 
topological structure if is normed vector space the norm induces metric notion of distance and therefore topology on this metric is defined in the natural way the distance between two vectors and is given by 
this topology is precisely the weakest topology which makes continuous and which is compatible with the linear structure of in the following sense the vector addition is jointly continuous with respect to this topology 
this follows directly from the triangle inequality 
the scalar multiplication where is the underlying scalar field of is jointly continuous 
this follows from the triangle inequality and homogeneity of the norm similarly for any seminormed vector space we can define the distance between two vectors and as 
this turns the seminormed space into pseudometric space notice this is weaker than metric and allows the definition of notions such as continuity and convergence 
to put it more abstractly every seminormed vector space is topological vector space and thus carries topological structure which is induced by the semi norm 
of special interest are complete normed spaces which are known as banach spaces 
every normed vector space sits as dense subspace inside some banach space this banach space is essentially uniquely defined by and is called the completion of two norms on the same vector space are called equivalent if they define the same topology 
on finite dimensional vector space all norms are equivalent but this is not true for infinite dimensional vector spaces 
all norms on finite dimensional vector space are equivalent from topological viewpoint as they induce the same topology although the resulting metric spaces need not be the same 
and since any euclidean space is complete we can thus conclude that all finite dimensional normed vector spaces are banach spaces 
normed vector space is locally compact if and only if the unit ball is compact which is the case if and only if is finite dimensional this is consequence of riesz lemma 
in fact more general result is true topological vector space is locally compact if and only if it is finite dimensional 
the point here is that we don assume the topology comes from norm 
the topology of seminormed vector space has many nice properties 
given neighbourhood system around we can construct all other neighbourhood systems as with moreover there exists neighbourhood basis for the origin consisting of absorbing and convex sets 
as this property is very useful in functional analysis generalizations of normed vector spaces with this property are studied under the name locally convex spaces 
norm or seminorm on topological vector space is continuous if and only if the topology that induces on is coarser than meaning which happens if and only if there exists some open ball in such as maybe for example that is open in said different such that 
normable spaces topological vector space is called normable if there exists norm on such that the canonical metric induces the topology on the following theorem is due to kolmogorov kolmogorov normability criterion hausdorff topological vector space is normable if and only if there exists convex von neumann bounded neighborhood of product of family of normable spaces is normable if and only if only finitely many of the spaces are non trivial that is 
furthermore the quotient of normable space by closed vector subspace is normable and if in addition topology is given by norm then the map given by inf is well defined norm on that induces the quotient topology on if is hausdorff locally convex topological vector space then the following are equivalent is normable 
has bounded neighborhood of the origin 
the strong dual space of is normable 
the strong dual space of is metrizable furthermore is finite dimensional if and only if is normable here denotes endowed with the weak topology 
the topology of the fr chet space as defined in the article on spaces of test functions and distributions is defined by countable family of norms but it is not normable space because there does not exist any norm on such that the topology that this norm induces is equal to even if metrizable topological vector space has topology that is defined by family of norms then it may nevertheless still fail to be normable space meaning that its topology can not be defined by any single norm 
an example of such space is the fr chet space whose definition can be found in the article on spaces of test functions and distributions because its topology is defined by countable family of norms but it is not normable space because there does not exist any norm on such that the topology this norm induces is equal to in fact the topology of locally convex space can be defined by family of norms on if and only if there exists at least one continuous norm on 
linear maps and dual spaces the most important maps between two normed vector spaces are the continuous linear maps 
together with these maps normed vector spaces form category 
the norm is continuous function on its vector space 
all linear maps between finite dimensional vector spaces are also continuous 
an isometry between two normed vector spaces is linear map which preserves the norm meaning for all vectors 
isometries are always continuous and injective 
surjective isometry between the normed vector spaces and is called an isometric isomorphism and and are called isometrically isomorphic 
isometrically isomorphic normed vector spaces are identical for all practical purposes 
when speaking of normed vector spaces we augment the notion of dual space to take the norm into account 
the dual of normed vector space is the space of all continuous linear maps from to the base field the complexes or the reals such linear maps are called functionals 
the norm of functional is defined as the supremum of where ranges over all unit vectors that is vectors of norm in this turns into normed vector space 
an important theorem about continuous linear functionals on normed vector spaces is the hahn banach theorem 
normed spaces as quotient spaces of seminormed spaces the definition of many normed spaces in particular banach spaces involves seminorm defined on vector space and then the normed space is defined as the quotient space by the subspace of elements of seminorm zero 
for instance with the spaces the function defined by is seminorm on the vector space of all functions on which the lebesgue integral on the right hand side is defined and finite 
however the seminorm is equal to zero for any function supported on set of lebesgue measure zero 
these functions form subspace which we quotient out making them equivalent to the zero function 
finite product spaces given seminormed spaces with seminorms denote the product space by where vector addition defined as and scalar multiplication defined as define new function by which is seminorm on the function is norm if and only if all are norms 
more generally for each real the map defined by is semi norm 
for each this defines the same topological space 
straightforward argument involving elementary linear algebra shows that the only finite dimensional seminormed spaces are those arising as the product space of normed space and space with trivial seminorm 
consequently many of the more interesting examples and applications of seminormed spaces occur for infinite dimensional vector spaces 
see also banach space normed vector spaces which are complete with respect to the metric induced by the norm banach mazur compactum set of dimensional subspaces of normed space made into compact metric space 
finsler manifold where the length of each tangent vector is determined by norm inner product space normed vector spaces where the norm is given by an inner product kolmogorov normability criterion characterization of normable spaces locally convex topological vector space vector space with topology defined by convex open sets space mathematics mathematical set with some added structure topological vector space vector space with notion of nearness references bibliography rudin walter 
international series in pure and applied mathematics 
new york ny mcgraw hill science engineering math 
isbn oclc banach stefan 
th orie des op rations lin aires theory of linear operations pdf 
monografie matematyczne in french 
warszawa subwencji funduszu kultury narodowej 
zbl archived from the original pdf on retrieved rolewicz stefan functional analysis and control theory linear systems mathematics and its applications east european series vol 
translated from the polish by ewa bednarczuk ed 
dordrecht warsaw reidel publishing co pwn polish scientific publishers pp 
xvi doi isbn mr oclc schaefer 
new york ny springer new york imprint springer 
isbn oclc tr ves fran ois 
topological vector spaces distributions and kernels 
external links media related to normed spaces at wikimedia commons
digital signature is mathematical scheme for verifying the authenticity of digital messages or documents
valid digital signature where the prerequisites are satisfied gives recipient very high confidence that the message was created by known sender authenticity and that the message was not altered in transit integrity digital signatures are standard element of most cryptographic protocol suites and are commonly used for software distribution financial transactions contract management software and in other cases where it is important to detect forgery or tampering
digital signatures are often used to implement electronic signatures which includes any electronic data that carries the intent of signature but not all electronic signatures use digital signatures
electronic signatures have legal significance in some countries including canada south africa the united states algeria turkey india brazil indonesia mexico saudi arabia uruguay switzerland chile and the countries of the european union digital signatures employ asymmetric cryptography
in many instances they provide layer of validation and security to messages sent through non secure channel properly implemented digital signature gives the receiver reason to believe the message was sent by the claimed sender
digital signatures are equivalent to traditional handwritten signatures in many respects but properly implemented digital signatures are more difficult to forge than the handwritten type
digital signature schemes in the sense used here are cryptographically based and must be implemented properly to be effective
they can also provide non repudiation meaning that the signer cannot successfully claim they did not sign message while also claiming their private key remains secret
further some non repudiation schemes offer timestamp for the digital signature so that even if the private key is exposed the signature is valid
digitally signed messages may be anything representable as bitstring examples include electronic mail contracts or message sent via some other cryptographic protocol
definition digital signature scheme typically consists of three algorithms key generation algorithm that selects private key uniformly at random from set of possible private keys
the algorithm outputs the private key and corresponding public key
signing algorithm that given message and private key produces signature
signature verifying algorithm that given the message public key and signature either accepts or rejects the message claim to authenticity two main properties are required
first the authenticity of signature generated from fixed message and fixed private key can be verified by using the corresponding public key
secondly it should be computationally infeasible to generate valid signature for party without knowing that party private key
digital signature is an authentication mechanism that enables the creator of the message to attach code that acts as signature
the digital signature algorithm dsa developed by the national institute of standards and technology is one of many examples of signing algorithm
in the following discussion refers to unary number
formally digital signature scheme is triple of probabilistic polynomial time algorithms satisfying key generator generates public key pk and corresponding private key sk on input where is the security parameter
signing returns tag on the inputs the private key sk and string
verifying outputs accepted or rejected on the inputs the public key pk string and tag for correctness and must satisfy pr pk sk pk sk accepted digital signature scheme is secure if for every non uniform probabilistic polynomial time adversary pr pk sk as sk pk pk accepted negl where as sk denotes that has access to the oracle sk denotes the set of the queries on made by which knows the public key pk and the security parameter and denotes that the adversary may not directly query the string on history in whitfield diffie and martin hellman first described the notion of digital signature scheme although they only conjectured that such schemes existed based on functions that are trapdoor one way permutations
soon afterwards ronald rivest adi shamir and len adleman invented the rsa algorithm which could be used to produce primitive digital signatures although only as proof of concept plain rsa signatures are not secure
the first widely marketed software package to offer digital signature was lotus notes released in which used the rsa algorithm other digital signature schemes were soon developed after rsa the earliest being lamport signatures merkle signatures also known as merkle trees or simply hash trees and rabin signatures in shafi goldwasser silvio micali and ronald rivest became the first to rigorously define the security requirements of digital signature schemes
they described hierarchy of attack models for signature schemes and also presented the gmr signature scheme the first that could be proved to prevent even an existential forgery against chosen message attack which is the currently accepted security definition for signature schemes
the first such scheme which is not built on trapdoor functions but rather on family of function with much weaker required property of one way permutation was presented by moni naor and moti yung
method one digital signature scheme of many is based on rsa
to create signature keys generate an rsa key pair containing modulus that is the product of two random secret distinct large primes along with integers and such that mod where is euler totient function
the signer public key consists of and and the signer secret key contains to sign message the signer computes signature such that md mod where md is modular exponentiation operation
to verify the receiver checks that mod
several early signature schemes were of similar type they involve the use of trapdoor permutation such as the rsa function or in the case of the rabin signature scheme computing square modulo composite trapdoor permutation family is family of permutations specified by parameter that is easy to compute in the forward direction but is difficult to compute in the reverse direction without already knowing the private key trapdoor
trapdoor permutations can be used for digital signature schemes where computing the reverse direction with the secret key is required for signing and computing the forward direction is used to verify signatures
used directly this type of signature scheme is vulnerable to key only existential forgery attack
to create forgery the attacker picks random signature and uses the verification procedure to determine the message corresponding to that signature
in practice however this type of signature is not used directly but rather the message to be signed is first hashed to produce short digest that is then padded to larger width comparable to then signed with the reverse trapdoor function
this forgery attack then only produces the padded hash function output that corresponds to but not message that leads to that value which does not lead to an attack
in the random oracle model hash then sign an idealized version of that practice where hash and padding combined have close to possible outputs this form of signature is existentially unforgeable even against chosen plaintext attack there are several reasons to sign such hash or message digest instead of the whole document
for efficiency the signature will be much shorter and thus save time since hashing is generally much faster than signing in practice
for compatibility messages are typically bit strings but some signature schemes operate on other domains such as in the case of rsa numbers modulo composite number
hash function can be used to convert an arbitrary input into the proper format
for integrity without the hash function the text to be signed may have to be split separated in blocks small enough for the signature scheme to act on them directly
however the receiver of the signed blocks is not able to recognize if all the blocks are present and in the appropriate order
applications as organizations move away from paper documents with ink signatures or authenticity stamps digital signatures can provide added assurances of the evidence to provenance identity and status of an electronic document as well as acknowledging informed consent and approval by signatory
the united states government printing office gpo publishes electronic versions of the budget public and private laws and congressional bills with digital signatures
universities including penn state university of chicago and stanford are publishing electronic student transcripts with digital signatures
below are some common reasons for applying digital signature to communications authentication although messages may often include information about the entity sending message that information may not be accurate
digital signatures can be used to authenticate the identity of the source messages
when ownership of digital signature secret key is bound to specific user valid signature shows that the message was sent by that user
the importance of high confidence in sender authenticity is especially obvious in financial context
for example suppose bank branch office sends instructions to the central office requesting change in the balance of an account
if the central office is not convinced that such message is truly sent from an authorized source acting on such request could be grave mistake
integrity in many scenarios the sender and receiver of message may have need for confidence that the message has not been altered during transmission
although encryption hides the contents of message it may be possible to change an encrypted message without understanding it
some encryption algorithms called nonmalleable prevent this but others do not
however if message is digitally signed any change in the message after signature invalidates the signature
furthermore there is no efficient way to modify message and its signature to produce new message with valid signature because this is still considered to be computationally infeasible by most cryptographic hash functions see collision resistance
non repudiation non repudiation or more specifically non repudiation of origin is an important aspect of digital signatures
by this property an entity that has signed some information cannot at later time deny having signed it
similarly access to the public key only does not enable fraudulent party to fake valid signature
note that these authentication non repudiation etc
properties rely on the secret key not having been revoked prior to its usage
public revocation of key pair is required ability else leaked secret keys would continue to implicate the claimed owner of the key pair
checking revocation status requires an online check checking certificate revocation list or via the online certificate status protocol
very roughly this is analogous to vendor who receives credit cards first checking online with the credit card issuer to find if given card has been reported lost or stolen
of course with stolen key pairs the theft is often discovered only after the secret key use to sign bogus certificate for espionage purpose
notions of security in their foundational paper goldwasser micali and rivest lay out hierarchy of attack models against digital signatures in key only attack the attacker is only given the public verification key
in known message attack the attacker is given valid signatures for variety of messages known by the attacker but not chosen by the attacker
in an adaptive chosen message attack the attacker first learns signatures on arbitrary messages of the attacker choice they also describe hierarchy of attack results total break results in the recovery of the signing key
universal forgery attack results in the ability to forge signatures for any message
selective forgery attack results in signature on message of the adversary choice
an existential forgery merely results in some valid message signature pair not already known to the adversary the strongest notion of security therefore is security against existential forgery under an adaptive chosen message attack
additional security precautions putting the private key on smart card all public key private key cryptosystems depend entirely on keeping the private key secret
private key can be stored on user computer and protected by local password but this has two disadvantages the user can only sign documents on that particular computer the security of the private key depends entirely on the security of the computera more secure alternative is to store the private key on smart card
many smart cards are designed to be tamper resistant although some designs have been broken notably by ross anderson and his students
in typical digital signature implementation the hash calculated from the document is sent to the smart card whose cpu signs the hash using the stored private key of the user and then returns the signed hash
typically user must activate their smart card by entering personal identification number or pin code thus providing two factor authentication
it can be arranged that the private key never leaves the smart card although this is not always implemented
if the smart card is stolen the thief will still need the pin code to generate digital signature
this reduces the security of the scheme to that of the pin system although it still requires an attacker to possess the card
mitigating factor is that private keys if generated and stored on smart cards are usually regarded as difficult to copy and are assumed to exist in exactly one copy
thus the loss of the smart card may be detected by the owner and the corresponding certificate can be immediately revoked
private keys that are protected by software only may be easier to copy and such compromises are far more difficult to detect
using smart card readers with separate keyboard entering pin code to activate the smart card commonly requires numeric keypad
some card readers have their own numeric keypad
this is safer than using card reader integrated into pc and then entering the pin using that computer keyboard
readers with numeric keypad are meant to circumvent the eavesdropping threat where the computer might be running keystroke logger potentially compromising the pin code
specialized card readers are also less vulnerable to tampering with their software or hardware and are often eal certified
other smart card designs smart card design is an active field and there are smart card schemes which are intended to avoid these particular problems despite having few security proofs so far
using digital signatures only with trusted applications one of the main differences between digital signature and written signature is that the user does not see what they sign
the user application presents hash code to be signed by the digital signing algorithm using the private key
an attacker who gains control of the user pc can possibly replace the user application with foreign substitute in effect replacing the user own communications with those of the attacker
this could allow malicious application to trick user into signing any document by displaying the user original on screen but presenting the attacker own documents to the signing application
to protect against this scenario an authentication system can be set up between the user application word processor email client etc
and the signing application
the general idea is to provide some means for both the user application and signing application to verify each other integrity
for example the signing application may require all requests to come from digitally signed binaries
using network attached hardware security module one of the main differences between cloud based digital signature service and locally provided one is risk
many risk averse companies including governments financial and medical institutions and payment processors require more secure standards like fips level and fips certification to ensure the signature is validated and secure
wysiwys technically speaking digital signature applies to string of bits whereas humans and applications believe that they sign the semantic interpretation of those bits
in order to be semantically interpreted the bit string must be transformed into form that is meaningful for humans and applications and this is done through combination of hardware and software based processes on computer system
the problem is that the semantic interpretation of bits can change as function of the processes used to transform the bits into semantic content
it is relatively easy to change the interpretation of digital document by implementing changes on the computer system where the document is being processed
from semantic perspective this creates uncertainty about what exactly has been signed
wysiwys what you see is what you sign means that the semantic interpretation of signed message cannot be changed
in particular this also means that message cannot contain hidden information that the signer is unaware of and that can be revealed after the signature has been applied
wysiwys is requirement for the validity of digital signatures but this requirement is difficult to guarantee because of the increasing complexity of modern computer systems
the term wysiwys was coined by peter landrock and torben pedersen to describe some of the principles in delivering secure and legally binding digital signatures for pan european projects
digital signatures versus ink on paper signatures an ink signature could be replicated from one document to another by copying the image manually or digitally but to have credible signature copies that can resist some scrutiny is significant manual or technical skill and to produce ink signature copies that resist professional scrutiny is very difficult
digital signatures cryptographically bind an electronic identity to an electronic document and the digital signature cannot be copied to another document
paper contracts sometimes have the ink signature block on the last page and the previous pages may be replaced after signature is applied
digital signatures can be applied to an entire document such that the digital signature on the last page will indicate tampering if any data on any of the pages have been altered but this can also be achieved by signing with ink and numbering all pages of the contract
some digital signature algorithms rsa dsa ecdsa eddsa rsa with sha ecdsa with sha elgamal signature scheme as the predecessor to dsa and variants schnorr signature and pointcheval stern signature algorithm rabin signature algorithm pairing based schemes such as bls ntrusign is an example of digital signature scheme based on hard lattice problems undeniable signatures aggregate signatureru signature scheme that supports aggregation given signatures on messages from users it is possible to aggregate all these signatures into single signature whose size is constant in the number of users
this single signature will convince the verifier that the users did indeed sign the original messages
scheme by mihir bellare and gregory neven may be used with bitcoin
signatures with efficient protocols are signature schemes that facilitate efficient cryptographic protocols such as zero knowledge proofs or secure computation
the current state of use legal and practical most digital signature schemes share the following goals regardless of cryptographic theory or legal provision quality algorithms some public key algorithms are known to be insecure as practical attacks against them having been discovered
quality implementations an implementation of good algorithm or protocol with mistake will not work
users and their software must carry out the signature protocol properly
the private key must remain private if the private key becomes known to any other party that party can produce perfect digital signatures of anything
the public key owner must be verifiable public key associated with bob actually came from bob
this is commonly done using public key infrastructure pki and the public key user association is attested by the operator of the pki called certificate authority
for open pkis in which anyone can request such an attestation universally embodied in cryptographically protected public key certificate the possibility of mistaken attestation is non trivial
commercial pki operators have suffered several publicly known problems
such mistakes could lead to falsely signed and thus wrongly attributed documents
closed pki systems are more expensive but less easily subverted in this way only if all of these conditions are met will digital signature actually be any evidence of who sent the message and therefore of their assent to its contents
legal enactment cannot change this reality of the existing engineering possibilities though some such have not reflected this actuality
legislatures being importuned by businesses expecting to profit from operating pki or by the technological avant garde advocating new solutions to old problems have enacted statutes and or regulations in many jurisdictions authorizing endorsing encouraging or permitting digital signatures and providing for or limiting their legal effect
the first appears to have been in utah in the united states followed closely by the states massachusetts and california
other countries have also passed statutes or issued regulations in this area as well and the un has had an active model law project for some time
these enactments or proposed enactments vary from place to place have typically embodied expectations at variance optimistically or pessimistically with the state of the underlying cryptographic engineering and have had the net effect of confusing potential users and specifiers nearly all of whom are not cryptographically knowledgeable
adoption of technical standards for digital signatures have lagged behind much of the legislation delaying more or less unified engineering position on interoperability algorithm choice key lengths and so on what the engineering is attempting to provide
industry standards some industries have established common interoperability standards for the use of digital signatures between members of the industry and with regulators
these include the automotive network exchange for the automobile industry and the safe biopharma association for the healthcare industry
using separate key pairs for signing and encryption in several countries digital signature has status somewhat like that of traditional pen and paper signature as in the eu digital signature directive and eu follow on legislation
generally these provisions mean that anything digitally signed legally binds the signer of the document to the terms therein
for that reason it is often thought best to use separate key pairs for encrypting and signing
using the encryption key pair person can engage in an encrypted conversation regarding real estate transaction but the encryption does not legally sign every message he or she sends
only when both parties come to an agreement do they sign contract with their signing keys and only then are they legally bound by the terms of specific document
after signing the document can be sent over the encrypted link
if signing key is lost or compromised it can be revoked to mitigate any future transactions
if an encryption key is lost backup or key escrow should be utilized to continue viewing encrypted content
signing keys should never be backed up or escrowed unless the backup destination is securely encrypted
see also cfr advanced electronic signature blind signature detached signature digital certificate digital signature in estonia electronic lab notebook electronic signature electronic signatures and law esign india gnu privacy guard public key infrastructure public key fingerprint server based signatures probabilistic signature scheme notes references goldreich oded foundations of cryptography basic tools cambridge cambridge university press isbn goldreich oded foundations of cryptography ii basic applications publ
press isbn pass rafael course in cryptography pdf retrieved december further reading katz and lindell introduction to modern cryptography chapman hall crc press lorna brazell electronic signatures and identities law and regulation nd edn london sweet maxwell dennis campbell editor commerce and the law of digital signatures oceana publications
schellenkens electronic signatures authentication technology from legal perspective tmc asser press
jeremiah buckley john kromer margo tank and david whitaker the law of electronic signatures rd edition west publishing
digital evidence and electronic signature law review free open source
in probability theory and statistics variance is the expectation of the squared deviation of random variable from its population mean or sample mean 
variance is measure of dispersion meaning it is measure of how far set of numbers is spread out from their average value 
variance has central role in statistics where some ideas that use it include descriptive statistics statistical inference hypothesis testing goodness of fit and monte carlo sampling 
variance is an important tool in the sciences where statistical analysis of data is common 
the variance is the square of the standard deviation the second central moment of distribution and the covariance of the random variable with itself and it is often represented by var or an advantage of variance as measure of dispersion is that it is more amenable to algebraic manipulation than other measures of dispersion such as the expected absolute deviation for example the variance of sum of uncorrelated random variables is equal to the sum of their variances 
disadvantage of the variance for practical applications is that unlike the standard deviation its units differ from the random variable which is why the standard deviation is more commonly reported as measure of dispersion once the calculation is finished 
there are two distinct concepts that are both called variance 
one as discussed above is part of theoretical probability distribution and is defined by an equation 
the other variance is characteristic of set of observations 
when variance is calculated from observations those observations are typically measured from real world system 
if all possible observations of the system are present then the calculated variance is called the population variance 
normally however only subset is available and the variance calculated from this is called the sample variance 
the variance calculated from sample is considered an estimate of the full population variance 
there are multiple ways to calculate an estimate of the population variance as discussed in the section below 
the two kinds of variance are closely related 
to see how consider that theoretical probability distribution can be used as generator of hypothetical observations 
if an infinite number of observations are generated using distribution then the sample variance calculated from that infinite set will match the value calculated using the distribution equation for variance 
etymology the term variance was first introduced by ronald fisher in his paper the correlation between relatives on the supposition of mendelian inheritance the great body of available statistics show us that the deviations of human measurement from its mean follow very closely the normal law of errors and therefore that the variability may be uniformly measured by the standard deviation corresponding to the square root of the mean square error 
when there are two independent causes of variability capable of producing in an otherwise uniform population distributions with standard deviations and it is found that the distribution when both causes act together has standard deviation it is therefore desirable in analysing the causes of variability to deal with the square of the standard deviation as the measure of variability 
we shall term this quantity the variance definition the variance of random variable is the expected value of the squared deviation from the mean of var 
this definition encompasses random variables that are generated by processes that are discrete continuous neither or mixed 
the variance can also be thought of as the covariance of random variable with itself var cov 
the variance is also equivalent to the second cumulant of probability distribution that generates the variance is typically designated as var or sometimes as or or symbolically as or simply pronounced sigma squared 
the expression for the variance can be expanded as follows var in other words the variance of is equal to the mean of the square of minus the square of the mean of this equation should not be used for computations using floating point arithmetic because it suffers from catastrophic cancellation if the two components of the equation are similar in magnitude 
for other numerically stable alternatives see algorithms for calculating variance 
discrete random variable if the generator of random variable is discrete with probability mass function then var where is the expected value 
when such discrete weighted variance is specified by weights whose sum is not then one divides by the sum of the weights 
the variance of collection of equally likely values can be written as var where is the average value 
that is the variance of set of equally likely values can be equivalently expressed without directly referring to the mean in terms of squared deviations of all pairwise squared distances of points from each other var 
absolutely continuous random variable if the random variable has probability density function and is the corresponding cumulative distribution function then var or equivalently var where is the expected value of given by 
in these formulas the integrals with respect to and are lebesgue and lebesgue stieltjes integrals respectively 
if the function is riemann integrable on every finite interval then var where the integral is an improper riemann integral 
examples exponential distribution the exponential distribution with parameter is continuous distribution whose probability density function is given by on the interval 
its mean can be shown to be using integration by parts and making use of the expected value already calculated we have thus the variance of is given by var 
fair die fair six sided die can be modeled as discrete random variable with outcomes through each with equal probability 
the expected value of is therefore the variance of is var the general formula for the variance of the outcome of an sided die is var 
commonly used probability distributions the following table lists the variance for some commonly used probability distributions 
properties basic properties variance is non negative because the squares are positive or zero var the variance of constant is zero 
var conversely if the variance of random variable is then it is almost surely constant 
that is it always has the same value var 
issues of finiteness if distribution does not have finite expected value as is the case for the cauchy distribution then the variance cannot be finite either 
however some distributions may not have finite variance despite their expected value being finite 
an example is pareto distribution whose index satisfies 
decomposition the general formula for variance decomposition or the law of total variance is if and are two random variables and the variance of exists then var var var 
the conditional expectation of given and the conditional variance var may be understood as follows 
given any particular value of the random variable there is conditional expectation given the event this quantity depends on the particular value it is function 
that same function evaluated at the random variable is the conditional expectation 
in particular if is discrete random variable assuming possible values with corresponding probabilities then in the formula for total variance the first term on the right hand side becomes var where var 
similarly the second term on the right hand side becomes var where and thus the total variance is given by var 
similar formula is applied in analysis of variance where the corresponding formula is total between within here refers to the mean of the squares 
in linear regression analysis the corresponding formula is total regression residual 
this can also be derived from the additivity of variances since the total observed score is the sum of the predicted score and the error score where the latter two are uncorrelated 
similar decompositions are possible for the sum of squared deviations sum of squares total between within total regression residual 
calculation from the cdf the population variance for non negative random variable can be expressed in terms of the cumulative distribution function using this expression can be used to calculate the variance in situations where the cdf but not the density can be conveniently expressed 
characteristic property the second moment of random variable attains the minimum value when taken around the first moment mean of the random variable 
conversely if continuous function satisfies for all random variables then it is necessarily of the form where this also holds in the multidimensional case 
units of measurement unlike the expected absolute deviation the variance of variable has units that are the square of the units of the variable itself 
for example variable measured in meters will have variance measured in meters squared 
for this reason describing data sets via their standard deviation or root mean square deviation is often preferred over using the variance 
in the dice example the standard deviation is slightly larger than the expected absolute deviation of the standard deviation and the expected absolute deviation can both be used as an indicator of the spread of distribution 
the standard deviation is more amenable to algebraic manipulation than the expected absolute deviation and together with variance and its generalization covariance is used frequently in theoretical statistics however the expected absolute deviation tends to be more robust as it is less sensitive to outliers arising from measurement anomalies or an unduly heavy tailed distribution 
propagation addition and multiplication by constant variance is invariant with respect to changes in location parameter 
that is if constant is added to all values of the variable the variance is unchanged var var 
if all values are scaled by constant the variance is scaled by the square of that constant var var 
the variance of sum of two random variables is given by var var var cov var var var cov where cov is the covariance 
linear combinations in general for the sum of random variables the variance becomes var cov var cov see also general bienaym identity 
these results lead to the variance of linear combination as var cov var cov var cov 
if the random variables are such that cov then they are said to be uncorrelated 
it follows immediately from the expression given earlier that if the random variables are uncorrelated then the variance of their sum is equal to the sum of their variances or expressed symbolically var var 
since independent random variables are always uncorrelated see covariance uncorrelatedness and independence the equation above holds in particular when the random variables are independent 
thus independence is sufficient but not necessary for the variance of the sum to equal the sum of the variances 
matrix notation for the variance of linear combination define as column vector of random variables and as column vector of scalars therefore is linear combination of these random variables where denotes the transpose of also let be the covariance matrix of the variance of is then given by var this implies that the variance of the mean can be written as with column vector of ones var var 
sum of variables sum of uncorrelated variables one reason for the use of the variance in preference to other measures of dispersion is that the variance of the sum or the difference of uncorrelated random variables is the sum of their variances var var 
this statement is called the bienaym formula and was discovered in it is often made with the stronger condition that the variables are independent but being uncorrelated suffices 
so if all the variables have the same variance then since division by is linear transformation this formula immediately implies that the variance of their mean is var var var that is the variance of the mean decreases when increases 
this formula for the variance of the mean is used in the definition of the standard error of the sample mean which is used in the central limit theorem 
to prove the initial statement it suffices to show that var var var 
the general result then follows by induction 
starting with the definition var using the linearity of the expectation operator and the assumption of independence or uncorrelatedness of and this further simplifies as follows var var var 
sum of correlated variables sum of correlated variables with fixed sample size in general the variance of the sum of variables is the sum of their covariances var cov var cov 
note the second equality comes from the fact that cov xi xi var xi 
here cov is the covariance which is zero for independent random variables if it exists 
the formula states that the variance of sum is equal to the sum of all elements in the covariance matrix of the components 
the next expression states equivalently that the variance of the sum is the sum of the diagonal of covariance matrix plus two times the sum of its upper triangular elements or its lower triangular elements this emphasizes that the covariance matrix is symmetric 
this formula is used in the theory of cronbach alpha in classical test theory 
so if the variables have equal variance and the average correlation of distinct variables is then the variance of their mean is var this implies that the variance of the mean increases with the average of the correlations 
in other words additional correlated observations are not as effective as additional independent observations at reducing the uncertainty of the mean 
moreover if the variables have unit variance for example if they are standardized then this simplifies to var this formula is used in the spearman brown prediction formula of classical test theory 
this converges to if goes to infinity provided that the average correlation remains constant or converges too 
so for the variance of the mean of standardized variables with equal correlations or converging average correlation we have lim var therefore the variance of the mean of large number of standardized variables is approximately equal to their average correlation 
this makes clear that the sample mean of correlated variables does not generally converge to the population mean even though the law of large numbers states that the sample mean will converge for independent variables 
sum of uncorrelated variables with random sample size there are cases when sample is taken without knowing in advance how many observations will be acceptable according to some criterion 
in such cases the sample size is random variable whose variation adds to the variation of such that var var var which follows from the law of total variance 
if has poisson distribution then var with estimator so the estimator of var becomes giving se weighted sum of variables the scaling property and the bienaym formula along with the property of the covariance cov ax by ab cov jointly imply that var var var cov 
this implies that in weighted sum of variables the variable with the largest weight will have disproportionally large weight in the variance of the total 
for example if and are uncorrelated and the weight of is two times the weight of then the weight of the variance of will be four times the weight of the variance of the expression above can be extended to weighted sum of multiple variables var var cov product of variables product of independent variables if two variables and are independent the variance of their product is given by var var var var var 
equivalently using the basic properties of expectation it is given by var 
product of statistically dependent variables in general if two variables are statistically dependent then the variance of their product is given by var cov cov var var cov arbitrary functions the delta method uses second order taylor expansions to approximate the variance of function of one or more random variables see taylor expansions for the moments of functions of random variables 
for example the approximate variance of function of one variable is given by var var provided that is twice differentiable and that the mean and variance of are finite 
population variance and sample variance real world observations such as the measurements of yesterday rain throughout the day typically cannot be complete sets of all possible observations that could be made 
as such the variance calculated from the finite set will in general not match the variance that would have been calculated from the full population of possible observations 
this means that one estimates the mean and variance from limited set of observations by using an estimator equation 
the estimator is function of the sample of observations drawn without observational bias from the whole population of potential observations 
in this example that sample would be the set of actual measurements of yesterday rainfall from available rain gauges within the geography of interest 
the simplest estimators for population mean and population variance are simply the mean and variance of the sample the sample mean and uncorrected sample variance these are consistent estimators they converge to the correct value as the number of samples increases but can be improved 
estimating the population variance by taking the sample variance is close to optimal in general but can be improved in two ways 
most simply the sample variance is computed as an average of squared deviations about the sample mean by dividing by however using values other than improves the estimator in various ways 
four common values for the denominator are and is the simplest population variance of the sample eliminates bias minimizes mean squared error for the normal distribution and mostly eliminates bias in unbiased estimation of standard deviation for the normal distribution 
firstly if the true population mean is unknown then the sample variance which uses the sample mean in place of the true mean is biased estimator it underestimates the variance by factor of correcting by this factor dividing by instead of is called bessel correction 
the resulting estimator is unbiased and is called the corrected sample variance or unbiased sample variance 
for example when the variance of single observation about the sample mean itself is obviously zero regardless of the population variance 
if the mean is determined in some other way than from the same samples used to estimate the variance then this bias does not arise and the variance can safely be estimated as that of the samples about the independently known mean 
secondly the sample variance does not generally minimize mean squared error between sample variance and population variance 
correcting for bias often makes this worse one can always choose scale factor that performs better than the corrected sample variance though the optimal scale factor depends on the excess kurtosis of the population see mean squared error variance and introduces bias 
this always consists of scaling down the unbiased estimator dividing by number larger than and is simple example of shrinkage estimator one shrinks the unbiased estimator towards zero 
for the normal distribution dividing by instead of or minimizes mean squared error 
the resulting estimator is biased however and is known as the biased sample variation 
population variance in general the population variance of finite population of size with values xi is given by where the population mean is the population variance can also be computed using this is true because the population variance matches the variance of the generating probability distribution 
in this sense the concept of population can be extended to continuous random variables with infinite populations 
sample variance biased sample variance in many practical situations the true variance of population is not known priori and must be computed somehow 
when dealing with extremely large populations it is not possible to count every object in the population so the computation must be performed on sample of the population 
sample variance can also be applied to the estimation of the variance of continuous distribution from sample of that distribution 
we take sample with replacement of values yn from the population where and estimate the variance on the basis of this sample 
directly taking the variance of the sample data gives the average of the squared deviations here denotes the sample mean since the yi are selected randomly both and are random variables 
their expected values can be evaluated by averaging over the ensemble of all possible samples of size from the population 
for this gives hence gives an estimate of the population variance that is biased by factor of for this reason is referred to as the biased sample variance 
unbiased sample variance correcting for this bias yields the unbiased sample variance denoted either estimator may be simply referred to as the sample variance when the version can be determined by context 
the same proof is also applicable for samples taken from continuous probability distribution 
the use of the term is called bessel correction and it is also used in sample covariance and the sample standard deviation the square root of variance 
the square root is concave function and thus introduces negative bias by jensen inequality which depends on the distribution and thus the corrected sample standard deviation using bessel correction is biased 
the unbiased estimation of standard deviation is technically involved problem though for the normal distribution using the term yields an almost unbiased estimator 
the unbiased sample variance is statistic for the function meaning that it is obtained by averaging sample statistic over element subsets of the population 
distribution of the sample variance being function of random variables the sample variance is itself random variable and it is natural to study its distribution 
in the case that yi are independent observations from normal distribution cochran theorem shows that follows scaled chi squared distribution see also asymptotic properties as direct consequence it follows that and var var var if the yi are independent and identically distributed but not necessarily normally distributed then var where is the kurtosis of the distribution and is the fourth central moment 
if the conditions of the law of large numbers hold for the squared observations is consistent estimator of 
one can see indeed that the variance of the estimator tends asymptotically to zero 
an asymptotically equivalent formula was given in kenney and keeping rose and smith and weisstein 
samuelson inequality samuelson inequality is result that states bounds on the values that individual observations in sample can take given that the sample mean and biased variance have been calculated 
values must lie within the limits 
relations with the harmonic and arithmetic means it has been shown that for sample of positive real numbers max where ymax is the maximum of the sample is the arithmetic mean is the harmonic mean of the sample and is the biased variance of the sample 
this bound has been improved and it is known that variance is bounded by max max max min min min where ymin is the minimum of the sample 
tests of equality of variances the test of equality of variances and the chi square tests are adequate when the sample is normally distributed 
non normality makes testing for the equality of two or more variances more difficult 
several non parametric tests have been proposed these include the barton david ansari freund siegel tukey test the capon test mood test the klotz test and the sukhatme test 
the sukhatme test applies to two variances and requires that both medians be known and equal to zero 
the mood klotz capon and barton david ansari freund siegel tukey tests also apply to two variances 
they allow the median to be unknown but do require that the two medians are equal 
the lehmann test is parametric test of two variances 
of this test there are several variants known 
other tests of the equality of variances include the box test the box anderson test and the moses test 
resampling methods which include the bootstrap and the jackknife may be used to test the equality of variances 
moment of inertia the variance of probability distribution is analogous to the moment of inertia in classical mechanics of corresponding mass distribution along line with respect to rotation about its center of mass 
it is because of this analogy that such things as the variance are called moments of probability distributions 
the covariance matrix is related to the moment of inertia tensor for multivariate distributions 
the moment of inertia of cloud of points with covariance matrix of is given by tr 
this difference between moment of inertia in physics and in statistics is clear for points that are gathered along line 
suppose many points are close to the axis and distributed along it 
the covariance matrix might look like 
that is there is the most variance in the direction 
physicists would consider this to have low moment about the axis so the moment of inertia tensor is 
semivariance the semivariance is calculated in the same manner as the variance but only those observations that fall below the mean are included in the calculation it is also described as specific measure in different fields of application 
for skewed distributions the semivariance can provide additional information that variance does not for inequalities associated with the semivariance see chebyshev inequality semivariances 
generalizations for complex variables if is scalar complex valued random variable with values in then its variance is where is the complex conjugate of this variance is real scalar 
for vector valued random variables as matrix if is vector valued random variable with values in and thought of as column vector then natural generalization of variance is where and is the transpose of and so is row vector 
the result is positive semi definite square matrix commonly referred to as the variance covariance matrix or simply as the covariance matrix 
if is vector and complex valued random variable with values in then the covariance matrix is where is the conjugate transpose of this matrix is also positive semi definite and square 
as scalar another generalization of variance for vector valued random variables which results in scalar value rather than in matrix is the generalized variance det the determinant of the covariance matrix 
the generalized variance can be shown to be related to the multidimensional scatter of points around their mean different generalization is obtained by considering the euclidean distance between the random variable and its mean 
this results in tr which is the trace of the covariance matrix 
see also bhatia davis inequality coefficient of variation homoscedasticity least squares spectral analysis for computing frequency spectrum with spectral magnitudes in of variance or in db popoviciu inequality on variances measures for statistical dispersion variance stabilizing transformation types of variance correlation distance variance explained variance pooled variance pseudo variance references
symmetric key algorithms are algorithms for cryptography that use the same cryptographic keys for both the encryption of plaintext and the decryption of ciphertext
the keys may be identical or there may be simple transformation to go between the two keys
the keys in practice represent shared secret between two or more parties that can be used to maintain private information link
the requirement that both parties have access to the secret key is one of the main drawbacks of symmetric key encryption in comparison to public key encryption also known as asymmetric key encryption
however symmetric key encryption algorithms are usually better for bulk encryption
they have smaller key size which means less storage space and faster transmission
due to this asymmetric key encryption is often used to exchange the secret key for symmetric key encryption
types symmetric key encryption can use either stream ciphers or block ciphers
stream ciphers encrypt the digits typically bytes or letters in substitution ciphers of message one at time
an example is chacha
substitution ciphers are well known ciphers but can be easily decrypted using frequency table
block ciphers take number of bits and encrypt them in single unit padding the plaintext to achieve multiple of the block size
the advanced encryption standard aes algorithm approved by nist in december uses bit blocks
implementations examples of popular symmetric key algorithms include twofish serpent aes rijndael camellia salsa chacha blowfish cast kuznyechik rc des des skipjack safer and idea
use as cryptographic primitive symmetric ciphers are commonly used to achieve other cryptographic primitives than just encryption encrypting message does not guarantee that it will remain unchanged while encrypted
hence often message authentication code is added to ciphertext to ensure that changes to the ciphertext will be noted by the receiver
message authentication codes can be constructed from an aead cipher
however symmetric ciphers cannot be used for non repudiation purposes except by involving additional parties
see the iso iec standard
another application is to build hash functions from block ciphers
see one way compression function for descriptions of several such methods
construction of symmetric ciphers many modern block ciphers are based on construction proposed by horst feistel
feistel construction makes it possible to build invertible functions from other functions that are themselves not invertible
security of symmetric ciphers symmetric ciphers have historically been susceptible to known plaintext attacks chosen plaintext attacks differential cryptanalysis and linear cryptanalysis
careful construction of the functions for each round can greatly reduce the chances of successful attack
it is also possible to increase the key length or the rounds in the encryption process to better protect against attack
this however tends to increase the processing power and decrease the speed at which the process runs due to the amount of operations the system needs to do most modern symmetric key algorithms appear to be resistant to the threat of post quantum cryptography
quantum computers would exponentially increase the speed at which these ciphers can be decoded notably grover algorithm would take the square root of the time traditionally required for brute force attack although these vulnerabilities can be compensated for by doubling key length
for example bit aes cipher would not be secure against such an attack as it would reduce the time required to test all possible iterations from over quintillion years to about six months
by contrast it would still take quantum computer the same amount of time to decode bit aes cipher as it would conventional computer to decode bit aes cipher
for this reason aes is believed to be quantum resistant
key management key establishment symmetric key algorithms require both the sender and the recipient of message to have the same secret key
all early cryptographic systems required either the sender or the recipient to somehow receive copy of that secret key over physically secure channel
nearly all modern cryptographic systems still use symmetric key algorithms internally to encrypt the bulk of the messages but they eliminate the need for physically secure channel by using diffie hellman key exchange or some other public key protocol to securely come to agreement on fresh new secret key for each session conversation forward secrecy
key generation when used with asymmetric ciphers for key transfer pseudorandom key generators are nearly always used to generate the symmetric cipher session keys
however lack of randomness in those generators or in their initialization vectors is disastrous and has led to cryptanalytic breaks in the past
therefore it is essential that an implementation use source of high entropy for its initialization
reciprocal cipher reciprocal cipher is cipher where just as one enters the plaintext into the cryptography system to get the ciphertext one could enter the ciphertext into the same place in the system to get the plaintext
reciprocal cipher is also sometimes referred as self reciprocal cipher practically all mechanical cipher machines implement reciprocal cipher mathematical involution on each typed in letter
instead of designing two kinds of machines one for encrypting and one for decrypting all the machines can be identical and can be set up keyed the same way examples of reciprocal ciphers include atbash beaufort cipher enigma machine marie antoinette and axel von fersen communicated with self reciprocal cipher
the porta polyalphabetic cipher is self reciprocal
purple cipher rc rot xor cipher vatsyayana cipherthe majority of all modern ciphers can be classified as either stream cipher most of which use reciprocal xor cipher combiner or block cipher most of which use feistel cipher or lai massey scheme with reciprocal transformation in each round
in statistics mediation model seeks to identify and explain the mechanism or process that underlies an observed relationship between an independent variable and dependent variable via the inclusion of third hypothetical variable known as mediator variable also mediating variable intermediary variable or intervening variable 
rather than direct causal relationship between the independent variable and the dependent variable mediation model proposes that the independent variable influences the mediator variable which in turn influences the dependent variable 
thus the mediator variable serves to clarify the nature of the relationship between the independent and dependent variables mediation analyses are employed to understand known relationship by exploring the underlying mechanism or process by which one variable influences another variable through mediator variable 
in particular mediation analysis can contribute to better understanding the relationship between an independent variable and dependent variable when these variables do not have an obvious direct connection 
baron and kenny steps for mediation analysis baron and kenny laid out several requirements that must be met to form true mediation relationship 
they are outlined below using real world example 
see the diagram above for visual representation of the overall mediating relationship to be explained 
note hayes critiqued baron and kenny mediation steps approach and as of david kenny on his website stated that mediation can exist in the absence of significant total effect and therefore step below may not be needed 
this situation is sometimes referred to as inconsistent mediation 
later publications by hayes also questioned the concepts of full or partial mediation and advocated for these terms along with the classical mediation steps approach outlined below to be abandoned 
step regress the dependent variable on the independent variable to confirm that the independent variable is significant predictor of the dependent variable independent variable dependent variable is significant step regress the mediator on the independent variable to confirm that the independent variable is significant predictor of the mediator 
if the mediator is not associated with the independent variable then it couldn possibly mediate anything independent variable mediator is significant step regress the dependent variable on both the mediator and independent variable to confirm that the mediator is significant predictor of the dependent variable and the strength of the coefficient of the previously significant independent variable in step is now greatly reduced if not rendered nonsignificant 
is significant should be smaller in absolute value than the original effect for the independent variable above example the following example drawn from howell explains each step of baron and kenny requirements to understand further how mediation effect is characterized 
step and step use simple regression analysis whereas step uses multiple regression analysis 
how you were parented independent variable predicts how confident you feel about parenting your own children dependent variable 
how you were parented independent variable predicts your feelings of competence and self esteem mediator 
your feelings of competence and self esteem mediator predict how confident you feel about parenting your own children dependent variable while controlling for how you were parented independent variable such findings would lead to the conclusion implying that your feelings of competence and self esteem mediate the relationship between how you were parented and how confident you feel about parenting your own children 
if step does not yield significant result one may still have grounds to move to step sometimes there is actually significant relationship between independent and dependent variables but because of small sample sizes or other extraneous factors there could not be enough power to predict the effect that actually exists 
direct versus indirect effects in the diagram shown above the indirect effect is the product of path coefficients and 
the direct effect is the coefficient 
the direct effect measures the extent to which the dependent variable changes when the independent variable increases by one unit and the mediator variable remains unaltered 
in contrast the indirect effect measures the extent to which the dependent variable changes when the independent variable is held constant and the mediator variable changes by the amount it would have changed had the independent variable increased by one unit 
in linear systems the total effect is equal to the sum of the direct and indirect ab in the model above 
in nonlinear models the total effect is not generally equal to the sum of the direct and indirect effects but to modified combination of the two 
full versus partial mediation mediator variable can either account for all or some of the observed relationship between two variables 
full mediation maximum evidence for mediation also called full mediation would occur if inclusion of the mediation variable drops the relationship between the independent variable and dependent variable see pathway in diagram above to zero 
partial mediation partial mediation maintains that the mediating variable accounts for some but not all of the relationship between the independent variable and dependent variable 
partial mediation implies that there is not only significant relationship between the mediator and the dependent variable but also some direct relationship between the independent and dependent variable 
in order for either full or partial mediation to be established the reduction in variance explained by the independent variable must be significant as determined by one of several tests such as the sobel test 
the effect of an independent variable on the dependent variable can become nonsignificant when the mediator is introduced simply because trivial amount of variance is explained not true mediation 
thus it is imperative to show significant reduction in variance explained by the independent variable before asserting either full or partial mediation 
it is possible to have statistically significant indirect effects in the absence of total effect 
this can be explained by the presence of several mediating paths that cancel each other out and become noticeable when one of the cancelling mediators is controlled for 
this implies that the terms partial and full mediation should always be interpreted relative to the set of variables that are present in the model 
in all cases the operation of fixing variable must be distinguished from that of controlling for variable which has been inappropriately used in the literature 
the former stands for physically fixing while the latter stands for conditioning on adjusting for or adding to the regression model 
the two notions coincide only when all error terms not shown in the diagram are statistically uncorrelated 
when errors are correlated adjustments must be made to neutralize those correlations before embarking on mediation analysis see bayesian networks 
sobel test sobel test is performed to determine if the relationship between the independent variable and dependent variable has been significantly reduced after inclusion of the mediator variable 
in other words this test assesses whether mediation effect is significant 
it examines the relationship between the independent variable and the dependent variable compared to the relationship between the independent variable and dependent variable including the mediation factor 
the sobel test is more accurate than the baron and kenny steps explained above however it does have low statistical power 
as such large sample sizes are required in order to have sufficient power to detect significant effects 
this is because the key assumption of sobel test is the assumption of normality 
because sobel test evaluates given sample on the normal distribution small sample sizes and skewness of the sampling distribution can be problematic see normal distribution for more details 
thus the rule of thumb as suggested by mackinnon et al is that sample size of is required to detect small effect sample size of is sufficient in detecting medium effect and sample size of is required to detect large effect 
the equation for sobel is preacher hayes bootstrap method the bootstrapping method provides some advantages to the sobel test primarily an increase in power 
the preacher and hayes bootstrapping method is non parametric test 
as such the bootstrap method does not violate assumptions of normality and is therefore recommended for small sample sizes 
bootstrapping involves repeatedly randomly sampling observations with replacement from the data set to compute the desired statistic in each resample 
computing over hundreds or thousands of bootstrap resamples provide an approximation of the sampling distribution of the statistic of interest 
the preacher hayes method provides point estimates and confidence intervals by which one can assess the significance or nonsignificance of mediation effect 
point estimates reveal the mean over the number of bootstrapped samples and if zero does not fall between the resulting confidence intervals of the bootstrapping method one can confidently conclude that there is significant mediation effect to report 
significance of mediation as outlined above there are few different options one can choose from to evaluate mediation model 
bootstrapping is becoming the most popular method of testing mediation because it does not require the normality assumption to be met and because it can be effectively utilized with smaller sample sizes 
however mediation continues to be most frequently determined using the logic of baron and kenny or the sobel test 
it is becoming increasingly more difficult to publish tests of mediation based purely on the baron and kenny method or tests that make distributional assumptions such as the sobel test 
thus it is important to consider your options when choosing which test to conduct 
approaches to mediation while the concept of mediation as defined within psychology is theoretically appealing the methods used to study mediation empirically have been challenged by statisticians and epidemiologists and interpreted formally 
experimental causal chain design an experimental causal chain design is used when the proposed mediator is experimentally manipulated 
such design implies that one manipulates some controlled third variable that they have reason to believe could be the underlying mechanism of given relationship 
measurement of mediation design measurement of mediation design can be conceptualized as statistical approach 
such design implies that one measures the proposed intervening variable and then uses statistical analyses to establish mediation 
this approach does not involve manipulation of the hypothesized mediating variable but only involves measurement 
criticisms of mediation measurement experimental approaches to mediation must be carried out with caution 
first it is important to have strong theoretical support for the exploratory investigation of potential mediating variable 
criticism of mediation approach rests on the ability to manipulate and measure mediating variable 
thus one must be able to manipulate the proposed mediator in an acceptable and ethical fashion 
as such one must be able to measure the intervening process without interfering with the outcome 
the mediator must also be able to establish construct validity of manipulation 
one of the most common criticisms of the measurement of mediation approach is that it is ultimately correlational design 
consequently it is possible that some other third variable independent from the proposed mediator could be responsible for the proposed effect 
however researchers have worked hard to provide counter evidence to this disparagement 
specifically the following counter arguments have been put forward temporal precedence for example if the independent variable precedes the dependent variable in time this would provide evidence suggesting directional and potentially causal link from the independent variable to the dependent variable 
nonspuriousness and or no confounds for example should one identify other third variables and prove that they do not alter the relationship between the independent variable and the dependent variable he she would have stronger argument for their mediation effect 
see other rd variables below mediation can be an extremely useful and powerful statistical test however it must be used properly 
it is important that the measures used to assess the mediator and the dependent variable are theoretically distinct and that the independent variable and mediator cannot interact 
should there be an interaction between the independent variable and the mediator one would have grounds to investigate moderation 
other third variables confounding another model that is often tested is one in which competing variables in the model are alternative potential mediators or an unmeasured cause of the dependent variable 
an additional variable in causal model may obscure or confound the relationship between the independent and dependent variables 
potential confounders are variables that may have causal impact on both the independent variable and dependent variable 
they include common sources of measurement error as discussed above as well as other influences shared by both the independent and dependent variables 
in experimental studies there is special concern about aspects of the experimental manipulation or setting that may account for study effects rather than the motivating theoretical factor 
any of these problems may produce spurious relationships between the independent and dependent variables as measured 
ignoring confounding variable may bias empirical estimates of the causal effect of the independent variable 
suppression suppressor variable increases the predictive validity of another variable when included in regression equation 
suppression can occur when single causal variable is related to an outcome variable through two separate mediator variables and when one of those mediated effects is positive and one is negative 
in such case each mediator variable suppresses or conceals the effect that is carried through the other mediator variable 
for example higher intelligence scores causal variable may cause an increase in error detection mediator variable which in turn may cause decrease in errors made at work on an assembly line an outcome variable at the same time intelligence could also cause an increase in boredom which in turn may cause an increase in errors 
thus in one causal path intelligence decreases errors and in the other it increases them 
when neither mediator is included in the analysis intelligence appears to have no effect or weak effect on errors 
however when boredom is controlled intelligence will appear to decrease errors and when error detection is controlled intelligence will appear to increase errors 
if intelligence could be increased while only boredom was held constant errors would decrease if intelligence could be increased while holding only error detection constant errors would increase 
in general the omission of suppressors or confounders will lead to either an underestimation or an overestimation of the effect of on thereby either reducing or artificially inflating the magnitude of relationship between two variables 
moderators other important third variables are moderators 
moderators are variables that can make the relationship between two variables either stronger or weaker 
such variables further characterize interactions in regression by affecting the direction and or strength of the relationship between and moderating relationship can be thought of as an interaction 
it occurs when the relationship between variables and depends on the level of see moderation for further discussion 
moderated mediation mediation and moderation can co occur in statistical models 
it is possible to mediate moderation and moderate mediation 
moderated mediation is when the effect of the treatment on the mediator and or the partial effect on the dependent variable depend in turn on levels of another variable moderator 
essentially in moderated mediation mediation is first established and then one investigates if the mediation effect that describes the relationship between the independent variable and dependent variable is moderated by different levels of another variable moderator 
this definition has been outlined by muller judd and yzerbyt and preacher rucker and hayes 
models of moderated mediation there are five possible models of moderated mediation as illustrated in the diagrams below 
in the first model the independent variable also moderates the relationship between the mediator and the dependent variable 
the second possible model of moderated mediation involves new variable which moderates the relationship between the independent variable and the mediator the path 
the third model of moderated mediation involves new moderator variable which moderates the relationship between the mediator and the dependent variable the path 
moderated mediation can also occur when one moderating variable affects both the relationship between the independent variable and the mediator the path and the relationship between the mediator and the dependent variable the path 
the fifth and final possible model of moderated mediation involves two new moderator variables one moderating the path and the other moderating the path 
in addition to the models mentioned above new variable can also exist which moderates the relationship between the independent variable and mediator the path while at the same time have the new variable moderate the relationship between the independent variable and dependent variable the path 
mediated moderation mediated moderation is variant of both moderation and mediation 
this is where there is initially overall moderation and the direct effect of the moderator variable on the outcome is mediated 
the main difference between mediated moderation and moderated mediation is that for the former there is initial overall moderation and this effect is mediated and for the latter there is no moderation but the effect of either the treatment on the mediator path is moderated or the effect of the mediator on the outcome path is moderated in order to establish mediated moderation one must first establish moderation meaning that the direction and or the strength of the relationship between the independent and dependent variables path differs depending on the level of third variable the moderator variable 
researchers next look for the presence of mediated moderation when they have theoretical reason to believe that there is fourth variable that acts as the mechanism or process that causes the relationship between the independent variable and the moderator path or between the moderator and the dependent variable path 
example the following is published example of mediated moderation in psychological research 
participants were presented with an initial stimulus prime that made them think of morality or made them think of might 
they then participated in the prisoner dilemma game pdg in which participants pretend that they and their partner in crime have been arrested and they must decide whether to remain loyal to their partner or to compete with their partner and cooperate with the authorities 
the researchers found that prosocial individuals were affected by the morality and might primes whereas proself individuals were not 
thus social value orientation proself vs prosocial moderated the relationship between the prime independent variable morality vs might and the behaviour chosen in the pdg dependent variable competitive vs cooperative 
the researchers next looked for the presence of mediated moderation effect 
regression analyses revealed that the type of prime morality vs might mediated the moderating relationship of participants social value orientation on pdg behaviour 
prosocial participants who experienced the morality prime expected their partner to cooperate with them so they chose to cooperate themselves 
prosocial participants who experienced the might prime expected their partner to compete with them which made them more likely to compete with their partner and cooperate with the authorities 
in contrast participants with pro self social value orientation always acted competitively 
regression equations for moderated mediation and mediated moderation muller judd and yzerbyt outline three fundamental models that underlie both moderated mediation and mediated moderation 
mo represents the moderator variable me represents the mediator variable and represents the measurement error of each regression equation 
step moderation of the relationship between the independent variable and the dependent variable also called the overall treatment effect path in the diagram 
to establish overall moderation the regression weight must be significant first step for establishing mediated moderation 
establishing moderated mediation requires that there be no moderation effect so the regression weight must not be significant 
step moderation of the relationship between the independent variable and the mediator path 
if the regression weight is significant the moderator affects the relationship between the independent variable and the mediator 
step moderation of both the relationship between the independent and dependent variables path and the relationship between the mediator and the dependent variable path 
if both in step and in step are significant the moderator affects the relationship between the independent variable and the mediator path 
if both in step and in step are significant the moderator affects the relationship between the mediator and the dependent variable path 
either or both of the conditions above may be true 
causal mediation analysis fixing versus conditioning mediation analysis quantifies the extent to which variable participates in the transmittance of change from cause to its effect 
it is inherently causal notion hence it cannot be defined in statistical terms 
traditionally however the bulk of mediation analysis has been conducted within the confines of linear regression with statistical terminology masking the causal character of the relationships involved 
this led to difficulties biases and limitations that have been alleviated by modern methods of causal analysis based on causal diagrams and counterfactual logic 
the source of these difficulties lies in defining mediation in terms of changes induced by adding third variables into regression equation 
such statistical changes are epiphenomena which sometimes accompany mediation but in general fail to capture the causal relationships that mediation analysis aims to quantify 
the basic premise of the causal approach is that it is not always appropriate to control for the mediator when we seek to estimate the direct effect of on see the figure above 
the classical rationale for controlling for is that if we succeed in preventing from changing then whatever changes we measure in are attributable solely to variations in and we are justified then in proclaiming the effect observed as direct effect of on 
unfortunately controlling for does not physically prevent from changing it merely narrows the analyst attention to cases of equal values 
moreover the language of probability theory does not possess the notation to express the idea of preventing from changing or physically holding constant 
the only operator probability provides is conditioning which is what we do when we control for or add as regressor in the equation for the result is that instead of physically holding constant say at and comparing for units under to those under we allow to vary but ignore all units except those in which achieves the value these two operations are fundamentally different and yield different results except in the case of no omitted variables 
to illustrate assume that the error terms of and are correlated 
under such conditions the structural coefficient and between and and between and can no longer be estimated by regressing on and in fact the regression slopes may both be nonzero even when is zero 
this has two consequences 
first new strategies must be devised for estimating the structural coefficients and second the basic definitions of direct and indirect effects must go beyond regression analysis and should invoke an operation that mimics fixing rather than conditioning on definitions such an operator denoted do was defined in pearl and it operates by removing the equation of and replacing it by constant for example if the basic mediation model consists of the equations then after applying the operator do the model becomes and after applying the operator do the model becomes where the functions and as well as the distributions of the error terms and remain unaltered 
if we further rename the variables and resulting from do as and respectively we obtain what came to be known as potential outcomes or structural counterfactuals 
these new variables provide convenient notation for defining direct and indirect effects 
in particular four types of effects have been defined for the transition from to total effect controlled direct effect natural direct effect natural indirect effect where stands for expectation taken over the error terms 
these effects have the following interpretations te measures the expected increase in the outcome as changes from to while the mediator is allowed to track the change in as dictated by the function 
cde measures the expected increase in the outcome as changes from to while the mediator is fixed at pre specified level uniformly over the entire population nde measures the expected increase in as changes from to while setting the mediator variable to whatever value it would have obtained under before the change 
nie measures the expected increase in when the is held constant at and changes to whatever value it would have attained for each individual under the difference te nde measures the extent to which mediation is necessary for explaining the effect while the nie measures the extent to which mediation is sufficient for sustaining it controlled version of the indirect effect does not exist because there is no way of disabling the direct effect by fixing variable to constant 
according to these definitions the total effect can be decomposed as sum where nier stands for the reverse transition from to it becomes additive in linear systems where reversal of transitions entails sign reversal 
the power of these definitions lies in their generality they are applicable to models with arbitrary nonlinear interactions arbitrary dependencies among the disturbances and both continuous and categorical variables 
the mediation formula in linear analysis all effects are determined by sums of products of structural coefficients giving independent of therefore all effects are estimable whenever the model is identified 
in non linear systems more stringent conditions are needed for estimating the direct and indirect effects 
for example if no confounding exists and are mutually independent the following formulas can be derived 
the last two equations are called mediation formulas and have become the target of estimation in many studies of mediation 
they give distribution free expressions for direct and indirect effects and demonstrate that despite the arbitrary nature of the error distributions and the functions and mediated effects can nevertheless be estimated from data using regression 
the analyses of moderated mediation and mediating moderators fall as special cases of the causal mediation analysis and the mediation formulas identify how various interactions coefficients contribute to the necessary and sufficient components of mediation 
example assume the model takes the form where the parameter quantifies the degree to which modifies the effect of on even when all parameters are estimated from data it is still not obvious what combinations of parameters measure the direct and indirect effect of on or more practically how to assess the fraction of the total effect that is explained by mediation and the fraction of that is owed to mediation 
in linear analysis the former fraction is captured by the product the latter by the difference and the two quantities coincide 
in the presence of interaction however each fraction demands separate analysis as dictated by the mediation formula which yields thus the fraction of output response for which mediation would be sufficient is while the fraction for which mediation would be necessary is 
these fractions involve non obvious combinations of the model parameters and can be constructed mechanically with the help of the mediation formula 
significantly due to interaction direct effect can be sustained even when the parameter vanishes and moreover total effect can be sustained even when both the direct and indirect effects vanish 
this illustrates that estimating parameters in isolation tells us little about the effect of mediation and more generally mediation and moderation are intertwined and cannot be assessed separately 
references as of june this article is derived in whole or in part from causal analysis in theory and practice 
the copyright holder has licensed the content in manner that permits reuse under cc by sa and gfdl 
all relevant terms must be followed 
notes bibliographypreacher kristopher hayes andrew 
spss and sas procedures for estimating indirect effects in simple mediation models 
behavior research methods instruments and computers 
pmid preacher kristopher hayes andrew 
asymptotic and resampling strategies for assessing and comparing indirect effects in multiple mediator models 
pmid preacher zyphur zhang 
general multilevel sem framework for assessing multilevel mediation 
pmid baron and kenny 
the moderator mediator variable distinction in social psychological research conceptual strategic and statistical considerations journal of personality and social psychology vol 
statistical power analysis for the behavioral sciences nd ed 
new york ny academic press 
beyond baron and kenny statistical mediation analysis in the new millennium 
statistical methods for psychology th ed 
belmot ca cengage learning 
advances in statistical methods for substance abuse prevention research 
doi pmc pmid preacher kelley 
effect sizes measures for mediation models quantitative strategies for communicating indirect effects 
pmid rucker preacher tormala 
mediation analysis in social psychology current practices and new recommendations 
social and personality psychology compass 
asymptotic confidence intervals for indirect effects in structural equation models 
jstor spencer zanna fong 
establishing causal chain why experiments are often more effective than mediational analyses in examining psychological processes 
journal of personality and social psychology 
the mediation formula guide to the assessment of causal pathways in nonlinear models 
in berzuini dawid bernardinelli 
causality statistical perspectives and applications 
chichester uk john wiley and sons ltd pp 
research methods in psychology th ed pp 
new york mcgraw hill 
the determiners of behavior at choice point 
degrees of hunger reward and nonreward and maze learning in rats 
university of california publications in psychology 
explanation in causal inference 
external links summary of mediation methods at psychwiki example of causal mediation using propensity scores the methodology center penn state university book on moderation and mediation analysis including an introduction to the process macro for spss and sas andrew hayes ohio state university online text of the determiner of behavior at choice point kenneth maccorquodale and paul meehl on distinction between hypothetical constructs and intervening variables classics in the history of psychology retr
the sample mean or empirical mean and the sample covariance are statistics computed from sample of data on one or more random variables 
the sample mean is the average value or mean value of sample of numbers taken from larger population of numbers where population indicates not number of people but the entirety of relevant data whether collected or not 
sample of companies sales from the fortune might be used for convenience instead of looking at the population all companies sales 
the sample mean is used as an estimator for the population mean the average value in the entire population where the estimate is more likely to be close to the population mean if the sample is large and representative 
the reliability of the sample mean is estimated using the standard error which in turn is calculated using the variance of the sample 
if the sample is random the standard error falls with the size of the sample and the sample mean distribution approaches the normal distribution as the sample size increases 
the term sample mean can also be used to refer to vector of average values when the statistician is looking at the values of several variables in the sample 
the sales profits and employees of sample of fortune companies 
in this case there is not just sample variance for each variable but sample variance covariance matrix or simply covariance matrix showing also the relationship between each pair of variables 
this would be matrix when variables are being considered 
the sample covariance is useful in judging the reliability of the sample means as estimators and is also useful as an estimate of the population covariance matrix 
due to their ease of calculation and other desirable characteristics the sample mean and sample covariance are widely used in statistics to represent the location and dispersion of the distribution of values in the sample and to estimate the values for the population 
definition of the sample mean the sample mean is the average of the values of variable in sample which is the sum of those values divided by the number of values 
using mathematical notation if sample of observations on variable is taken from the population the sample mean is under this definition if the sample is taken from the population then the sample mean is as compared to the population mean of even if sample is random it is rarely perfectly representative and other samples would have other sample means even if the samples were all from the same population 
the sample for example would have sample mean of if the statistician is interested in variables rather than one each observation having value for each of those variables the overall sample mean consists of sample means for individual variables 
let be the ith independently drawn observation on the jth random variable 
these observations can be arranged into column vectors each with entries with the column vector giving the th observations of all variables being denoted 
the sample mean vector is column vector whose th element is the average value of the observations of the jth variable thus the sample mean vector contains the average of the observations for each variable and is written definition of sample covariance the sample covariance matrix is by matrix with entries where is an estimate of the covariance between the jth variable and the kth variable of the population underlying the data 
in terms of the observation vectors the sample covariance is 
alternatively arranging the observation vectors as the columns of matrix so that which is matrix of rows and columns 
here the sample covariance matrix can be computed as where is an by vector of ones 
if the observations are arranged as rows instead of columns so is now row vector and is an matrix whose column is the vector of observations on variable then applying transposes in the appropriate places yields 
like covariance matrices for random vector sample covariance matrices are positive semi definite 
to prove it note that for any matrix the matrix is positive semi definite 
furthermore covariance matrix is positive definite if and only if the rank of the 
vectors is unbiasedness the sample mean and the sample covariance matrix are unbiased estimates of the mean and the covariance matrix of the random vector row vector whose jth element is one of the random variables 
the sample covariance matrix has in the denominator rather than due to variant of bessel correction in short the sample covariance relies on the difference between each observation and the sample mean but the sample mean is slightly correlated with each observation since it is defined in terms of all observations 
if the population mean is known the analogous unbiased estimate using the population mean has in the denominator 
this is an example of why in probability and statistics it is essential to distinguish between random variables upper case letters and realizations of the random variables lower case letters 
the maximum likelihood estimate of the covariance for the gaussian distribution case has in the denominator as well 
the ratio of to approaches for large so the maximum likelihood estimate approximately equals the unbiased estimate when the sample is large 
distribution of the sample mean for each random variable the sample mean is good estimator of the population mean where good estimator is defined as being efficient and unbiased 
of course the estimator will likely not be the true value of the population mean since different samples drawn from the same distribution will give different sample means and hence different estimates of the true mean 
thus the sample mean is random variable not constant and consequently has its own distribution 
for random sample of observations on the jth random variable the sample mean distribution itself has mean equal to the population mean and variance equal to where is the population variance 
the arithmetic mean of population or population mean is often denoted the sample mean the arithmetic mean of sample of values drawn from the population makes good estimator of the population mean as its expected value is equal to the population mean that is it is an unbiased estimator 
the sample mean is random variable not constant since its calculated value will randomly differ depending on which members of the population are sampled and consequently it will have its own distribution 
for random sample of independent observations the expected value of the sample mean is and the variance of the sample mean is var if the samples are not independent but correlated then special care has to be taken in order to avoid the problem of pseudoreplication 
if the population is normally distributed then the sample mean is normally distributed as follows 
if the population is not normally distributed the sample mean is nonetheless approximately normally distributed if is large and 
this is consequence of the central limit theorem 
weighted samples in weighted sample each vector each set of single observations on each of the random variables is assigned weight without loss of generality assume that the weights are normalized 
if they are not divide the weights by their sum 
then the weighted mean vector is given by and the elements of the weighted covariance matrix are 
if all weights are the same the weighted mean and covariance reduce to the biased sample mean and covariance mentioned above 
criticism the sample mean and sample covariance are not robust statistics meaning that they are sensitive to outliers 
as robustness is often desired trait particularly in real world applications robust alternatives may prove desirable notably quantile based statistics such as the sample median for location and interquartile range iqr for dispersion 
other alternatives include trimming and winsorising as in the trimmed mean and the winsorized mean 
see also estimation of covariance matrices scatter matrix unbiased estimation of standard deviation references
in the field of multivariate statistics kernel principal component analysis kernel pca is an extension of principal component analysis pca using techniques of kernel methods 
using kernel the originally linear operations of pca are performed in reproducing kernel hilbert space 
background linear pca recall that conventional pca operates on zero centered data that is where is one of the multivariate observations 
it operates by diagonalizing the covariance matrix in other words it gives an eigendecomposition of the covariance matrix which can be rewritten as for see also covariance matrix as linear operator introduction of the kernel to pca to understand the utility of kernel pca particularly for clustering observe that while points cannot in general be linearly separated in dimensions they can almost always be linearly separated in dimensions 
that is given points if we map them to an dimensional space with where it is easy to construct hyperplane that divides the points into arbitrary clusters 
of course this creates linearly independent vectors so there is no covariance on which to perform eigendecomposition explicitly as we would in linear pca 
instead in kernel pca non trivial arbitrary function is chosen that is never calculated explicitly allowing the possibility to use very high dimensional if we never have to actually evaluate the data in that space 
since we generally try to avoid working in the space which we will call the feature space we can create the by kernel which represents the inner product space see gramian matrix of the otherwise intractable feature space 
the dual form that arises in the creation of kernel allows us to mathematically formulate version of pca in which we never actually solve the eigenvectors and eigenvalues of the covariance matrix in the space see kernel trick 
the elements in each column of represent the dot product of one point of the transformed data with respect to all the transformed points points 
some well known kernels are shown in the example below 
because we are never working directly in the feature space the kernel formulation of pca is restricted in that it computes not the principal components themselves but the projections of our data onto those components 
to evaluate the projection from point in the feature space onto the kth principal component where superscript means the component not powers of we note that denotes dot product which is simply the elements of the kernel it seems all that left is to calculate and normalize the which can be done by solving the eigenvector equation where is the number of data points in the set and and are the eigenvalues and eigenvectors of then to normalize the eigenvectors we require that care must be taken regarding the fact that whether or not has zero mean in its original space it is not guaranteed to be centered in the feature space which we never compute explicitly 
since centered data is required to perform an effective principal component analysis we centralize to become where denotes by matrix for which each element takes value we use to perform the kernel pca algorithm described above 
one caveat of kernel pca should be illustrated here 
in linear pca we can use the eigenvalues to rank the eigenvectors based on how much of the variation of the data is captured by each principal component 
this is useful for data dimensionality reduction and it could also be applied to kpca 
however in practice there are cases that all variations of the data are same 
this is typically caused by wrong choice of kernel scale 
large datasets in practice large data set leads to large and storing may become problem 
one way to deal with this is to perform clustering on the dataset and populate the kernel with the means of those clusters 
since even this method may yield relatively large it is common to compute only the top eigenvalues and eigenvectors of the eigenvalues are calculated in this way 
example consider three concentric clouds of points shown we wish to use kernel pca to identify these groups 
the color of the points does not represent information involved in the algorithm but only shows how the transformation relocates the data points 
first consider the kernel applying this to kernel pca yields the next image 
now consider gaussian kernel that is this kernel is measure of closeness equal to when the points coincide and equal to at infinity 
note in particular that the first principal component is enough to distinguish the three different groups which is impossible using only linear pca because linear pca operates only in the given in this case two dimensional space in which these concentric point clouds are not linearly separable 
applications kernel pca has been demonstrated to be useful for novelty detection and image de noising 
see also cluster analysis nonlinear dimensionality reduction spectral clustering references
in music theory the circle of fifths is way of organizing the chromatic pitches as sequence of perfect fifths 
this is strictly true in the standard tone equal temperament system using different system requires one interval of diminished sixth to be treated as fifth 
if is chosen as starting point the sequence is continuing the pattern from returns the sequence to its starting point of this order places the most closely related key signatures adjacent to one another 
it is usually illustrated in the form of circle 
definition the circle of fifths organizes pitches in sequence of perfect fifths generally shown as circle with the pitches and their corresponding keys in clockwise progression 
musicians and composers often use the circle of fifths to describe the musical relationships between pitches 
its design is helpful in composing and harmonizing melodies building chords and modulating to different keys within composition using the system of just intonation perfect fifth consists of two pitches with frequency ratio of but generating twelve successive perfect fifths in this way does not result in return to the pitch class of the starting note 
to adjust for this instruments are generally tuned with the equal temperament system 
twelve equal temperament fifths lead to note exactly seven octaves above the initial tone this results in perfect fifth that is equivalent to seven equal temperament semitones 
the top of the circle shows the key of major with no sharps or flats 
proceeding clockwise the pitches ascend by fifths 
the key signatures associated with those pitches also change the key of has one sharp the key of has sharps and so on 
similarly proceeding counterclockwise from the top of the circle the notes change by descending fifths and the key signatures change accordingly the key of has one flat the key of has flats and so on 
some keys at the bottom of the circle can be notated either in sharps or in flats 
starting at any pitch and ascending by fifth generates all twelve tones before returning to the beginning pitch class pitch class consists of all of the notes indicated by given letter regardless of octave all for example belong to the same pitch class 
moving counterclockwise the pitches descend by fifth but ascending by perfect fourth will lead to the same note an octave higher therefore in the same pitch class 
moving counter clockwise from could be thought of as descending by fifth to or ascending by fourth to structure and use diatonic key signatures each of the twelve pitches can serve as the tonic of major or minor key and each of these keys will have diatonic scale associated with it 
the circle diagram shows the number of sharps or flats in each key signature with the major key indicated by capital letter and the minor key indicated by lower case letter 
major and minor keys that have the same key signature are referred to as relative major and relative minor of one another 
modulation and chord progression tonal music often modulates to new tonal center whose key signature differs from the original by only one flat or sharp 
these closely related keys are fifth apart from each other and are therefore adjacent in the circle of fifths 
chord progressions also often move between chords whose roots are related by perfect fifth making the circle of fifths useful in illustrating the harmonic distance between chords 
the circle of fifths is used to organize and describe the harmonic function of chords 
chords can progress in pattern of ascending perfect fourths alternately viewed as descending perfect fifths in functional succession 
this can be shown by the circle of fifths in which therefore scale degree ii is closer to the dominant than scale degree iv 
in this view the tonic is considered the end point of chord progression derived from the circle of fifths 
according to richard franko goldman harmony in western music the iv chord is in the simplest mechanisms of diatonic relationships at the greatest distance from in terms of the descending circle of fifths it leads away from rather than toward it 
he states that the progression ii an authentic cadence would feel more final or resolved than iv plagal cadence 
goldman concurs with nattiez who argues that the chord on the fourth degree appears long before the chord on ii and the subsequent final in the progression iv viio iii vi ii and is farther from the tonic there as well 
in this and related articles upper case roman numerals indicate major triads while lower case roman numerals indicate minor triads 
circle closure in non equal tuning systems using the exact ratio of frequencies to define perfect fifth just intonation does not quite result in return to the pitch class of the starting note after going around the circle of fifths 
equal temperament tuning produces fifths that return to tone exactly seven octaves above the initial tone and makes the frequency ratio of each half step the same 
an equal tempered fifth has frequency ratio of or about approximately two cents narrower than justly tuned fifth at ratio of ascending by justly tuned fifths fails to close the circle by an excess of approximately cents roughly quarter of semitone an interval known as the pythagorean comma 
in pythagorean tuning this problem is solved by markedly shortening the width of one of the twelve fifths which makes it severely dissonant 
this anomalous fifth is called the wolf fifth humorous reference to wolf howling an off pitch note 
the quarter comma meantone tuning system uses eleven fifths slightly narrower than the equally tempered fifth and requires much wider and even more dissonant wolf fifth to close the circle 
more complex tuning systems based on just intonation such as limit tuning use at most eight justly tuned fifths and at least three non just fifths some slightly narrower and some slightly wider than the just fifth to close the circle 
other tuning systems use up to tones the original tones and more between them in order to close the circle of fifths 
history the circle of fifths developed in the late and early to theorize the modulation of the baroque era see baroque era 
the first circle of fifths diagram appears in the grammatika of the composer and theorist nikolay diletsky who intended to present music theory as tool for composition 
it was the first of its kind aimed at teaching russian audience how to write western style polyphonic compositions 
circle of fifths diagram was independently created by german composer and theorist johann david heinichen in his neu erfundene und gr ndliche anweisung which he called the musical circle german musicalischer circul 
this was also published in his der general bass in der composition 
heinichen placed the relative minor key next to the major key which did not reflect the actual proximity of keys 
johann mattheson and others attempted to improve this david kellner proposed having the major keys on one circle and the relative minor keys on second inner circle 
this was later developed into chordal space incorporating the parallel minor as well some sources imply that the circle of fifths was known in antiquity by pythagoras 
this is misunderstanding and an anachronism 
tuning by fifths so called pythagorean tuning dates to ancient mesopotamia see music of mesopotamia music theory though they did not extend this to twelve note scale stopping at seven 
the pythagorean comma was calculated by euclid and by chinese mathematicians in the huainanzi see pythagorean comma history 
thus it was known in antiquity that cycle of twelve fifths was almost exactly seven octaves more practically alternating ascending fifths and descending fourths was almost exactly an octave 
however this was theoretical knowledge and was not used to construct repeating twelve tone scale nor to modulate 
this was done later in meantone temperament and twelve tone equal temperament which allowed modulation while still being in tune but did not develop in europe until about 
use in musical pieces from the baroque music era and the classical era of music and in western popular music traditional music and folk music when pieces or songs modulate to new key these modulations are often associated with the circle of fifths 
in practice compositions rarely make use of the entire circle of fifths 
more commonly composers make use of the compositional idea of the cycle of ths when music moves consistently through smaller or larger segment of the tonal structural resources which the circle abstractly represents 
the usual practice is to derive the circle of fifths progression from the seven tones of the diatonic scale rather from the full range of twelve tones present in the chromatic scale 
in this diatonic version of the circle one of the fifths is not true fifth it is tritone or diminished fifth 
between and in the natural diatonic scale 
without sharps or flats 
here is how the circle of fifths derives through permutation from the diatonic major scale and from the natural minor scale the following is the basic sequence of chords that can be built over the major bass line and over the minor adding sevenths to the chords creates greater sense of forward momentum to the harmony baroque era according to richard taruskin arcangelo corelli was the most influential composer to establish the pattern as standard harmonic trope it was precisely in corelli time the late seventeenth century that the circle of fifths was being theorized as the main propellor of harmonic motion and it was corelli more than any one composer who put that new idea into telling practice 
the circle of fifths progression occurs frequently in the music of bach 
in the following from jauchzet gott in allen landen bwv even when the solo bass line implies rather than states the chords involved handel uses circle of fifths progression as the basis for the passacaglia movement from his harpsichord suite no 
baroque composers learnt to enhance the propulsive force of the harmony engendered by the circle of fifths by adding sevenths to most of the constituent chords 
these sevenths being dissonances create the need for resolution thus turning each progression of the circle into simultaneous reliever and re stimulator of harmonic tension hence harnessed for expressive purposes 
striking passages that illustrate the use of sevenths occur in the aria pena tiranna in handel opera amadigi di gaula and in bach keyboard arrangement of alessandro marcello concerto for oboe and strings 
nineteenth century during the nineteenth century composers made use of the circle of fifths to enhance the expressive character of their music 
franz schubert poignant impromptu in flat major contains such passage as does the intermezzo movement from mendelssohn string quartet no robert schumann evocative child falling asleep from his kinderszenen springs surprise at the end of the progression the piece ends on an minor chord instead of the expected tonic minor 
in wagner opera tterd mmerung cycle of fifths progression occurs in the music which transitions from the end of the prologue into the first scene of act set in the imposing hall of the wealthy gibichungs 
status and reputation are written all over the motifs assigned to gunther chief of the gibichung clan jazz and popular music the enduring popularity of the circle of fifths as both form building device and as an expressive musical trope is evident in the number of standard popular songs composed during the twentieth century 
it is also favored as vehicle for improvisation by jazz musicians 
bart howard fly me to the moon the song opens with pattern of descending phrases in essence the hook of the song presented with soothing predictability almost as if the future direction of the melody is dictated by the opening five notes 
the harmonic progression for its part rarely departs from the circle of fifths 
jerome kern all the things you are ray noble cherokee 
many jazz musicians have found this particularly challenging as the middle eight progresses so rapidly through the circle creating series of ii progressions that temporarily pass through several tonalities 
kosmo prevert and mercer autumn leaves the beatles you never give me your money mike oldfield incantations carlos santana europa earth cry heaven smile gloria gaynor will survive pet shop boys it sin donna summer love to love you baby related concepts diatonic circle of fifths the diatonic circle of fifths is the circle of fifths encompassing only members of the diatonic scale 
therefore it contains diminished fifth in major between and see structure implies multiplicity 
the circle progression is commonly circle of fifths through the diatonic chords including one diminished chord 
circle progression in major with chords iv viio iii vi ii is shown below 
chromatic circle the circle of fifths is closely related to the chromatic circle which also arranges the twelve equal tempered pitch classes in circular ordering 
key difference between the two circles is that the chromatic circle can be understood as continuous space where every point on the circle corresponds to conceivable pitch class and every conceivable pitch class corresponds to point on the circle 
by contrast the circle of fifths is fundamentally discrete structure and there is no obvious way to assign pitch classes to each of its points 
in this sense the two circles are mathematically quite different 
however the twelve equal tempered pitch classes can be represented by the cyclic group of order twelve or equivalently the residue classes modulo twelve the group has four generators which can be identified with the ascending and descending semitones and the ascending and descending perfect fifths 
the semitonal generator gives rise to the chromatic circle while the perfect fifth gives rise to the circle of fifths 
relation with chromatic scale the circle of fifths or fourths may be mapped from the chromatic scale by multiplication and vice versa 
to map between the circle of fifths and the chromatic scale in integer notation multiply by and for the circle of fourths multiply by 
here is demonstration of this procedure 
start off with an ordered tuple tone row of integers representing the notes of the chromatic scale 
now multiply the entire tuple by and then apply modulo reduction to each of the numbers subtract from each number as many times as necessary until the number becomes smaller than which is equivalent to which is the circle of fifths 
note that this is enharmonically equivalent to 
enharmonic equivalents theoretical keys and the spiral of fifths equal temperament tuning does not use the exact ratio of frequencies that defines perfect fifth wheras the system of just intonation uses this exact ratio 
ascending by fifths in equal temperament leads to return to the starting pitch class starting with and ascending by fifths leads to another after twelve iterations 
this does not occur if an exact ratio is used just intonation 
the adjustment made in equal temperament tuning is called the pythagorean comma 
because of this difference pitches that are enharmonically equivalent in equal temperament tuning and are not equivalent when using just intonation 
in just intonation the sequence of fifths can therefore be visualized as spiral not circle sequence of twelve fifths results in comma pump by the pythagorean comma visualized as going up level in the spiral 
see also circle closure in non equal tuning systems 
without enharmonic equivalence continuing sequence of fifths results in notes with double accidentals double sharps or double flats 
when using equal temperament these can be replaced by an enharmonically equivalent note 
keys with double sharps or flats in the key signatures are called theoretical keys their use is extremely rare 
notation in these cases is not standardized 
the default behaviour of lilypond pictured above writes single sharps or flats in the circle of fifths order before proceeding to double sharps or flats 
this is the format used in john foulds world requiem op 
which ends with the key signature of major as displayed above 
the sharps in the key signature of major here proceed single sharps or flats in the key signature are sometimes repeated as courtesy 
max reger supplement to the theory of modulation which contains minor key signatures on pp 
these have at the start and also at the end with double flat symbol going the convention of lilypond and foulds would suppress the initial 
sometimes the double signs are written at the beginning of the key signature followed by the single signs 
for example the key signature is notated as 
this convention is used by victor ewald by the program finale software and by some theoretical works 
see also approach chord sonata form well temperament circle of fifths text table pitch constellation multiplicative group of integers modulo notes references barnett gregory 
tonal organization in seventeenth century music theory 
in thomas christensen ed 
the cambridge history of western music theory 
cambridge cambridge university press 
the jazz standards guide to the repertoire 
isbn goldman richard franko 
harmony in western music 
theoretical work of late seventeenth century muscovy nikolai diletskii grammatika and the earliest circle of fifths 
journal of the american musicological society 
between modes and keys german theory 
prelude to musical geometry 
the college mathematics journal 
jstor archived from the original on retrieved nattiez jean jacques 
music and discourse toward semiology of music translated by carolyn abbate 
princeton new jersey princeton university press 
originally published in french as musicologie rale et miologie 
the oxford history of western music music in the seventeenth and eighteenth centuries 
further reading indy vincent 
cours de composition musicale 
paris durand et fils 
between modes and keys german theory 
the complete idiot guide to music theory nd ed 
indianapolis in alpha isbn purwins hendrik 
profiles of pitch classes circularity of relative pitch and key experiments models computational music analysis and perspectives 
berlin technische universit berlin 
purwins hendrik benjamin blankertz and klaus obermayer 
toroidal models in tonal theory and pitch class analysis 
in computing in musicology tonal theory for the digital age 
external links decoding the circle of vths interactive circle of fifths interactive circle of fifths for guitarists
in statistical modeling regression analysis is set of statistical processes for estimating the relationships between dependent variable often called the outcome or response variable or label in machine learning parlance and one or more independent variables often called predictors covariates explanatory variables or features 
the most common form of regression analysis is linear regression in which one finds the line or more complex linear combination that most closely fits the data according to specific mathematical criterion 
for example the method of ordinary least squares computes the unique line or hyperplane that minimizes the sum of squared differences between the true data and that line or hyperplane 
for specific mathematical reasons see linear regression this allows the researcher to estimate the conditional expectation or population average value of the dependent variable when the independent variables take on given set of values 
less common forms of regression use slightly different procedures to estimate alternative location parameters quantile regression or necessary condition analysis or estimate the conditional expectation across broader collection of non linear models nonparametric regression 
regression analysis is primarily used for two conceptually distinct purposes 
first regression analysis is widely used for prediction and forecasting where its use has substantial overlap with the field of machine learning 
second in some situations regression analysis can be used to infer causal relationships between the independent and dependent variables 
importantly regressions by themselves only reveal relationships between dependent variable and collection of independent variables in fixed dataset 
to use regressions for prediction or to infer causal relationships respectively researcher must carefully justify why existing relationships have predictive power for new context or why relationship between two variables has causal interpretation 
the latter is especially important when researchers hope to estimate causal relationships using observational data 
history the earliest form of regression was the method of least squares which was published by legendre in and by gauss in legendre and gauss both applied the method to the problem of determining from astronomical observations the orbits of bodies about the sun mostly comets but also later the then newly discovered minor planets 
gauss published further development of the theory of least squares in including version of the gauss markov theorem 
the term regression was coined by francis galton in the th century to describe biological phenomenon 
the phenomenon was that the heights of descendants of tall ancestors tend to regress down towards normal average phenomenon also known as regression toward the mean 
for galton regression had only this biological meaning but his work was later extended by udny yule and karl pearson to more general statistical context 
in the work of yule and pearson the joint distribution of the response and explanatory variables is assumed to be gaussian 
this assumption was weakened by fisher in his works of and fisher assumed that the conditional distribution of the response variable is gaussian but the joint distribution need not be 
in this respect fisher assumption is closer to gauss formulation of in the and economists used electromechanical desk calculators to calculate regressions 
before it sometimes took up to hours to receive the result from one regression regression methods continue to be an area of active research 
in recent decades new methods have been developed for robust regression regression involving correlated responses such as time series and growth curves regression in which the predictor independent variable or response variables are curves images graphs or other complex data objects regression methods accommodating various types of missing data nonparametric regression bayesian methods for regression regression in which the predictor variables are measured with error regression with more predictor variables than observations and causal inference with regression 
regression model in practice researchers first select model they would like to estimate and then use their chosen method ordinary least squares to estimate the parameters of that model 
regression models involve the following components the unknown parameters often denoted as scalar or vector the independent variables which are observed in data and are often denoted as vector where denotes row of data 
the dependent variable which are observed in data and often denoted using the scalar the error terms which are not directly observed in data and are often denoted using the scalar in various fields of application different terminologies are used in place of dependent and independent variables 
most regression models propose that is function of and with representing an additive error term that may stand in for un modeled determinants of or random statistical noise the researchers goal is to estimate the function that most closely fits the data 
to carry out regression analysis the form of the function must be specified 
sometimes the form of this function is based on knowledge about the relationship between and that does not rely on the data 
if no such knowledge is available flexible or convenient form for is chosen 
for example simple univariate regression may propose suggesting that the researcher believes to be reasonable approximation for the statistical process generating the data 
once researchers determine their preferred statistical model different forms of regression analysis provide tools to estimate the parameters for example least squares including its most common variant ordinary least squares finds the value of that minimizes the sum of squared errors given regression method will ultimately provide an estimate of usually denoted to distinguish the estimate from the true unknown parameter value that generated the data 
using this estimate the researcher can then use the fitted value for prediction or to assess the accuracy of the model in explaining the data 
whether the researcher is intrinsically interested in the estimate or the predicted value will depend on context and their goals 
as described in ordinary least squares least squares is widely used because the estimated function approximates the conditional expectation 
however alternative variants least absolute deviations or quantile regression are useful when researchers want to model other functions 
it is important to note that there must be sufficient data to estimate regression model 
for example suppose that researcher has access to rows of data with one dependent and two independent variables 
suppose further that the researcher wants to estimate bivariate linear model via least squares if the researcher only has access to data points then they could find infinitely many combinations that explain the data equally well any combination can be chosen that satisfies all of which lead to and are therefore valid solutions that minimize the sum of squared residuals 
to understand why there are infinitely many options note that the system of equations is to be solved for unknowns which makes the system underdetermined 
alternatively one can visualize infinitely many dimensional planes that go through fixed points 
more generally to estimate least squares model with distinct parameters one must have distinct data points 
if then there does not generally exist set of parameters that will perfectly fit the data 
the quantity appears often in regression analysis and is referred to as the degrees of freedom in the model 
moreover to estimate least squares model the independent variables 
must be linearly independent one must not be able to reconstruct any of the independent variables by adding and multiplying the remaining independent variables 
as discussed in ordinary least squares this condition ensures that is an invertible matrix and therefore that unique solution exists 
underlying assumptions by itself regression is simply calculation using the data 
in order to interpret the output of regression as meaningful statistical quantity that measures real world relationships researchers often rely on number of classical assumptions 
these assumptions often include the sample is representative of the population at large 
the independent variables are measured with no error 
deviations from the model have an expected value of zero conditional on covariates the variance of the residuals is constant across observations homoscedasticity 
the residuals are uncorrelated with one another 
mathematically the variance covariance matrix of the errors is diagonal handful of conditions are sufficient for the least squares estimator to possess desirable properties in particular the gauss markov assumptions imply that the parameter estimates will be unbiased consistent and efficient in the class of linear unbiased estimators 
practitioners have developed variety of methods to maintain some or all of these desirable properties in real world settings because these classical assumptions are unlikely to hold exactly 
for example modeling errors in variables can lead to reasonable estimates independent variables are measured with errors 
heteroscedasticity consistent standard errors allow the variance of to change across values of correlated errors that exist within subsets of the data or follow specific patterns can be handled using clustered standard errors geographic weighted regression or newey west standard errors among other techniques 
when rows of data correspond to locations in space the choice of how to model within geographic units can have important consequences 
the subfield of econometrics is largely focused on developing techniques that allow researchers to make reasonable real world conclusions in real world settings where classical assumptions do not hold exactly 
linear regression in linear regression the model specification is that the dependent variable is linear combination of the parameters but need not be linear in the independent variables 
for example in simple linear regression for modeling data points there is one independent variable and two parameters and straight line in multiple linear regression there are several independent variables or functions of independent variables 
adding term in to the preceding regression gives parabola this is still linear regression although the expression on the right hand side is quadratic in the independent variable it is linear in the parameters and in both cases is an error term and the subscript indexes particular observation 
returning our attention to the straight line case given random sample from the population we estimate the population parameters and obtain the sample linear regression model the residual is the difference between the value of the dependent variable predicted by the model and the true value of the dependent variable one method of estimation is ordinary least squares 
this method obtains parameter estimates that minimize the sum of squared residuals ssr minimization of this function results in set of normal equations set of simultaneous linear equations in the parameters which are solved to yield the parameter estimators in the case of simple regression the formulas for the least squares estimates are where is the mean average of the values and is the mean of the values 
under the assumption that the population error term has constant variance the estimate of that variance is given by this is called the mean square error mse of the regression 
the denominator is the sample size reduced by the number of model parameters estimated from the same data for regressors or if an intercept is used 
in this case so the denominator is the standard errors of the parameter estimates are given by under the further assumption that the population error term is normally distributed the researcher can use these estimated standard errors to create confidence intervals and conduct hypothesis tests about the population parameters 
general linear model in the more general multiple regression model there are independent variables where is the th observation on the th independent variable 
if the first independent variable takes the value for all then is called the regression intercept 
the least squares parameter estimates are obtained from normal equations 
the residual can be written as the normal equations are in matrix notation the normal equations are written as where the element of is the element of the column vector is and the element of is thus is is and is the solution is 
diagnostics once regression model has been constructed it may be important to confirm the goodness of fit of the model and the statistical significance of the estimated parameters 
commonly used checks of goodness of fit include the squared analyses of the pattern of residuals and hypothesis testing 
statistical significance can be checked by an test of the overall fit followed by tests of individual parameters 
interpretations of these diagnostic tests rest heavily on the model assumptions 
although examination of the residuals can be used to invalidate model the results of test or test are sometimes more difficult to interpret if the model assumptions are violated 
for example if the error term does not have normal distribution in small samples the estimated parameters will not follow normal distributions and complicate inference 
with relatively large samples however central limit theorem can be invoked such that hypothesis testing may proceed using asymptotic approximations 
limited dependent variables limited dependent variables which are response variables that are categorical variables or are variables constrained to fall only in certain range often arise in econometrics 
the response variable may be non continuous limited to lie on some subset of the real line 
for binary zero or one variables if analysis proceeds with least squares linear regression the model is called the linear probability model 
nonlinear models for binary dependent variables include the probit and logit model 
the multivariate probit model is standard method of estimating joint relationship between several binary dependent variables and some independent variables 
for categorical variables with more than two values there is the multinomial logit 
for ordinal variables with more than two values there are the ordered logit and ordered probit models 
censored regression models may be used when the dependent variable is only sometimes observed and heckman correction type models may be used when the sample is not randomly selected from the population of interest 
an alternative to such procedures is linear regression based on polychoric correlation or polyserial correlations between the categorical variables 
such procedures differ in the assumptions made about the distribution of the variables in the population 
if the variable is positive with low values and represents the repetition of the occurrence of an event then count models like the poisson regression or the negative binomial model may be used 
nonlinear regression when the model function is not linear in the parameters the sum of squares must be minimized by an iterative procedure 
this introduces many complications which are summarized in differences between linear and non linear least squares 
interpolation and extrapolation regression models predict value of the variable given known values of the variables 
prediction within the range of values in the dataset used for model fitting is known informally as interpolation 
prediction outside this range of the data is known as extrapolation 
performing extrapolation relies strongly on the regression assumptions 
the further the extrapolation goes outside the data the more room there is for the model to fail due to differences between the assumptions and the sample data or the true values 
it is generally advised that when performing extrapolation one should accompany the estimated value of the dependent variable with prediction interval that represents the uncertainty 
such intervals tend to expand rapidly as the values of the independent variable moved outside the range covered by the observed data 
for such reasons and others some tend to say that it might be unwise to undertake extrapolation however this does not cover the full set of modeling errors that may be made in particular the assumption of particular form for the relation between and properly conducted regression analysis will include an assessment of how well the assumed form is matched by the observed data but it can only do so within the range of values of the independent variables actually available 
this means that any extrapolation is particularly reliant on the assumptions being made about the structural form of the regression relationship 
best practice advice here is that linear in variables and linear in parameters relationship should not be chosen simply for computational convenience but that all available knowledge should be deployed in constructing regression model 
if this knowledge includes the fact that the dependent variable cannot go outside certain range of values this can be made use of in selecting the model even if the observed dataset has no values particularly near such bounds 
the implications of this step of choosing an appropriate functional form for the regression can be great when extrapolation is considered 
at minimum it can ensure that any extrapolation arising from fitted model is realistic or in accord with what is known 
power and sample size calculations there are no generally agreed methods for relating the number of observations versus the number of independent variables in the model 
one method conjectured by good and hardin is where is the sample size is the number of independent variables and is the number of observations needed to reach the desired precision if the model had only one independent variable 
for example researcher is building linear regression model using dataset that contains patients 
if the researcher decides that five observations are needed to precisely define straight line then the maximum number of independent variables the model can support is because log log 
other methods although the parameters of regression model are usually estimated using the method of least squares other methods which have been used include bayesian methods 
bayesian linear regression percentage regression for situations where reducing percentage errors is deemed more appropriate 
least absolute deviations which is more robust in the presence of outliers leading to quantile regression nonparametric regression requires large number of observations and is computationally intensive scenario optimization leading to interval predictor models distance metric learning which is learned by the search of meaningful distance metric in given input space 
software all major statistical software packages perform least squares regression analysis and inference 
simple linear regression and multiple regression using least squares can be done in some spreadsheet applications and on some calculators 
while many statistical software packages can perform various types of nonparametric and robust regression these methods are less standardized 
different software packages implement different methods and method with given name may be implemented differently in different packages 
specialized regression software has been developed for use in fields such as survey analysis and neuroimaging 
see also references further reading william kruskal and judith tanur ed 
linear hypotheses international encyclopedia of statistics 
free press evan williams regression pp 
analysis of variance pp 
regression and correlation analysis new palgrave dictionary of economics pp 
birkes david and dodge alternative methods of regression 
isbn chatfield calculating interval forecasts journal of business and economic statistics pp 
applied regression analysis rd ed 
applied regression analysis linear models and related methods 
sage hardle applied nonparametric regression isbn meade nigel islam towhidul 
prediction intervals for growth curve forecasts 
sen srivastava regression analysis theory methods and applications springer verlag berlin th printing 
strutz data fitting and uncertainty practical introduction to weighted least squares and beyond 
vieweg teubner isbn stulp freek and olivier sigaud 
many regression algorithms one unified model review 
https doi org neunet 
operations and production systems with multiple objectives 
external links regression analysis encyclopedia of mathematics ems press earliest uses regression basic history and references what is multiple regression used for 
multiple regression regression of weakly correlated data how linear regression mistakes can appear when range is much smaller than range
in cryptography the elliptic curve digital signature algorithm ecdsa offers variant of the digital signature algorithm dsa which uses elliptic curve cryptography
key and signature size as with elliptic curve cryptography in general the bit size of the private key believed to be needed for ecdsa is about twice the size of the security level in bits
for example at security level of bits meaning an attacker requires maximum of about operations to find the private key the size of an ecdsa private key would be bits
on the other hand the signature size is the same for both dsa and ecdsa approximately bits where is the security level measured in bits that is about bits for security level of bits
signature generation algorithm suppose alice wants to send signed message to bob
initially they must agree on the curve parameters curve
in addition to the field and equation of the curve we need base point of prime order on the curve is the multiplicative order of the point the order of the base point must be prime
indeed we assume that every nonzero element of the ring is invertible so that must be field
it implies that must be prime cf
alice creates key pair consisting of private key integer randomly selected in the interval and public key curve point we use to denote elliptic curve point multiplication by scalar
for alice to sign message she follows these steps calculate hash
here hash is cryptographic hash function such as sha with the output converted to an integer
let be the leftmost bits of where is the bit length of the group order note that can be greater than but not longer
select cryptographically secure random integer from
calculate the curve point calculate mod if go back to step calculate mod if go back to step the signature is the pair
and mod is also valid signature
as the standard notes it is not only required for to be secret but it is also crucial to select different for different signatures otherwise the equation in step can be solved for the private key given two signatures and employing the same unknown for different known messages and an attacker can calculate and and since all operations in this paragraph are done modulo the attacker can find
since the attacker can now calculate the private key this implementation failure was used for example to extract the signing key used for the playstation gaming console another way ecdsa signature may leak private keys is when is generated by faulty random number generator
such failure in random number generation caused users of android bitcoin wallet to lose their funds in august to ensure that is unique for each message one may bypass random number generation completely and generate deterministic signatures by deriving from both the message and the private key
signature verification algorithm for bob to authenticate alice signature he must have copy of her public key curve point bob can verify is valid curve point as follows check that is not equal to the identity element and its coordinates are otherwise valid check that lies on the curve check that after that bob follows these steps verify that and are integers in
if not the signature is invalid
calculate hash where hash is the same function used in the signature generation
let be the leftmost bits of calculate mod and mod calculate the curve point if then the signature is invalid
the signature is valid if mod invalid otherwise note that an efficient implementation would compute inverse mod only once
also using shamir trick sum of two scalar multiplications can be calculated faster than two scalar multiplications done independently
correctness of the algorithm it is not immediately obvious why verification even functions correctly
to see why denote as the curve point computed in step of verification from the definition of the public key as because elliptic curve scalar multiplication distributes over addition expanding the definition of and from verification step collecting the common term expanding the definition of from signature step since the inverse of an inverse is the original element and the product of an element inverse and the element is the identity we are left with from the definition of this is verification step this shows only that correctly signed message will verify correctly many other properties are required for secure signature algorithm
public key recovery given message and alice signature on that message bob can potentially recover alice public key verify that and are integers in
if not the signature is invalid
calculate curve point where is one of etc
provided is not too large for field element and is value such that the curve equation is satisfied
note that there may be several curve points satisfying these conditions and each different value results in distinct recovered key
calculate hash where hash is the same function used in the signature generation
let be the leftmost bits of calculate mod and mod calculate the curve point the signature is valid if matches alice public key
the signature is invalid if all the possible points have been tried and none match alice public key note that an invalid signature or signature from different message will result in the recovery of an incorrect public key
the recovery algorithm can only be used to check validity of signature if the signer public key or its hash is known beforehand
correctness of the recovery algorithm start with the definition of from recovery step from the definition from signing step because elliptic curve scalar multiplication distributes over addition expanding the definition of and from recovery step expanding the definition of from signature step since the product of an element inverse and the element is the identity we are left with the first and second terms cancel each other out from the definition of this is alice public key
this shows that correctly signed message will recover the correct public key provided additional information was shared to uniquely calculate curve point from signature value security in december group calling itself fail verflow announced recovery of the ecdsa private key used by sony to sign software for the playstation game console
however this attack only worked because sony did not properly implement the algorithm because was static instead of random
as pointed out in the signature generation algorithm section above this makes solvable rendering the entire algorithm useless on march two researchers published an iacr paper demonstrating that it is possible to retrieve tls private key of server using openssl that authenticates with elliptic curves dsa over binary field via timing attack
the vulnerability was fixed in openssl in august it was revealed that bugs in some implementations of the java class securerandom sometimes generated collisions in the value
this allowed hackers to recover private keys giving them the same control over bitcoin transactions as legitimate keys owners had using the same exploit that was used to reveal the ps signing key on some android app implementations which use java and rely on ecdsa to authenticate transactions this issue can be prevented by an unpredictable generation of deterministic procedure as described by rfc
concerns some concerns expressed about ecdsa political concerns the trustworthiness of nist produced curves being questioned after revelations that the nsa willingly inserts backdoors into software hardware components and published standards were made well known cryptographers have expressed doubts about how the nist curves were designed and voluntary tainting has already been proved in the past
see also the libssh curve introduction
nevertheless proof that the named nist curves exploit rare weakness is missing yet
technical concerns the difficulty of properly implementing the standard its slowness and design flaws which reduce security in insufficiently defensive implementations
implementations below is list of cryptographic libraries that provide support for ecdsa botan bouncy castle cryptlib crypto crypto api linux gnutls libgcrypt libressl mbed tls microsoft cryptoapi openssl wolfcrypt see also eddsa rsa cryptosystem references further reading accredited standards committee asc issues new standard for public key cryptography ecdsa oct source accredited standards committee american national standard public key cryptography for the financial services industry the elliptic curve digital signature algorithm ecdsa november certicom research standards for efficient cryptography sec elliptic curve cryptography version may pez and dahab an overview of elliptic curve cryptography technical report ic state university of campinas daniel bernstein pippenger exponentiation algorithm daniel brown generic groups collision resistance and ecdsa designs codes and cryptography eprint version ian blake gadiel seroussi and nigel smart editors advances in elliptic curve cryptography london mathematical society lecture note series cambridge university press hankerson vanstone menezes
guide to elliptic curve cryptography
external links digital signature standard includes info on ecdsa the elliptic curve digital signature algorithm ecdsa provides an in depth guide on ecdsa
linear combination of atomic orbitals or lcao is quantum superposition of atomic orbitals and technique for calculating molecular orbitals in quantum chemistry
in quantum mechanics electron configurations of atoms are described as wavefunctions
in mathematical sense these wave functions are the basis set of functions the basis functions which describe the electrons of given atom
in chemical reactions orbital wavefunctions are modified
the electron cloud shape is changed according to the type of atoms participating in the chemical bond
it was introduced in by sir john lennard jones with the description of bonding in the diatomic molecules of the first main row of the periodic table but had been used earlier by linus pauling for
mathematical description an initial assumption is that the number of molecular orbitals is equal to the number of atomic orbitals included in the linear expansion
in sense atomic orbitals combine to form molecular orbitals which can be numbered to and which may not all be the same
the expression linear expansion for the th molecular orbital would be or where is molecular orbital represented as the sum of atomic orbitals each multiplied by corresponding coefficient and numbered to represents which atomic orbital is combined in the term
the coefficients are the weights of the contributions of the atomic orbitals to the molecular orbital
the hartree fock method is used to obtain the coefficients of the expansion
the orbitals are thus expressed as linear combinations of basis functions and the basis functions are single electron functions which may or may not be centered on the nuclei of the component atoms of the molecule
in either case the basis functions are usually also referred to as atomic orbitals even though only in the former case this name seems to be adequate
the atomic orbitals used are typically those of hydrogen like atoms since these are known analytically
slater type orbitals but other choices are possible such as the gaussian functions from standard basis sets or the pseudo atomic orbitals from plane wave pseudopotentials
by minimizing the total energy of the system an appropriate set of coefficients of the linear combinations is determined
this quantitative approach is now known as the hartree fock method
however since the development of computational chemistry the lcao method often refers not to an actual optimization of the wave function but to qualitative discussion which is very useful for predicting and rationalizing results obtained via more modern methods
in this case the shape of the molecular orbitals and their respective energies are deduced approximately from comparing the energies of the atomic orbitals of the individual atoms or molecular fragments and applying some recipes known as level repulsion and the like
the graphs that are plotted to make this discussion clearer are called correlation diagrams
the required atomic orbital energies can come from calculations or directly from experiment via koopmans theorem
this is done by using the symmetry of the molecules and orbitals involved in bonding and thus is sometimes called symmetry adapted linear combination salc
the first step in this process is assigning point group to the molecule
each operation in the point group is performed upon the molecule
the number of bonds that are unmoved is the character of that operation
this reducible representation is decomposed into the sum of irreducible representations
these irreducible representations correspond to the symmetry of the orbitals involved
molecular orbital diagrams provide simple qualitative lcao treatment
the ckel method the extended ckel method and the pariser parr pople method provide some quantitative theories
see also quantum chemistry computer programs hartree fock method basis set chemistry tight binding holstein herring method external links lcao chemistry umeche maine edu link references
weighted least squares wls also known as weighted linear regression is generalization of ordinary least squares and linear regression in which knowledge of the variance of observations is incorporated into the regression 
wls is also specialization of generalized least squares 
introduction special case of generalized least squares called weighted least squares can be used when all the off diagonal entries of the covariance matrix of the residuals are null the variances of the observations along the covariance matrix diagonal may still be unequal heteroscedasticity 
the fit of model to data point is measured by its residual defined as the difference between measured value of the dependent variable and the value predicted by the model 
if the errors are uncorrelated and have equal variance then the function is minimised at such that the gauss markov theorem shows that when this is so is best linear unbiased estimator blue 
if however the measurements are uncorrelated but have different uncertainties modified approach might be adopted 
aitken showed that when weighted sum of squared residuals is minimized is the blue if each weight is equal to the reciprocal of the variance of the measurement the gradient equations for this sum of squares are which in linear least squares system give the modified normal equations when the observational errors are uncorrelated and the weight matrix is diagonal these may be written as if the errors are correlated the resulting estimator is the blue if the weight matrix is equal to the inverse of the variance covariance matrix of the observations 
when the errors are uncorrelated it is convenient to simplify the calculations to factor the weight matrix as the normal equations can then be written in the same form as ordinary least squares where we define the following scaled matrix and vector diag diag this is type of whitening transformation the last expression involves an entrywise division 
for non linear least squares systems similar argument shows that the normal equations should be modified as follows 
note that for empirical tests the appropriate is not known for sure and must be estimated 
for this feasible generalized least squares fgls techniques may be used in this case it is specialized for diagonal covariance matrix thus yielding feasible weighted least squares solution 
if the uncertainty of the observations is not known from external sources then the weights could be estimated from the given observations 
this can be useful for example to identify outliers 
after the outliers have been removed from the data set the weights should be reset to one 
motivation in some cases the observations may be weighted for example they may not be equally reliable 
in this case one can minimize the weighted sum of squares where wi is the weight of the ith observation and is the diagonal matrix of such weights 
the weights should ideally be equal to the reciprocal of the variance of the measurement 
this implies that the observations are uncorrelated 
if the observations are correlated the expression applies 
in this case the weight matrix should ideally be equal to the inverse of the variance covariance matrix of the observations 
the normal equations are then this method is used in iteratively reweighted least squares 
parameter errors and correlation the estimated parameter values are linear combinations of the observed values therefore an expression for the estimated variance covariance matrix of the parameter estimates can be obtained by error propagation from the errors in the observations 
let the variance covariance matrix for the observations be denoted by and that of the estimated parameters by 
then when this simplifies to when unit weights are used the identity matrix it is implied that the experimental errors are uncorrelated and all equal where is the priori variance of an observation 
in any case is approximated by the reduced chi squared where is the minimum value of the weighted objective function the denominator is the number of degrees of freedom see effective degrees of freedom for generalizations for the case of correlated observations 
in all cases the variance of the parameter estimate is given by and the covariance between the parameter estimates and is given by the standard deviation is the square root of variance and the correlation coefficient is given by 
these error estimates reflect only random errors in the measurements 
the true uncertainty in the parameters is larger due to the presence of systematic errors which by definition cannot be quantified 
note that even though the observations may be uncorrelated the parameters are typically correlated 
parameter confidence limits it is often assumed for want of any concrete evidence but often appealing to the central limit theorem see normal distribution occurrence and applications that the error on each observation belongs to normal distribution with mean of zero and standard deviation under that assumption the following probabilities can be derived for single scalar parameter estimate in terms of its estimated standard error given here that the interval encompasses the true coefficient value that the interval encompasses the true coefficient value that the interval encompasses the true coefficient valuethe assumption is not unreasonable when if the experimental errors are normally distributed the parameters will belong to student distribution with degrees of freedom 
when student distribution approximates normal distribution 
note however that these confidence limits cannot take systematic error into account 
also parameter errors should be quoted to one significant figure only as they are subject to sampling error when the number of observations is relatively small chebychev inequality can be used for an upper bound on probabilities regardless of any assumptions about the distribution of experimental errors the maximum probabilities that parameter will be more than or standard deviations away from its expectation value are and respectively 
residual values and correlation the residuals are related to the observations by where is the idempotent matrix known as the hat matrix and is the identity matrix 
the variance covariance matrix of the residuals is given by thus the residuals are correlated even if the observations are not 
when the sum of weighted residual values is equal to zero whenever the model function contains constant term 
left multiply the expression for the residuals by xt wt say for example that the first term of the model is constant so that for all in that case it follows that thus in the motivational example above the fact that the sum of residual values is equal to zero is not accidental but is consequence of the presence of the constant term in the model 
if experimental error follows normal distribution then because of the linear relationship between residuals and observations so should residuals but since the observations are only sample of the population of all possible observations the residuals should belong to student distribution 
studentized residuals are useful in making statistical test for an outlier when particular residual appears to be excessively large 
see also iteratively reweighted least squares heteroscedasticity consistent standard errors weighted mean references
dependent and independent variables are variables in mathematical modeling statistical modeling and experimental sciences 
dependent variables receive this name because in an experiment their values are studied under the supposition or demand that they depend by some law or rule by mathematical function on the values of other variables 
independent variables in turn are not seen as depending on any other variable in the scope of the experiment in question 
in this sense some common independent variables are time space density mass fluid flow rate and previous values of some observed value of interest 
human population size to predict future values the dependent variable of the two it is always the dependent variable whose variation is being studied by altering inputs also known as regressors in statistical context 
in an experiment any variable that can be attributed value without attributing value to any other variable is called an independent variable 
models and experiments test the effects that the independent variables have on the dependent variables 
sometimes even if their influence is not of direct interest independent variables may be included for other reasons such as to account for their potential confounding effect 
mathematics in mathematics function is rule for taking an input in the simplest case number or set of numbers and providing an output which may also be number 
symbol that stands for an arbitrary input is called an independent variable while symbol that stands for an arbitrary output is called dependent variable 
the most common symbol for the input is and the most common symbol for the output is the function itself is commonly written it is possible to have multiple independent variables or multiple dependent variables 
for instance in multivariable calculus one often encounters functions of the form where is dependent variable and and are independent variables 
functions with multiple outputs are often referred to as vector valued functions 
modeling in mathematical modeling the dependent variable is studied to see if and how much it varies as the independent variables vary 
in the simple stochastic linear model yi bxi ei the term yi is the ith value of the dependent variable and xi is the ith value of the independent variable 
the term ei is known as the error and contains the variability of the dependent variable not explained by the independent variable 
with multiple independent variables the model is yi bxi bxi bxi ei where is the number of independent variables the linear regression model is now discussed 
to use linear regression scatter plot of data is generated with as the independent variable and as the dependent variable 
this is also called bivariate dataset xi yi 
the simple linear regression model takes the form of yi bxi ui for in this case ui un are independent random variables 
this occurs when the measurements do not influence each other 
through propagation of independence the independence of ui implies independence of yi even though each yi has different expectation value 
each ui has an expectation value of and variance of expectation of yi proof the line of best fit for the bivariate dataset takes the form and is called the regression line 
and correspond to the intercept and slope respectively 
simulation in simulation the dependent variable is changed in response to changes in the independent variables 
statistics in an experiment the variable manipulated by an experimenter is something that is proven to work called an independent variable 
the dependent variable is the event expected to change when the independent variable is manipulated in data mining tools for multivariate statistics and machine learning the dependent variable is assigned role as target variable or in some tools as label attribute while an independent variable may be assigned role as regular variable 
known values for the target variable are provided for the training data set and test data set but should be predicted for other data 
the target variable is used in supervised learning algorithms but not in unsupervised learning 
statistics synonyms depending on the context an independent variable is sometimes called predictor variable regressor covariate manipulated variable explanatory variable exposure variable see reliability theory risk factor see medical statistics feature in machine learning and pattern recognition or input variable 
in econometrics the term control variable is usually used instead of covariate 
explanatory variable is preferred by some authors over independent variable when the quantities treated as independent variables may not be statistically independent or independently manipulable by the researcher 
if the independent variable is referred to as an explanatory variable then the term response variable is preferred by some authors for the dependent variable from the economics community the independent variables are also called exogenous 
depending on the context dependent variable is sometimes called response variable regressand criterion predicted variable measured variable explained variable experimental variable responding variable outcome variable output variable target or label 
in economics endogenous variables are usually referencing the target 
explained variable is preferred by some authors over dependent variable when the quantities treated as dependent variables may not be statistically dependent 
if the dependent variable is referred to as an explained variable then the term predictor variable is preferred by some authors for the independent variable variables may also be referred to by their form continuous or categorical which in turn may be binary dichotomous nominal categorical and ordinal categorical among others 
an example is provided by the analysis of trend in sea level by woodworth 
here the dependent variable and variable of most interest was the annual mean sea level at given location for which series of yearly values were available 
the primary independent variable was time 
use was made of covariate consisting of yearly values of annual mean atmospheric pressure at sea level 
the results showed that inclusion of the covariate allowed improved estimates of the trend against time to be obtained compared to analyses which omitted the covariate 
other variables variable may be thought to alter the dependent or independent variables but may not actually be the focus of the experiment 
so that the variable will be kept constant or monitored to try to minimize its effect on the experiment 
such variables may be designated as either controlled variable control variable or fixed variable 
extraneous variables if included in regression analysis as independent variables may aid researcher with accurate response parameter estimation prediction and goodness of fit but are not of substantive interest to the hypothesis under examination 
for example in study examining the effect of post secondary education on lifetime earnings some extraneous variables might be gender ethnicity social class genetics intelligence age and so forth 
variable is extraneous only when it can be assumed or shown to influence the dependent variable 
if included in regression it can improve the fit of the model 
if it is excluded from the regression and if it has non zero covariance with one or more of the independent variables of interest its omission will bias the regression result for the effect of that independent variable of interest 
this effect is called confounding or omitted variable bias in these situations design changes and or controlling for variable statistical control is necessary 
extraneous variables are often classified into three types subject variables which are the characteristics of the individuals being studied that might affect their actions 
these variables include age gender health status mood background etc 
blocking variables or experimental variables are characteristics of the persons conducting the experiment which might influence how person behaves 
gender the presence of racial discrimination language or other factors may qualify as such variables 
situational variables are features of the environment in which the study or research was conducted which have bearing on the outcome of the experiment in negative way 
included are the air temperature level of activity lighting and time of day in modelling variability that is not covered by the independent variable is designated by and is known as the residual side effect error unexplained share residual variable disturbance or tolerance 
examples effect of fertilizer on plant growths in study measuring the influence of different quantities of fertilizer on plant growth the independent variable would be the amount of fertilizer used 
the dependent variable would be the growth in height or mass of the plant 
the controlled variables would be the type of plant the type of fertilizer the amount of sunlight the plant gets the size of the pots etc effect of drug dosage on symptom severity in study of how different doses of drug affect the severity of symptoms researcher could compare the frequency and intensity of symptoms when different doses are administered 
here the independent variable is the dose and the dependent variable is the frequency intensity of symptoms effect of temperature on pigmentation in measuring the amount of color removed from beetroot samples at different temperatures temperature is the independent variable and amount of pigment removed is the dependent variable effect of sugar added in coffee the taste varies with the amount of sugar added in the coffee 
here the sugar is the independent variable while the taste is the dependent variable 
see also abscissa and ordinate blocking statistics latent variable versus observable variable notes references
database encryption can generally be defined as process that uses an algorithm to transform data stored in database into cipher text that is incomprehensible without first being decrypted
it can therefore be said that the purpose of database encryption is to protect the data stored in database from being accessed by individuals with potentially malicious intentions
the act of encrypting database also reduces the incentive for individuals to hack the aforementioned database as meaningless encrypted data is of little to no use for hackers
there are multiple techniques and technologies available for database encryption the most important of which will be detailed in this article
transparent external database encryption transparent data encryption often abbreviated as tde is used to encrypt an entire database which therefore involves encrypting data at rest
data at rest can generally be defined as inactive data that is not currently being edited or pushed across network
as an example text file stored on computer is at rest until it is opened and edited
data at rest are stored on physical storage media solutions such as tapes or hard disk drives
the act of storing large amounts of sensitive data on physical storage media naturally raises concerns of security and theft
tde ensures that the data on physical storage media cannot be read by malicious individuals that may have the intention to steal them
data that cannot be read is worthless thus reducing the incentive for theft
perhaps the most important strength that is attributed to tde is its transparency
given that tde encrypts all data it can be said that no applications need to be altered in order for tde to run correctly
it is important to note that tde encrypts the entirety of the database as well as backups of the database
the transparent element of tde has to do with the fact that tde encrypts on the page level which essentially means that data is encrypted when stored and decrypted when it is called into the system memory
the contents of the database are encrypted using symmetric key that is often referred to as database encryption key
column level encryption in order to explain column level encryption it is important to outline basic database structure
typical relational database is divided into tables that are divided into columns that each have rows of data
whilst tde usually encrypts an entire database column level encryption allows for individual columns within database to be encrypted
it is important to establish that the granularity of column level encryption causes specific strengths and weaknesses to arise when compared to encrypting an entire database
firstly the ability to encrypt individual columns allows for column level encryption to be significantly more flexible when compared to encryption systems that encrypt an entire database such as tde
secondly it is possible to use an entirely unique and separate encryption key for each column within database
this effectively increases the difficulty of generating rainbow tables which thus implies that the data stored within each column is less likely to be lost or leaked
the main disadvantage associated with column level database encryption is speed or loss thereof
encrypting separate columns with different unique keys in the same database can cause database performance to decrease and additionally also decreases the speed at which the contents of the database can be indexed or searched
field level encryption experimental work is being done on providing database operations like searching or arithmetical operations on encrypted fields without the need to decrypt them
strong encryption is required to be randomized different result must be generated each time
this is known as probabilistic encryption
field level encryption is weaker than randomized encryption but it allows users to test for equality without decrypting the data
filesystem level encryption encrypting file system efs it is important to note that traditional database encryption techniques normally encrypt and decrypt the contents of database
databases are managed by database management systems dbms that run on top of an existing operating system os
this raises potential security concern as an encrypted database may be running on an accessible and potentially vulnerable operating system
efs can encrypt data that is not part of database system which implies that the scope of encryption for efs is much wider when compared to system such as tde that is only capable of encrypting database files
whilst efs does widen the scope of encryption it also decreases database performance and can cause administration issues as system administrators require operating system access to use efs
due to the issues concerning performance efs is not typically used in databasing applications that require frequent database input and output
in order to offset the performance issues it is often recommended that efs systems be used in environments with few users
full disk encryption bitlocker does not have the same performance concerns associated with efs
symmetric and asymmetric database encryption symmetric database encryption symmetric encryption in the context of database encryption involves private key being applied to data that is stored and called from database
this private key alters the data in way that causes it to be unreadable without first being decrypted
data is encrypted when saved and decrypted when opened given that the user knows the private key
thus if the data is to be shared through database the receiving individual must have copy of the secret key used by the sender in order to decrypt and view the data
clear disadvantage related to symmetric encryption is that sensitive data can be leaked if the private key is spread to individuals that should not have access to the data
however given that only one key is involved in the encryption process it can generally be said that speed is an advantage of symmetric encryption
asymmetric database encryption asymmetric encryption expands on symmetric encryption by incorporating two different types of keys into the encryption method private and public keys
public key can be accessed by anyone and is unique to one user whereas private key is secret key that is unique to and only known by one user
in most scenarios the public key is the encryption key whereas the private key is the decryption key
as an example if individual would like to send message to individual using asymmetric encryption he would encrypt the message using individual public key and then send the encrypted version
individual would then be able to decrypt the message using his private key
individual would not be able to decrypt individual message as individual private key is not the same as individual private key
asymmetric encryption is often described as being more secure in comparison to symmetric database encryption given that private keys do not need to be shared as two separate keys handle encryption and decryption processes
for performance reasons asymmetric encryption is used in key management rather than to encrypt the data which is usually done with symmetric encryption
key management the symmetric asymmetric database encryption section introduced the concept of public and private keys with basic examples in which users exchange keys
the act of exchanging keys becomes impractical from logistical point of view when many different individuals need to communicate with each other
in database encryption the system handles the storage and exchange of keys
this process is called key management
if encryption keys are not managed and stored properly highly sensitive data may be leaked
additionally if key management system deletes or loses key the information that was encrypted via said key is essentially rendered lost as well
the complexity of key management logistics is also topic that needs to be taken into consideration
as the number of application that firm uses increases the number of keys that need to be stored and managed increases as well
thus it is necessary to establish way in which keys from all applications can be managed through single channel which is also known as enterprise key management
enterprise key management solutions are sold by great number of suppliers in the technology industry
these systems essentially provide centralised key management solution that allows administrators to manage all keys in system through one hub
thus it can be said that the introduction of enterprise key management solutions has the potential to lessen the risks associated with key management in the context of database encryption as well as to reduce the logistical troubles that arise when many individuals attempt to manually share keys
hashing hashing is used in database systems as method to protect sensitive data such as passwords however it is also used to improve the efficiency of database referencing
inputted data is manipulated by hashing algorithm
the hashing algorithm converts the inputted data into string of fixed length that can then be stored in database
hashing systems have two crucially important characteristics that will now be outlined
firstly hashes are unique and repeatable
as an example running the word cat through the same hashing algorithm multiple times will always yield the same hash however it is extremely difficult to find word that will return the same hash that cat does
secondly hashing algorithms are not reversible
to relate this back to the example provided above it would be nearly impossible to convert the output of the hashing algorithm back to the original input which was cat
in the context of database encryption hashing is often used in password systems
when user first creates their password it is run through hashing algorithm and saved as hash
when the user logs back into the website the password that they enter is run through the hashing algorithm and is then compared to the stored hash
given the fact that hashes are unique if both hashes match then it is said that the user inputted the correct password
one example of popular hash function is sha secure hash algorithm
salting one issue that arises when using hashing for password management in the context of database encryption is the fact that malicious user could potentially use an input to hash table rainbow table for the specific hashing algorithm that the system uses
this would effectively allow the individual to decrypt the hash and thus have access to stored passwords
solution for this issue is to salt the hash
salting is the process of encrypting more than just the password in database
the more information that is added to string that is to be hashed the more difficult it becomes to collate rainbow tables
as an example system may combine user email and password into single hash
this increase in the complexity of hash means that it is far more difficult and thus less likely for rainbow tables to be generated
this naturally implies that the threat of sensitive data loss is minimised through salting hashes
pepper some systems incorporate pepper in addition to salts in their hashing systems
pepper systems are controversial however it is still necessary to explain their use
pepper is value that is added to hashed password that has been salted
this pepper is often unique to one website or service and it is important to note that the same pepper is usually added to all passwords saved in database
in theory the inclusion of peppers in password hashing systems has the potential to decrease the risk of rainbow input hash tables given the system level specificity of peppers however the real world benefits of pepper implementation are highly disputed
application level encryption in application level encryption the process of encrypting data is completed by the application that has been used to generate or modify the data that is to be encrypted
essentially this means that data is encrypted before it is written to the database
this unique approach to encryption allows for the encryption process to be tailored to each user based on the information such as entitlements or roles that the application knows about its users according to eugene pilyankevich application level encryption is becoming good practice for systems with increased security requirements with general drift toward perimeter less and more exposed cloud systems
advantages of application level encryption one of the most important advantages of application level encryption is the fact that application level encryption has the potential to simplify the encryption process used by company
if an application encrypts the data that it writes modifies from database then secondary encryption tool will not need to be integrated into the system
the second main advantage relates to the overarching theme of theft
given that data is encrypted before it is written to the server hacker would need to have access to the database contents as well as the applications that were used to encrypt and decrypt the contents of the database in order to decrypt sensitive data
disadvantages of application level encryption the first important disadvantage of application level encryption is that applications used by firm will need to be modified to encrypt data themselves
this has the potential to consume significant amount of time and other resources
given the nature of opportunity cost firms may not believe that application level encryption is worth the investment
in addition application level encryption may have limiting effect on database performance
if all data on database is encrypted by multitude of different applications then it becomes impossible to index or search data on the database
to ground this in reality in the form of basic example it would be impossible to construct glossary in single language for book that was written in languages
lastly the complexity of key management increases as multiple different applications need to have the authority and access to encrypt data and write it to the database
risks of database encryption when discussing the topic of database encryption it is imperative to be aware of the risks that are involved in the process
the first set of risks are related to key management
if private keys are not managed in an isolated system system administrators with malicious intentions may have the ability to decrypt sensitive data using keys that they have access to
the fundamental principle of keys also gives rise to potentially devastating risk if keys are lost then the encrypted data is essentially lost as well as decryption without keys is almost impossible
in statistics principal component regression pcr is regression analysis technique that is based on principal component analysis pca 
more specifically pcr is used for estimating the unknown regression coefficients in standard linear regression model 
in pcr instead of regressing the dependent variable on the explanatory variables directly the principal components of the explanatory variables are used as regressors 
one typically uses only subset of all the principal components for regression making pcr kind of regularized procedure and also type of shrinkage estimator 
often the principal components with higher variances the ones based on eigenvectors corresponding to the higher eigenvalues of the sample variance covariance matrix of the explanatory variables are selected as regressors 
however for the purpose of predicting the outcome the principal components with low variances may also be important in some cases even more important one major use of pcr lies in overcoming the multicollinearity problem which arises when two or more of the explanatory variables are close to being collinear 
pcr can aptly deal with such situations by excluding some of the low variance principal components in the regression step 
in addition by usually regressing on only subset of all the principal components pcr can result in dimension reduction through substantially lowering the effective number of parameters characterizing the underlying model 
this can be particularly useful in settings with high dimensional covariates 
also through appropriate selection of the principal components to be used for regression pcr can lead to efficient prediction of the outcome based on the assumed model 
the principle the pcr method may be broadly divided into three major steps perform pca on the observed data matrix for the explanatory variables to obtain the principal components and then usually select subset based on some appropriate criteria of the principal components so obtained for further use 
now regress the observed vector of outcomes on the selected principal components as covariates using ordinary least squares regression linear regression to get vector of estimated regression coefficients with dimension equal to the number of selected principal components now transform this vector back to the scale of the actual covariates using the selected pca loadings the eigenvectors corresponding to the selected principal components to get the final pcr estimator with dimension equal to the total number of covariates for estimating the regression coefficients characterizing the original model 
details of the method data representation let denote the vector of observed outcomes and denote the corresponding data matrix of observed covariates where and denote the size of the observed sample and the number of covariates respectively with each of the rows of denotes one set of observations for the dimensional covariate and the respective entry of denotes the corresponding observed outcome 
data pre processing assume that and each of the columns of have already been centered so that all of them have zero empirical means 
this centering step is crucial at least for the columns of since pcr involves the use of pca on and pca is sensitive to centering of the data 
underlying model following centering the standard gauss markov linear regression model for on can be represented as where denotes the unknown parameter vector of regression coefficients and denotes the vector of random errors with and var for some unknown variance parameter objective the primary goal is to obtain an efficient estimator for the parameter based on the data 
one frequently used approach for this is ordinary least squares regression which assuming is full column rank gives the unbiased estimator of pcr is another technique that may be used for the same purpose of estimating pca step pcr starts by performing pca on the centered data matrix for this let denote the singular value decomposition of where diag with denoting the non negative singular values of while the columns of and are both orthonormal sets of vectors denoting the left and right singular vectors of respectively 
the principal components gives spectral decomposition of where diag diag with denoting the non negative eigenvalues also known as the principal values of while the columns of denote the corresponding orthonormal set of eigenvectors 
then and respectively denote the principal component and the principal component direction or pca loading corresponding to the th largest principal value for each 
derived covariates for any let denote the matrix with orthonormal columns consisting of the first columns of let denote the matrix having the first principal components as its columns 
may be viewed as the data matrix obtained by using the transformed covariates instead of using the original covariates the pcr estimator let denote the vector of estimated regression coefficients obtained by ordinary least squares regression of the response vector on the data matrix then for any the final pcr estimator of based on using the first principal components is given by fundamental characteristics and applications of the pcr estimator two basic properties the fitting process for obtaining the pcr estimator involves regressing the response vector on the derived data matrix which has orthogonal columns for any since the principal components are mutually orthogonal to each other 
thus in the regression step performing multiple linear regression jointly on the selected principal components as covariates is equivalent to carrying out independent simple linear regressions or univariate regressions separately on each of the selected principal components as covariate 
when all the principal components are selected for regression so that then the pcr estimator is equivalent to the ordinary least squares estimator 
thus this is easily seen from the fact that and also observing that is an orthogonal matrix 
variance reduction for any the variance of is given by var diag in particular var var hence for all we have var var thus for all we have var var where indicates that square symmetric matrix is non negative definite 
consequently any given linear form of the pcr estimator has lower variance compared to that of the same linear form of the ordinary least squares estimator 
addressing multicollinearity under multicollinearity two or more of the covariates are highly correlated so that one can be linearly predicted from the others with non trivial degree of accuracy 
consequently the columns of the data matrix that correspond to the observations for these covariates tend to become linearly dependent and therefore tends to become rank deficient losing its full column rank structure 
more quantitatively one or more of the smaller eigenvalues of get very close or become exactly equal to under such situations 
the variance expressions above indicate that these small eigenvalues have the maximum inflation effect on the variance of the least squares estimator thereby destabilizing the estimator significantly when they are close to this issue can be effectively addressed through using pcr estimator obtained by excluding the principal components corresponding to these small eigenvalues 
dimension reduction pcr may also be used for performing dimension reduction 
to see this let denote any matrix having orthonormal columns for any 
suppose now that we want to approximate each of the covariate observations through the rank linear transformation for some 
then it can be shown that is minimized at the matrix with the first principal component directions as columns and the corresponding dimensional derived covariates 
thus the dimensional principal components provide the best linear approximation of rank to the observed data matrix the corresponding reconstruction error is given by thus any potential dimension reduction may be achieved by choosing the number of principal components to be used through appropriate thresholding on the cumulative sum of the eigenvalues of since the smaller eigenvalues do not contribute significantly to the cumulative sum the corresponding principal components may be continued to be dropped as long as the desired threshold limit is not exceeded 
the same criteria may also be used for addressing the multicollinearity issue whereby the principal components corresponding to the smaller eigenvalues may be ignored as long as the threshold limit is maintained 
regularization effect since the pcr estimator typically uses only subset of all the principal components for regression it can be viewed as some sort of regularized procedure 
more specifically for any the pcr estimator denotes the regularized solution to the following constrained minimization problem min subject to 
the constraint may be equivalently written as where 
thus when only proper subset of all the principal components are selected for regression the pcr estimator so obtained is based on hard form of regularization that constrains the resulting solution to the column space of the selected principal component directions and consequently restricts it to be orthogonal to the excluded directions 
optimality of pcr among class of regularized estimators given the constrained minimization problem as defined above consider the following generalized version of it min subject to where denotes any full column rank matrix of order with let denote the corresponding solution 
thus arg min subject to then the optimal choice of the restriction matrix for which the corresponding estimator achieves the minimum prediction error is given by where diag 
quite clearly the resulting optimal estimator is then simply given by the pcr estimator based on the first principal components 
efficiency since the ordinary least squares estimator is unbiased for we have var mse where mse denotes the mean squared error 
now if for some we additionally have then the corresponding is also unbiased for and therefore var mse 
we have already seen that var var which then implies mse mse for that particular thus in that case the corresponding would be more efficient estimator of compared to based on using the mean squared error as the performance criteria 
in addition any given linear form of the corresponding would also have lower mean squared error compared to that of the same linear form of now suppose that for given then the corresponding is biased for however since var var it is still possible that mse mse especially if is such that the excluded principal components correspond to the smaller eigenvalues thereby resulting in lower bias 
in order to ensure efficient estimation and prediction performance of pcr as an estimator of park proposes the following guideline for selecting the principal components to be used for regression drop the principal component if and only if practical implementation of this guideline of course requires estimates for the unknown model parameters and in general they may be estimated using the unrestricted least squares estimates obtained from the original full model 
park however provides slightly modified set of estimates that may be better suited for this purpose unlike the criteria based on the cumulative sum of the eigenvalues of which is probably more suited for addressing the multicollinearity problem and for performing dimension reduction the above criteria actually attempts to improve the prediction and estimation efficiency of the pcr estimator by involving both the outcome as well as the covariates in the process of selecting the principal components to be used in the regression step 
alternative approaches with similar goals include selection of the principal components based on cross validation or the mallow cp criteria 
often the principal components are also selected based on their degree of association with the outcome 
shrinkage effect of pcr in general pcr is essentially shrinkage estimator that usually retains the high variance principal components corresponding to the higher eigenvalues of as covariates in the model and discards the remaining low variance components corresponding to the lower eigenvalues of 
thus it exerts discrete shrinkage effect on the low variance components nullifying their contribution completely in the original model 
in contrast the ridge regression estimator exerts smooth shrinkage effect through the regularization parameter or the tuning parameter inherently involved in its construction 
while it does not completely discard any of the components it exerts shrinkage effect over all of them in continuous manner so that the extent of shrinkage is higher for the low variance components and lower for the high variance components 
frank and friedman conclude that for the purpose of prediction itself the ridge estimator owing to its smooth shrinkage effect is perhaps better choice compared to the pcr estimator having discrete shrinkage effect 
in addition the principal components are obtained from the eigen decomposition of that involves the observations for the explanatory variables only 
therefore the resulting pcr estimator obtained from using these principal components as covariates need not necessarily have satisfactory predictive performance for the outcome 
somewhat similar estimator that tries to address this issue through its very construction is the partial least squares pls estimator 
similar to pcr pls also uses derived covariates of lower dimensions 
however unlike pcr the derived covariates for pls are obtained based on using both the outcome as well as the covariates 
while pcr seeks the high variance directions in the space of the covariates pls seeks the directions in the covariate space that are most useful for the prediction of the outcome 
variant of the classical pcr known as the supervised pcr was proposed 
in spirit similar to that of pls it attempts at obtaining derived covariates of lower dimensions based on criterion that involves both the outcome as well as the covariates 
the method starts by performing set of simple linear regressions or univariate regressions wherein the outcome vector is regressed separately on each of the covariates taken one at time 
then for some the first covariates that turn out to be the most correlated with the outcome based on the degree of significance of the corresponding estimated regression coefficients are selected for further use 
conventional pcr as described earlier is then performed but now it is based on only the data matrix corresponding to the observations for the selected covariates 
the number of covariates used and the subsequent number of principal components used are usually selected by cross validation 
generalization to kernel settings the classical pcr method as described above is based on classical pca and considers linear regression model for predicting the outcome based on the covariates 
however it can be easily generalized to kernel machine setting whereby the regression function need not necessarily be linear in the covariates but instead it can belong to the reproducing kernel hilbert space associated with any arbitrary possibly non linear symmetric positive definite kernel 
the linear regression model turns out to be special case of this setting when the kernel function is chosen to be the linear kernel 
in general under the kernel machine setting the vector of covariates is first mapped into high dimensional potentially infinite dimensional feature space characterized by the kernel function chosen 
the mapping so obtained is known as the feature map and each of its coordinates also known as the feature elements corresponds to one feature may be linear or non linear of the covariates 
the regression function is then assumed to be linear combination of these feature elements 
thus the underlying regression model in the kernel machine setting is essentially linear regression model with the understanding that instead of the original set of covariates the predictors are now given by the vector potentially infinite dimensional of feature elements obtained by transforming the actual covariates using the feature map 
however the kernel trick actually enables us to operate in the feature space without ever explicitly computing the feature map 
it turns out that it is only sufficient to compute the pairwise inner products among the feature maps for the observed covariate vectors and these inner products are simply given by the values of the kernel function evaluated at the corresponding pairs of covariate vectors 
the pairwise inner products so obtained may therefore be represented in the form of symmetric non negative definite matrix also known as the kernel matrix 
pcr in the kernel machine setting can now be implemented by first appropriately centering this kernel matrix say with respect to the feature space and then performing kernel pca on the centered kernel matrix say whereby an eigendecomposition of is obtained 
kernel pcr then proceeds by usually selecting subset of all the eigenvectors so obtained and then performing standard linear regression of the outcome vector on these selected eigenvectors 
the eigenvectors to be used for regression are usually selected using cross validation 
the estimated regression coefficients having the same dimension as the number of selected eigenvectors along with the corresponding selected eigenvectors are then used for predicting the outcome for future observation 
in machine learning this technique is also known as spectral regression 
clearly kernel pcr has discrete shrinkage effect on the eigenvectors of quite similar to the discrete shrinkage effect of classical pcr on the principal components as discussed earlier 
however the feature map associated with the chosen kernel could potentially be infinite dimensional and hence the corresponding principal components and principal component directions could be infinite dimensional as well 
therefore these quantities are often practically intractable under the kernel machine setting 
kernel pcr essentially works around this problem by considering an equivalent dual formulation based on using the spectral decomposition of the associated kernel matrix 
under the linear regression model which corresponds to choosing the kernel function as the linear kernel this amounts to considering spectral decomposition of the corresponding kernel matrix and then regressing the outcome vector on selected subset of the eigenvectors of so obtained 
it can be easily shown that this is the same as regressing the outcome vector on the corresponding principal components which are finite dimensional in this case as defined in the context of the classical pcr 
thus for the linear kernel the kernel pcr based on dual formulation is exactly equivalent to the classical pcr based on primal formulation 
however for arbitrary and possibly non linear kernels this primal formulation may become intractable owing to the infinite dimensionality of the associated feature map 
thus classical pcr becomes practically infeasible in that case but kernel pcr based on the dual formulation still remains valid and computationally scalable 
see also principal component analysis partial least squares regression ridge regression canonical correlation deming regression total sum of squares references further reading amemiya takeshi
in theoretical computer science and cryptography trapdoor function is function that is easy to compute in one direction yet difficult to compute in the opposite direction finding its inverse without special information called the trapdoor
trapdoor functions are special case of one way functions and are widely used in public key cryptography in mathematical terms if is trapdoor function then there exists some secret information such that given and it is easy to compute consider padlock and its key
it is trivial to change the padlock from open to closed without using the key by pushing the shackle into the lock mechanism
opening the padlock easily however requires the key to be used
here the key is the trapdoor and the padlock is the trapdoor function
an example of simple mathematical trapdoor is is the product of two prime numbers
what are those numbers
typical brute force solution would be to try dividing by several prime numbers until finding the answer
however if one is told that is one of the numbers one can find the answer by entering into any calculator
this example is not sturdy trapdoor function modern computers can guess all of the possible answers within second but this sample problem could be improved by using the product of two much larger primes
trapdoor functions came to prominence in cryptography in the mid with the publication of asymmetric or public key encryption techniques by diffie hellman and merkle
indeed diffie hellman coined the term
several function classes had been proposed and it soon became obvious that trapdoor functions are harder to find than was initially thought
for example an early suggestion was to use schemes based on the subset sum problem
this turned out rather quickly to be unsuitable
as of the best known trapdoor function family candidates are the rsa and rabin families of functions
both are written as exponentiation modulo composite number and both are related to the problem of prime factorization
functions related to the hardness of the discrete logarithm problem either modulo prime or in group defined over an elliptic curve are not known to be trapdoor functions because there is no known trapdoor information about the group that enables the efficient computation of discrete logarithms
trapdoor in cryptography has the very specific aforementioned meaning and is not to be confused with backdoor these are frequently used interchangeably which is incorrect
backdoor is deliberate mechanism that is added to cryptographic algorithm key pair generation algorithm digital signing algorithm etc
or operating system for example that permits one or more unauthorized parties to bypass or subvert the security of the system in some fashion
definition trapdoor function is collection of one way functions satisfying the following conditions there exists probabilistic polynomial time ppt sampling algorithm gen
gen tk with satisfies tk in which is some polynomial
each tk is called the trapdoor corresponding to each trapdoor can be efficiently sampled
given input there also exists ppt algorithm that outputs dk
that is each dk can be efficiently sampled
for any there exists ppt algorithm that correctly computes fk
for any there exists ppt algorithm
for any dk let fk tk and then we have fk fk
that is given trapdoor it is easy to invert
for any without trapdoor tk for any ppt algorithm the probability to correctly invert fk given fk find pre image such that fk fk is negligible if each function in the collection above is one way permutation then the collection is also called trapdoor permutation
examples in the following two examples we always assume it is difficult to factorize large composite number see integer factorization
rsa assumption in this example the inverse of modulo euler totient function of is the trapdoor mod if the factorization of is known then can be computed
with this the inverse of can be computed mod and then given we can find mod mod mod its hardness follows from the rsa assumption
rabin quadratic residue assumption let be large composite number such that where and are large primes such that mod mod and kept confidential to the adversary
the problem is to compute given such that mod
the trapdoor is the factorization of with the trapdoor the solutions of can be given as where mod mod mod mod mod mod
see chinese remainder theorem for more details
note that given primes and we can find mod and mod
here the conditions mod and mod guarantee that the solutions and can be well defined
see also one way function notes references diffie hellman new directions in cryptography pdf ieee transactions on information theory citeseerx doi tit pass rafael course in cryptography pdf retrieved november goldwasser shafi lecture notes on cryptography pdf retrieved november ostrovsky rafail foundations of cryptography pdf retrieved november dodis yevgeniy introduction to cryptography lecture notes fall retrieved december lindell yehuda foundations of cryptography pdf retrieved december
the circle of fifths text table shows the number of flats or sharps in each of the diatonic musical scales and keys 
both major and minor keys have no flats or sharps 
in the table minor keys are written with lowercase letters for brevity 
however in common guitar tabs notation minor key is designated with lowercase 
for example minor is am and sharp minor is 
the small interval between equivalent notes such as sharp and flat is the pythagorean comma 
minor scales start with major scales start with 
see also circle of fifths key signature musical notation notes
the axolotl from classical nahuatl tl lo listen ambystoma mexicanum is paedomorphic salamander closely related to the tiger salamander 
axolotls are unusual among amphibians in that they reach adulthood without undergoing metamorphosis 
instead of taking to the land adults remain aquatic and gilled 
the species was originally found in several lakes underlying what is now mexico city such as lake xochimilco and lake chalco 
these lakes were drained by spanish settlers after the conquest of the aztec empire leading to the destruction of much of the axolotl natural habitat 
axolotls should not be confused with the larval stage of the closely related tiger salamander tigrinum which are widespread in much of north america and occasionally become paedomorphic 
neither should they be confused with mudpuppies necturus spp 
fully aquatic salamanders from different family that are not closely related to the axolotl but bear superficial resemblance as of wild axolotls were near extinction due to urbanization in mexico city and consequent water pollution as well as the introduction of invasive species such as tilapia and perch 
they are listed as critically endangered in the wild with decreasing population of around to adult individuals by the international union for conservation of nature and natural resources iucn and are listed under appendix ii of the convention on international trade in endangered species cites 
axolotls are used extensively in scientific research due to their ability to regenerate limbs gills and parts of their eyes and brains 
axolotls were also sold as food in mexican markets and were staple in the aztec diet 
description sexually mature adult axolotl at age months ranges in length from to cm to in although size close to cm in is most common and greater than cm in is rare 
axolotls possess features typical of salamander larvae including external gills and caudal fin extending from behind the head to the vent 
external gills are usually lost when salamander species mature into adulthood although the axolotl maintains this feature 
this is due to their neoteny evolution where axolotls are much more aquatic than other salamander species their heads are wide and their eyes are lidless 
their limbs are underdeveloped and possess long thin digits 
males are identified by their swollen cloacae lined with papillae while females are noticeable for their wider bodies full of eggs 
three pairs of external gill stalks rami originate behind their heads and are used to move oxygenated water 
the external gill rami are lined with filaments fimbriae to increase surface area for gas exchange 
four gill slits lined with gill rakers are hidden underneath the external gills which prevent food from entering and allow particles to filter through 
axolotls have barely visible vestigial teeth which develop during metamorphosis 
the primary method of feeding is by suction during which their rakers interlock to close the gill slits 
external gills are used for respiration although buccal pumping gulping air from the surface may also be used to provide oxygen to their lungs 
buccal pumping can occur in two stroke manner that pumps air from the mouth to the lungs and with four stroke that reverses this pathway with compression forces 
axolotls have four pigmentation genes when mutated they create different color variants 
the normal wild type animal is brown tan with gold speckles and an olive undertone 
the five more common mutant colors are leucistic pale pink with black eyes golden albino golden with gold eyes xanthic grey with black eyes albino pale pink white with red eyes which is more common in axolotls than some other creatures and melanoid all black dark blue with no gold speckling or olive tone 
in addition there is wide individual variability in the size frequency and intensity of the gold speckling and at least one variant that develops black and white piebald appearance on reaching maturity 
because pet breeders frequently cross the variant colors double homozygous mutants are common in the pet trade especially white pink animals with pink eyes that are double homozygous mutants for both the albino and leucistic trait 
axolotls also have some limited ability to alter their color to provide better camouflage by changing the relative size and thickness of their melanophores 
habitat and ecology the axolotl is native only to the freshwater of lake xochimilco and lake chalco in the valley of mexico 
lake chalco no longer exists having been drained as flood control measure and lake xochimilco remains remnant of its former self existing mainly as canals 
the water temperature in xochimilco rarely rises above although it may fall to in the winter and perhaps lower surveys in and found and axolotls per square kilometer in its lake xochimilco habitat respectively 
four month long search in however turned up no surviving individuals in the wild 
just month later two wild ones were spotted in network of canals leading from xochimilco the wild population has been put under heavy pressure by the growth of mexico city 
the axolotl is currently on the international union for conservation of nature annual red list of threatened species 
non native fish such as african tilapia and asian carp have also recently been introduced to the waters 
these new fish have been eating the axolotls young as well as their primary source of food axolotls are members of the tiger salamander or ambystoma tigrinum species complex along with all other mexican species of ambystoma 
their habitat is like that of most neotenic species high altitude body of water surrounded by risky terrestrial environment 
these conditions are thought to favor neoteny 
however terrestrial population of mexican tiger salamanders occupies and breeds in the axolotl habitat the axolotl is carnivorous consuming small prey such as mollusks worms insects other arthropods and small fish in the wild 
axolotls locate food by smell and will snap at any potential meal sucking the food into their stomachs with vacuum force 
use as model organism today the axolotl is still used in research as model organism and large numbers are bred in captivity 
they are especially easy to breed compared to other salamanders in their family which are rarely captive bred due to the demands of terrestrial life 
one attractive feature for research is the large and easily manipulated embryo which allows viewing of the full development of vertebrate 
axolotls are used in heart defect studies due to the presence of mutant gene that causes heart failure in embryos 
since the embryos survive almost to hatching with no heart function the defect is very observable 
the axolotl is also considered an ideal animal model for the study of neural tube closure due to the similarities between human and axolotl neural plate and tube formation the axolotl neural tube unlike the frog is not hidden under layer of superficial epithelium 
there are also mutations affecting other organ systems some of which are not well characterized and others that are 
the genetics of the color variants of the axolotl have also been widely studied 
regeneration the feature of the axolotl that attracts most attention is its healing ability the axolotl does not heal by scarring and is capable of the regeneration of entire lost appendages in period of months and in certain cases more vital structures such as tail limb central nervous system and tissues of the eye and heart 
they can even restore less vital parts of their brains 
they can also readily accept transplants from other individuals including eyes and parts of the brain restoring these alien organs to full functionality 
in some cases axolotls have been known to repair damaged limb as well as regenerating an additional one ending up with an extra appendage that makes them attractive to pet owners as novelty 
in metamorphosed individuals however the ability to regenerate is greatly diminished 
the axolotl is therefore used as model for the development of limbs in vertebrates 
there are three basic requirements for regeneration of the limb the wound epithelium nerve signaling and the presence of cells from the different limb axes 
wound epidermis is quickly formed by the cells to cover up the site of the wound 
in the following days the cells of the wound epidermis divide and grow quickly forming blastema which means the wound is ready to heal and undergo patterning to form the new limb 
it is believed that during limb generation axolotls have different system to regulate their internal macrophage level and suppress inflammation as scarring prevents proper healing and regeneration 
however this belief has been questioned by other studies 
axolotl regenerative properties leave the species as the perfect model to study the process of stem cells and its own neoteny feature 
current research can record specific examples of these regenerative properties through tracking cell fates and behaviors lineage tracing skin triploid cell grafts pigmentation imaging electroporation tissue clearing and lineage tracing from dye labeling 
the newer technologies of germline modification and transgenesis are better suited for live imaging the regenerative processes that occur for axolotls 
genome the billion base pair long sequence of the axolotl genome was published in and was the largest animal genome completed at the time 
it revealed species specific genetic pathways that may be responsible for limb regeneration 
although the axolotl genome is about times as large as the human genome it encodes similar number of proteins namely the human genome encodes about proteins 
the size difference is mostly explained by large fraction of repetitive sequences but such repeated elements also contribute to increased median intron sizes bp which are and times that observed in human bp mouse bp and tibetan frog bp respectively 
neoteny when most amphibians are young they live in water and they use gills that can breathe in the water 
when they become adults they go through process called metamorphosis in which they lose their gills and start living on land 
however the axolotl is unusual in that it has lack of thyroid stimulating hormone which is needed for the thyroid to produce thyroxine in order for the axolotl to go through metamorphosis therefore it keeps its gills and lives in water all its life even after it becomes an adult and is able to reproduce 
its body has the capacity to go through metamorphosis if given the necessary hormone but axolotls do not produce it and must be exposed to it from an external source after which an axolotl undergoes an artificially induced metamorphosis and begins living on land 
one method of artificial metamorphosis induction is through an injection of iodine which is used in the production of thyroid hormones 
an axolotl undergoing metamorphosis experiences number of physiological changes that help them adapt to life on land 
these include increased muscle tone in limbs the absorption of gills and fins into the body the development of eyelids and reduction in the skin permeability to water allowing the axolotl to stay more easily hydrated when on land 
the lungs of an axolotl though present alongside gills after reaching non metamorphosed adulthood develop further during metamorphosis an axolotl that has gone through metamorphosis resembles an adult plateau tiger salamander though the axolotl differs in its longer toes 
the process of artificially inducing metamorphosis can often result in death during or even following successful attempt and so casual hobbyists are generally discouraged from attempting to induce metamorphosis in pet axolotls neoteny is the term for reaching sexual maturity without undergoing metamorphosis 
many other species within the axolotl genus are also either entirely neotenic or have neotenic populations 
sirens and necturus are other neotenic salamanders although unlike axolotls they cannot be induced to metamorphose by an injection of iodine or thyroxine hormone 
the genes responsible for neoteny in laboratory animals may have been identified however they are not linked in wild populations suggesting artificial selection is the cause of complete neoteny in laboratory and pet axolotls six adult axolotls including leucistic specimen were shipped from mexico city to the jardin des plantes in paris in unaware of their neoteny auguste dum ril was surprised when instead of the axolotl he found in the vivarium new species similar to the salamander 
this discovery was the starting point of research about neoteny 
it is not certain that ambystoma velasci specimens were not included in the original shipment 
vilem laufberger in prague used thyroid hormone injections to induce an axolotl to grow into terrestrial adult salamander 
the experiment was repeated by englishman julian huxley who was unaware the experiment had already been done using ground thyroids 
since then experiments have been done often with injections of iodine or various thyroid hormones used to induce metamorphosis neoteny has been observed in all salamander families in which it seems to be survival mechanism in aquatic environments only of mountain and hill with little food and in particular with little iodine 
in this way salamanders can reproduce and survive in the form of smaller larval stage which is aquatic and requires lower quality and quantity of food compared to the big adult which is terrestrial 
if the salamander larvae ingest sufficient amount of iodine directly or indirectly through cannibalism they quickly begin metamorphosis and transform into bigger terrestrial adults with higher dietary requirements 
in fact in some high mountain lakes there live dwarf forms of salmonids that are caused by deficiencies in food and in particular iodine which causes cretinism and dwarfism due to hypothyroidism as it does in humans 
captive care the axolotl is popular exotic pet like its relative the tiger salamander ambystoma tigrinum 
as for all poikilothermic organisms lower temperatures result in slower metabolism and very unhealthily reduced appetite 
temperatures at approximately to are suggested for captive axolotls to ensure sufficient food intake stress resulting from more than day exposure to lower temperatures may quickly lead to disease and death and temperatures higher than may lead to metabolic rate increase also causing stress and eventually death 
chlorine commonly added to tapwater is harmful to axolotls 
single axolotl typically requires litre us gallon tank 
axolotls spend the majority of the time at the bottom of the tank 
salts such as holtfreter solution are often added to the water to prevent infection in captivity axolotls eat variety of readily available foods including trout and salmon pellets frozen or live bloodworms earthworms and waxworms 
axolotls can also eat feeder fish but care should be taken as fish may contain parasites substrates are another important consideration for captive axolotls as axolotls like other amphibians and reptiles tend to ingest bedding material together with food and are commonly prone to gastrointestinal obstruction and foreign body ingestion 
some common substrates used for animal enclosures can be harmful for amphibians and reptiles 
gravel common in aquarium use should not be used and is recommended that any sand consists of smooth particles with grain size of under mm 
one guide to axolotl care for laboratories notes that bowel obstructions are common cause of death and recommends that no items with diameter below cm or approximately the size of the animal head should be available to the animal there is some evidence that axolotls might seek out appropriately sized gravel for use as gastroliths based on experiments conducted at the university of manitoba axolotl colony but these studies are outdated and not conclusive 
as there is no conclusive evidence pointing to gastrolith use gravel should be avoided due to the high risk of impaction 
cultural significance the species is named after the aztec deity xolotl who transformed himself into an axolotl 
they continue to play an outsized cultural role in mexico and have appeared in cartoons and murals in it was announced that the axolotl will be featured on the new design for mexico peso banknote along with images of maize and chinampas 
see also mudpuppies olm texas salamander texas blind salamander lake patzcuaro salamander barred tiger salamander amphibious fish handfish regenerative biomedicine references external links ambystomatidae at curlie follow the eggs hatchlings and juveniles mating dance and laying eggs follow the eggs and hatchlings nd batch indiana axolotl colony university of ky axolotl colony mystical amphibian venerated by aztecs nears extinction the animal that everywhere and nowhere axolotl 
the tao of axolotl thetolteciching com on folklore
in western music the adjectives major and minor may describe chord scale or key 
as such composition movement section or phrase may be referred to by its key including whether that key is major or minor 
intervals some intervals may be referred to as major and minor 
major interval is one semitone larger than minor interval 
the words perfect diminished and augmented are also used to describe the quality of an interval 
only the intervals of second third sixth and seventh and the compound intervals based on them may be major or minor or rarely diminished or augmented 
unisons fourths fifths and octaves and their compound interval must be perfect or rarely diminished or augmented 
in western music minor chord sounds darker than major chord 
scales and chords the other uses of major and minor generally refer to scales and chords that contain major third or minor third respectively 
major scale is scale in which the third scale degree the mediant is major third above the tonic note 
in minor scale the third degree is minor third above the tonic 
similarly in major triad or major seventh chord the third is major third above the chord root 
in minor triad or minor seventh chord the third is minor third above the root 
keys the hallmark that distinguishes major keys from minor is whether the third scale degree is major or minor 
as musicologist roger kamien explains the crucial difference is that in the minor scale there is only half step between nd and rd note and between th and th note as compared to the major scales where the difference between rd and th note and between th and th note is half step 
this alteration in the third degree greatly changes the mood of the music and music based on minor scales tends to be considered to sound serious or melancholic minor keys are sometimes said to have more interesting possibly darker sound than plain major scales 
harry partch considers minor as the immutable faculty of ratios which in turn represent an immutable faculty of the human ear 
the minor key and scale are also considered less justifiable than the major with paul hindemith calling it clouding of major and moritz hauptmann calling it falsehood of the major changes of mode which involve the alteration of the third and mode mixture are often analyzed as minor changes unless structurally supported because the root and overall key and tonality remain unchanged 
this is in contrast with for instance transposition 
transposition is done by moving all intervals up or down certain constant interval and does change the key but not the mode which requires the alteration of intervals 
the use of triads only available in the minor mode such as the use of major in major is relatively decorative chromaticism considered to add color and weaken the sense of key without entirely destroying or losing it 
intonation and tuning musical tuning of intervals is expressed by the ratio between the pitches frequencies 
simple fractions can sound more harmonious than complex fractions for instance an octave is simple ratio and fifth is the relatively simple ratio 
the table below gives approximations of scale to ratios that are rounded to be as simple as possible 
in just intonation minor chord is often but not exclusively tuned in the frequency ratio play 
in tone equal temperament tet which is now the most common tuning system in the west minor chord has semitones between the root and third between the third and fifth and between the root and fifth 
in tet the perfect fifth cents is only about two cents narrower than the just tuned perfect fifth cents but the minor third cents is noticeably about cents narrower than the just minor third cents 
moreover the minor third cents more closely approximates the limit limit music minor third play cents the nineteenth harmonic with about two cents error alexander ellis proposes that the conflict between mathematicians and physicists on one hand and practicing musicians on the other regarding the supposed inferiority of the minor chord and scale to the major may be explained due to physicists comparison of just minor and major triads in which case minor comes out the loser versus the musicians comparison of the equal tempered triads in which case minor comes out the winner since the et major third is about cents sharp from the just major third cents but just about four cents narrower than the limit major third cents while the et minor third closely approximates the minor third which many find pleasing 
advanced theory in the neo riemannian theory the minor mode is considered the inverse of the major mode an upside down major scale based on theoretical undertones rather than actual overtones harmonics see also utonality 
the root of the minor triad is thus considered the top of the fifth which in the united states is called the fifth 
so in minor the tonic is actually and the leading tone is half step rather than in major the root being and the leading tone half step 
also since all chords are analyzed as having tonic subdominant or dominant function with for instance in minor being considered the tonic parallel us relative tp the use of minor mode root chord progressions in major such as major major major is analyzed as sp dp the minor subdominant parallel see parallel chord the minor dominant parallel and the major tonic 
see also gypsy scale list of major minor compositions music written in all major and or minor keys otonality and utonality references
in probability theory and statistics collection of random variables is independent and identically distributed if each random variable has the same probability distribution as the others and all are mutually independent 
this property is usually abbreviated as iid or iid 
iid was first defined in statistics and finds application in different fields such as data mining and signal processing 
introduction in statistics we commonly deal with random samples 
random sample can be thought of as set of objects that are chosen randomly 
or more formally it sequence of independent identically distributed iid random variables 
in other words the terms random sample and iid are basically one and the same 
in statistics we usually say random sample but in probability it more common to say iid identically distributed means that there are no overall trends the distribution doesn fluctuate and all items in the sample are taken from the same probability distribution 
independent means that the sample items are all independent events 
in other words they aren connected to each other in any way 
in other words knowledge of the value of one variable gives no information about the value of the other and vice versa 
application independent and identically distributed random variables are often used as an assumption which tends to simplify the underlying mathematics 
in practical applications of statistical modeling however the assumption may or may not be realistic the 
assumption is also used in central limit theorem which states that the probability distribution of the sum or average of 
variables with finite variance approaches normal distribution often the 
assumption arises in the context of sequences of random variables 
then independent and identically distributed implies that an element in the sequence is independent of the random variables that came before it 
in this way an 
sequence is different from markov sequence where the probability distribution for the nth random variable is function of the previous random variable in the sequence for first order markov sequence 
sequence does not imply the probabilities for all elements of the sample space or event space must be the same 
for example repeated throws of loaded dice will produce sequence that is despite the outcomes being biased 
definition definition for two random variables suppose that the random variables and are defined to assume values in let and be the cumulative distribution functions of and respectively and denote their joint cumulative distribution function by 
two random variables and are identically distributed if and only if two random variables and are independent if and only if 
see further independence probability theory two random variables 
two random variables and are 
if they are independent and identically distributed 
if and only if definition for more than two random variables the definition extends naturally to more than two random variables 
we say that random variables are 
if they are independent see further independence probability theory more than two random variables and identically distributed 
if and only if where denotes the joint cumulative distribution function of definition for independence in probability theory two events are called independent if and only if and 
in the following ab is short for and 
suppose there are the two events of the experiment if there is possibility 
generally the occurrence of has an effect on the probability of which is called conditional probability and only when the occurrence of has no effect on the occurrence of there is 
note if then are mutually independent which cannot be established with mutually incompatible at the same time that is independence must be compatible and mutual exclusion must be related 
suppose are three events 
if ab bc ac abc are satisfied then the events are independent of each other 
more general definition is there are events an 
if the probabilities of the product events for any events are equal to the product of the probabilities of each event then the events an are independent of each other 
examples example sequence of outcomes of spins of fair or unfair roulette wheel is 
one implication of this is that if the roulette ball lands on red for example times in row the next spin is no more or less likely to be black than on any other spin see the gambler fallacy 
sequence of fair or loaded dice rolls is 
sequence of fair or unfair coin flips is 
in signal processing and image processing the notion of transformation to 
implies two specifications the 
the signal level must be balanced on the time axis 
the signal spectrum must be flattened 
transformed by filtering such as deconvolution to white noise signal 
signal where all frequencies are equally present 
example toss coin times and record how many times does the coin lands on head 
independent each outcome of landing will not affect the other outcome which means the results are independent from each other 
identically distributed if the coin is homogeneous material each time the probability for head is which means the probability is identical for each time 
example roll dice times and record how many time the result is independent each outcome of the dice will not affect the next one which means the results are independent from each other 
identically distributed if the dice is homogeneous material each time the probability for the number is which means the probability is identical for each time 
example choose card from standard deck of cards containing cards then place the card back in the deck 
repeat it for times 
record the number of king appears independent each outcome of the card will not affect the next one which means the results are independent from each other 
identically distributed after drawing one card from it each time the probability for king is which means the probability is identical for each time 
generalizations many results that were first proven under the assumption that the random variables are 
have been shown to be true even under weaker distributional assumption 
exchangeable random variables the most general notion which shares the main properties of 
variables are exchangeable random variables introduced by bruno de finetti 
exchangeability means that while variables may not be independent future ones behave like past ones formally any value of finite sequence is as likely as any permutation of those values the joint probability distribution is invariant under the symmetric group 
this provides useful generalization for example sampling without replacement is not independent but is exchangeable 
vy process in stochastic calculus 
variables are thought of as discrete time vy process each variable gives how much one changes from one time to another 
for example sequence of bernoulli trials is interpreted as the bernoulli process 
one may generalize this to include continuous time vy processes and many vy processes can be seen as limits of 
variables for instance the wiener process is the limit of the bernoulli process 
in machine learning why assume the data in machine learning are independent and identically distributed 
machine learning uses currently acquired massive quantities of data to deliver faster more accurate results 
therefore we need to use historical data with overall representativeness 
if the data obtained is not representative of the overall situation then the rules will be summarized badly or wrongly 
hypothesis the number of individual cases in the training sample can be greatly reduced 
this assumption makes maximization very easy to calculate mathematically 
observing the assumption of independent and identical distribution in mathematics simplifies the calculation of the likelihood function in optimization problems 
because of the assumption of independence the likelihood function can be written like this 
in order to maximize the probability of the observed event take the log function and maximize the parameter that is to say to compute log where log log log log 
log the computer is very efficient to calculate multiple additions but it is not efficient to calculate the multiplication 
this simplification is the core reason for the increase in computational efficiency 
and this log transformation is also in the process of maximizing turning many exponential functions into linear functions 
for two reasons this hypothesis is easy to use the central limit theorem in practical applications 
even if the sample comes from more complex non gaussian distribution it can also approximate well 
because it can be simplified from the central limit theorem to gaussian distribution 
for large number of observable samples the sum of many random variables will have an approximately normal distribution 
the second reason is that the accuracy of the model depends on the simplicity and representative power of the model unit as well as the data quality 
because the simplicity of the unit makes it easy to interpret and scale and the representative power scale out of the unit improves the model accuracy 
like in deep neural network each neuron is very simple but has strong representative power layer by layer to represent more complex features to improve model accuracy 
see also de finetti theorem pairwise independent variables central limit theorem references further reading
in mathematics the dimension of vector space is the cardinality the number of vectors of basis of over its base field 
it is sometimes called hamel dimension after georg hamel or algebraic dimension to distinguish it from other types of dimension 
for every vector space there exists basis and all bases of vector space have equal cardinality as result the dimension of vector space is uniquely defined 
we say is finite dimensional if the dimension of is finite and infinite dimensional if its dimension is infinite 
the dimension of the vector space over the field can be written as dim or as read dimension of over 
when can be inferred from context dim is typically written 
examples the vector space has as standard basis and therefore dim more generally dim and even more generally dim for any field the complex numbers are both real and complex vector space we have dim and dim so the dimension depends on the base field 
the only vector space with dimension is the vector space consisting only of its zero element 
properties if is linear subspace of then dim dim 
to show that two finite dimensional vector spaces are equal the following criterion can be used if is finite dimensional vector space and is linear subspace of with dim dim then the space has the standard basis where is the th column of the corresponding identity matrix 
therefore has dimension any two finite dimensional vector spaces over with the same dimension are isomorphic 
any bijective map between their bases can be uniquely extended to bijective linear map between the vector spaces 
if is some set vector space with dimension over can be constructed as follows take the set of all functions such that for all but finitely many in these functions can be added and multiplied with elements of to obtain the desired vector space 
an important result about dimensions is given by the rank nullity theorem for linear maps 
if is field extension then is in particular vector space over furthermore every vector space is also vector space 
the dimensions are related by the formula in particular every complex vector space of dimension is real vector space of dimension some formulae relate the dimension of vector space with the cardinality of the base field and the cardinality of the space itself 
if is vector space over field then and if the dimension of is denoted by dim then if dim is finite then dim if dim is infinite then max dim 
generalizations vector space can be seen as particular case of matroid and in the latter there is well defined notion of dimension 
the length of module and the rank of an abelian group both have several properties similar to the dimension of vector spaces 
the krull dimension of commutative ring named after wolfgang krull is defined to be the maximal number of strict inclusions in an increasing chain of prime ideals in the ring 
trace the dimension of vector space may alternatively be characterized as the trace of the identity operator 
for instance tr id tr this appears to be circular definition but it allows useful generalizations 
firstly it allows for definition of notion of dimension when one has trace but no natural sense of basis 
for example one may have an algebra with maps the inclusion of scalars called the unit and map corresponding to trace called the counit 
the composition is scalar being linear operator on dimensional space corresponds to trace of identity and gives notion of dimension for an abstract algebra 
in practice in bialgebras this map is required to be the identity which can be obtained by normalizing the counit by dividing by dimension tr so in these cases the normalizing constant corresponds to dimension 
alternatively it may be possible to take the trace of operators on an infinite dimensional space in this case finite trace is defined even though no finite dimension exists and gives notion of dimension of the operator 
these fall under the rubric of trace class operators on hilbert space or more generally nuclear operators on banach space 
subtler generalization is to consider the trace of family of operators as kind of twisted dimension 
this occurs significantly in representation theory where the character of representation is the trace of the representation hence scalar valued function on group whose value on the identity is the dimension of the representation as representation sends the identity in the group to the identity matrix tr dim the other values of the character can be viewed as twisted dimensions and find analogs or generalizations of statements about dimensions to statements about characters or representations 
sophisticated example of this occurs in the theory of monstrous moonshine the invariant is the graded dimension of an infinite dimensional graded representation of the monster group and replacing the dimension with the character gives the mckay thompson series for each element of the monster group 
see also fractal dimension ratio providing statistical index of complexity variation with scale krull dimension in mathematics dimension of ring matroid rank maximum size of an independent set of the matroid rank linear algebra dimension of the column space of matrix topological dimension also called lebesgue covering dimension notes references sources axler sheldon 
linear algebra done right 
undergraduate texts in mathematics rd ed 
external links mit linear algebra lecture on independence basis and dimension by gilbert strang at mit opencourseware
linear least squares lls is the least squares approximation of linear functions to data 
it is set of formulations for solving statistical problems involved in linear regression including variants for ordinary unweighted weighted and generalized correlated residuals 
numerical methods for linear least squares include inverting the matrix of the normal equations and orthogonal decomposition methods 
main formulations the three main linear least squares formulations are ordinary least squares ols is the most common estimator 
ols estimates are commonly used to analyze both experimental and observational data 
the ols method minimizes the sum of squared residuals and leads to closed form expression for the estimated value of the unknown parameter vector where is vector whose ith element is the ith observation of the dependent variable and is matrix whose ij element is the ith observation of the jth independent variable 
the estimator is unbiased and consistent if the errors have finite variance and are uncorrelated with the regressors where is the transpose of row of the matrix it is also efficient under the assumption that the errors have finite variance and are homoscedastic meaning that xi does not depend on the condition that the errors are uncorrelated with the regressors will generally be satisfied in an experiment but in the case of observational data it is difficult to exclude the possibility of an omitted covariate that is related to both the observed covariates and the response variable 
the existence of such covariate will generally lead to correlation between the regressors and the response variable and hence to an inconsistent estimator of the condition of homoscedasticity can fail with either experimental or observational data 
if the goal is either inference or predictive modeling the performance of ols estimates can be poor if multicollinearity is present unless the sample size is large 
weighted least squares wls are used when heteroscedasticity is present in the error terms of the model 
generalized least squares gls is an extension of the ols method that allows efficient estimation of when either heteroscedasticity or correlations or both are present among the error terms of the model as long as the form of heteroscedasticity and correlation is known independently of the data 
to handle heteroscedasticity when the error terms are uncorrelated with each other gls minimizes weighted analogue to the sum of squared residuals from ols regression where the weight for the ith case is inversely proportional to var 
this special case of gls is called weighted least squares 
the gls solution to an estimation problem is where is the covariance matrix of the errors 
gls can be viewed as applying linear transformation to the data so that the assumptions of ols are met for the transformed data 
for gls to be applied the covariance structure of the errors must be known up to multiplicative constant 
alternative formulations other formulations include iteratively reweighted least squares irls is used when heteroscedasticity or correlations or both are present among the error terms of the model but where little is known about the covariance structure of the errors independently of the data 
in the first iteration ols or gls with provisional covariance structure is carried out and the residuals are obtained from the fit 
based on the residuals an improved estimate of the covariance structure of the errors can usually be obtained 
subsequent gls iteration is then performed using this estimate of the error structure to define the weights 
the process can be iterated to convergence but in many cases only one iteration is sufficient to achieve an efficient estimate of instrumental variables regression iv can be performed when the regressors are correlated with the errors 
in this case we need the existence of some auxiliary instrumental variables zi such that zi if is the matrix of instruments then the estimator can be given in closed form as optimal instruments regression is an extension of classical iv regression to the situation where zi total least squares tls is an approach to least squares estimation of the linear regression model that treats the covariates and response variable in more geometrically symmetric manner than ols 
it is one approach to handling the errors in variables problem and is also sometimes used even when the covariates are assumed to be error free percentage least squares focuses on reducing percentage errors which is useful in the field of forecasting or time series analysis 
it is also useful in situations where the dependent variable has wide range without constant variance as here the larger residuals at the upper end of the range would dominate if ols were used 
when the percentage or relative error is normally distributed least squares percentage regression provides maximum likelihood estimates 
percentage regression is linked to multiplicative error model whereas ols is linked to models containing an additive error term constrained least squares indicates linear least squares problem with additional constraints on the solution 
objective function in ols assuming unweighted observations the optimal value of the objective function is found by substituting the optimal expression for the coefficient vector where the latter equality holding since is symmetric and idempotent 
it can be shown from this that under an appropriate assignment of weights the expected value of is if instead unit weights are assumed the expected value of is where is the variance of each observation 
if it is assumed that the residuals belong to normal distribution the objective function being sum of weighted squared residuals will belong to chi squared distribution with degrees of freedom 
some illustrative percentile values of are given in the following table 
these values can be used for statistical criterion as to the goodness of fit 
when unit weights are used the numbers should be divided by the variance of an observation 
for wls the ordinary objective function above is replaced for weighted average of residuals 
discussion in statistics and mathematics linear least squares is an approach to fitting mathematical or statistical model to data in cases where the idealized value provided by the model for any data point is expressed linearly in terms of the unknown parameters of the model 
the resulting fitted model can be used to summarize the data to predict unobserved values from the same system and to understand the mechanisms that may underlie the system 
mathematically linear least squares is the problem of approximately solving an overdetermined system of linear equations where is not an element of the column space of the matrix the approximate solution is realized as an exact solution to where is the projection of onto the column space of the best approximation is then that which minimizes the sum of squared differences between the data values and their corresponding modeled values 
the approach is called linear least squares since the assumed function is linear in the parameters to be estimated 
linear least squares problems are convex and have closed form solution that is unique provided that the number of data points used for fitting equals or exceeds the number of unknown parameters except in special degenerate situations 
in contrast non linear least squares problems generally must be solved by an iterative procedure and the problems can be non convex with multiple optima for the objective function 
if prior distributions are available then even an underdetermined system can be solved using the bayesian mmse estimator 
in statistics linear least squares problems correspond to particularly important type of statistical model called linear regression which arises as particular form of regression analysis 
one basic form of such model is an ordinary least squares model 
the present article concentrates on the mathematical aspects of linear least squares problems with discussion of the formulation and interpretation of statistical regression models and statistical inferences related to these being dealt with in the articles just mentioned 
see outline of regression analysis for an outline of the topic 
properties if the experimental errors are uncorrelated have mean of zero and constant variance the gauss markov theorem states that the least squares estimator has the minimum variance of all estimators that are linear combinations of the observations 
in this sense it is the best or optimal estimator of the parameters 
note particularly that this property is independent of the statistical distribution function of the errors 
in other words the distribution function of the errors need not be normal distribution 
however for some probability distributions there is no guarantee that the least squares solution is even possible given the observations still in such cases it is the best estimator that is both linear and unbiased 
for example it is easy to show that the arithmetic mean of set of measurements of quantity is the least squares estimator of the value of that quantity 
if the conditions of the gauss markov theorem apply the arithmetic mean is optimal whatever the distribution of errors of the measurements might be 
however in the case that the experimental errors do belong to normal distribution the least squares estimator is also maximum likelihood estimator these properties underpin the use of the method of least squares for all types of data fitting even when the assumptions are not strictly valid 
limitations an assumption underlying the treatment given above is that the independent variable is free of error 
in practice the errors on the measurements of the independent variable are usually much smaller than the errors on the dependent variable and can therefore be ignored 
when this is not the case total least squares or more generally errors in variables models or rigorous least squares should be used 
this can be done by adjusting the weighting scheme to take into account errors on both the dependent and independent variables and then following the standard procedure in some cases the weighted normal equations matrix xtx is ill conditioned 
when fitting polynomials the normal equations matrix is vandermonde matrix 
vandermonde matrices become increasingly ill conditioned as the order of the matrix increases 
in these cases the least squares estimate amplifies the measurement noise and may be grossly inaccurate 
various regularization techniques can be applied in such cases the most common of which is called ridge regression 
if further information about the parameters is known for example range of possible values of then various techniques can be used to increase the stability of the solution 
for example see constrained least squares 
another drawback of the least squares estimator is the fact that the norm of the residuals is minimized whereas in some cases one is truly interested in obtaining small error in the parameter small value of 
however since the true parameter is necessarily unknown this quantity cannot be directly minimized 
if prior probability on is known then bayes estimator can be used to minimize the mean squared error 
the least squares method is often applied when no prior is known 
surprisingly when several parameters are being estimated jointly better estimators can be constructed an effect known as stein phenomenon 
for example if the measurement error is gaussian several estimators are known which dominate or outperform the least squares technique the best known of these is the james stein estimator 
this is an example of more general shrinkage estimators that have been applied to regression problems 
applications polynomial fitting models are polynomials in an independent variable straight line quadratic cubic quartic and higher polynomials 
for regression with high order polynomials the use of orthogonal polynomials is recommended 
numerical smoothing and differentiation this is an application of polynomial fitting 
multinomials in more than one independent variable including surface fitting curve fitting with splines chemometrics calibration curve standard addition gran plot analysis of mixtures uses in data fitting the primary application of linear least squares is in data fitting 
given set of data points consisting of experimentally measured values taken at values of an independent variable may be scalar or vector quantities and given model function with it is desired to find the parameters such that the model function best fits the data 
in linear least squares linearity is meant to be with respect to parameters so here the functions may be nonlinear with respect to the variable ideally the model function fits the data exactly so for all this is usually not possible in practice as there are more data points than there are parameters to be determined 
the approach chosen then is to find the minimal possible value of the sum of squares of the residuals so to minimize the function after substituting for and then for this minimization problem becomes the quadratic minimization problem above with and the best fit can be found by solving the normal equations 
example as result of an experiment four data points were obtained and shown in red in the diagram on the right 
we hope to find line that best fits these four points 
in other words we would like to find the numbers and that approximately solve the overdetermined linear system of four equations in two unknowns in some best sense 
represents the residual at each point between the curve fit and the data the least squares approach to solving this problem is to try to make the sum of the squares of these residuals as small as possible that is to find the minimum of the function the minimum is determined by calculating the partial derivatives of with respect to and and setting them to zero this results in system of two equations in two unknowns called the normal equations which when solved give and the equation is the line of best fit 
the residuals that is the differences between the values from the observations and the predicated variables by using the line of best fit are then found to be and see the diagram on the right 
the minimum value of the sum of squares of the residuals is more generally one can have regressors and linear model using quadratic model importantly in linear least squares we are not restricted to using line as the model as in the above example 
for instance we could have chosen the restricted quadratic model this model is still linear in the parameter so we can still perform the same analysis constructing system of equations from the data points the partial derivatives with respect to the parameters this time there is only one are again computed and set to and solved leading to the resulting best fit model 
see also line line intersection nearest point to non intersecting lines an application line fitting nonlinear least squares regularized least squares simple linear regression partial least squares regression linear function references further reading bevington philip robinson keith 
data reduction and error analysis for the physical sciences 
external links least squares fitting from mathworld least squares fitting polynomial from mathworld
in statistics econometrics epidemiology and related disciplines the method of instrumental variables iv is used to estimate causal relationships when controlled experiments are not feasible or when treatment is not successfully delivered to every unit in randomized experiment 
intuitively ivs are used when an explanatory variable of interest is correlated with the error term in which case ordinary least squares and anova give biased results 
valid instrument induces changes in the explanatory variable but has no independent effect on the dependent variable allowing researcher to uncover the causal effect of the explanatory variable on the dependent variable 
instrumental variable methods allow for consistent estimation when the explanatory variables covariates are correlated with the error terms in regression model 
such correlation may occur when changes in the dependent variable change the value of at least one of the covariates reverse causation there are omitted variables that affect both the dependent and independent variables or the covariates are subject to non random measurement error explanatory variables that suffer from one or more of these issues in the context of regression are sometimes referred to as endogenous 
in this situation ordinary least squares produces biased and inconsistent estimates 
however if an instrument is available consistent estimates may still be obtained 
an instrument is variable that does not itself belong in the explanatory equation but is correlated with the endogenous explanatory variables conditionally on the value of other covariates 
in linear models there are two main requirements for using ivs the instrument must be correlated with the endogenous explanatory variables conditionally on the other covariates 
if this correlation is strong then the instrument is said to have strong first stage 
weak correlation may provide misleading inferences about parameter estimates and standard errors 
the instrument cannot be correlated with the error term in the explanatory equation conditionally on the other covariates 
in other words the instrument cannot suffer from the same problem as the original predicting variable 
if this condition is met then the instrument is said to satisfy the exclusion restriction 
history first use of an instrument variable occurred in book by philip wright best known for his excellent description of the production transport and sale of vegetable and animal oils in the early in the united states while in olav reiers applied the same approach in the context of errors in variables models in his dissertation giving the method its name wright attempted to determine the supply and demand for butter using panel data on prices and quantities sold in the united states 
the idea was that regression analysis could produce demand or supply curve because they are formed by the path between prices and quantities demanded or supplied 
the problem was that the observational data did not form demand or supply curve as such but rather cloud of point observations that took different shapes under varying market conditions 
it seemed that making deductions from the data remained elusive 
the problem was that price affected both supply and demand so that function describing only one of the two could not be constructed directly from the observational data 
wright correctly concluded that he needed variable that correlated with either demand or supply but not both that is an instrumental variable 
after much deliberation wright decided to use regional rainfall as his instrumental variable he concluded that rainfall affected grass production and hence milk production and ultimately butter supply but not butter demand 
in this way he was able to construct regression equation with only the instrumental variable of price and supply 
theory while the ideas behind iv extend to broad class of models very common context for iv is in linear regression 
traditionally an instrumental variable is defined as variable that is correlated with the independent variable and uncorrelated with the error term in the linear equation is vector 
is matrix usually with column of ones and perhaps with additional columns for other covariates 
consider how an instrument allows to be recovered 
recall that ols solves for such that cov when we minimize the sum of squared errors min the first order condition is exactly 
if the true model is believed to have cov due to any of the reasons listed above for example if there is an omitted variable which affects both and separately then this ols procedure will not yield the causal impact of on ols will simply pick the parameter that makes the resulting errors appear uncorrelated with consider for simplicity the single variable case 
suppose we are considering regression with one variable and constant perhaps no other covariates are necessary or perhaps we have partialed out any other relevant covariates in this case the coefficient on the regressor of interest is given by cov var 
substituting for gives cov var cov var cov var cov var cov var where is what the estimated coefficient vector would be if were not correlated with in this case it can be shown that is an unbiased estimator of if cov in the underlying model that we believe then ols gives coefficient which does not reflect the underlying causal effect of interest 
iv helps to fix this problem by identifying the parameters not based on whether is uncorrelated with but based on whether another variable is uncorrelated with if theory suggests that is related to the first stage but uncorrelated with the exclusion restriction then iv may identify the causal parameter of interest where ols fails 
because there are multiple specific ways of using and deriving iv estimators even in just the linear case iv sls gmm we save further discussion for the estimation section below 
example informally in attempting to estimate the causal effect of some variable on another an instrument is third variable which affects only through its effect on for example suppose researcher wishes to estimate the causal effect of smoking on general health 
correlation between health and smoking does not imply that smoking causes poor health because other variables such as depression may affect both health and smoking or because health may affect smoking 
it is at best difficult and expensive to conduct controlled experiments on smoking status in the general population 
the researcher may attempt to estimate the causal effect of smoking on health from observational data by using the tax rate for tobacco products as an instrument for smoking 
the tax rate for tobacco products is reasonable choice for an instrument because the researcher assumes that it can only be correlated with health through its effect on smoking 
if the researcher then finds tobacco taxes and state of health to be correlated this may be viewed as evidence that smoking causes changes in health 
angrist and krueger present survey of the history and uses of instrumental variable techniques 
graphical definition of course iv techniques have been developed among much broader class of non linear models 
general definitions of instrumental variables using counterfactual and graphical formalism were given by pearl 
the graphical definition requires that satisfy the following conditions where stands for separation and stands for the graph in which all arrows entering are cut off 
the counterfactual definition requires that satisfies where yx stands for the value that would attain had been and stands for independence 
if there are additional covariates then the above definitions are modified so that qualifies as an instrument if the given criteria hold conditional on the essence of pearl definition is the equations of interest are structural not regression 
the error term stands for all exogenous factors that affect when is held constant 
the instrument should be independent of the instrument should not affect when is held constant exclusion restriction 
the instrument should not be independent of these conditions do not rely on specific functional form of the equations and are applicable therefore to nonlinear equations where can be non additive see non parametric analysis 
they are also applicable to system of multiple equations in which and other factors affect through several intermediate variables 
an instrumental variable need not be cause of proxy of such cause may also be used if it satisfies conditions 
the exclusion restriction condition is redundant it follows from conditions and 
selecting suitable instruments since is unobserved the requirement that be independent of cannot be inferred from data and must instead be determined from the model structure the data generating process 
causal graphs are representation of this structure and the graphical definition given above can be used to quickly determine whether variable qualifies as an instrumental variable given set of covariates to see how consider the following example 
suppose that we wish to estimate the effect of university tutoring program on grade point average gpa 
the relationship between attending the tutoring program and gpa may be confounded by number of factors 
students who attend the tutoring program may care more about their grades or may be struggling with their work 
this confounding is depicted in the figures on the right through the bidirected arc between tutoring program and gpa 
if students are assigned to dormitories at random the proximity of the student dorm to the tutoring program is natural candidate for being an instrumental variable 
however what if the tutoring program is located in the college library 
in that case proximity may also cause students to spend more time at the library which in turn improves their gpa see figure 
using the causal graph depicted in the figure we see that proximity does not qualify as an instrumental variable because it is connected to gpa through the path proximity library hours gpa in 
however if we control for library hours by adding it as covariate then proximity becomes an instrumental variable since proximity is separated from gpa given library hours in 
now suppose that we notice that student natural ability affects his or her number of hours in the library as well as his or her gpa as in figure using the causal graph we see that library hours is collider and conditioning on it opens the path proximity library hours gpa 
as result proximity cannot be used as an instrumental variable 
finally suppose that library hours does not actually affect gpa because students who do not study in the library simply study elsewhere as in figure in this case controlling for library hours still opens spurious path from proximity to gpa 
however if we do not control for library hours and remove it as covariate then proximity can again be used an instrumental variable 
estimation we now revisit and expand upon the mechanics of iv in greater detail 
suppose the data are generated by process of the form where indexes observations is the th value of the dependent variable is vector of the th values of the independent variable and constant is the th value of an unobserved error term representing all causes of other than and is an unobserved parameter vector the parameter vector is the causal effect on of one unit change in each element of holding all other causes of constant 
the econometric goal is to estimate for simplicity sake assume the draws of are uncorrelated and that they are drawn from distributions with the same variance that is that the errors are serially uncorrelated and homoskedastic 
suppose also that regression model of nominally the same form is proposed 
given random sample of observations from this process the ordinary least squares estimator is where and denote column vectors of length this equation is similar to the equation involving cov in the introduction this is the matrix version of that equation 
when and are uncorrelated under certain regularity conditions the second term has an expected value conditional on of zero and converges to zero in the limit so the estimator is unbiased and consistent 
when and the other unmeasured causal variables collapsed into the term are correlated however the ols estimator is generally biased and inconsistent for in this case it is valid to use the estimates to predict values of given values of but the estimate does not recover the causal effect of on to recover the underlying parameter we introduce set of variables that is highly correlated with each endogenous component of but in our underlying model is not correlated with for simplicity one might consider to be matrix composed of column of constants and one endogenous variable and to be consisting of column of constants and one instrumental variable 
however this technique generalizes to being matrix of constant and say endogenous variables with being matrix composed of constant and instruments 
in the discussion that follows we will assume that is matrix and leave this value unspecified 
an estimator in which and are both matrices is referred to as just identified 
suppose that the relationship between each endogenous component xi and the instruments is given by the most common iv specification uses the following estimator this specification approaches the true parameter as the sample gets large so long as in the true model as long as in the underlying process which generates the data the appropriate use of the iv estimator will identify this parameter 
this works because iv solves for the unique parameter that satisfies and therefore hones in on the true underlying parameter as the sample size grows 
now an extension suppose that there are more instruments than there are covariates in the equation of interest so that is matrix with this is often called the over identified case 
in this case the generalized method of moments gmm can be used 
the gmm iv estimator is where refers to the projection matrix this expression collapses to the first when the number of instruments is equal to the number of covariates in the equation of interest 
the over identified iv is therefore generalization of the just identified iv 
there is an equivalent under identified estimator for the case where since the parameters are the solutions to set of linear equations an under identified model using the set of equations does not have unique solution 
interpretation as two stage least squares one computational method which can be used to calculate iv estimates is two stage least squares sls or tsls 
in the first stage each explanatory variable that is an endogenous covariate in the equation of interest is regressed on all of the exogenous variables in the model including both exogenous covariates in the equation of interest and the excluded instruments 
the predicted values from these regressions are obtained stage regress each column of on errors and save the predicted values in the second stage the regression of interest is estimated as usual except that in this stage each endogenous covariate is replaced with the predicted values from the first stage stage regress on the predicted values from the first stage which gives sls this method is only valid in linear models 
for categorical endogenous covariates one might be tempted to use different first stage than ordinary least squares such as probit model for the first stage followed by ols for the second 
this is commonly known in the econometric literature as the forbidden regression because second stage iv parameter estimates are consistent only in special cases 
the resulting estimator of is numerically identical to the expression displayed above 
small correction must be made to the sum of squared residuals in the second stage fitted model in order that the covariance matrix of is calculated correctly 
non parametric analysis when the form of the structural equations is unknown an instrumental variable can still be defined through the equations where and are two arbitrary functions and is independent of unlike linear models however measurements of and do not allow for the identification of the average causal effect of on denoted ace ace pr do 
balke and pearl derived tight bounds on ace and showed that these can provide valuable information on the sign and size of ace in linear analysis there is no test to falsify the assumption the is instrumental relative to the pair 
this is not the case when is discrete 
pearl has shown that for all and the following constraint called instrumental inequality must hold whenever satisfies the two equations above max max pr 
interpretation under treatment effect heterogeneity the exposition above assumes that the causal effect of interest does not vary across observations that is that is constant 
generally different subjects will respond in different ways to changes in the treatment when this possibility is recognized the average effect in the population of change in on may differ from the effect in given subpopulation 
for example the average effect of job training program may substantially differ across the group of people who actually receive the training and the group which chooses not to receive training 
for these reasons iv methods invoke implicit assumptions on behavioral response or more generally assumptions over the correlation between the response to treatment and propensity to receive treatment the standard iv estimator can recover local average treatment effects late rather than average treatment effects ate 
imbens and angrist demonstrate that the linear iv estimate can be interpreted under weak conditions as weighted average of local average treatment effects where the weights depend on the elasticity of the endogenous regressor to changes in the instrumental variables 
roughly that means that the effect of variable is only revealed for the subpopulations affected by the observed changes in the instruments and that subpopulations which respond most to changes in the instruments will have the largest effects on the magnitude of the iv estimate 
for example if researcher uses presence of land grant college as an instrument for college education in an earnings regression she identifies the effect of college on earnings in the subpopulation which would obtain college degree if college is present but which would not obtain degree if college is not present 
this empirical approach does not without further assumptions tell the researcher anything about the effect of college among people who would either always or never get college degree regardless of whether local college exists 
weak instruments problem as bound jaeger and baker note problem is caused by the selection of weak instruments instruments that are poor predictors of the endogenous question predictor in the first stage equation 
in this case the prediction of the question predictor by the instrument will be poor and the predicted values will have very little variation 
consequently they are unlikely to have much success in predicting the ultimate outcome when they are used to replace the question predictor in the second stage equation 
in the context of the smoking and health example discussed above tobacco taxes are weak instruments for smoking if smoking status is largely unresponsive to changes in taxes 
if higher taxes do not induce people to quit smoking or not start smoking then variation in tax rates tells us nothing about the effect of smoking on health 
if taxes affect health through channels other than through their effect on smoking then the instruments are invalid and the instrumental variables approach may yield misleading results 
for example places and times with relatively health conscious populations may both implement high tobacco taxes and exhibit better health even holding smoking rates constant so we would observe correlation between health and tobacco taxes even if it were the case that smoking has no effect on health 
in this case we would be mistaken to infer causal effect of smoking on health from the observed correlation between tobacco taxes and health 
testing for weak instruments the strength of the instruments can be directly assessed because both the endogenous covariates and the instruments are observable 
common rule of thumb for models with one endogenous regressor is the statistic against the null that the excluded instruments are irrelevant in the first stage regression should be larger than 
statistical inference and hypothesis testing when the covariates are exogenous the small sample properties of the ols estimator can be derived in straightforward manner by calculating moments of the estimator conditional on when some of the covariates are endogenous so that instrumental variables estimation is implemented simple expressions for the moments of the estimator cannot be so obtained 
generally instrumental variables estimators only have desirable asymptotic not finite sample properties and inference is based on asymptotic approximations to the sampling distribution of the estimator 
even when the instruments are uncorrelated with the error in the equation of interest and when the instruments are not weak the finite sample properties of the instrumental variables estimator may be poor 
for example exactly identified models produce finite sample estimators with no moments so the estimator can be said to be neither biased nor unbiased the nominal size of test statistics may be substantially distorted and the estimates may commonly be far away from the true value of the parameter 
testing the exclusion restriction the assumption that the instruments are not correlated with the error term in the equation of interest is not testable in exactly identified models 
if the model is overidentified there is information available which may be used to test this assumption 
the most common test of these overidentifying restrictions called the sargan hansen test is based on the observation that the residuals should be uncorrelated with the set of exogenous variables if the instruments are truly exogenous 
the sargan hansen test statistic can be calculated as the number of observations multiplied by the coefficient of determination from the ols regression of the residuals onto the set of exogenous variables 
this statistic will be asymptotically chi squared with degrees of freedom under the null that the error term is uncorrelated with the instruments 
see also control function econometrics optimal instruments references further reading greene william 
econometric analysis sixth ed 
upper saddle river pearson prentice hall 
isbn gujarati damodar porter dawn 
basic econometrics fifth ed 
new york mcgraw hill irwin 
lectures on advanced econometric theory 
introductory econometrics modern approach fifth international ed 
mason oh south western 
quasi likelihood methods for count data handbook of applied econometrics volume ed 
pesaran and schmidt oxford blackwell pp 
terza estimating count models with endogenous switching sample selection and endogenous treatment effects 
journal of econometrics pp 
econometric analysis of cross section and panel data mit press cambridge massachusetts 
external links chapter from daniel mcfadden textbook econometrics lecture topic instrumental variable on youtube by mark thoma 
econometrics lecture topic two stages least square on youtube by mark thoma
in music closely related key or close key is one sharing many common tones with an original key as opposed to distantly related key or distant key 
in music harmony there are six of them five share all or all except one pitches with key with which it is being compared and is adjacent to it on the circle of fifths and its relative major or minor and one shares the same tonic 
such keys are the most commonly used destinations or transpositions in modulation because of their strong structural links with the home key 
distant keys may be reached sequentially through closely related keys by chain modulation for example to to for example one principle that every composer of haydn day classical music era kept in mind was over all unity of tonality 
no piece dared wander too far from its tonic key and no piece in four movement form dared to present tonality not closely related to the key of the whole series 
for example the first movement of mozart piano sonata no 
modulates only to closely related keys the dominant supertonic and submediant given major key tonic the related keys are ii supertonic the relative minor of the subdominant iii mediant the relative minor of the dominant iv subdominant one less sharp or one more flat around circle of fifths dominant one more sharp or one fewer flat around circle of fifths vi submediant or relative minor different tonic same key signature parallel minor same tonic different key signature specifically starting from minor key the closely related keys are the mediant or relative major iii the subdominant iv the minor dominant the submediant vi the subtonic vii and the parallel major 
in the key of minor when we translate them to keys we get major minor minor major major majoranother view of closely related keys is that there are six closely related keys based on the tonic and the remaining triads of the diatonic scale excluding the dissonant diminished triads 
four of the five differ by one accidental one has the same key signature and one uses the parallel modal form 
in the key of major these would be minor minor major major minor and minor 
despite being three sharps or flats away from the original key in the circle of fifths parallel keys are also considered as closely related keys as the tonal center is the same and this makes this key have an affinity with the original key 
in modern music the closeness of relation between any two keys or sets of pitches may be determined by the number of tones they share in common which allows one to consider modulations not occurring in standard major minor tonality 
for example in music based on the pentatonic scale containing pitches and modulating fifth higher gives the collection of pitches and having four of five tones in common 
however modulating up tritone would produce which shares no common tones with the original scale 
thus the scale fifth higher is very closely related while the scale tritone higher is not 
other modulations may be placed in order from closest to most distant depending upon the number of common tones 
another view in modern music notably in bart common tonic produces closely related keys the other scales being the six other modes 
this usage can be found in several of the mikrokosmos piano pieces 
when modulation causes the new key to traverse the bottom of the circle of fifths this may give rise to theoretical key containing eight or more sharps or flats in its notated key signature in such case notational conventions require recasting the new section in its enharmonically equivalent key 
andranik tangian suggests and visualizations of key chord proximity for both all major and all minor keys chords by locating them along single subdominant dominant axis which wraps torus that is then unfolded 
see also chromatic mediant common chord music monotonality parallel and counter parallel pitch space references further reading howard hanson harmonic materials of modern music 
appleton century crofts inc
public key cryptography or asymmetric cryptography is the field of cryptographic systems that use pairs of related keys
each key pair consists of public key and corresponding private key
key pairs are generated with cryptographic algorithms based on mathematical problems termed one way functions
security of public key cryptography depends on keeping the private key secret the public key can be openly distributed without compromising security in public key encryption system anyone with public key can encrypt message yielding ciphertext but only those who know the corresponding private key can decrypt the ciphertext to obtain the original message for example journalist can publish the public key of an encryption key pair on web site so that sources can send secret messages to the news organization in ciphertext
only the journalist who knows the corresponding private key can decrypt the ciphertexts to obtain the sources messages an eavesdropper reading email on its way to the journalist can decrypt the ciphertexts
however public key encryption doesn conceal metadata like what computer source used to send message when they sent it or how long it is
public key encryption on its own also doesn tell the recipient anything about who sent message it just conceals the content of message in ciphertext that can only be decrypted with the private key
in digital signature system sender can use private key together with message to create signature
anyone with the corresponding public key can verify whether the signature matches the message but forger who doesn know the private key can find any message signature pair that will pass verification with the public key for example software publisher can create signature key pair and include the public key in software installed on computers
later the publisher can distribute an update to the software signed using the private key and any computer receiving an update can confirm it is genuine by verifying the signature using the public key
as long as the software publisher keeps the private key secret even if forger can distribute malicious updates to computers they can convince the computers that any malicious updates are genuine
public key algorithms are fundamental security primitives in modern cryptosystems including applications and protocols which offer assurance of the confidentiality authenticity and non repudiability of electronic communications and data storage
they underpin numerous internet standards such as transport layer security tls ssh mime and pgp
some public key algorithms provide key distribution and secrecy diffie hellman key exchange some provide digital signatures digital signature algorithm and some provide both rsa
compared to symmetric encryption asymmetric encryption is rather slower than good symmetric encryption too slow for many purposes
today cryptosystems such as tls secure shell use both symmetric encryption and asymmetric encryption often by using asymmetric encryption to securely exchange secret key which is then used for symmetric encryption
description before the mid all cipher systems used symmetric key algorithms in which the same cryptographic key is used with the underlying algorithm by both the sender and the recipient who must both keep it secret
of necessity the key in every such system had to be exchanged between the communicating parties in some secure way prior to any use of the system for instance via secure channel
this requirement is never trivial and very rapidly becomes unmanageable as the number of participants increases or when secure channels aren available or when as is sensible cryptographic practice keys are frequently changed
in particular if messages are meant to be secure from other users separate key is required for each possible pair of users
by contrast in public key system the public keys can be disseminated widely and openly and only the corresponding private keys need be kept secret by its owner
two of the best known uses of public key cryptography are public key encryption in which message is encrypted with the intended recipient public key
for properly chosen and used algorithms messages cannot in practice be decrypted by anyone who does not possess the matching private key who is thus presumed to be the owner of that key and so the person associated with the public key
this can be used to ensure confidentiality of message
digital signatures in which message is signed with the sender private key and can be verified by anyone who has access to the sender public key
this verification proves that the sender had access to the private key and therefore is very likely to be the person associated with the public key
it also proves that the signature was prepared for that exact message since verification will fail for any other message one could devise without using the private key one important issue is confidence proof that particular public key is authentic
that it is correct and belongs to the person or entity claimed and has not been tampered with or replaced by some perhaps malicious third party
there are several possible approaches including public key infrastructure pki in which one or more third parties known as certificate authorities certify ownership of key pairs
tls relies upon this
this implies that the pki system software hardware and management is trust able by all involved
web of trust which decentralizes authentication by using individual endorsements of links between user and the public key belonging to that user
pgp uses this approach in addition to lookup in the domain name system dns
the dkim system for digitally signing emails also uses this approach
applications the most obvious application of public key encryption system is for encrypting communication to provide confidentiality message that sender encrypts using the recipient public key which can be decrypted only by the recipient paired private key
another application in public key cryptography is the digital signature
digital signature schemes can be used for sender authentication
non repudiation systems use digital signatures to ensure that one party cannot successfully dispute its authorship of document or communication
further applications built on this foundation include digital cash password authenticated key agreement time stamping services and non repudiation protocols
hybrid cryptosystems because asymmetric key algorithms are nearly always much more computationally intensive than symmetric ones it is common to use public private asymmetric key exchange algorithm to encrypt and exchange symmetric key which is then used by symmetric key cryptography to transmit data using the now shared symmetric key for symmetric key encryption algorithm
pgp ssh and the ssl tls family of schemes use this procedure they are thus called hybrid cryptosystems
the initial asymmetric cryptography based key exchange to share server generated symmetric key from the server to client has the advantage of not requiring that symmetric key be pre shared manually such as on printed paper or discs transported by courier while providing the higher data throughput of symmetric key cryptography over asymmetric key cryptography for the remainder of the shared connection
weaknesses as with all security related systems it is important to identify potential weaknesses
aside from poor choice of an asymmetric key algorithm there are few which are widely regarded as satisfactory or too short key length the chief security risk is that the private key of pair becomes known
all security of messages authentication etc will then be lost
algorithms all public key schemes are in theory susceptible to brute force key search attack
however such an attack is impractical if the amount of computation needed to succeed termed the work factor by claude shannon is out of reach of all potential attackers
in many cases the work factor can be increased by simply choosing longer key
but other algorithms may inherently have much lower work factors making resistance to brute force attack from longer keys irrelevant
some special and specific algorithms have been developed to aid in attacking some public key encryption algorithms both rsa and elgamal encryption have known attacks that are much faster than the brute force approach
none of these are sufficiently improved to be actually practical however
major weaknesses have been found for several formerly promising asymmetric key algorithms
the knapsack packing algorithm was found to be insecure after the development of new attack
as with all cryptographic functions public key implementations may be vulnerable to side channel attacks that exploit information leakage to simplify the search for secret key
these are often independent of the algorithm being used
research is underway to both discover and to protect against new attacks
alteration of public keys another potential security vulnerability in using asymmetric keys is the possibility of man in the middle attack in which the communication of public keys is intercepted by third party the man in the middle and then modified to provide different public keys instead
encrypted messages and responses must in all instances be intercepted decrypted and re encrypted by the attacker using the correct public keys for the different communication segments so as to avoid suspicion
communication is said to be insecure where data is transmitted in manner that allows for interception also called sniffing
these terms refer to reading the sender private data in its entirety
communication is particularly unsafe when interceptions can be prevented or monitored by the sender man in the middle attack can be difficult to implement due to the complexities of modern security protocols
however the task becomes simpler when sender is using insecure media such as public networks the internet or wireless communication
in these cases an attacker can compromise the communications infrastructure rather than the data itself
hypothetical malicious staff member at an internet service provider isp might find man in the middle attack relatively straightforward
capturing the public key would only require searching for the key as it gets sent through the isp communications hardware in properly implemented asymmetric key schemes this is not significant risk
in some advanced man in the middle attacks one side of the communication will see the original data while the other will receive malicious variant
asymmetric man in the middle attacks can prevent users from realizing their connection is compromised
this remains so even when one user data is known to be compromised because the data appears fine to the other user
this can lead to confusing disagreements between users such as it must be on your end
when neither user is at fault
hence man in the middle attacks are only fully preventable when the communications infrastructure is physically controlled by one or both parties such as via wired route inside the sender own building
in summation public keys are easier to alter when the communications hardware used by sender is controlled by an attacker
public key infrastructure one approach to prevent such attacks involves the use of public key infrastructure pki set of roles policies and procedures needed to create manage distribute use store and revoke digital certificates and manage public key encryption
however this has potential weaknesses
for example the certificate authority issuing the certificate must be trusted by all participating parties to have properly checked the identity of the key holder to have ensured the correctness of the public key when it issues certificate to be secure from computer piracy and to have made arrangements with all participants to check all their certificates before protected communications can begin
web browsers for instance are supplied with long list of self signed identity certificates from pki providers these are used to check the bona fides of the certificate authority and then in second step the certificates of potential communicators
an attacker who could subvert one of those certificate authorities into issuing certificate for bogus public key could then mount man in the middle attack as easily as if the certificate scheme were not used at all
in an alternative scenario rarely discussed an attacker who penetrates an authority servers and obtains its store of certificates and keys public and private would be able to spoof masquerade decrypt and forge transactions without limit
despite its theoretical and potential problems this approach is widely used
examples include tls and its predecessor ssl which are commonly used to provide security for web browser transactions for example to securely send credit card details to an online store
aside from the resistance to attack of particular key pair the security of the certification hierarchy must be considered when deploying public key systems
some certificate authority usually purpose built program running on server computer vouches for the identities assigned to specific private keys by producing digital certificate
public key digital certificates are typically valid for several years at time so the associated private keys must be held securely over that time
when private key used for certificate creation higher in the pki server hierarchy is compromised or accidentally disclosed then man in the middle attack is possible making any subordinate certificate wholly insecure
examples examples of well regarded asymmetric key techniques for varied purposes include diffie hellman key exchange protocol dss digital signature standard which incorporates the digital signature algorithm elgamal elliptic curve cryptography elliptic curve digital signature algorithm ecdsa elliptic curve diffie hellman ecdh ed and ed eddsa and ecdh eddh various password authenticated key agreement techniques paillier cryptosystem rsa encryption algorithm pkcs cramer shoup cryptosystem yak authenticated key agreement protocolexamples of asymmetric key algorithms not yet widely adopted include ntruencrypt cryptosystem kyber mceliece cryptosystemexamples of notable yet insecure asymmetric key algorithms include merkle hellman knapsack cryptosystemexamples of protocols using asymmetric key algorithms include mime gpg an implementation of openpgp and an internet standard emv emv certificate authority ipsec pgp zrtp secure voip protocol transport layer security standardized by ietf and its predecessor secure socket layer silc ssh bitcoin off the record messaging history during the early history of cryptography two parties would rely upon key that they would exchange by means of secure but non cryptographic method such as face to face meeting or trusted courier
this key which both parties must then keep absolutely secret could then be used to exchange encrypted messages
number of significant practical difficulties arise with this approach to distributing keys
anticipation in his book the principles of science william stanley jevons wrote can the reader say what two numbers multiplied together will produce the number
think it unlikely that anyone but myself will ever know
here he described the relationship of one way functions to cryptography and went on to discuss specifically the factorization problem used to create trapdoor function
in july mathematician solomon golomb said jevons anticipated key feature of the rsa algorithm for public key cryptography although he certainly did not invent the concept of public key cryptography
classified discovery in james ellis british cryptographer at the uk government communications headquarters gchq conceived of the possibility of non secret encryption now called public key cryptography but could see no way to implement it
in his colleague clifford cocks implemented what has become known as the rsa encryption algorithm giving practical method of non secret encryption and in another gchq mathematician and cryptographer malcolm williamson developed what is now known as diffie hellman key exchange
the scheme was also passed to the us national security agency
both organisations had military focus and only limited computing power was available in any case the potential of public key cryptography remained unrealised by either organization judged it most important for military use if you can share your key rapidly and electronically you have major advantage over your opponent
only at the end of the evolution from berners lee designing an open internet architecture for cern its adaptation and adoption for the arpanet did public key cryptography realise its full potential
ralph benjamin these discoveries were not publicly acknowledged for years until the research was declassified by the british government in
public discovery in an asymmetric key cryptosystem was published by whitfield diffie and martin hellman who influenced by ralph merkle work on public key distribution disclosed method of public key agreement
this method of key exchange which uses exponentiation in finite field came to be known as diffie hellman key exchange
this was the first published practical method for establishing shared secret key over an authenticated but not confidential communications channel without using prior shared secret
merkle public key agreement technique became known as merkle puzzles and was invented in and only published in this makes asymmetric encryption rather new field in cryptography although cryptography itself dates back more than years in generalization of cocks scheme was independently invented by ron rivest adi shamir and leonard adleman all then at mit
the latter authors published their work in in martin gardner scientific american column and the algorithm came to be known as rsa from their initials
rsa uses exponentiation modulo product of two very large primes to encrypt and decrypt performing both public key encryption and public key digital signatures
its security is connected to the extreme difficulty of factoring large integers problem for which there is no known efficient general technique though prime factorization may be obtained through brute force attacks this grows much more difficult the larger the prime factors are
description of the algorithm was published in the mathematical games column in the august issue of scientific american since the large number and variety of encryption digital signature key agreement and other techniques have been developed including the rabin cryptosystem elgamal encryption dsa and elliptic curve cryptography
see also notes references external links oral history interview with martin hellman charles babbage institute university of minnesota
leading cryptography scholar martin hellman discusses the circumstances and fundamental insights of his invention of public key cryptography with collaborators whitfield diffie and ralph merkle at stanford university in the mid
an account of how gchq kept their invention of pke secret until
in statistics ordinary least squares ols is type of linear least squares method for choosing the unknown parameters in linear regression model with fixed level one effects of linear function of set of explanatory variables by the principle of least squares minimizing the sum of the squares of the differences between the observed dependent variable values of the variable being observed in the input dataset and the output of the linear function of the independent variable 
geometrically this is seen as the sum of the squared distances parallel to the axis of the dependent variable between each data point in the set and the corresponding point on the regression surface the smaller the differences the better the model fits the data 
the resulting estimator can be expressed by simple formula especially in the case of simple linear regression in which there is single regressor on the right side of the regression equation 
the ols estimator is consistent for the level one fixed effects when the regressors are exogenous and forms perfect colinearity rank condition consistent for the variance estimate of the residuals when regressors have finite fourth moments and by the gauss markov theorem optimal in the class of linear unbiased estimators when the errors are homoscedastic and serially uncorrelated 
under these conditions the method of ols provides minimum variance mean unbiased estimation when the errors have finite variances 
under the additional assumption that the errors are normally distributed with zero mean ols is the maximum likelihood estimator that outperforms any non linear unbiased estimator 
linear model suppose the data consists of observations each observation includes scalar response and column vector of parameters regressors in linear regression model the response variable is linear function of the regressors or in vector form where as introduced previously is column vector of the th observation of all the explanatory variables is vector of unknown parameters and the scalar represents unobserved random variables errors of the th observation 
accounts for the influences upon the responses from sources other than the explanators this model can also be written in matrix notation as where and are vectors of the response variables and the errors of the observations and is an matrix of regressors also sometimes called the design matrix whose row is and contains the th observations on all the explanatory variables 
as rule the constant term is always included in the set of regressors say by taking for all the coefficient corresponding to this regressor is called the intercept 
regressors do not have to be independent there can be any desired relationship between the regressors so long as it is not linear relationship 
for instance we might suspect the response depends linearly both on value and its square in which case we would include one regressor whose value is just the square of another regressor 
in that case the model would be quadratic in the second regressor but none the less is still considered linear model because the model is still linear in the parameters 
matrix vector formulation consider an overdetermined system of linear equations in unknown coefficients with note for linear model as above not all elements in contains information on the data points 
the first column is populated with ones only the other columns contain actual data 
so here is equal to the number of regressors plus one 
this can be written in matrix form as where 
such system usually has no exact solution so the goal is instead to find the coefficients which fit the equations best in the sense of solving the quadratic minimization problem where the objective function is given by justification for choosing this criterion is given in properties below 
this minimization problem has unique solution provided that the columns of the matrix are linearly independent given by solving the so called normal equations the matrix is known as the normal matrix or gram matrix and the matrix is known as the moment matrix of regressand by regressors 
finally is the coefficient vector of the least squares hyperplane expressed as or 
estimation suppose is candidate value for the parameter vector the quantity yi xitb called the residual for the th observation measures the vertical distance between the data point xi yi and the hyperplane xtb and thus assesses the degree of fit between the actual data and the model 
the sum of squared residuals ssr also called the error sum of squares ess or residual sum of squares rss is measure of the overall model fit where denotes the matrix transpose and the rows of denoting the values of all the independent variables associated with particular value of the dependent variable are xi xit 
the value of which minimizes this sum is called the ols estimator for the function is quadratic in with positive definite hessian and therefore this function possesses unique global minimum at which can be given by the explicit formula proof argmin the product xt is gram matrix and its inverse is the cofactor matrix of closely related to its covariance matrix 
the matrix xt xt xt is called the moore penrose pseudoinverse matrix of this formulation highlights the point that estimation can be carried out if and only if there is no perfect multicollinearity between the explanatory variables which would cause the gram matrix to have no inverse 
after we have estimated the fitted values or predicted values from the regression will be where xtx xt is the projection matrix onto the space spanned by the columns of this matrix is also sometimes called the hat matrix because it puts hat onto the variable another matrix closely related to is the annihilator matrix in this is projection matrix onto the space orthogonal to both matrices and are symmetric and idempotent meaning that and and relate to the data matrix via identities px and mx matrix creates the residuals from the regression using these residuals we can estimate the value of using the reduced chi squared statistic the denominator is the statistical degrees of freedom 
the first quantity is the ols estimate for whereas the second is the mle estimate for 
the two estimators are quite similar in large samples the first estimator is always unbiased while the second estimator is biased but has smaller mean squared error 
in practice is used more often since it is more convenient for the hypothesis testing 
the square root of is called the regression standard error standard error of the regression or standard error of the equation it is common to assess the goodness of fit of the ols regression by comparing how much the initial variation in the sample can be reduced by regressing onto the coefficient of determination is defined as ratio of explained variance to the total variance of the dependent variable in the cases where the regression sum of squares equals the sum of squares of residuals where tss is the total sum of squares for the dependent variable and is an matrix of ones 
is centering matrix which is equivalent to regression on constant it simply subtracts the mean from variable 
in order for to be meaningful the matrix of data on regressors must contain column vector of ones to represent the constant whose coefficient is the regression intercept 
in that case will always be number between and with values close to indicating good degree of fit 
the variance in the prediction of the independent variable as function of the dependent variable is given in the article polynomial least squares 
simple linear regression model if the data matrix contains only two variables constant and scalar regressor xi then this is called the simple regression model 
this case is often considered in the beginner statistics classes as it provides much simpler formulas even suitable for manual calculation 
the parameters are commonly denoted as the least squares estimates in this case are given by simple formulas alternative derivations in the previous section the least squares estimator was obtained as value that minimizes the sum of squared residuals of the model 
however it is also possible to derive the same estimator from other approaches 
in all cases the formula for ols estimator remains the same xtx xty the only difference is in how we interpret this result 
projection for mathematicians ols is an approximate solution to an overdetermined system of linear equations where is the unknown 
assuming the system cannot be solved exactly the number of equations is much larger than the number of unknowns we are looking for solution that could provide the smallest discrepancy between the right and left hand sides 
in other words we are looking for the solution that satisfies min where is the standard norm in the dimensional euclidean space rn 
the predicted quantity is just certain linear combination of the vectors of regressors 
thus the residual vector will have the smallest length when is projected orthogonally onto the linear subspace spanned by the columns of the ols estimator in this case can be interpreted as the coefficients of vector decomposition of py along the basis of in other words the gradient equations at the minimum can be written as geometrical interpretation of these equations is that the vector of residuals is orthogonal to the column space of since the dot product is equal to zero for any conformal vector this means that is the shortest of all possible vectors that is the variance of the residuals is the minimum possible 
this is illustrated at the right 
introducing and matrix with the assumption that matrix is non singular and kt cf 
orthogonal projections the residual vector should satisfy the following equation 
the equation and solution of linear least squares are thus described as follows another way of looking at it is to consider the regression line to be weighted average of the lines passing through the combination of any two points in the dataset 
although this way of calculation is more computationally expensive it provides better intuition on ols 
maximum likelihood the ols estimator is identical to the maximum likelihood estimator mle under the normality assumption for the error terms 
proof this normality assumption has historical importance as it provided the basis for the early work in linear regression analysis by yule and pearson 
from the properties of mle we can infer that the ols estimator is asymptotically efficient in the sense of attaining the cram rao bound for variance if the normality assumption is satisfied 
generalized method of moments in iid case the ols estimator can also be viewed as gmm estimator arising from the moment conditions these moment conditions state that the regressors should be uncorrelated with the errors 
since xi is vector the number of moment conditions is equal to the dimension of the parameter vector and thus the system is exactly identified 
this is the so called classical gmm case when the estimator does not depend on the choice of the weighting matrix 
note that the original strict exogeneity assumption xi implies far richer set of moment conditions than stated above 
in particular this assumption implies that for any vector function the moment condition xi will hold 
however it can be shown using the gauss markov theorem that the optimal choice of function is to take which results in the moment equation posted above 
properties assumptions there are several different frameworks in which the linear regression model can be cast in order to make the ols technique applicable 
each of these settings produces the same formulas and same results 
the only difference is the interpretation and the assumptions which have to be imposed in order for the method to give meaningful results 
the choice of the applicable framework depends mostly on the nature of data in hand and on the inference task which has to be performed 
one of the lines of difference in interpretation is whether to treat the regressors as random variables or as predefined constants 
in the first case random design the regressors xi are random and sampled together with the yi from some population as in an observational study 
this approach allows for more natural study of the asymptotic properties of the estimators 
in the other interpretation fixed design the regressors are treated as known constants set by design and is sampled conditionally on the values of as in an experiment 
for practical purposes this distinction is often unimportant since estimation and inference is carried out while conditioning on all results stated in this article are within the random design framework 
classical linear regression model the classical model focuses on the finite sample estimation and inference meaning that the number of observations is fixed 
this contrasts with the other approaches which study the asymptotic behavior of ols and in which the number of observations is allowed to grow to infinity 
the linear functional form must coincide with the form of the actual data generating process 
the errors in the regression should have conditional mean zero the immediate consequence of the exogeneity assumption is that the errors have mean zero and that the regressors are uncorrelated with the errors xt the exogeneity assumption is critical for the ols theory 
if it holds then the regressor variables are called exogenous 
if it doesn then those regressors that are correlated with the error term are called endogenous and the ols estimator becomes biased 
in such case the method of instrumental variables may be used to carry out inference no linear dependence 
the regressors in must all be linearly independent 
mathematically this means that the matrix must have full column rank almost surely pr rank usually it is also assumed that the regressors have finite moments up to at least the second moment 
then the matrix qxx xtx is finite and positive semi definite 
when this assumption is violated the regressors are called linearly dependent or perfectly multicollinear 
in such case the value of the regression coefficient cannot be learned although prediction of values is still possible for new values of the regressors that lie in the same linearly dependent subspace spherical errors var where in is the identity matrix in dimension and is parameter which determines the variance of each observation 
this is considered nuisance parameter in the model although usually it is also estimated 
if this assumption is violated then the ols estimates are still valid but no longer efficient 
it is customary to split this assumption into two parts homoscedasticity which means that the error term has the same variance in each observation 
when this requirement is violated this is called heteroscedasticity in such case more efficient estimator would be weighted least squares 
if the errors have infinite variance then the ols estimates will also have infinite variance although by the law of large numbers they will nonetheless tend toward the true values so long as the errors have zero mean 
in this case robust estimation techniques are recommended 
no autocorrelation the errors are uncorrelated between observations for this assumption may be violated in the context of time series data panel data cluster samples hierarchical data repeated measures data longitudinal data and other data with dependencies 
in such cases generalized least squares provides better alternative than the ols 
another expression for autocorrelation is serial correlation normality 
it is sometimes additionally assumed that the errors have normal distribution conditional on the regressors 
this assumption is not needed for the validity of the ols method although certain additional finite sample properties can be established in case when it does especially in the area of hypotheses testing 
also when the errors are normal the ols estimator is equivalent to the maximum likelihood estimator mle and therefore it is asymptotically efficient in the class of all regular estimators 
importantly the normality assumption applies only to the error terms contrary to popular misconception the response dependent variable is not required to be normally distributed 
independent and identically distributed iid in some applications especially with cross sectional data an additional assumption is imposed that all observations are independent and identically distributed 
this means that all observations are taken from random sample which makes all the assumptions listed earlier simpler and easier to interpret 
also this framework allows one to state asymptotic results as the sample size which are understood as theoretical possibility of fetching new independent observations from the data generating process 
the list of assumptions in this case is iid observations xi yi is independent from and has the same distribution as xj yj for all no perfect multicollinearity qxx xi xit is positive definite matrix exogeneity xi homoscedasticity var xi 
time series model the stochastic process is co integrating 
the regressors are predetermined xi for all the matrix qxx xi xit is of full rank and hence positive definite is martingale difference sequence with finite matrix of second moments qxx xi xit 
finite sample properties first of all under the strict exogeneity assumption the ols estimators and are unbiased meaning that their expected values coincide with the true values of the parameters proof if the strict exogeneity does not hold as is the case with many time series models where exogeneity is assumed only with respect to the past shocks but not the future ones then these estimators will be biased in finite samples 
the variance covariance matrix or simply covariance matrix of is equal to var in particular the standard error of each coefficient is equal to square root of the th diagonal element of this matrix 
the estimate of this standard error is obtained by replacing the unknown quantity with its estimate 
thus it can also be easily shown that the estimator is uncorrelated with the residuals from the model cov the gauss markov theorem states that under the spherical errors assumption that is the errors should be uncorrelated and homoscedastic the estimator is efficient in the class of linear unbiased estimators 
this is called the best linear unbiased estimator blue 
efficiency should be understood as if we were to find some other estimator which would be linear in and unbiased then var var in the sense that this is nonnegative definite matrix 
this theorem establishes optimality only in the class of linear unbiased estimators which is quite restrictive 
depending on the distribution of the error terms other non linear estimators may provide better results than ols 
assuming normality the properties listed so far are all valid regardless of the underlying distribution of the error terms 
however if you are willing to assume that the normality assumption holds that is that in then additional properties of the ols estimators can be stated 
the estimator is normally distributed with mean and variance as given before 
this estimator reaches the cram rao bound for the model and thus is optimal in the class of all unbiased estimators 
note that unlike the gauss markov theorem this result establishes optimality among both linear and non linear estimators but only in the case of normally distributed error terms 
the estimator will be proportional to the chi squared distribution the variance of this estimator is equal to which does not attain the cram rao bound of 
however it was shown that there are no unbiased estimators of with variance smaller than that of the estimator 
if we are willing to allow biased estimators and consider the class of estimators that are proportional to the sum of squared residuals ssr of the model then the best in the sense of the mean squared error estimator in this class will be ssr which even beats the cram rao bound in case when there is only one regressor moreover the estimators and are independent the fact which comes in useful when constructing the and tests for the regression 
influential observations as was mentioned before the estimator is linear in meaning that it represents linear combination of the dependent variables yi 
the weights in this linear combination are functions of the regressors and generally are unequal 
the observations with high weights are called influential because they have more pronounced effect on the value of the estimator 
to analyze which observations are influential we remove specific th observation and consider how much the estimated quantities are going to change similarly to the jackknife method 
it can be shown that the change in the ols estimator for will be equal to where hj xjt xtx xj is the th diagonal element of the hat matrix and xj is the vector of regressors corresponding to the th observation 
similarly the change in the predicted value for th observation resulting from omitting that observation from the dataset will be equal to from the properties of the hat matrix hj and they sum up to so that on average hj 
these quantities hj are called the leverages and observations with high hj are called leverage points 
usually the observations with high leverage ought to be scrutinized more carefully in case they are erroneous or outliers or in some other way atypical of the rest of the dataset 
partitioned regression sometimes the variables and corresponding parameters in the regression can be logically split into two groups so that the regression takes form where and have dimensions and are and vectors with the frisch waugh lovell theorem states that in this regression the residuals and the ols estimate will be numerically identical to the residuals and the ols estimate for in the following regression where is the annihilator matrix for regressors 
the theorem can be used to establish number of theoretical results 
for example having regression with constant and another regressor is equivalent to subtracting the means from the dependent variable and the regressor and then running the regression for the de meaned variables but without the constant term 
constrained estimation suppose it is known that the coefficients in the regression satisfy system of linear equations where is matrix of full rank and is vector of known constants where in this case least squares estimation is equivalent to minimizing the sum of squared residuals of the model subject to the constraint the constrained least squares cls estimator can be given by an explicit formula 
this expression for the constrained estimator is valid as long as the matrix xtx is invertible 
it was assumed from the beginning of this article that this matrix is of full rank and it was noted that when the rank condition fails will not be identifiable 
however it may happen that adding the restriction makes identifiable in which case one would like to find the formula for the estimator 
the estimator is equal to where is matrix such that the matrix is non singular and rtq such matrix can always be found although generally it is not unique 
the second formula coincides with the first in case when xtx is invertible 
large sample properties the least squares estimators are point estimates of the linear regression model parameters however generally we also want to know how close those estimates might be to the true values of parameters 
in other words we want to construct the interval estimates 
since we haven made any assumption about the distribution of error term it is impossible to infer the distribution of the estimators and nevertheless we can apply the central limit theorem to derive their asymptotic properties as sample size goes to infinity 
while the sample size is necessarily finite it is customary to assume that is large enough so that the true distribution of the ols estimator is close to its asymptotic limit 
we can show that under the model assumptions the least squares estimator for is consistent that is converges in probability to and asymptotically normal proof where 
intervals using this asymptotic distribution approximate two sided confidence intervals for the th component of the vector can be constructed as at the confidence level where denotes the quantile function of standard normal distribution and jj is the th diagonal element of matrix 
similarly the least squares estimator for is also consistent and asymptotically normal provided that the fourth moment of exists with limiting distribution 
these asymptotic distributions can be used for prediction testing hypotheses constructing other estimators etc as an example consider the problem of prediction 
suppose is some point within the domain of distribution of the regressors and one wants to know what the response variable would have been at that point 
the mean response is the quantity whereas the predicted response is 
clearly the predicted response is random variable its distribution can be derived from that of which allows construct confidence intervals for mean response to be constructed at the confidence level 
hypothesis testing two hypothesis tests are particularly widely used 
first one wants to know if the estimated regression equation is any better than simply predicting that all values of the response variable equal its sample mean if not it is said to have no explanatory power 
the null hypothesis of no explanatory value of the estimated regression is tested using an test 
if the calculated value is found to be large enough to exceed its critical value for the pre chosen level of significance the null hypothesis is rejected and the alternative hypothesis that the regression has explanatory power is accepted 
otherwise the null hypothesis of no explanatory power is accepted 
second for each explanatory variable of interest one wants to know whether its estimated coefficient differs significantly from zero that is whether this particular explanatory variable in fact has explanatory power in predicting the response variable 
here the null hypothesis is that the true coefficient is zero 
this hypothesis is tested by computing the coefficient statistic as the ratio of the coefficient estimate to its standard error 
if the statistic is larger than predetermined value the null hypothesis is rejected and the variable is found to have explanatory power with its coefficient significantly different from zero 
otherwise the null hypothesis of zero value of the true coefficient is accepted 
in addition the chow test is used to test whether two subsamples both have the same underlying true coefficient values 
the sum of squared residuals of regressions on each of the subsets and on the combined data set are compared by computing an statistic if this exceeds critical value the null hypothesis of no difference between the two subsets is rejected otherwise it is accepted 
example with real data the following data set gives average heights and weights for american women aged source the world almanac and book of facts 
when only one dependent variable is being modeled scatterplot will suggest the form and strength of the relationship between the dependent variable and regressors 
it might also reveal outliers heteroscedasticity and other aspects of the data that may complicate the interpretation of fitted regression model 
the scatterplot suggests that the relationship is strong and can be approximated as quadratic function 
ols can handle non linear relationships by introducing the regressor height 
the regression model then becomes multiple linear model the output from most popular statistical packages will look similar to this in this table the value column gives the least squares estimates of parameters the std error column shows standard errors of each coefficient estimate the statistic and value columns are testing whether any of the coefficients might be equal to zero 
the statistic is calculated simply as if the errors follow normal distribution follows student distribution 
under weaker conditions is asymptotically normal 
large values of indicate that the null hypothesis can be rejected and that the corresponding coefficient is not zero 
the second column value expresses the results of the hypothesis test as significance level 
conventionally values smaller than are taken as evidence that the population coefficient is nonzero 
squared is the coefficient of determination indicating goodness of fit of the regression 
this statistic will be equal to one if fit is perfect and to zero when regressors have no explanatory power whatsoever 
this is biased estimate of the population squared and will never decrease if additional regressors are added even if they are irrelevant 
adjusted squared is slightly modified version of designed to penalize for the excess number of regressors which do not add to the explanatory power of the regression 
this statistic is always smaller than can decrease as new regressors are added and even be negative for poorly fitting models log likelihood is calculated under the assumption that errors follow normal distribution 
even though the assumption is not very reasonable this statistic may still find its use in conducting lr tests 
durbin watson statistic tests whether there is any evidence of serial correlation between the residuals 
as rule of thumb the value smaller than will be an evidence of positive correlation 
akaike information criterion and schwarz criterion are both used for model selection 
generally when comparing two alternative models smaller values of one of these criteria will indicate better model 
standard error of regression is an estimate of standard error of the error term 
total sum of squares model sum of squared and residual sum of squares tell us how much of the initial variation in the sample were explained by the regression 
statistic tries to test the hypothesis that all coefficients except the intercept are equal to zero 
this statistic has distribution under the null hypothesis and normality assumption and its value indicates probability that the hypothesis is indeed true 
note that when errors are not normal this statistic becomes invalid and other tests such as wald test or lr test should be used 
ordinary least squares analysis often includes the use of diagnostic plots designed to detect departures of the data from the assumed form of the model 
these are some of the common diagnostic plots residuals against the explanatory variables in the model 
non linear relation between these variables suggests that the linearity of the conditional mean function may not hold 
different levels of variability in the residuals for different levels of the explanatory variables suggests possible heteroscedasticity 
residuals against explanatory variables not in the model 
any relation of the residuals to these variables would suggest considering these variables for inclusion in the model 
residuals against the fitted values 
residuals against the preceding residual 
this plot may identify serial correlations in the residuals an important consideration when carrying out statistical inference using regression models is how the data were sampled 
in this example the data are averages rather than measurements on individual women 
the fit of the model is very good but this does not imply that the weight of an individual woman can be predicted with high accuracy based only on her height 
sensitivity to rounding this example also demonstrates that coefficients determined by these calculations are sensitive to how the data is prepared 
the heights were originally given rounded to the nearest inch and have been converted and rounded to the nearest centimetre 
since the conversion factor is one inch to cm this is not an exact conversion 
the original inches can be recovered by round and then re converted to metric without rounding 
if this is done the results become using either of these equations to predict the weight of woman gives similar values kg with rounding vs kg without rounding 
thus seemingly small variation in the data has real effect on the coefficients but small effect on the results of the equation 
while this may look innocuous in the middle of the data range it could become significant at the extremes or in the case where the fitted model is used to project outside the data range extrapolation 
this highlights common error this example is an abuse of ols which inherently requires that the errors in the independent variable in this case height are zero or at least negligible 
the initial rounding to nearest inch plus any actual measurement errors constitute finite and non negligible error 
as result the fitted parameters are not the best estimates they are presumed to be 
though not totally spurious the error in the estimation will depend upon relative size of the and errors 
another example with less real data problem statement we can use the least square mechanism to figure out the equation of two body orbit in polar base co ordinates 
the equation typically used is cos where is the radius of how far the object is from one of the bodies 
in the equation the parameters and are used to determine the path of the orbit 
we have measured the following data 
we need to find the least squares approximation of and for the given data 
solution first we need to represent and in linear form 
so we are going to rewrite the equation as cos 
now we can use this form to represent our observational data as where is and is and is constructed by the first column being the coefficient of and the second column being the coefficient of and is the values for the respective so and 
on solving we get so and see also bayesian least squares fama macbeth regression nonlinear least squares numerical methods for linear least squares nonlinear system identification references further reading dougherty christopher 
introduction to econometrics nd ed 
new york oxford university press 
isbn gujarati damodar porter dawn 
basic econometics fifth ed 
boston mcgraw hill irwin 
isbn heij christiaan boer paul franses philip kloek teun van dijk herman 
econometric methods with applications in business and economics st ed 
oxford oxford university press 
isbn hill carter griffiths william lim guay 
principles of econometrics rd ed 
hoboken nj john wiley sons 
the simple regression model 
introductory econometrics modern approach th ed 
mason oh cengage learning
in statistics econometrics epidemiology and related disciplines the method of instrumental variables iv is used to estimate causal relationships when controlled experiments are not feasible or when treatment is not successfully delivered to every unit in randomized experiment 
intuitively ivs are used when an explanatory variable of interest is correlated with the error term in which case ordinary least squares and anova give biased results 
valid instrument induces changes in the explanatory variable but has no independent effect on the dependent variable allowing researcher to uncover the causal effect of the explanatory variable on the dependent variable 
instrumental variable methods allow for consistent estimation when the explanatory variables covariates are correlated with the error terms in regression model 
such correlation may occur when changes in the dependent variable change the value of at least one of the covariates reverse causation there are omitted variables that affect both the dependent and independent variables or the covariates are subject to non random measurement error explanatory variables that suffer from one or more of these issues in the context of regression are sometimes referred to as endogenous 
in this situation ordinary least squares produces biased and inconsistent estimates 
however if an instrument is available consistent estimates may still be obtained 
an instrument is variable that does not itself belong in the explanatory equation but is correlated with the endogenous explanatory variables conditionally on the value of other covariates 
in linear models there are two main requirements for using ivs the instrument must be correlated with the endogenous explanatory variables conditionally on the other covariates 
if this correlation is strong then the instrument is said to have strong first stage 
weak correlation may provide misleading inferences about parameter estimates and standard errors 
the instrument cannot be correlated with the error term in the explanatory equation conditionally on the other covariates 
in other words the instrument cannot suffer from the same problem as the original predicting variable 
if this condition is met then the instrument is said to satisfy the exclusion restriction 
history first use of an instrument variable occurred in book by philip wright best known for his excellent description of the production transport and sale of vegetable and animal oils in the early in the united states while in olav reiers applied the same approach in the context of errors in variables models in his dissertation giving the method its name wright attempted to determine the supply and demand for butter using panel data on prices and quantities sold in the united states 
the idea was that regression analysis could produce demand or supply curve because they are formed by the path between prices and quantities demanded or supplied 
the problem was that the observational data did not form demand or supply curve as such but rather cloud of point observations that took different shapes under varying market conditions 
it seemed that making deductions from the data remained elusive 
the problem was that price affected both supply and demand so that function describing only one of the two could not be constructed directly from the observational data 
wright correctly concluded that he needed variable that correlated with either demand or supply but not both that is an instrumental variable 
after much deliberation wright decided to use regional rainfall as his instrumental variable he concluded that rainfall affected grass production and hence milk production and ultimately butter supply but not butter demand 
in this way he was able to construct regression equation with only the instrumental variable of price and supply 
theory while the ideas behind iv extend to broad class of models very common context for iv is in linear regression 
traditionally an instrumental variable is defined as variable that is correlated with the independent variable and uncorrelated with the error term in the linear equation is vector 
is matrix usually with column of ones and perhaps with additional columns for other covariates 
consider how an instrument allows to be recovered 
recall that ols solves for such that cov when we minimize the sum of squared errors min the first order condition is exactly 
if the true model is believed to have cov due to any of the reasons listed above for example if there is an omitted variable which affects both and separately then this ols procedure will not yield the causal impact of on ols will simply pick the parameter that makes the resulting errors appear uncorrelated with consider for simplicity the single variable case 
suppose we are considering regression with one variable and constant perhaps no other covariates are necessary or perhaps we have partialed out any other relevant covariates in this case the coefficient on the regressor of interest is given by cov var 
substituting for gives cov var cov var cov var cov var cov var where is what the estimated coefficient vector would be if were not correlated with in this case it can be shown that is an unbiased estimator of if cov in the underlying model that we believe then ols gives coefficient which does not reflect the underlying causal effect of interest 
iv helps to fix this problem by identifying the parameters not based on whether is uncorrelated with but based on whether another variable is uncorrelated with if theory suggests that is related to the first stage but uncorrelated with the exclusion restriction then iv may identify the causal parameter of interest where ols fails 
because there are multiple specific ways of using and deriving iv estimators even in just the linear case iv sls gmm we save further discussion for the estimation section below 
example informally in attempting to estimate the causal effect of some variable on another an instrument is third variable which affects only through its effect on for example suppose researcher wishes to estimate the causal effect of smoking on general health 
correlation between health and smoking does not imply that smoking causes poor health because other variables such as depression may affect both health and smoking or because health may affect smoking 
it is at best difficult and expensive to conduct controlled experiments on smoking status in the general population 
the researcher may attempt to estimate the causal effect of smoking on health from observational data by using the tax rate for tobacco products as an instrument for smoking 
the tax rate for tobacco products is reasonable choice for an instrument because the researcher assumes that it can only be correlated with health through its effect on smoking 
if the researcher then finds tobacco taxes and state of health to be correlated this may be viewed as evidence that smoking causes changes in health 
angrist and krueger present survey of the history and uses of instrumental variable techniques 
graphical definition of course iv techniques have been developed among much broader class of non linear models 
general definitions of instrumental variables using counterfactual and graphical formalism were given by pearl 
the graphical definition requires that satisfy the following conditions where stands for separation and stands for the graph in which all arrows entering are cut off 
the counterfactual definition requires that satisfies where yx stands for the value that would attain had been and stands for independence 
if there are additional covariates then the above definitions are modified so that qualifies as an instrument if the given criteria hold conditional on the essence of pearl definition is the equations of interest are structural not regression 
the error term stands for all exogenous factors that affect when is held constant 
the instrument should be independent of the instrument should not affect when is held constant exclusion restriction 
the instrument should not be independent of these conditions do not rely on specific functional form of the equations and are applicable therefore to nonlinear equations where can be non additive see non parametric analysis 
they are also applicable to system of multiple equations in which and other factors affect through several intermediate variables 
an instrumental variable need not be cause of proxy of such cause may also be used if it satisfies conditions 
the exclusion restriction condition is redundant it follows from conditions and 
selecting suitable instruments since is unobserved the requirement that be independent of cannot be inferred from data and must instead be determined from the model structure the data generating process 
causal graphs are representation of this structure and the graphical definition given above can be used to quickly determine whether variable qualifies as an instrumental variable given set of covariates to see how consider the following example 
suppose that we wish to estimate the effect of university tutoring program on grade point average gpa 
the relationship between attending the tutoring program and gpa may be confounded by number of factors 
students who attend the tutoring program may care more about their grades or may be struggling with their work 
this confounding is depicted in the figures on the right through the bidirected arc between tutoring program and gpa 
if students are assigned to dormitories at random the proximity of the student dorm to the tutoring program is natural candidate for being an instrumental variable 
however what if the tutoring program is located in the college library 
in that case proximity may also cause students to spend more time at the library which in turn improves their gpa see figure 
using the causal graph depicted in the figure we see that proximity does not qualify as an instrumental variable because it is connected to gpa through the path proximity library hours gpa in 
however if we control for library hours by adding it as covariate then proximity becomes an instrumental variable since proximity is separated from gpa given library hours in 
now suppose that we notice that student natural ability affects his or her number of hours in the library as well as his or her gpa as in figure using the causal graph we see that library hours is collider and conditioning on it opens the path proximity library hours gpa 
as result proximity cannot be used as an instrumental variable 
finally suppose that library hours does not actually affect gpa because students who do not study in the library simply study elsewhere as in figure in this case controlling for library hours still opens spurious path from proximity to gpa 
however if we do not control for library hours and remove it as covariate then proximity can again be used an instrumental variable 
estimation we now revisit and expand upon the mechanics of iv in greater detail 
suppose the data are generated by process of the form where indexes observations is the th value of the dependent variable is vector of the th values of the independent variable and constant is the th value of an unobserved error term representing all causes of other than and is an unobserved parameter vector the parameter vector is the causal effect on of one unit change in each element of holding all other causes of constant 
the econometric goal is to estimate for simplicity sake assume the draws of are uncorrelated and that they are drawn from distributions with the same variance that is that the errors are serially uncorrelated and homoskedastic 
suppose also that regression model of nominally the same form is proposed 
given random sample of observations from this process the ordinary least squares estimator is where and denote column vectors of length this equation is similar to the equation involving cov in the introduction this is the matrix version of that equation 
when and are uncorrelated under certain regularity conditions the second term has an expected value conditional on of zero and converges to zero in the limit so the estimator is unbiased and consistent 
when and the other unmeasured causal variables collapsed into the term are correlated however the ols estimator is generally biased and inconsistent for in this case it is valid to use the estimates to predict values of given values of but the estimate does not recover the causal effect of on to recover the underlying parameter we introduce set of variables that is highly correlated with each endogenous component of but in our underlying model is not correlated with for simplicity one might consider to be matrix composed of column of constants and one endogenous variable and to be consisting of column of constants and one instrumental variable 
however this technique generalizes to being matrix of constant and say endogenous variables with being matrix composed of constant and instruments 
in the discussion that follows we will assume that is matrix and leave this value unspecified 
an estimator in which and are both matrices is referred to as just identified 
suppose that the relationship between each endogenous component xi and the instruments is given by the most common iv specification uses the following estimator this specification approaches the true parameter as the sample gets large so long as in the true model as long as in the underlying process which generates the data the appropriate use of the iv estimator will identify this parameter 
this works because iv solves for the unique parameter that satisfies and therefore hones in on the true underlying parameter as the sample size grows 
now an extension suppose that there are more instruments than there are covariates in the equation of interest so that is matrix with this is often called the over identified case 
in this case the generalized method of moments gmm can be used 
the gmm iv estimator is where refers to the projection matrix this expression collapses to the first when the number of instruments is equal to the number of covariates in the equation of interest 
the over identified iv is therefore generalization of the just identified iv 
there is an equivalent under identified estimator for the case where since the parameters are the solutions to set of linear equations an under identified model using the set of equations does not have unique solution 
interpretation as two stage least squares one computational method which can be used to calculate iv estimates is two stage least squares sls or tsls 
in the first stage each explanatory variable that is an endogenous covariate in the equation of interest is regressed on all of the exogenous variables in the model including both exogenous covariates in the equation of interest and the excluded instruments 
the predicted values from these regressions are obtained stage regress each column of on errors and save the predicted values in the second stage the regression of interest is estimated as usual except that in this stage each endogenous covariate is replaced with the predicted values from the first stage stage regress on the predicted values from the first stage which gives sls this method is only valid in linear models 
for categorical endogenous covariates one might be tempted to use different first stage than ordinary least squares such as probit model for the first stage followed by ols for the second 
this is commonly known in the econometric literature as the forbidden regression because second stage iv parameter estimates are consistent only in special cases 
the resulting estimator of is numerically identical to the expression displayed above 
small correction must be made to the sum of squared residuals in the second stage fitted model in order that the covariance matrix of is calculated correctly 
non parametric analysis when the form of the structural equations is unknown an instrumental variable can still be defined through the equations where and are two arbitrary functions and is independent of unlike linear models however measurements of and do not allow for the identification of the average causal effect of on denoted ace ace pr do 
balke and pearl derived tight bounds on ace and showed that these can provide valuable information on the sign and size of ace in linear analysis there is no test to falsify the assumption the is instrumental relative to the pair 
this is not the case when is discrete 
pearl has shown that for all and the following constraint called instrumental inequality must hold whenever satisfies the two equations above max max pr 
interpretation under treatment effect heterogeneity the exposition above assumes that the causal effect of interest does not vary across observations that is that is constant 
generally different subjects will respond in different ways to changes in the treatment when this possibility is recognized the average effect in the population of change in on may differ from the effect in given subpopulation 
for example the average effect of job training program may substantially differ across the group of people who actually receive the training and the group which chooses not to receive training 
for these reasons iv methods invoke implicit assumptions on behavioral response or more generally assumptions over the correlation between the response to treatment and propensity to receive treatment the standard iv estimator can recover local average treatment effects late rather than average treatment effects ate 
imbens and angrist demonstrate that the linear iv estimate can be interpreted under weak conditions as weighted average of local average treatment effects where the weights depend on the elasticity of the endogenous regressor to changes in the instrumental variables 
roughly that means that the effect of variable is only revealed for the subpopulations affected by the observed changes in the instruments and that subpopulations which respond most to changes in the instruments will have the largest effects on the magnitude of the iv estimate 
for example if researcher uses presence of land grant college as an instrument for college education in an earnings regression she identifies the effect of college on earnings in the subpopulation which would obtain college degree if college is present but which would not obtain degree if college is not present 
this empirical approach does not without further assumptions tell the researcher anything about the effect of college among people who would either always or never get college degree regardless of whether local college exists 
weak instruments problem as bound jaeger and baker note problem is caused by the selection of weak instruments instruments that are poor predictors of the endogenous question predictor in the first stage equation 
in this case the prediction of the question predictor by the instrument will be poor and the predicted values will have very little variation 
consequently they are unlikely to have much success in predicting the ultimate outcome when they are used to replace the question predictor in the second stage equation 
in the context of the smoking and health example discussed above tobacco taxes are weak instruments for smoking if smoking status is largely unresponsive to changes in taxes 
if higher taxes do not induce people to quit smoking or not start smoking then variation in tax rates tells us nothing about the effect of smoking on health 
if taxes affect health through channels other than through their effect on smoking then the instruments are invalid and the instrumental variables approach may yield misleading results 
for example places and times with relatively health conscious populations may both implement high tobacco taxes and exhibit better health even holding smoking rates constant so we would observe correlation between health and tobacco taxes even if it were the case that smoking has no effect on health 
in this case we would be mistaken to infer causal effect of smoking on health from the observed correlation between tobacco taxes and health 
testing for weak instruments the strength of the instruments can be directly assessed because both the endogenous covariates and the instruments are observable 
common rule of thumb for models with one endogenous regressor is the statistic against the null that the excluded instruments are irrelevant in the first stage regression should be larger than 
statistical inference and hypothesis testing when the covariates are exogenous the small sample properties of the ols estimator can be derived in straightforward manner by calculating moments of the estimator conditional on when some of the covariates are endogenous so that instrumental variables estimation is implemented simple expressions for the moments of the estimator cannot be so obtained 
generally instrumental variables estimators only have desirable asymptotic not finite sample properties and inference is based on asymptotic approximations to the sampling distribution of the estimator 
even when the instruments are uncorrelated with the error in the equation of interest and when the instruments are not weak the finite sample properties of the instrumental variables estimator may be poor 
for example exactly identified models produce finite sample estimators with no moments so the estimator can be said to be neither biased nor unbiased the nominal size of test statistics may be substantially distorted and the estimates may commonly be far away from the true value of the parameter 
testing the exclusion restriction the assumption that the instruments are not correlated with the error term in the equation of interest is not testable in exactly identified models 
if the model is overidentified there is information available which may be used to test this assumption 
the most common test of these overidentifying restrictions called the sargan hansen test is based on the observation that the residuals should be uncorrelated with the set of exogenous variables if the instruments are truly exogenous 
the sargan hansen test statistic can be calculated as the number of observations multiplied by the coefficient of determination from the ols regression of the residuals onto the set of exogenous variables 
this statistic will be asymptotically chi squared with degrees of freedom under the null that the error term is uncorrelated with the instruments 
see also control function econometrics optimal instruments references further reading greene william 
econometric analysis sixth ed 
upper saddle river pearson prentice hall 
isbn gujarati damodar porter dawn 
basic econometrics fifth ed 
new york mcgraw hill irwin 
lectures on advanced econometric theory 
introductory econometrics modern approach fifth international ed 
mason oh south western 
quasi likelihood methods for count data handbook of applied econometrics volume ed 
pesaran and schmidt oxford blackwell pp 
terza estimating count models with endogenous switching sample selection and endogenous treatment effects 
journal of econometrics pp 
econometric analysis of cross section and panel data mit press cambridge massachusetts 
external links chapter from daniel mcfadden textbook econometrics lecture topic instrumental variable on youtube by mark thoma 
econometrics lecture topic two stages least square on youtube by mark thoma
symmetric key algorithms are algorithms for cryptography that use the same cryptographic keys for both the encryption of plaintext and the decryption of ciphertext 
the keys may be identical or there may be simple transformation to go between the two keys 
the keys in practice represent shared secret between two or more parties that can be used to maintain private information link 
the requirement that both parties have access to the secret key is one of the main drawbacks of symmetric key encryption in comparison to public key encryption also known as asymmetric key encryption 
however symmetric key encryption algorithms are usually better for bulk encryption 
they have smaller key size which means less storage space and faster transmission 
due to this asymmetric key encryption is often used to exchange the secret key for symmetric key encryption 
types symmetric key encryption can use either stream ciphers or block ciphers 
stream ciphers encrypt the digits typically bytes or letters in substitution ciphers of message one at time 
an example is chacha 
substitution ciphers are well known ciphers but can be easily decrypted using frequency table 
block ciphers take number of bits and encrypt them in single unit padding the plaintext to achieve multiple of the block size 
the advanced encryption standard aes algorithm approved by nist in december uses bit blocks 
implementations examples of popular symmetric key algorithms include twofish serpent aes rijndael camellia salsa chacha blowfish cast kuznyechik rc des des skipjack safer and idea 
use as cryptographic primitive symmetric ciphers are commonly used to achieve other cryptographic primitives than just encryption encrypting message does not guarantee that it will remain unchanged while encrypted 
hence often message authentication code is added to ciphertext to ensure that changes to the ciphertext will be noted by the receiver 
message authentication codes can be constructed from an aead cipher 
however symmetric ciphers cannot be used for non repudiation purposes except by involving additional parties 
see the iso iec standard 
another application is to build hash functions from block ciphers 
see one way compression function for descriptions of several such methods 
construction of symmetric ciphers many modern block ciphers are based on construction proposed by horst feistel 
feistel construction makes it possible to build invertible functions from other functions that are themselves not invertible 
security of symmetric ciphers symmetric ciphers have historically been susceptible to known plaintext attacks chosen plaintext attacks differential cryptanalysis and linear cryptanalysis 
careful construction of the functions for each round can greatly reduce the chances of successful attack 
it is also possible to increase the key length or the rounds in the encryption process to better protect against attack 
this however tends to increase the processing power and decrease the speed at which the process runs due to the amount of operations the system needs to do most modern symmetric key algorithms appear to be resistant to the threat of post quantum cryptography 
quantum computers would exponentially increase the speed at which these ciphers can be decoded notably grover algorithm would take the square root of the time traditionally required for brute force attack although these vulnerabilities can be compensated for by doubling key length 
for example bit aes cipher would not be secure against such an attack as it would reduce the time required to test all possible iterations from over quintillion years to about six months 
by contrast it would still take quantum computer the same amount of time to decode bit aes cipher as it would conventional computer to decode bit aes cipher 
for this reason aes is believed to be quantum resistant 
key management key establishment symmetric key algorithms require both the sender and the recipient of message to have the same secret key 
all early cryptographic systems required either the sender or the recipient to somehow receive copy of that secret key over physically secure channel 
nearly all modern cryptographic systems still use symmetric key algorithms internally to encrypt the bulk of the messages but they eliminate the need for physically secure channel by using diffie hellman key exchange or some other public key protocol to securely come to agreement on fresh new secret key for each session conversation forward secrecy 
key generation when used with asymmetric ciphers for key transfer pseudorandom key generators are nearly always used to generate the symmetric cipher session keys 
however lack of randomness in those generators or in their initialization vectors is disastrous and has led to cryptanalytic breaks in the past 
therefore it is essential that an implementation use source of high entropy for its initialization 
reciprocal cipher reciprocal cipher is cipher where just as one enters the plaintext into the cryptography system to get the ciphertext one could enter the ciphertext into the same place in the system to get the plaintext 
reciprocal cipher is also sometimes referred as self reciprocal cipher practically all mechanical cipher machines implement reciprocal cipher mathematical involution on each typed in letter 
instead of designing two kinds of machines one for encrypting and one for decrypting all the machines can be identical and can be set up keyed the same way examples of reciprocal ciphers include atbash beaufort cipher enigma machine marie antoinette and axel von fersen communicated with self reciprocal cipher 
the porta polyalphabetic cipher is self reciprocal 
purple cipher rc rot xor cipher vatsyayana cipherthe majority of all modern ciphers can be classified as either stream cipher most of which use reciprocal xor cipher combiner or block cipher most of which use feistel cipher or lai massey scheme with reciprocal transformation in each round
linear least squares lls is the least squares approximation of linear functions to data 
it is set of formulations for solving statistical problems involved in linear regression including variants for ordinary unweighted weighted and generalized correlated residuals 
numerical methods for linear least squares include inverting the matrix of the normal equations and orthogonal decomposition methods 
main formulations the three main linear least squares formulations are ordinary least squares ols is the most common estimator 
ols estimates are commonly used to analyze both experimental and observational data 
the ols method minimizes the sum of squared residuals and leads to closed form expression for the estimated value of the unknown parameter vector where is vector whose ith element is the ith observation of the dependent variable and is matrix whose ij element is the ith observation of the jth independent variable 
the estimator is unbiased and consistent if the errors have finite variance and are uncorrelated with the regressors where is the transpose of row of the matrix it is also efficient under the assumption that the errors have finite variance and are homoscedastic meaning that xi does not depend on the condition that the errors are uncorrelated with the regressors will generally be satisfied in an experiment but in the case of observational data it is difficult to exclude the possibility of an omitted covariate that is related to both the observed covariates and the response variable 
the existence of such covariate will generally lead to correlation between the regressors and the response variable and hence to an inconsistent estimator of the condition of homoscedasticity can fail with either experimental or observational data 
if the goal is either inference or predictive modeling the performance of ols estimates can be poor if multicollinearity is present unless the sample size is large 
weighted least squares wls are used when heteroscedasticity is present in the error terms of the model 
generalized least squares gls is an extension of the ols method that allows efficient estimation of when either heteroscedasticity or correlations or both are present among the error terms of the model as long as the form of heteroscedasticity and correlation is known independently of the data 
to handle heteroscedasticity when the error terms are uncorrelated with each other gls minimizes weighted analogue to the sum of squared residuals from ols regression where the weight for the ith case is inversely proportional to var 
this special case of gls is called weighted least squares 
the gls solution to an estimation problem is where is the covariance matrix of the errors 
gls can be viewed as applying linear transformation to the data so that the assumptions of ols are met for the transformed data 
for gls to be applied the covariance structure of the errors must be known up to multiplicative constant 
alternative formulations other formulations include iteratively reweighted least squares irls is used when heteroscedasticity or correlations or both are present among the error terms of the model but where little is known about the covariance structure of the errors independently of the data 
in the first iteration ols or gls with provisional covariance structure is carried out and the residuals are obtained from the fit 
based on the residuals an improved estimate of the covariance structure of the errors can usually be obtained 
subsequent gls iteration is then performed using this estimate of the error structure to define the weights 
the process can be iterated to convergence but in many cases only one iteration is sufficient to achieve an efficient estimate of instrumental variables regression iv can be performed when the regressors are correlated with the errors 
in this case we need the existence of some auxiliary instrumental variables zi such that zi if is the matrix of instruments then the estimator can be given in closed form as optimal instruments regression is an extension of classical iv regression to the situation where zi total least squares tls is an approach to least squares estimation of the linear regression model that treats the covariates and response variable in more geometrically symmetric manner than ols 
it is one approach to handling the errors in variables problem and is also sometimes used even when the covariates are assumed to be error free percentage least squares focuses on reducing percentage errors which is useful in the field of forecasting or time series analysis 
it is also useful in situations where the dependent variable has wide range without constant variance as here the larger residuals at the upper end of the range would dominate if ols were used 
when the percentage or relative error is normally distributed least squares percentage regression provides maximum likelihood estimates 
percentage regression is linked to multiplicative error model whereas ols is linked to models containing an additive error term constrained least squares indicates linear least squares problem with additional constraints on the solution 
objective function in ols assuming unweighted observations the optimal value of the objective function is found by substituting the optimal expression for the coefficient vector where the latter equality holding since is symmetric and idempotent 
it can be shown from this that under an appropriate assignment of weights the expected value of is if instead unit weights are assumed the expected value of is where is the variance of each observation 
if it is assumed that the residuals belong to normal distribution the objective function being sum of weighted squared residuals will belong to chi squared distribution with degrees of freedom 
some illustrative percentile values of are given in the following table 
these values can be used for statistical criterion as to the goodness of fit 
when unit weights are used the numbers should be divided by the variance of an observation 
for wls the ordinary objective function above is replaced for weighted average of residuals 
discussion in statistics and mathematics linear least squares is an approach to fitting mathematical or statistical model to data in cases where the idealized value provided by the model for any data point is expressed linearly in terms of the unknown parameters of the model 
the resulting fitted model can be used to summarize the data to predict unobserved values from the same system and to understand the mechanisms that may underlie the system 
mathematically linear least squares is the problem of approximately solving an overdetermined system of linear equations where is not an element of the column space of the matrix the approximate solution is realized as an exact solution to where is the projection of onto the column space of the best approximation is then that which minimizes the sum of squared differences between the data values and their corresponding modeled values 
the approach is called linear least squares since the assumed function is linear in the parameters to be estimated 
linear least squares problems are convex and have closed form solution that is unique provided that the number of data points used for fitting equals or exceeds the number of unknown parameters except in special degenerate situations 
in contrast non linear least squares problems generally must be solved by an iterative procedure and the problems can be non convex with multiple optima for the objective function 
if prior distributions are available then even an underdetermined system can be solved using the bayesian mmse estimator 
in statistics linear least squares problems correspond to particularly important type of statistical model called linear regression which arises as particular form of regression analysis 
one basic form of such model is an ordinary least squares model 
the present article concentrates on the mathematical aspects of linear least squares problems with discussion of the formulation and interpretation of statistical regression models and statistical inferences related to these being dealt with in the articles just mentioned 
see outline of regression analysis for an outline of the topic 
properties if the experimental errors are uncorrelated have mean of zero and constant variance the gauss markov theorem states that the least squares estimator has the minimum variance of all estimators that are linear combinations of the observations 
in this sense it is the best or optimal estimator of the parameters 
note particularly that this property is independent of the statistical distribution function of the errors 
in other words the distribution function of the errors need not be normal distribution 
however for some probability distributions there is no guarantee that the least squares solution is even possible given the observations still in such cases it is the best estimator that is both linear and unbiased 
for example it is easy to show that the arithmetic mean of set of measurements of quantity is the least squares estimator of the value of that quantity 
if the conditions of the gauss markov theorem apply the arithmetic mean is optimal whatever the distribution of errors of the measurements might be 
however in the case that the experimental errors do belong to normal distribution the least squares estimator is also maximum likelihood estimator these properties underpin the use of the method of least squares for all types of data fitting even when the assumptions are not strictly valid 
limitations an assumption underlying the treatment given above is that the independent variable is free of error 
in practice the errors on the measurements of the independent variable are usually much smaller than the errors on the dependent variable and can therefore be ignored 
when this is not the case total least squares or more generally errors in variables models or rigorous least squares should be used 
this can be done by adjusting the weighting scheme to take into account errors on both the dependent and independent variables and then following the standard procedure in some cases the weighted normal equations matrix xtx is ill conditioned 
when fitting polynomials the normal equations matrix is vandermonde matrix 
vandermonde matrices become increasingly ill conditioned as the order of the matrix increases 
in these cases the least squares estimate amplifies the measurement noise and may be grossly inaccurate 
various regularization techniques can be applied in such cases the most common of which is called ridge regression 
if further information about the parameters is known for example range of possible values of then various techniques can be used to increase the stability of the solution 
for example see constrained least squares 
another drawback of the least squares estimator is the fact that the norm of the residuals is minimized whereas in some cases one is truly interested in obtaining small error in the parameter small value of 
however since the true parameter is necessarily unknown this quantity cannot be directly minimized 
if prior probability on is known then bayes estimator can be used to minimize the mean squared error 
the least squares method is often applied when no prior is known 
surprisingly when several parameters are being estimated jointly better estimators can be constructed an effect known as stein phenomenon 
for example if the measurement error is gaussian several estimators are known which dominate or outperform the least squares technique the best known of these is the james stein estimator 
this is an example of more general shrinkage estimators that have been applied to regression problems 
applications polynomial fitting models are polynomials in an independent variable straight line quadratic cubic quartic and higher polynomials 
for regression with high order polynomials the use of orthogonal polynomials is recommended 
numerical smoothing and differentiation this is an application of polynomial fitting 
multinomials in more than one independent variable including surface fitting curve fitting with splines chemometrics calibration curve standard addition gran plot analysis of mixtures uses in data fitting the primary application of linear least squares is in data fitting 
given set of data points consisting of experimentally measured values taken at values of an independent variable may be scalar or vector quantities and given model function with it is desired to find the parameters such that the model function best fits the data 
in linear least squares linearity is meant to be with respect to parameters so here the functions may be nonlinear with respect to the variable ideally the model function fits the data exactly so for all this is usually not possible in practice as there are more data points than there are parameters to be determined 
the approach chosen then is to find the minimal possible value of the sum of squares of the residuals so to minimize the function after substituting for and then for this minimization problem becomes the quadratic minimization problem above with and the best fit can be found by solving the normal equations 
example as result of an experiment four data points were obtained and shown in red in the diagram on the right 
we hope to find line that best fits these four points 
in other words we would like to find the numbers and that approximately solve the overdetermined linear system of four equations in two unknowns in some best sense 
represents the residual at each point between the curve fit and the data the least squares approach to solving this problem is to try to make the sum of the squares of these residuals as small as possible that is to find the minimum of the function the minimum is determined by calculating the partial derivatives of with respect to and and setting them to zero this results in system of two equations in two unknowns called the normal equations which when solved give and the equation is the line of best fit 
the residuals that is the differences between the values from the observations and the predicated variables by using the line of best fit are then found to be and see the diagram on the right 
the minimum value of the sum of squares of the residuals is more generally one can have regressors and linear model using quadratic model importantly in linear least squares we are not restricted to using line as the model as in the above example 
for instance we could have chosen the restricted quadratic model this model is still linear in the parameter so we can still perform the same analysis constructing system of equations from the data points the partial derivatives with respect to the parameters this time there is only one are again computed and set to and solved leading to the resulting best fit model 
see also line line intersection nearest point to non intersecting lines an application line fitting nonlinear least squares regularized least squares simple linear regression partial least squares regression linear function references further reading bevington philip robinson keith 
data reduction and error analysis for the physical sciences 
external links least squares fitting from mathworld least squares fitting polynomial from mathworld
public key cryptography or asymmetric cryptography is the field of cryptographic systems that use pairs of related keys 
each key pair consists of public key and corresponding private key 
key pairs are generated with cryptographic algorithms based on mathematical problems termed one way functions 
security of public key cryptography depends on keeping the private key secret the public key can be openly distributed without compromising security in public key encryption system anyone with public key can encrypt message yielding ciphertext but only those who know the corresponding private key can decrypt the ciphertext to obtain the original message for example journalist can publish the public key of an encryption key pair on web site so that sources can send secret messages to the news organization in ciphertext 
only the journalist who knows the corresponding private key can decrypt the ciphertexts to obtain the sources messages an eavesdropper reading email on its way to the journalist can decrypt the ciphertexts 
however public key encryption doesn conceal metadata like what computer source used to send message when they sent it or how long it is 
public key encryption on its own also doesn tell the recipient anything about who sent message it just conceals the content of message in ciphertext that can only be decrypted with the private key 
in digital signature system sender can use private key together with message to create signature 
anyone with the corresponding public key can verify whether the signature matches the message but forger who doesn know the private key can find any message signature pair that will pass verification with the public key for example software publisher can create signature key pair and include the public key in software installed on computers 
later the publisher can distribute an update to the software signed using the private key and any computer receiving an update can confirm it is genuine by verifying the signature using the public key 
as long as the software publisher keeps the private key secret even if forger can distribute malicious updates to computers they can convince the computers that any malicious updates are genuine 
public key algorithms are fundamental security primitives in modern cryptosystems including applications and protocols which offer assurance of the confidentiality authenticity and non repudiability of electronic communications and data storage 
they underpin numerous internet standards such as transport layer security tls ssh mime and pgp 
some public key algorithms provide key distribution and secrecy diffie hellman key exchange some provide digital signatures digital signature algorithm and some provide both rsa 
compared to symmetric encryption asymmetric encryption is rather slower than good symmetric encryption too slow for many purposes 
today cryptosystems such as tls secure shell use both symmetric encryption and asymmetric encryption often by using asymmetric encryption to securely exchange secret key which is then used for symmetric encryption 
description before the mid all cipher systems used symmetric key algorithms in which the same cryptographic key is used with the underlying algorithm by both the sender and the recipient who must both keep it secret 
of necessity the key in every such system had to be exchanged between the communicating parties in some secure way prior to any use of the system for instance via secure channel 
this requirement is never trivial and very rapidly becomes unmanageable as the number of participants increases or when secure channels aren available or when as is sensible cryptographic practice keys are frequently changed 
in particular if messages are meant to be secure from other users separate key is required for each possible pair of users 
by contrast in public key system the public keys can be disseminated widely and openly and only the corresponding private keys need be kept secret by its owner 
two of the best known uses of public key cryptography are public key encryption in which message is encrypted with the intended recipient public key 
for properly chosen and used algorithms messages cannot in practice be decrypted by anyone who does not possess the matching private key who is thus presumed to be the owner of that key and so the person associated with the public key 
this can be used to ensure confidentiality of message 
digital signatures in which message is signed with the sender private key and can be verified by anyone who has access to the sender public key 
this verification proves that the sender had access to the private key and therefore is very likely to be the person associated with the public key 
it also proves that the signature was prepared for that exact message since verification will fail for any other message one could devise without using the private key one important issue is confidence proof that particular public key is authentic 
that it is correct and belongs to the person or entity claimed and has not been tampered with or replaced by some perhaps malicious third party 
there are several possible approaches including public key infrastructure pki in which one or more third parties known as certificate authorities certify ownership of key pairs 
tls relies upon this 
this implies that the pki system software hardware and management is trust able by all involved 
web of trust which decentralizes authentication by using individual endorsements of links between user and the public key belonging to that user 
pgp uses this approach in addition to lookup in the domain name system dns 
the dkim system for digitally signing emails also uses this approach 
applications the most obvious application of public key encryption system is for encrypting communication to provide confidentiality message that sender encrypts using the recipient public key which can be decrypted only by the recipient paired private key 
another application in public key cryptography is the digital signature 
digital signature schemes can be used for sender authentication 
non repudiation systems use digital signatures to ensure that one party cannot successfully dispute its authorship of document or communication 
further applications built on this foundation include digital cash password authenticated key agreement time stamping services and non repudiation protocols 
hybrid cryptosystems because asymmetric key algorithms are nearly always much more computationally intensive than symmetric ones it is common to use public private asymmetric key exchange algorithm to encrypt and exchange symmetric key which is then used by symmetric key cryptography to transmit data using the now shared symmetric key for symmetric key encryption algorithm 
pgp ssh and the ssl tls family of schemes use this procedure they are thus called hybrid cryptosystems 
the initial asymmetric cryptography based key exchange to share server generated symmetric key from the server to client has the advantage of not requiring that symmetric key be pre shared manually such as on printed paper or discs transported by courier while providing the higher data throughput of symmetric key cryptography over asymmetric key cryptography for the remainder of the shared connection 
weaknesses as with all security related systems it is important to identify potential weaknesses 
aside from poor choice of an asymmetric key algorithm there are few which are widely regarded as satisfactory or too short key length the chief security risk is that the private key of pair becomes known 
all security of messages authentication etc will then be lost 
algorithms all public key schemes are in theory susceptible to brute force key search attack 
however such an attack is impractical if the amount of computation needed to succeed termed the work factor by claude shannon is out of reach of all potential attackers 
in many cases the work factor can be increased by simply choosing longer key 
but other algorithms may inherently have much lower work factors making resistance to brute force attack from longer keys irrelevant 
some special and specific algorithms have been developed to aid in attacking some public key encryption algorithms both rsa and elgamal encryption have known attacks that are much faster than the brute force approach 
none of these are sufficiently improved to be actually practical however 
major weaknesses have been found for several formerly promising asymmetric key algorithms 
the knapsack packing algorithm was found to be insecure after the development of new attack 
as with all cryptographic functions public key implementations may be vulnerable to side channel attacks that exploit information leakage to simplify the search for secret key 
these are often independent of the algorithm being used 
research is underway to both discover and to protect against new attacks 
alteration of public keys another potential security vulnerability in using asymmetric keys is the possibility of man in the middle attack in which the communication of public keys is intercepted by third party the man in the middle and then modified to provide different public keys instead 
encrypted messages and responses must in all instances be intercepted decrypted and re encrypted by the attacker using the correct public keys for the different communication segments so as to avoid suspicion 
communication is said to be insecure where data is transmitted in manner that allows for interception also called sniffing 
these terms refer to reading the sender private data in its entirety 
communication is particularly unsafe when interceptions can be prevented or monitored by the sender man in the middle attack can be difficult to implement due to the complexities of modern security protocols 
however the task becomes simpler when sender is using insecure media such as public networks the internet or wireless communication 
in these cases an attacker can compromise the communications infrastructure rather than the data itself 
hypothetical malicious staff member at an internet service provider isp might find man in the middle attack relatively straightforward 
capturing the public key would only require searching for the key as it gets sent through the isp communications hardware in properly implemented asymmetric key schemes this is not significant risk 
in some advanced man in the middle attacks one side of the communication will see the original data while the other will receive malicious variant 
asymmetric man in the middle attacks can prevent users from realizing their connection is compromised 
this remains so even when one user data is known to be compromised because the data appears fine to the other user 
this can lead to confusing disagreements between users such as it must be on your end 
when neither user is at fault 
hence man in the middle attacks are only fully preventable when the communications infrastructure is physically controlled by one or both parties such as via wired route inside the sender own building 
in summation public keys are easier to alter when the communications hardware used by sender is controlled by an attacker 
public key infrastructure one approach to prevent such attacks involves the use of public key infrastructure pki set of roles policies and procedures needed to create manage distribute use store and revoke digital certificates and manage public key encryption 
however this has potential weaknesses 
for example the certificate authority issuing the certificate must be trusted by all participating parties to have properly checked the identity of the key holder to have ensured the correctness of the public key when it issues certificate to be secure from computer piracy and to have made arrangements with all participants to check all their certificates before protected communications can begin 
web browsers for instance are supplied with long list of self signed identity certificates from pki providers these are used to check the bona fides of the certificate authority and then in second step the certificates of potential communicators 
an attacker who could subvert one of those certificate authorities into issuing certificate for bogus public key could then mount man in the middle attack as easily as if the certificate scheme were not used at all 
in an alternative scenario rarely discussed an attacker who penetrates an authority servers and obtains its store of certificates and keys public and private would be able to spoof masquerade decrypt and forge transactions without limit 
despite its theoretical and potential problems this approach is widely used 
examples include tls and its predecessor ssl which are commonly used to provide security for web browser transactions for example to securely send credit card details to an online store 
aside from the resistance to attack of particular key pair the security of the certification hierarchy must be considered when deploying public key systems 
some certificate authority usually purpose built program running on server computer vouches for the identities assigned to specific private keys by producing digital certificate 
public key digital certificates are typically valid for several years at time so the associated private keys must be held securely over that time 
when private key used for certificate creation higher in the pki server hierarchy is compromised or accidentally disclosed then man in the middle attack is possible making any subordinate certificate wholly insecure 
examples examples of well regarded asymmetric key techniques for varied purposes include diffie hellman key exchange protocol dss digital signature standard which incorporates the digital signature algorithm elgamal elliptic curve cryptography elliptic curve digital signature algorithm ecdsa elliptic curve diffie hellman ecdh ed and ed eddsa and ecdh eddh various password authenticated key agreement techniques paillier cryptosystem rsa encryption algorithm pkcs cramer shoup cryptosystem yak authenticated key agreement protocolexamples of asymmetric key algorithms not yet widely adopted include ntruencrypt cryptosystem kyber mceliece cryptosystemexamples of notable yet insecure asymmetric key algorithms include merkle hellman knapsack cryptosystemexamples of protocols using asymmetric key algorithms include mime gpg an implementation of openpgp and an internet standard emv emv certificate authority ipsec pgp zrtp secure voip protocol transport layer security standardized by ietf and its predecessor secure socket layer silc ssh bitcoin off the record messaging history during the early history of cryptography two parties would rely upon key that they would exchange by means of secure but non cryptographic method such as face to face meeting or trusted courier 
this key which both parties must then keep absolutely secret could then be used to exchange encrypted messages 
number of significant practical difficulties arise with this approach to distributing keys 
anticipation in his book the principles of science william stanley jevons wrote can the reader say what two numbers multiplied together will produce the number 
think it unlikely that anyone but myself will ever know 
here he described the relationship of one way functions to cryptography and went on to discuss specifically the factorization problem used to create trapdoor function 
in july mathematician solomon golomb said jevons anticipated key feature of the rsa algorithm for public key cryptography although he certainly did not invent the concept of public key cryptography 
classified discovery in james ellis british cryptographer at the uk government communications headquarters gchq conceived of the possibility of non secret encryption now called public key cryptography but could see no way to implement it 
in his colleague clifford cocks implemented what has become known as the rsa encryption algorithm giving practical method of non secret encryption and in another gchq mathematician and cryptographer malcolm williamson developed what is now known as diffie hellman key exchange 
the scheme was also passed to the us national security agency 
both organisations had military focus and only limited computing power was available in any case the potential of public key cryptography remained unrealised by either organization judged it most important for military use if you can share your key rapidly and electronically you have major advantage over your opponent 
only at the end of the evolution from berners lee designing an open internet architecture for cern its adaptation and adoption for the arpanet did public key cryptography realise its full potential 
ralph benjamin these discoveries were not publicly acknowledged for years until the research was declassified by the british government in 
public discovery in an asymmetric key cryptosystem was published by whitfield diffie and martin hellman who influenced by ralph merkle work on public key distribution disclosed method of public key agreement 
this method of key exchange which uses exponentiation in finite field came to be known as diffie hellman key exchange 
this was the first published practical method for establishing shared secret key over an authenticated but not confidential communications channel without using prior shared secret 
merkle public key agreement technique became known as merkle puzzles and was invented in and only published in this makes asymmetric encryption rather new field in cryptography although cryptography itself dates back more than years in generalization of cocks scheme was independently invented by ron rivest adi shamir and leonard adleman all then at mit 
the latter authors published their work in in martin gardner scientific american column and the algorithm came to be known as rsa from their initials 
rsa uses exponentiation modulo product of two very large primes to encrypt and decrypt performing both public key encryption and public key digital signatures 
its security is connected to the extreme difficulty of factoring large integers problem for which there is no known efficient general technique though prime factorization may be obtained through brute force attacks this grows much more difficult the larger the prime factors are 
description of the algorithm was published in the mathematical games column in the august issue of scientific american since the large number and variety of encryption digital signature key agreement and other techniques have been developed including the rabin cryptosystem elgamal encryption dsa and elliptic curve cryptography 
see also notes references external links oral history interview with martin hellman charles babbage institute university of minnesota 
leading cryptography scholar martin hellman discusses the circumstances and fundamental insights of his invention of public key cryptography with collaborators whitfield diffie and ralph merkle at stanford university in the mid 
an account of how gchq kept their invention of pke secret until
aids is caused by human immunodeficiency virus hiv which originated in non human primates in central and west africa 
while various sub groups of the virus acquired human infectivity at different times the present pandemic had its origins in the emergence of one specific strain hiv subgroup in opoldville in the belgian congo now kinshasa in the democratic republic of the congo in the there are two types of hiv hiv and hiv 
hiv is more virulent easily transmitted and is the cause of the vast majority of hiv infections globally 
the pandemic strain of hiv is closely related to virus found in chimpanzees of the subspecies pan troglodytes troglodytes which live in the forests of the central african nations of cameroon equatorial guinea gabon the republic of the congo and the central african republic 
hiv is less transmittable and is largely confined to west africa along with its closest relative virus of the sooty mangabey cercocebus atys atys an old world monkey inhabiting southern senegal guinea bissau guinea sierra leone liberia and western ivory coast 
transmission from non humans to humans research in this area is conducted using molecular phylogenetics comparing viral genomic sequences to determine relatedness 
hiv from chimpanzees and gorillas to humans scientists generally accept that the known strains or groups of hiv are most closely related to the simian immunodeficiency viruses sivs endemic in wild ape populations of west central african forests 
in particular each of the known hiv strains is either closely related to the siv that infects the chimpanzee subspecies pan troglodytes troglodytes sivcpz or closely related to the siv that infects western lowland gorillas gorilla gorilla gorilla called sivgor 
the pandemic hiv strain group or main and rare strain found only in few cameroonian people group are clearly derived from sivcpz strains endemic in pan troglodytes troglodytes chimpanzee populations living in cameroon 
another very rare hiv strain group is clearly derived from sivgor strains of cameroon 
finally the primate ancestor of hiv group strain infecting people mostly from cameroon but also from neighbouring countries was confirmed in to be sivgor 
the pandemic hiv group is most closely related to the sivcpz collected from the southeastern rain forests of cameroon modern east province near the sangha river 
thus this region is presumably where the virus was first transmitted from chimpanzees to humans 
however reviews of the epidemiological evidence of early hiv infection in stored blood samples and of old cases of aids in central africa have led many scientists to believe that hiv group early human centre was probably not in cameroon but rather further south in the democratic republic of the congo then the belgian congo more probably in its capital city kinshasa formerly opoldville using hiv sequences preserved in human biological samples along with estimates of viral mutation rates scientists calculate that the jump from chimpanzee to human probably happened during the late th or early th century time of rapid urbanisation and colonisation in equatorial africa 
exactly when the zoonosis occurred is not known 
some molecular dating studies suggest that hiv group had its most recent common ancestor mrca that is started to spread in the human population in the early th century probably between and study published in analyzing viral sequences recovered from biopsy made in kinshasa in along with previously known sequences suggested common ancestor between and with central estimates varying between and 
genetic recombination had earlier been thought to seriously confound such phylogenetic analysis but later work has suggested that recombination is not likely to systematically bias results although recombination is expected to increase variance 
the results of phylogenetics study support the later work and indicate that hiv evolves fairly reliably 
further research was hindered due to the primates being critically endangered 
sample analyses resulted in little data due to the rarity of experimental material 
the researchers however were able to hypothesize phylogeny from the gathered data 
they were also able to use the molecular clock of specific strain of hiv to determine the initial date of transmission which is estimated to be around 
hiv from sooty mangabeys to humans similar research has been undertaken with siv strains collected from several wild sooty mangabey cercocebus atys atys sivsmm populations of the west african nations of sierra leone liberia and ivory coast 
the resulting phylogenetic analyses show that the viruses most closely related to the two strains of hiv that spread considerably in humans hiv groups and are the sivsmm found in the sooty mangabeys of the tai forest in western ivory coast there are six additional known hiv groups each having been found in just one person 
they all seem to derive from independent transmissions from sooty mangabeys to humans 
groups and have been found in two people from liberia groups and have been discovered in two people from sierra leone and groups and have been detected in two people from the ivory coast 
these hiv strains are probably dead end infections and each of them is most closely related to sivsmm strains from sooty mangabeys living in the same country where the human infection was found molecular dating studies suggest that both the epidemic groups and started to spread among humans between and with the central estimates varying between and 
bushmeat practice according to the natural transfer theory also called hunter theory or bushmeat theory in the simplest and most plausible explanation for the cross species transmission of siv or hiv post mutation the virus was transmitted from an ape or monkey to human when hunter or bushmeat vendor handler was bitten or cut while hunting or butchering the animal 
the resulting exposure to blood or other bodily fluids of the animal can result in siv infection 
prior to wwii some sub saharan africans were forced out of the rural areas because of the european demand for resources 
since rural africans were not keen to pursue agricultural practices in the jungle they turned to non domesticated animals as their primary source of meat 
this over exposure to bushmeat and malpractice of butchery increased blood to blood contact which then increased the probability of transmission 
recent serological survey showed that human infections by siv are not rare in central africa the percentage of people showing seroreactivity to antigens evidence of current or past siv infection was among the general population of cameroon in villages where bushmeat is hunted or used and in the most exposed people of these villages 
how the siv virus would have transformed into hiv after infection of the hunter or bushmeat handler from the ape monkey is still matter of debate although natural selection would favour any viruses capable of adjusting so that they could infect and reproduce in the cells of human host 
emergence unresolved questions about hiv origins and emergence the discovery of the main hiv siv phylogenetic relationships permits explaining broad hiv biogeography the early centres of the hiv groups were in central africa where the primate reservoirs of the related sivcpz and sivgor viruses chimpanzees and gorillas exist similarly the hiv groups had their centres in west africa where sooty mangabeys which harbour the related sivsmm virus exist 
however these relationships do not explain more detailed patterns of biogeography such as why epidemic hiv groups and only evolved in the ivory coast which is one of only six countries harbouring the sooty mangabey 
it is also unclear why the sivcpz endemic in the chimpanzee subspecies pan troglodytes schweinfurthii inhabiting the democratic republic of congo central african republic rwanda burundi uganda and tanzania did not spawn an epidemic hiv strain to humans while the democratic republic of congo was the main centre of hiv group virus descended from sivcpz strains of subspecies pan troglodytes troglodytes that does not exist in this country 
it is clear that the several hiv and hiv strains descend from sivcpz sivgor and sivsmm viruses and that bushmeat practice provides the most plausible cause of cross species transfer to humans 
however some loose ends remain 
it is not yet explained why only four hiv groups hiv groups and and hiv groups and spread considerably in human populations despite bushmeat practices being widespread in central and west africa and the resulting human siv infections being common it also remains unexplained why all epidemic hiv groups emerged in humans nearly simultaneously and only in the th century despite very old human exposure to siv phylogenetic study demonstrated that siv is at least tens of thousands of years old 
origin and epidemic emergence several of the theories of hiv origin accept the established knowledge of the hiv siv phylogenetic relationships and also accept that bushmeat practice was the most likely cause of the initial transfer to humans 
all of them propose that the simultaneous epidemic emergences of four hiv groups in the late th early th century and the lack of previous known emergences are explained by new factor that appeared in the relevant african regions in that timeframe 
these new factor would have acted either to increase human exposures to siv to help it to adapt to the human organism by mutation thus enhancing its between humans transmissibility or to cause an initial burst of transmissions crossing an epidemiological threshold and therefore increasing the probability of continued spread 
genetic studies of the virus suggested in that the most recent common ancestor of the hiv group dates back to the belgian congo city of opoldville modern kinshasa circa proponents of this dating link the hiv epidemic with the emergence of colonialism and growth of large colonial african cities leading to social changes including higher degree of non monogamous sexual activity the spread of prostitution and the concomitant high frequency of genital ulcer diseases such as syphilis in nascent colonial cities in study conducted by scientists from the university of oxford and the university of leuven in belgium revealed that because approximately one million people every year would flow through the prominent city of kinshasa which served as the origin of the first known hiv cases in the passengers riding on the region belgian railway trains were able to spread the virus to larger areas 
the study also identified roaring sex trade rapid population growth and unsterilised needles used in health clinics as other factors which contributed to the emergence of the africa hiv epidemic 
social changes and urbanization beatrice hahn paul sharp and their colleagues proposed that the epidemic emergence of hiv most likely reflects changes in population structure and behaviour in africa during the th century and perhaps medical interventions that provided the opportunity for rapid human to human spread of the virus 
after the scramble for africa started in the european colonial powers established cities towns and other colonial stations 
largely masculine labor force was hastily recruited to work in fluvial and sea ports railways other infrastructures and in plantations 
this disrupted traditional tribal values and favored casual sexual activity with an increased number of partners 
in the nascent cities women felt relatively liberated from rural tribal rules and many remained unmarried or divorced during long periods this being rare in african traditional societies 
this was accompanied by unprecedented increase in people movements 
michael worobey and colleagues observed that the growth of cities probably played role in the epidemic emergence of hiv since the phylogenetic dating of the two older strains of hiv groups and suggest that these viruses started to spread soon after the main central african colonial cities were founded 
colonialism in africa amit chitnis diana rawls and jim moore proposed that hiv may have emerged epidemically as result of harsh conditions forced labor displacement and unsafe injection and vaccination practices associated with colonialism particularly in french equatorial africa 
the workers in plantations construction projects and other colonial enterprises were supplied with bushmeat which would have contributed to an increase in hunting and it follows higher incidence of human exposure to siv 
several historical sources support the view that bushmeat hunting indeed increased both because of the necessity to supply workers and because firearms became more widely available the colonial authorities also gave many vaccinations against smallpox and injections of which many would be made without sterilising the equipment between uses 
proposed that both these parenteral risks and the prostitution associated with forced labor camps could have caused serial transmission or serial passage of siv between humans see discussion of this in the next section 
in addition they proposed that the conditions of extreme stress associated with forced labor could depress the immune system of workers therefore prolonging the primary acute infection period of someone newly infected by siv thus increasing the odds of both adaptation of the virus to humans and of further transmissions the authors proposed that hiv originated in the area of french equatorial africa in the early th century when the colonial abuses and forced labor were at their peak 
later research established that these theories were mostly correct hiv groups and started to spread in humans in late th early th century 
in addition all groups of hiv descend from either sivcpz or sivgor from apes living to the west of the ubangi river either in countries that belonged to the french equatorial africa federation of colonies in equatorial guinea then spanish colony or in cameroon which was german colony between and and then fell to allied forces in world war and had most of its area administered by france in close association with french equatorial africa 
this theory was later dubbed heart of darkness by jim moore alluding to the book of the same title written by joseph conrad the main focus of which is colonial abuses in equatorial africa 
unsterile injections in several articles published since preston marx philip alcabes and ernest drucker proposed that hiv emerged because of rapid serial human to human transmission of siv after bushmeat hunter or handler became siv infected through unsafe or unsterile injections 
although both chitnis et al 
and sharp et al 
also suggested that this may have been one of the major risk factors at play in hiv emergence see above marx et al 
enunciated the underlying mechanisms in greater detail and wrote the first review of the injection campaigns made in colonial africa central to the marx et al 
argument is the concept of adaptation by serial passage or serial transmission an adventitious virus or other pathogen can increase its biological adaptation to new host species if it is rapidly transmitted between hosts while each host is still in the acute infection period 
this process favors the accumulation of adaptive mutations more rapidly therefore increasing the odds that better adapted viral variant will appear in the host before the immune system suppresses the virus 
such better adapted variants could then survive in the human host for longer than the short acute infection period in high numbers high viral load which would grant it more possibilities of epidemic spread 
reported experiments of cross species transfer of siv in captive monkeys some of which made by themselves in which the use of serial passage helped to adapt siv to the new monkey species after passage by three or four animals in agreement with this model is also the fact that while both hiv and hiv attain substantial viral loads in the human organism adventitious siv infecting humans seldom does so people with siv antibodies often have very low or even undetectable siv viral load 
this suggests that both hiv and hiv are adapted to humans and serial passage could have been the process responsible for it 
proposed that unsterile injections that is injections where the needle or syringe is reused without sterilization or cleaning between uses which were likely very prevalent in africa during both the colonial period and afterwards provided the mechanism of serial passage that permitted hiv to adapt to humans therefore explaining why it emerged epidemically only in the th century 
massive injections of the antibiotic era marx et al 
emphasize the massive number of injections administered in africa after antibiotics were introduced around as being the most likely implicated in the origin of hiv because by these times roughly in the period to injection intensity in africa was maximal 
they argued that serial passage chain of or transmissions between humans is an unlikely event the probability of transmission after needle reuse is something between and and only few people have an acute siv infection at any time and so hiv emergence may have required the very high frequency of injections of the antibiotic era the molecular dating studies place the initial spread of the epidemic hiv groups before that time see above 
according to marx et al these studies could have overestimated the age of the hiv groups because they depend on molecular clock assumption may not have accounted for the effects of natural selection in the viruses and the serial passage process alone would be associated with strong natural selection 
injection campaigns against sleeping sickness david gisselquist proposed that the mass injection campaigns to treat trypanosomiasis sleeping sickness in central africa were responsible for the emergence of hiv 
unlike marx et al gisselquist argued that the millions of unsafe injections administered during these campaigns were sufficient to spread rare hiv infections into an epidemic and that evolution of hiv through serial passage was not essential to the emergence of the hiv epidemic in the th century this theory focuses on injection campaigns that peaked in the period that is around the time the hiv groups started to spread 
it also focuses on the fact that many of the injections in these campaigns were intravenous which are more likely to transmit siv hiv than subcutaneous or intramuscular injections and many of the patients received many often more than injections per year therefore increasing the odds of siv serial passage 
other early injection campaigns jacques pin and annie claude labb reviewed the colonial health reports of cameroon and french equatorial africa for the period calculating the incidences of the diseases requiring intravenous injections 
they concluded that trypanosomiasis leprosy yaws and syphilis were responsible for most intravenous injections 
schistosomiasis tuberculosis and vaccinations against smallpox represented lower parenteral risks schistosomiasis cases were relatively few tuberculosis patients only became numerous after mid century and there were few smallpox vaccinations in the lifetime of each person the authors suggested that the very high prevalence of the hepatitis virus in southern cameroon and forested areas of french equatorial africa around can be better explained by the unsterile injections used to treat yaws because this disease was much more prevalent than syphilis trypanosomiasis and leprosy in these areas 
they suggested that all these parenteral risks caused not only the massive spread of hepatitis but also the spread of other pathogens and the emergence of hiv the same procedures could have exponentially amplified hiv from single hunter cook occupationally infected with sivcpz to several thousand patients treated with arsenicals or other drugs threshold beyond which sexual transmission could prosper 
they do not suggest specifically serial passage as the mechanism of adaptation 
according to pin book the origins of aids the virus can be traced to central african bush hunter in with colonial medical campaigns using improperly sterilized syringe and needles playing key role in enabling future epidemic 
pin concludes that aids spread silently in africa for decades fueled by urbanization and prostitution since the initial cross species infection 
pin also claims that the virus was brought to the americas by haitian teacher returning home from zaire in the 
sex tourism and contaminated blood transfusion centers ultimately propelled aids to public consciousness in the and worldwide pandemic 
genital ulcer diseases and evolution of sexual activity jo dinis de sousa viktor ller philippe lemey and anne mieke vandamme proposed that hiv became epidemic through sexual serial transmission in nascent colonial cities helped by high frequency of genital ulcers caused by genital ulcer diseases gud 
gud are simply sexually transmitted diseases that cause genital ulcers examples are syphilis chancroid lymphogranuloma venereum and genital herpes 
these diseases increase the probability of hiv transmission dramatically from around to per heterosexual act because the genital ulcers provide portal of viral entry and contain many activated cells expressing the ccr co receptor the main cell targets of hiv 
probable time interval of cross species transfer sousa et al 
use molecular dating techniques to estimate the time when each hiv group split from its closest siv lineage 
each hiv group necessarily crossed to humans between this time and the time when it started to spread the time of the mrca because after the mrca certainly all lineages were already in humans and before the split with the closest simian strain the lineage was in simian 
hiv groups and split from their closest sivs around and respectively 
this information together with the datations of the hiv groups mrcas mean that all hiv groups likely crossed to humans in the early th century 
strong genital ulcer disease incidence in nascent colonial cities the authors reviewed colonial medical articles and archived medical reports of the countries at or near the ranges of chimpanzees gorillas and sooty mangabeys and found that genital ulcer diseases guds peaked in the colonial cities during their early growth period up to 
the colonial authorities recruited men to work in railways fluvial and sea ports and other infrastructure projects and most of these men did not bring their wives with them 
then the highly male biased sex ratio favoured prostitution which in its turn caused an explosion of gud especially syphilis and chancroid 
after the mid people movements were more tightly controlled and mass surveys and treatments of arsenicals and other drugs were organized and so the gud incidences started to decline 
they declined even further after world war ii because of the heavy use of antibiotics so that by the late opoldville which is the probable center of hiv group had very low gud incidence 
similar processes happened in the cities of cameroon and ivory coast where hiv group and hiv respectively evolved therefore the peak gud incidences in cities have good temporal coincidence with the period when all main hiv groups crossed to humans and started to spread 
in addition the authors gathered evidence that syphilis and the other guds were like injections absent from the densely forested areas of central and west africa before organized colonialism socially disrupted these areas starting in the 
thus this theory also potentially explains why hiv emerged only after the late th century 
female genital mutilation uli linke has argued that the practice of female genital mutilation either or both of clitoridectomy and infibulation is responsible for the high incidence of aids in africa since intercourse with female who has undergone clitoridectomy is conducive to exchange of blood 
male circumcision distribution and hiv origins male circumcision may reduce the probability of hiv acquisition by men 
leaving aside blood transfusions the highest hiv transmissibility ever measured was from female prostitutes with prevalence of hiv to uncircumcised men with gud cumulative seroconverted to hiv after single sexual exposure 
there was no seroconversion in the absence of male gud 
reasoned that the adaptation and epidemic emergence of each hiv group may have required such extreme conditions and thus reviewed the existing ethnographic literature for patterns of male circumcision and hunting of apes and monkeys for bushmeat focusing on the period and on most of the ethnic groups living in central and west africa 
they also collected censuses and other literature showing the ethnic composition of colonial cities in this period 
then they estimated the circumcision frequencies of the central african cities over time 
charts reveal that male circumcision frequencies were much lower in several cities of western and central africa in the early th century than they are currently 
the reason is that many ethnic groups not performing circumcision by that time gradually adopted it to imitate other ethnic groups and enhance the social acceptance of their boys colonialism produced massive intermixing between african ethnic groups 
about of men in opoldville and douala in the early th century should be uncircumcised and these cities were the probable centers of hiv groups and respectively the authors studied early circumcision frequencies in cities of central and west africa to test if this variable correlated with hiv emergence 
this correlation was strong for hiv among west african cities that could have received immigrants infected with sivsmm the two cities from the ivory coast studied abidjan and bouak had much higher frequency of uncircumcised men than the others and epidemic hiv groups emerged initially in this country only 
this correlation was less clear for hiv in central africa 
computer simulations of hiv emergence sousa et al 
then built computer simulations to test if an ill adapted siv meaning simian immunodeficiency virus already infecting human but incapable of transmission beyond the short acute infection period could spread in colonial cities 
the simulations used parameters of sexual transmission obtained from the current hiv literature 
they modelled people sexual links with different levels of sexual partner change among different categories of people prostitutes single women with several partners year married women and men according to data obtained from modern studies of sexual activity in african cities 
the simulations let the parameters city size proportion of people married gud frequency male circumcision frequency and transmission parameters vary and explored several scenarios 
each scenario was run times to test the probability of siv generating long chains of sexual transmission 
the authors postulated that such long chains of sexual transmission were necessary for the siv strain to adapt better to humans becoming an hiv capable of further epidemic emergence 
the main result was that genital ulcer frequency was by far the most decisive factor 
for the gud levels prevailing in opoldville in the early th century long chains of siv transmission had high probability 
for the lower gud levels existing in the same city in the late see above they were much less likely 
and without gud situation typical of villages in forested equatorial africa before colonialism siv could not spread at all 
city size was not an important factor 
the authors propose that these findings explain the temporal patterns of hiv emergence no hiv emerging in tens of thousands of years of human slaughtering of apes and monkeys several hiv groups emerging in the nascent gud riddled colonial cities and no epidemically successful hiv group emerging in mid th century when gud was more controlled and cities were much bigger 
male circumcision had little to moderate effect in their simulations but given the geographical correlation found the authors propose that it could have had an indirect role either by increasing genital ulcer disease itself it is known that syphilis chancroid and several other guds have higher incidences in uncircumcised men or by permitting further spread of the hiv strain after the first chains of sexual transmission permitted adaptation to the human organism 
one of the main advantages of this theory is stressed by the authors it the theory also offers conceptual simplicity because it proposes as causal factors for siv adaptation to humans and initial spread the very same factors that most promote the continued spread of hiv nowadays promiscuous sic sex particularly involving sex workers gud and possibly lack of circumcision 
iatrogenic and other theories iatrogenic theories propose that medical interventions were responsible for hiv origins 
by proposing factors that only appeared in central and west africa after the late th century they seek to explain why all hiv groups also started after that 
the theories centred on the role of parenteral risks such as unsterile injections transfusions or smallpox vaccinations are accepted as plausible by most scientists of the field 
discredited hiv aids origins theories include several iatrogenic theories such as the polio vaccine hypothesis which argues that the early oral polio vaccines were contaminated with chimpanzee virus leading to the central african outbreak 
pathogenicity of siv in non human primates in most non human primate species natural siv infection does not cause fatal disease but see below 
comparison of the gene sequence of siv with hiv should therefore provide information about the factors necessary to cause disease in humans 
the factors that determine the virulence of hiv as compared to most sivs are only now being elucidated 
non human sivs contain nef gene that down regulates cd cd and mhc class expression most non human sivs therefore do not induce immunodeficiency the hiv nef gene however has lost its ability to down regulate cd which results in the immune activation and apoptosis that is characteristic of chronic hiv infection in addition long term survey of chimpanzees naturally infected with sivcpz in gombe national park tanzania found that contrary to the previous paradigm chimpanzees with sivcpz infection do experience an increased mortality and also suffer from human aids like illness 
siv pathogenicity in wild animals could exist in other chimpanzee subspecies and other primate species as well and stay unrecognized by lack of relevant long term studies 
history of spread david carr david carr was an apprentice printer usually mistakenly referred to as sailor carr had served in the navy between and from manchester england who died on august and was for some time mistakenly reported to have died from aids defining opportunistic infections adois 
following the failure of his immune system he succumbed to pneumonia 
doctors baffled by what he had died from preserved of his tissue samples for inspection 
in the tissues were found to be hiv positive 
however in second test by aids researcher david ho found that the strain of hiv present in the tissues was similar to those found in rather than an earlier strain which would have mutated considerably over the course of years 
he concluded that the dna samples provided actually came from patient with aids in the 
upon retesting david carr tissues he found no sign of the virus 
congolese man one of the earliest documented hiv infections was discovered in preserved blood sample taken in from man from opoldville in the belgian congo 
however it is unknown whether this anonymous person ever developed aids and died of its complications 
congolese woman second early documented hiv infection was discovered in preserved lymph node biopsy sample taken in from woman from opoldville belgian congo 
congolese man strain with large amount of the genetic material present was dated to from sample from year old man 
robert rayford in may year old african american robert rayford died at the st louis city hospital from kaposi sarcoma 
in researchers at tulane university school of medicine detected virus closely related or identical to hiv in his preserved blood and tissues 
the doctors who worked on his case at the time suspected he was prostitute or the victim of sexual abuse though the patient did not discuss his sexual history with them in detail 
ugandan children from to researchers drew blood from children in uganda to serve as controls for study of burkitt lymphoma 
in retroactive testing of the frozen blood serum indicated that antibodies to virus related to hiv were present in of the children 
arvid noe in and norwegian sailor with the alias name arvid noe his wife and his seven year old daughter died of aids 
the sailor had first presented symptoms in eight years after he first spent time in ports along the west african coastline 
gonorrhea infection during his first african voyage shows he was sexually active at this time 
tissue samples from the sailor and his wife were tested in and found to contain hiv group 
grethe rask grethe rask was danish surgeon who traveled to za re in then again in to aid the sick 
she was likely directly exposed to blood from many congolese patients one of whom infected her 
she became unwell from then returned to denmark in with her colleagues baffled by her symptoms 
she died of pneumocystis pneumonia in december her tissues were examined and tested by her colleagues and found positive in 
spread to the western hemisphere hiv strains were once thought to have arrived in new york city from haiti around it spread from new york city to san francisco around hiv is believed to have arrived in haiti from central africa possibly from the democratic republic of the congo around the current consensus is that hiv was introduced to haiti by an unknown individual or individuals who contracted it while working in the democratic republic of the congo circa mini epidemic followed and circa yet another unknown individual took hiv from haiti to the united states 
the vast majority of cases of aids outside sub saharan africa can be traced back to that single patient 
later numerous unrelated incidents of aids among haitian immigrants to the were recorded in the early 
also as evidenced by the case of robert rayford isolated occurrences of this infection may have been emerging as early as the virus eventually entered gay male communities in large united states cities where combination of casual multi partner sexual activity with individuals reportedly averaging over unprotected sexual partners per year and relatively high transmission rates associated with anal intercourse allowed it to spread explosively enough to finally be noticed because of the long incubation period of hiv up to decade or longer before symptoms of aids appear and because of the initially low incidence hiv was not noticed at first 
by the time the first reported cases of aids were found in large united states cities the prevalence of hiv infection in some communities had passed 
worldwide hiv infection has spread from urban to rural areas and has appeared in regions such as china and india 
canadian flight attendant theory canadian airline steward named ga tan dugas was referred to as case and later patient with the alphabet letter standing for outside southern california in an early aids study by dr william darrow of the centers for disease control 
because of this many people had considered dugas to be responsible for taking hiv to north america 
however hiv reached new york city around while dugas did not start work at air canada until in randy shilts book and the band played on and the movie based on it dugas is referred to as aids patient zero instead of patient but neither the book nor the movie states that he had been the first to bring the virus to north america 
he was incorrectly called patient zero because at least of the people known to be infected by hiv in had had sex with him or with person who had sexual intercourse with dugas 
homeless people and intravenous drug users in new york volunteer social worker called betty williams quaker who worked with the homeless in new york from the seventies and early eighties onwards has talked about people at that time whose death would be labelled as junkie flu or the dwindles 
in an interview for the act up oral history project in she said of course the horror stories came mainly concerning women who were injection drug users who had pcp pneumonia pneumocystis pneumonia and were told that they just had bronchitis 
she continues actually believe that aids kind of existed among this group of people first because if you look back there was something called junkie pneumonia there was something called the dwindles that addicts got and think this was another early aids population way too helpless to ever do anything for themselves on their own behalf 
julia epstein writes in her book altered conditions disease medicine and storytelling that as we uncover more of the early history of hiv infection it becomes clear that by at least the the virus was already making major inroads into the immune systems of number of diverse populations in the united states the retrospectively diagnosed epidemic of junkie pneumonia in new york city in the late for example and had for some time been causing devastation in several countries in africa 
anecdotal evidence suggests that so called junkie pneumonia first began to afflict heroin addicts in new york in in her book engendering aids deconstructing sex text and epidemic tamsin wilton writes people had been sickening and dying of mysterious conditions since the early conditions that we can retrospectively diagnose as aids related 
there was for example phenomenon known as junkie pneumonia which spread among some populations of injecting street drug users in the and which is now believed to have been caused by hiv infection 
melinda cooper writes in her book family values between neoliberalism and the new social conservatism it is plausible that these cases of aids did not come to light in the for the same reason that junkie pneumonia was not recognized as the sign of an emerging infectious disease the people in question had such precarious access to health care that news of their death was never communicated to public health authorities 
an article by pattrice maurer in the newspaper agenda from april explores some of the issues surrounding junkie pneumonia 
it starts in the late while the epidemic known as disco fever swept through the an epidemic known as junkie pneumonia raged among injection drug users in new york city 
it continues few people were aware that large numbers of injections drug users were inexplicably dying of pneumonia 
those few who did notice these deaths did not feel compelled to investigate the public health puzzle they posed 
the author opinion is that if anyone had bothered to investigate these deaths they would have found an immune system disorder that is now called aids steven thrasher writes in the guardian indeed those of us who study aids have long known that long before common symptoms such as kaposi sarcoma and pneumonia were showing up among hemophiliacs and gay men they were likely affecting homeless people who lived off society radar people who used iv intravenous drugs and those who avoided medical treatment out of fear 
chapter in the proceedings of the world conference of therapeutic communities th san francisco california september gives details about serum samples that were tested for signs of hiv then called htlv iii lav antibodies 
quoting we have also conducted historical studies of the epidemic in new york city using serum samples that were originally collected for other purposes 
we have sera from iv drug users that go back to the middle 
the first indication of htlv iii lav antibody presence is in one of eleven samples from of samples in of samples from and of samples from the htlv iii lav virus appears to have been introduced among iv drug users in the late in new york city 
anna thompson writes on the website thebody com in an article dated autumn many women were dying in the late of pneumonia cervical cancer and other illnesses complicated by mysteriously suppressed immune systems 
yet it was not until that case of aids in woman was first reported by the centers for disease control cdc 
she continues the cdc refusal to address women issues led to the overall perception that women do not get aids 
in an article published in aids cultural analysis cultural activism author douglas crimp draws attention to anecdotal evidence about junkie pneumonia 
quoting even these statistics are based on cdc epidemiology that continues to see the beginning of the epidemic as in spite of widespread anecdotal reporting of high rate of deaths throughout the from what was known as junkie pneumonia and was likely pneumocystis pneumonia 
the statistics crimp writes about were taken from new york times article from october about nyc department of health study that showed that of aids sufferers were people who injected drugs more than percent higher than previously reported 
quoting city health officials estimated that half of the city intravenous drug users were infected with the virus that causes aids the study hiv infection among intravenous drug users in manhattan new york city from through published in february seeks to understand long term trends in the spread of hiv among intravenous drug users idus 
aids surveillance data and studies which detail the number of persons who tested hiv positive in manhattan are used to compile information deemed critical to realising the extent of the aids epidemic 
it starts by stating that up to september idu was the risk behaviour in or of the first cases of aids in the us 
cases among idus in new york city in the same period numbered approximately third of national idu cases 
the study continues to outline the methodology used in the compilation of data 
it says that while truly representative samples of idus within community are probably impossible to obtain samples of idus entering treatment provide good source for monitoring trends 
in the results section it states quoting the first evidence for hiv infection among iv drug users in new york is from three cases of aids in children born in these cases were later reported to the new york city department of health aids surveillance unit 
these children did not receive any known transfusions prior to developing aids and were born to mothers known to be iv drug users 
it continues to outline that the earliest known case of aids in an adult idu occurred in mixed risk and that known cases among idus increased rapidly from the cases in mixed risk to cases in to cases in and to cases in statistics on the incidence of positive tests for hiv mainly using archived samples are out of in out of in out of in out of between out of and out of in out of in and out of in in the comments section it states the three cases in of apparent perinatal transmission mother to child from iv drug using women strongly suggest that the introduction of hiv into the iv drug use group occurred around or or perhaps even earlier 
it says that without extensive samples from this period it is not possible to be certain about the spread of hiv among idus but the samples from idus with chronic liver disease suggest that the rates of infection were below for the first or years after its introduction hiv is thought to have entered the population of people using intravenous drugs in new york city in approximately in spring the government of new york city underwent fiscal crisis which led to the closing of many social services with people who used intravenous drugs living in hostile sociopolitical and legal environment 
this fiscal crisis led to many agencies with health responsibilities being particularly hard hit which in turn might have led to an increase in hiv aids and tuberculosis tb 
quoting from american journal of public health study between and the department of health doh budget in ny was cut by and by the department had lost staff members of its workforce 
to achieve these reductions the department closed of district health centers cut million from its methadone program terminated the employment of of health educators and closed of child health stations and of chest clinics the units responsible for tb screening and diagnosis 
study published in the journal of the american medical association in linked tb and hiv aids severe and unusual presentation of overwhelming tuberculosis in appropriate clinical circumstances may be considered an infection predictive of the presence of aids 
further study from stated there was link between the rise in tb aids and drug users within the united states aids thus compounds the risk of acquiring tuberculosis and in the united states most patients with aids and tuberculosis have been drug users 
newsletter from spring by the national coalition of gay std services featured an article titled tuberculosis and aids connecticut that suggested an association between tb and aids within that state 
from grid to aids the aids epidemic officially began on june when the centers for disease control and prevention in its morbidity and mortality weekly report newsletter reported unusual clusters of pneumocystis pneumonia pcp caused by form of pneumocystis carinii now recognized as distinct species pneumocystis jirovecii in five homosexual men in los angeles 
over the next months more pcp clusters were discovered among otherwise healthy men in cities throughout the country along with other opportunistic diseases such as kaposi sarcoma and persistent generalized lymphadenopathy common in immunosuppressed patients 
in june report of group of cases amongst gay men in southern california suggested that sexually transmitted infectious agent might be the etiological agent 
the syndrome was initially termed grid or gay related immune deficiency other less common gay specific terms included gay compromise syndrome gay lymph node syndrome gay cancer gay plague homosexual syndrome community acquired immunodeficiency caid and acquired community immunodeficiency syndrome acids 
health authorities soon realized however that nearly half of the people identified with the syndrome were not homosexual men 
the same opportunistic infections were also reported among hemophiliacs users of intravenous drugs such as heroin and haitian immigrants leading some researchers to call it the disease 
by august the disease was being referred to by its new cdc coined name acquired immune deficiency syndrome aids 
activism by aids patients and families in new york city nathan fain larry kramer larry mass paul popham paul rapoport and edmund white officially established the gay men health crisis gmhc in also in michael callen and richard berkowitz published how to have sex in an epidemic one approach 
in this short work they described ways gay men could be sexual and affectionate while dramatically reducing the risk of contracting or spreading hiv 
both authors were themselves gay men living with aids 
this booklet was one of the first times men were advised to use condoms when having sexual relations with other men at the beginning of the aids epidemic in the there was very little information about the disease 
because aids affected stigmatized groups such as lgbtq people people of low socioeconomic status sex workers and addicts there was also initially little mass media coverage when the epidemic started 
however with the rise of activist groups composed of people suffering from aids either directly or through loved one more public attention was brought to the epidemic 
identification of the virus may lav in may team of doctors at the pasteur institute in france including fran oise barr sinoussi and luc montagnier reported that they had isolated new retrovirus from lymphoid ganglions that they believed was the cause of aids 
the virus was later named lymphadenopathy associated virus lav and sample was sent to the centers for disease control which was later passed to the national cancer institute nci 
may htlv iii in may team led by robert gallo of the united states confirmed the discovery of the virus but they renamed it human lymphotropic virus type iii htlv iii 
august arv dr jay levy group at the university of california san francisco also played role in the discovery of hiv 
he independently isolated the aids virus in and named it the aids associated retrovirus arv publishing his findings in the journal science in 
january both found to be the same in january number of more detailed reports were published concerning lav and htlv iii and by march it was clear that the viruses were the same indeed it was later determined that the virus isolated by the gallo lab was from the lymph nodes of the patient studied in the original report by montagnier and was the etiological agent of aids 
may the name hiv in may the international committee on taxonomy of viruses ruled that both names should be dropped and new name hiv human immunodeficiency virus be used 
nobel whether barr sinoussi and montagnier deserve more credit than gallo for the discovery of the virus that causes aids has been matter of considerable controversy 
barr sinoussi and montagnier were awarded the nobel prize in physiology or medicine for their discovery of human immunodeficiency virus and harald zur hausen also shared the prize for his discovery that human papilloma virus leads to cervical cancer but gallo was left out 
gallo said that it was disappointment that he was not named co recipient 
montagnier said he was surprised gallo was not recognized by the nobel committee it was important to prove that hiv was the cause of aids and gallo had very important role in that 
very sorry for robert gallo 
dr levy contribution to the discovery of hiv was also cited in the nobel prize ceremony 
case definition for epidemiological surveillance since june many definitions have been developed for epidemiological surveillance such as the bangui definition and the expanded world health organization aids case definition 
genetic studies according to study published in the proceedings of the national academy of sciences in team led by robert shafer at stanford university school of medicine discovered that the gray mouse lemur has an endogenous lentivirus the genus to which hiv belongs in its genetic makeup 
this suggests that lentiviruses have existed for at least million years much longer than the currently known existence of hiv 
in addition the time frame falls in the period when madagascar was still connected to what is now the african continent the said lemurs later developed immunity to the virus strain and survived an era when the lentivirus was widespread among other mammals 
the study was hailed as crucial as it fills the blanks in the origin of the virus as well as in its evolution and could be important in the development of new antiviral drugs in researchers reported that siv had infected monkeys in bioko for at least years 
previous to this time it was thought that siv infection in monkeys had happened over the past few hundred years 
scientists estimated that it would take similar amount of time before humans adapted naturally to hiv infection in the way monkeys in africa have adapted to siv and not suffer any harm from the infection czech study of the genome of malayan flying lemurs an order of mammals parallel to primates and sharing an immediate common ancestor with them found endogenous lentiviruses that emerged an estimated million years ago based on rates of viral mutation versus modern lentiviruses 
debunked hiv aids conspiracy theories aids denialism aids denialists argue that aids does not exist or that aids is not caused by hiv some of its proponents believe that aids is caused by lifestyle including sexuality or drug use and not by hiv 
both forms of aids denialism contradict scientific consensus 
the evidence that hiv causes aids is generally considered conclusive among pathologists 
most arguments for denialism are based on misrepresentations of outdated data 
the belief that hiv was created by the us government as bioweapon an idea invented by soviet propaganda operation is held by disproportionately high number of africans and african americans 
influence on bolsonaro conspiracy theorists influence reached peak in with brazilian president jair bolsonaro claiming that covid vaccines can lead to aids 
the supreme federal court of brazil ordered an investigation into bolsonaro for falsely claiming that covid vaccines could increase the risk of contracting aids 
see also timeline of hiv aids references further reading shilts randy 
and the band played on politics people and the aids epidemic 
new york st martin press 
isbn oclc brier jennier 
infectious ideas political responses to the aids crisis 
chapel hill university of north carolina press
dependent and independent variables are variables in mathematical modeling statistical modeling and experimental sciences 
dependent variables receive this name because in an experiment their values are studied under the supposition or demand that they depend by some law or rule by mathematical function on the values of other variables 
independent variables in turn are not seen as depending on any other variable in the scope of the experiment in question 
in this sense some common independent variables are time space density mass fluid flow rate and previous values of some observed value of interest 
human population size to predict future values the dependent variable of the two it is always the dependent variable whose variation is being studied by altering inputs also known as regressors in statistical context 
in an experiment any variable that can be attributed value without attributing value to any other variable is called an independent variable 
models and experiments test the effects that the independent variables have on the dependent variables 
sometimes even if their influence is not of direct interest independent variables may be included for other reasons such as to account for their potential confounding effect 
mathematics in mathematics function is rule for taking an input in the simplest case number or set of numbers and providing an output which may also be number 
symbol that stands for an arbitrary input is called an independent variable while symbol that stands for an arbitrary output is called dependent variable 
the most common symbol for the input is and the most common symbol for the output is the function itself is commonly written it is possible to have multiple independent variables or multiple dependent variables 
for instance in multivariable calculus one often encounters functions of the form where is dependent variable and and are independent variables 
functions with multiple outputs are often referred to as vector valued functions 
modeling in mathematical modeling the dependent variable is studied to see if and how much it varies as the independent variables vary 
in the simple stochastic linear model yi bxi ei the term yi is the ith value of the dependent variable and xi is the ith value of the independent variable 
the term ei is known as the error and contains the variability of the dependent variable not explained by the independent variable 
with multiple independent variables the model is yi bxi bxi bxi ei where is the number of independent variables the linear regression model is now discussed 
to use linear regression scatter plot of data is generated with as the independent variable and as the dependent variable 
this is also called bivariate dataset xi yi 
the simple linear regression model takes the form of yi bxi ui for in this case ui un are independent random variables 
this occurs when the measurements do not influence each other 
through propagation of independence the independence of ui implies independence of yi even though each yi has different expectation value 
each ui has an expectation value of and variance of expectation of yi proof the line of best fit for the bivariate dataset takes the form and is called the regression line 
and correspond to the intercept and slope respectively 
simulation in simulation the dependent variable is changed in response to changes in the independent variables 
statistics in an experiment the variable manipulated by an experimenter is something that is proven to work called an independent variable 
the dependent variable is the event expected to change when the independent variable is manipulated in data mining tools for multivariate statistics and machine learning the dependent variable is assigned role as target variable or in some tools as label attribute while an independent variable may be assigned role as regular variable 
known values for the target variable are provided for the training data set and test data set but should be predicted for other data 
the target variable is used in supervised learning algorithms but not in unsupervised learning 
statistics synonyms depending on the context an independent variable is sometimes called predictor variable regressor covariate manipulated variable explanatory variable exposure variable see reliability theory risk factor see medical statistics feature in machine learning and pattern recognition or input variable 
in econometrics the term control variable is usually used instead of covariate 
explanatory variable is preferred by some authors over independent variable when the quantities treated as independent variables may not be statistically independent or independently manipulable by the researcher 
if the independent variable is referred to as an explanatory variable then the term response variable is preferred by some authors for the dependent variable from the economics community the independent variables are also called exogenous 
depending on the context dependent variable is sometimes called response variable regressand criterion predicted variable measured variable explained variable experimental variable responding variable outcome variable output variable target or label 
in economics endogenous variables are usually referencing the target 
explained variable is preferred by some authors over dependent variable when the quantities treated as dependent variables may not be statistically dependent 
if the dependent variable is referred to as an explained variable then the term predictor variable is preferred by some authors for the independent variable variables may also be referred to by their form continuous or categorical which in turn may be binary dichotomous nominal categorical and ordinal categorical among others 
an example is provided by the analysis of trend in sea level by woodworth 
here the dependent variable and variable of most interest was the annual mean sea level at given location for which series of yearly values were available 
the primary independent variable was time 
use was made of covariate consisting of yearly values of annual mean atmospheric pressure at sea level 
the results showed that inclusion of the covariate allowed improved estimates of the trend against time to be obtained compared to analyses which omitted the covariate 
other variables variable may be thought to alter the dependent or independent variables but may not actually be the focus of the experiment 
so that the variable will be kept constant or monitored to try to minimize its effect on the experiment 
such variables may be designated as either controlled variable control variable or fixed variable 
extraneous variables if included in regression analysis as independent variables may aid researcher with accurate response parameter estimation prediction and goodness of fit but are not of substantive interest to the hypothesis under examination 
for example in study examining the effect of post secondary education on lifetime earnings some extraneous variables might be gender ethnicity social class genetics intelligence age and so forth 
variable is extraneous only when it can be assumed or shown to influence the dependent variable 
if included in regression it can improve the fit of the model 
if it is excluded from the regression and if it has non zero covariance with one or more of the independent variables of interest its omission will bias the regression result for the effect of that independent variable of interest 
this effect is called confounding or omitted variable bias in these situations design changes and or controlling for variable statistical control is necessary 
extraneous variables are often classified into three types subject variables which are the characteristics of the individuals being studied that might affect their actions 
these variables include age gender health status mood background etc 
blocking variables or experimental variables are characteristics of the persons conducting the experiment which might influence how person behaves 
gender the presence of racial discrimination language or other factors may qualify as such variables 
situational variables are features of the environment in which the study or research was conducted which have bearing on the outcome of the experiment in negative way 
included are the air temperature level of activity lighting and time of day in modelling variability that is not covered by the independent variable is designated by and is known as the residual side effect error unexplained share residual variable disturbance or tolerance 
examples effect of fertilizer on plant growths in study measuring the influence of different quantities of fertilizer on plant growth the independent variable would be the amount of fertilizer used 
the dependent variable would be the growth in height or mass of the plant 
the controlled variables would be the type of plant the type of fertilizer the amount of sunlight the plant gets the size of the pots etc effect of drug dosage on symptom severity in study of how different doses of drug affect the severity of symptoms researcher could compare the frequency and intensity of symptoms when different doses are administered 
here the independent variable is the dose and the dependent variable is the frequency intensity of symptoms effect of temperature on pigmentation in measuring the amount of color removed from beetroot samples at different temperatures temperature is the independent variable and amount of pigment removed is the dependent variable effect of sugar added in coffee the taste varies with the amount of sugar added in the coffee 
here the sugar is the independent variable while the taste is the dependent variable 
see also abscissa and ordinate blocking statistics latent variable versus observable variable notes references
weighted least squares wls also known as weighted linear regression is generalization of ordinary least squares and linear regression in which knowledge of the variance of observations is incorporated into the regression 
wls is also specialization of generalized least squares 
introduction special case of generalized least squares called weighted least squares can be used when all the off diagonal entries of the covariance matrix of the residuals are null the variances of the observations along the covariance matrix diagonal may still be unequal heteroscedasticity 
the fit of model to data point is measured by its residual defined as the difference between measured value of the dependent variable and the value predicted by the model 
if the errors are uncorrelated and have equal variance then the function is minimised at such that the gauss markov theorem shows that when this is so is best linear unbiased estimator blue 
if however the measurements are uncorrelated but have different uncertainties modified approach might be adopted 
aitken showed that when weighted sum of squared residuals is minimized is the blue if each weight is equal to the reciprocal of the variance of the measurement the gradient equations for this sum of squares are which in linear least squares system give the modified normal equations when the observational errors are uncorrelated and the weight matrix is diagonal these may be written as if the errors are correlated the resulting estimator is the blue if the weight matrix is equal to the inverse of the variance covariance matrix of the observations 
when the errors are uncorrelated it is convenient to simplify the calculations to factor the weight matrix as the normal equations can then be written in the same form as ordinary least squares where we define the following scaled matrix and vector diag diag this is type of whitening transformation the last expression involves an entrywise division 
for non linear least squares systems similar argument shows that the normal equations should be modified as follows 
note that for empirical tests the appropriate is not known for sure and must be estimated 
for this feasible generalized least squares fgls techniques may be used in this case it is specialized for diagonal covariance matrix thus yielding feasible weighted least squares solution 
if the uncertainty of the observations is not known from external sources then the weights could be estimated from the given observations 
this can be useful for example to identify outliers 
after the outliers have been removed from the data set the weights should be reset to one 
motivation in some cases the observations may be weighted for example they may not be equally reliable 
in this case one can minimize the weighted sum of squares where wi is the weight of the ith observation and is the diagonal matrix of such weights 
the weights should ideally be equal to the reciprocal of the variance of the measurement 
this implies that the observations are uncorrelated 
if the observations are correlated the expression applies 
in this case the weight matrix should ideally be equal to the inverse of the variance covariance matrix of the observations 
the normal equations are then this method is used in iteratively reweighted least squares 
parameter errors and correlation the estimated parameter values are linear combinations of the observed values therefore an expression for the estimated variance covariance matrix of the parameter estimates can be obtained by error propagation from the errors in the observations 
let the variance covariance matrix for the observations be denoted by and that of the estimated parameters by 
then when this simplifies to when unit weights are used the identity matrix it is implied that the experimental errors are uncorrelated and all equal where is the priori variance of an observation 
in any case is approximated by the reduced chi squared where is the minimum value of the weighted objective function the denominator is the number of degrees of freedom see effective degrees of freedom for generalizations for the case of correlated observations 
in all cases the variance of the parameter estimate is given by and the covariance between the parameter estimates and is given by the standard deviation is the square root of variance and the correlation coefficient is given by 
these error estimates reflect only random errors in the measurements 
the true uncertainty in the parameters is larger due to the presence of systematic errors which by definition cannot be quantified 
note that even though the observations may be uncorrelated the parameters are typically correlated 
parameter confidence limits it is often assumed for want of any concrete evidence but often appealing to the central limit theorem see normal distribution occurrence and applications that the error on each observation belongs to normal distribution with mean of zero and standard deviation under that assumption the following probabilities can be derived for single scalar parameter estimate in terms of its estimated standard error given here that the interval encompasses the true coefficient value that the interval encompasses the true coefficient value that the interval encompasses the true coefficient valuethe assumption is not unreasonable when if the experimental errors are normally distributed the parameters will belong to student distribution with degrees of freedom 
when student distribution approximates normal distribution 
note however that these confidence limits cannot take systematic error into account 
also parameter errors should be quoted to one significant figure only as they are subject to sampling error when the number of observations is relatively small chebychev inequality can be used for an upper bound on probabilities regardless of any assumptions about the distribution of experimental errors the maximum probabilities that parameter will be more than or standard deviations away from its expectation value are and respectively 
residual values and correlation the residuals are related to the observations by where is the idempotent matrix known as the hat matrix and is the identity matrix 
the variance covariance matrix of the residuals is given by thus the residuals are correlated even if the observations are not 
when the sum of weighted residual values is equal to zero whenever the model function contains constant term 
left multiply the expression for the residuals by xt wt say for example that the first term of the model is constant so that for all in that case it follows that thus in the motivational example above the fact that the sum of residual values is equal to zero is not accidental but is consequence of the presence of the constant term in the model 
if experimental error follows normal distribution then because of the linear relationship between residuals and observations so should residuals but since the observations are only sample of the population of all possible observations the residuals should belong to student distribution 
studentized residuals are useful in making statistical test for an outlier when particular residual appears to be excessively large 
see also iteratively reweighted least squares heteroscedasticity consistent standard errors weighted mean references
in machine learning boosting is an ensemble meta algorithm for primarily reducing bias and also variance in supervised learning and family of machine learning algorithms that convert weak learners to strong ones 
boosting is based on the question posed by kearns and valiant can set of weak learners create single strong learner 
weak learner is defined to be classifier that is only slightly correlated with the true classification it can label examples better than random guessing 
in contrast strong learner is classifier that is arbitrarily well correlated with the true classification 
robert schapire affirmative answer in paper to the question of kearns and valiant has had significant ramifications in machine learning and statistics most notably leading to the development of boosting when first introduced the hypothesis boosting problem simply referred to the process of turning weak learner into strong learner 
informally the hypothesis boosting problem asks whether an efficient learning algorithm that outputs hypothesis whose performance is only slightly better than random guessing 
weak learner implies the existence of an efficient algorithm that outputs hypothesis of arbitrary accuracy 
algorithms that achieve hypothesis boosting quickly became simply known as boosting 
freund and schapire arcing adapt at ive resampling and combining as general technique is more or less synonymous with boosting 
boosting algorithms while boosting is not algorithmically constrained most boosting algorithms consist of iteratively learning weak classifiers with respect to distribution and adding them to final strong classifier 
when they are added they are weighted in way that is related to the weak learners accuracy 
after weak learner is added the data weights are readjusted known as re weighting 
misclassified input data gain higher weight and examples that are classified correctly lose weight 
thus future weak learners focus more on the examples that previous weak learners misclassified 
there are many boosting algorithms 
the original ones proposed by robert schapire recursive majority gate formulation and yoav freund boost by majority were not adaptive and could not take full advantage of the weak learners 
schapire and freund then developed adaboost an adaptive boosting algorithm that won the prestigious del prize 
only algorithms that are provable boosting algorithms in the probably approximately correct learning formulation can accurately be called boosting algorithms 
other algorithms that are similar in spirit to boosting algorithms are sometimes called leveraging algorithms although they are also sometimes incorrectly called boosting algorithms the main variation between many boosting algorithms is their method of weighting training data points and hypotheses 
adaboost is very popular and the most significant historically as it was the first algorithm that could adapt to the weak learners 
it is often the basis of introductory coverage of boosting in university machine learning courses 
there are many more recent algorithms such as lpboost totalboost brownboost xgboost madaboost logitboost and others 
many boosting algorithms fit into the anyboost framework which shows that boosting performs gradient descent in function space using convex cost function 
object categorization in computer vision given images containing various known objects in the world classifier can be learned from them to automatically classify the objects in future images 
simple classifiers built based on some image feature of the object tend to be weak in categorization performance 
using boosting methods for object categorization is way to unify the weak classifiers in special way to boost the overall ability of categorization 
problem of object categorization object categorization is typical task of computer vision that involves determining whether or not an image contains some specific category of object 
the idea is closely related with recognition identification and detection 
appearance based object categorization typically contains feature extraction learning classifier and applying the classifier to new examples 
there are many ways to represent category of objects 
from shape analysis bag of words models or local descriptors such as sift etc 
examples of supervised classifiers are naive bayes classifiers support vector machines mixtures of gaussians and neural networks 
however research has shown that object categories and their locations in images can be discovered in an unsupervised manner as well 
status quo for object categorization the recognition of object categories in images is challenging problem in computer vision especially when the number of categories is large 
this is due to high intra class variability and the need for generalization across variations of objects within the same category 
objects within one category may look quite different 
even the same object may appear unalike under different viewpoint scale and illumination 
background clutter and partial occlusion add difficulties to recognition as well 
humans are able to recognize thousands of object types whereas most of the existing object recognition systems are trained to recognize only few 
human faces cars simple objects etc 
research has been very active on dealing with more categories and enabling incremental additions of new categories and although the general problem remains unsolved several multi category objects detectors for up to hundreds or thousands of categories have been developed 
one means is by feature sharing and boosting 
boosting for binary categorization adaboost can be used for face detection as an example of binary categorization 
the two categories are faces versus background 
the general algorithm is as follows form large set of simple features initialize weights for training images for rounds normalize the weights for available features from the set train classifier using single feature and evaluate the training error choose the classifier with the lowest error update the weights of the training images increase if classified wrongly by this classifier decrease if correctly form the final strong classifier as the linear combination of the classifiers coefficient larger if training error is small after boosting classifier constructed from features could yield detection rate under false positive rate another application of boosting for binary categorization is system that detects pedestrians using patterns of motion and appearance 
this work is the first to combine both motion information and appearance information as features to detect walking person 
it takes similar approach to the viola jones object detection framework 
boosting for multi class categorization compared with binary categorization multi class categorization looks for common features that can be shared across the categories at the same time 
they turn to be more generic edge like features 
during learning the detectors for each category can be trained jointly 
compared with training separately it generalizes better needs less training data and requires fewer features to achieve the same performance 
the main flow of the algorithm is similar to the binary case 
what is different is that measure of the joint training error shall be defined in advance 
during each iteration the algorithm chooses classifier of single feature features that can be shared by more categories shall be encouraged 
this can be done via converting multi class classification into binary one set of categories versus the rest or by introducing penalty error from the categories that do not have the feature of the classifier in the paper sharing visual features for multiclass and multiview object detection torralba et al 
used gentleboost for boosting and showed that when training data is limited learning via sharing features does much better job than no sharing given same boosting rounds 
also for given performance level the total number of features required and therefore the run time cost of the classifier for the feature sharing detectors is observed to scale approximately logarithmically with the number of class slower than linear growth in the non sharing case 
similar results are shown in the paper incremental learning of object detectors using visual shape alphabet yet the authors used adaboost for boosting 
convex vs non convex boosting algorithms boosting algorithms can be based on convex or non convex optimization algorithms 
convex algorithms such as adaboost and logitboost can be defeated by random noise such that they can learn basic and learnable combinations of weak hypotheses 
this limitation was pointed out by long servedio in however by multiple authors demonstrated that boosting algorithms based on non convex optimization such as brownboost can learn from noisy datasets and can specifically learn the underlying classifier of the long servedio dataset 
see also implementations scikit learn an open source machine learning library for python orange free data mining software suite module orange ensemble weka is machine learning set of tools that offers variate implementations of boosting algorithms like adaboost and logitboost package gbm generalized boosted regression models implements extensions to freund and schapire adaboost algorithm and friedman gradient boosting machine 
jboost adaboost logitboost robustboost boostexter and alternating decision trees package adabag applies multiclass adaboost adaboost samme and bagging package xgboost an implementation of gradient boosting for linear and tree based models 
notes references further reading yoav freund and robert schapire decision theoretic generalization of on line learning and an application to boosting journal of computer and system sciences robert schapire and yoram singer improved boosting algorithms using confidence rated predictors machine learning external links robert schapire the boosting approach to machine learning an overview msri mathematical sciences research institute workshop on nonlinear estimation and classification zhou zhi hua boosting years ccl keynote 
on the margin explanation of boosting algorithm pdf 
in proceedings of the st annual conference on learning theory colt 
on the doubt about margin explanation of boosting pdf
the chromatic circle is clock diagram for displaying relationships among the equal tempered pitch classes making up the familiar chromatic scale on circle 
explanation if one starts on any equal tempered pitch and repeatedly ascends by the musical interval of semitone one will eventually land on pitch with the same pitch class as the initial one having passed through all the other equal tempered chromatic pitch classes in between 
since the space is circular it is also possible to descend by semitone 
the chromatic circle is useful because it represents melodic distance which is often correlated with physical distance on musical instruments 
for instance to move from any on piano keyboard to the nearest one must move up four semitones corresponding to four clockwise steps on the chromatic circle 
one can also move down by eight semitones corresponding to eight counterclockwise steps on the pitch class circle 
larger motions on the piano or in pitch space can be represented in pitch class space by paths that wrap around the chromatic circle one or more times 
one can represent the twelve equal tempered pitch classes by the cyclic group of order twelve or equivalently the residue classes modulo twelve 
the group has four generators which can be identified with the ascending and descending semitones and the ascending and descending perfect fifths 
the semitonal generator gives rise to the chromatic circle while the perfect fifth gives rise to the circle of fifths 
comparison with circle of fifths key difference between the chromatic circle and the circle of fifths is that the former is truly continuous space every point on the circle corresponds to conceivable pitch class and every conceivable pitch class corresponds to point on the circle 
by contrast the circle of fifths is fundamentally discrete structure and there is no obvious way to assign pitch classes to each of its points 
pitch constellation pitch constellation is graphical representation of pitches used to describe musical scales modes chords or other groupings of pitches within an octave range 
it consists of circle with markings along the circumference or lines from the center which indicate pitches 
most pitch constellations use of subset of pitches chosen from the twelve pitch chromatic scale 
in this case the points on the circle are spaced like the twelve hour markings on an analog clock where each tick mark represents semitone 
scales and modes the pitch constellation provides an easy way to identify certain patterns and similarities between harmonic structures 
major scale consists of circle with markings at or and clock 
minor scale consists of circle with markings at or and clock 
the diagrams above show the two scales marked with scale degrees 
it can be observed that the tonic second fourth and fifth are shared while the minor scale flattens the third sixth and seventh notes relative to the major scale 
another observation is that the minor scale constellation is the same as the major scale but rotated degrees 
in the following drawing all of the major minor scales are drawn 
note that the constellation for all the major scales or all the minor scales are identical 
the different scales are generated by rotating the note overlay 
the notes that need to be sharpened flattened can be easily identified 
moreover if we draw all seven diatonic modes we can see them all as rotations of the ionian mode 
note also the significance of the clock point 
this corresponds to tritone 
the modes including pitches tritone from the tonic locrian and lydian are least used 
the clock and clock pitches are also important points corresponding to perfect fourth and perfect fifth respectively 
the most used scales modes major ionian mode minor aeolian mode and mixolydian include these pitches 
symmetric scales have simple representations in this scheme 
more exotic scales such as the pentatonic blues and octatonic can also be drawn and related to the common scales 
more complete list of musical scales and modes other overlays in previous sections we saw how various overlays scale degrees semi tone numbering notes can be used to notate the circumference of the constellation 
various other overlays can be laid around the constellation 
pitch ratios ratios of pitch frequencies 
note that once pitch constellation has been determined any number of overlays notes solf ge intervals etc 
may be placed on top for analysis comparison 
often generating one harmonic relationship from another is simply matter of rotating the overlay or constellation or shifting one or two pitch locations 
chords similarities between chords can also be observed as well as the significance of augmented diminished notes for triads we have the following and for seventh chords circle of fifths beginning with pitch constellation of chromatic scale the notes of circle of fifths can be easily generated 
starting at and moving across the circle and then one tick clockwise line is drawn with an arrow indicating the direction moved 
continuing from that point across the circle and one tick clockwise all points are connected 
moving through this pattern the notes of the circle of fifths can be determined 
one can also depict non tempered intervals on chromatic circle which allows one to depict commas small intervals particularly comma pumps 
for example using sequence of twelve just fifths ratio does not quite return to the starting point the size of the gap is the pythagorean comma resulting in broken circle of fifths 
technical note the ratio of the frequencies between two pitches in the constellation can be determined as follows 
take the length of the arc measured clockwise between the two points and divide by the circumference of the circle 
the frequency ratio is two raised to this power 
for example for fifth which is located at clock relative to the tonic the frequency ratio is references further reading brower candace cognitive theory of musical meaning journal of music theory duke university press doi jstor ku inskas darius symmetry in creative work of mikalojus konstantinas iurlionis pdf menotyra olson harry music physics and engineering dover publications isbn external links notenscheibe web application pitch constellations of scales triads intervals and the circle of fifths with basic audio on line app illustrating pitch constellations scaletapper iphone app which utilizes pitch constellations 
pdf of musical scales
among alternative tunings for guitar major thirds tuning is regular tuning in which each interval between successive open strings is major third in musical abbreviation
other names for major thirds tuning include major third tuning tuning all thirds tuning and augmented tuning
by definition major third interval separates two notes that differ by exactly four semitones one third of the twelve note octave
the spanish guitar tuning mixes four perfect fourths five semitones and one major third the latter occurring between the and strings this tuning which is used for acoustic and electric guitars is called standard in english convention that is followed in this article
while standard tuning is irregular mixing four fourths and one major third tunings are regular only major third intervals occur between the successive strings of the tunings for example the open augmented tuning
for each tuning the open strings form an augmented triad in two octaves
for guitars with six strings every major third tuning repeats its three open notes in two octaves so providing many options for fingering chords
by repeating open string notes and by having uniform intervals between strings major thirds tuning simplifies learning by beginners
these features also facilitate advanced guitarists improvisation precisely the aim of jazz guitarist ralph patt when he began popularizing major thirds tuning between and
avoiding standard tuning irregular intervals in standard tuning the successive open strings mix two types of intervals four perfect fourths and the major third between the and strings only major thirds occur as open string intervals for major thirds tuning which is also called major third tuning all thirds tuning and tuning
the most viable tunings are all of these tunings reduce the overall range of the instrument bit the first takes off the top of the range and the last takes off the bottom of the range
one popular tuning has the open strings which some guitarists have applied to the top six strings of seven string guitar with the low seventh string tuned to the low to restore the standard range
while tuning can use standard sets of guitar strings specialized string gauges have been recommended
the middle tunings are compromise each losing note or two off both the top and the bottom of the range
for example for six string guitars the tuning loses the two lowest semitones on the low string and the two highest semitones from the high string in standard tuning it can use string sets for standard tuning note that regardless of which note is chosen to start the tuning sequence there are only four distinct sets of open note pitch classes
the major thirds tunings respectively have the open notes properties major thirds tunings require less hand stretching than other tunings because each tuning packs the octave twelve notes into four consecutive frets
the major third intervals allow major chords and minor chords to be played with two three consecutive fingers on two consecutive frets
every major thirds tuning is regular and repetitive two properties that facilitate learning by beginners and improvisation by advanced guitarists
four frets for the four fingers in major thirds tuning the chromatic scale is arranged on three consecutive strings in four consecutive frets
this four fret arrangement facilitates the left hand technique for classical spanish guitar for each hand position of four frets the hand is stationary and the fingers move each finger being responsible for one fret
consequently three hand positions covering frets and partition the fingerboard of classical guitar which has exactly frets only two or three frets are needed for the guitar chords major minor and dominant sevenths which are emphasized in introductions to guitar playing and to the fundamentals of music
each major and minor chord can be played on two successive frets on three successive strings and therefore each needs only two fingers
other chords seconds fourths sevenths and ninths are played on only three successive frets
for fundamental chord fingerings major thirds tuning simplicity and consistency are not shared by standard tuning whose seventh chord fingering is discussed at the end of this section
repetition each major thirds tuning repeats its open notes after every two strings which results in two copies of the three open strings notes each in different octave
this repetition again simplifies the learning of chords and improvisation
this advantage is not shared by two popular regular tunings all fourths and all fifths tuning chord inversion is especially simple in major thirds tuning
chords are inverted simply by raising one or two notes three strings
the raised notes are played with the same finger as the original notes
thus major and minor chords are played on two frets in tuning even when they are inverted
in contrast inversions of chords in standard tuning require three fingers on span of four frets in standard tuning the shape of inversions depends on the involvement of the irregular major third
regular musical intervals in each regular tuning the musical intervals are the same for each pair of consecutive strings
other regular tunings include all fourths augmented fourths and all fifths tunings
for each regular tuning chord patterns may be moved around the fretboard property that simplifies beginners learning of chords and advanced players improvisation in contrast chords cannot be shifted around the fretboard in standard tuning which requires four chord shapes for the major chords there are separate fingerings for chords having root notes on one of the four strings three six
shifting chords vertical and diagonal the repetition of the major thirds tuning enables notes and chords to be raised one octave by being vertically shifted by three strings
notes and chords may be shifted diagonally in major thirds tuning by combining vertical shift of one string with horizontal shift of four frets like all regular tunings chords in the major third tuning can be moved across the fretboard ascending or descending major third for each string in standard tuning playing scales of one octave requires three patterns which depend on the string of the root note
chords cannot be shifted diagonally without changing finger patterns
standard tuning has four finger patterns for musical intervals four forms for basic major chords and three forms for the inversion of the basic major chords
open chords and beginning players major thirds tunings are unconventional open tunings in which the open strings form an augmented triad
in tunings the augmented fifth replaces the perfect fifth of the major triad which is used in conventional open tunings
for example the augmented triad has in place of the major triad the note is enharmonically equivalent to as noted above
consequently tunings are also called open augmented fifth tunings in french la guitare majeure quinte augment instructional literature uses standard tuning
traditionally course begins with the hand in first position that is with the left hand covering frets
beginning players first learn open chords belonging to the major keys and guitarists who play mainly open chords in these three major keys and their relative minor keys am em bm may prefer standard tuning over an tuning
in particular hobbyists playing folk music around campfire are well served by standard tuning
such hobbyists may also play major thirds tuning which also has many open chords with notes on five or six strings chords with five six strings have greater volume than chords with three four strings and so are useful for acoustic guitars for example acoustic electric guitars without amplification
intermediate guitarists do not limit themselves to one hand position and consequently open chords are only part of their chordal repertoire
in contemporary music master guitarists think diagonally and move up and down the strings fluency on the entire fretboard is needed particularly by guitarists playing jazz
according to its inventor ralph patt major thirds tuning makes the hard things easy and the easy things hard
this is never going to take the place of folk guitar and it not meant to
for difficult music and for where we are going in free jazz and even the old be bop jazz this is much easier way to play
left handed chords major thirds tuning is closely related to minor sixths tuning which is the regular tuning that is based on the minor sixth the interval of eight semitones
either ascending by major third or by descending by minor sixth one arrives at the same pitch class the same note representing pitches in different octaves
intervals paired like the pair of major third and minor sixth intervals are termed inverse intervals in the theory of music
consequently chord charts for minor sixths tunings may be used for left handed major thirds tunings conversely chord charts for major thirds tunings may be used for left handed minor sixths tunings
fingering of seventh chords major thirds tuning facilitates playing chords with closed voicings
in contrast standard tuning would require more hand stretching to play closed voice seventh chords and so standard tuning uses open voicings for many four note chords for example of dominant seventh chords
by definition dominant seventh is four note chord combining major chord and minor seventh
for example the seventh chord combines the major chord with
in standard tuning extending the root bass major chord to chord would span six frets such seventh chords contain some pretty serious stretches in the left hand
an illustration shows this voicing which would be extremely difficult to play in standard tuning besides the openly voiced chord that is conventional in standard tuning this open position chord is termed second inversion drop chord because the second highest note in the second inversion chord is lowered by an octave
disadvantages while major thirds tuning confers the numerous advantages detailed above it also introduces certain disadvantages as compared to the instrument standard tuning tuning decreases the overall range of the guitar this is why some players eventually resorted to and string instruments to regain that lost range simplifies the voicing of chords in close harmony but it makes certain common voicings in open harmony more difficult or even impossible facilitates moving and note chords up or down an octave but it makes the fingerings for and note multi octave chords more complex and awkward
history major thirds tuning was introduced in by jazz guitarist ralph patt
he was studying with gunther schuller whose twelve tone technique was invented for atonal composition by his teacher arnold schoenberg
patt was also inspired by the free jazz of ornette coleman and john coltrane
seeking guitar tuning that would facilitate improvisation using twelve tones he introduced major thirds tuning by perhaps in to achieve the open string range of standard spanish tuning patt started using seven string guitars in before settling on eight string guitars with high equivalently as their highest open notes
patt used major thirds tuning during all of his work as session musician after in new york
patt developed webpage with extensive information about major thirds tuning
see also minor thirds tuning repetitive open tunings approximate tunings non spanish classical guitars english its open tuning approximates russian its string open tuning approximates other open tunings open tuning approximates open tuning approximates open tuning approximates open tuning approximates open tuning approximates open tuning approximates open tuning approximates references footnotes citations bibliography further reading sethares bill january
alternate tuning guide pdf
madison wisconsin university of wisconsin department of electrical engineering
external links wolfowitz kiefer may august
chord diagrams for major thirds tuning pdf
dictionary of chords major minor dominant sevenths diagrams of sevenths major minor dominant half diminished arising in the tertian harmonization of the major scale on etc
retrieved april professors andreas griewank and william sethares each recommend discussions of major thirds tuning by two jazz guitarists sethares regular tunings and griewank ole kirkeby for and string guitars charts of intervals major minor and dominant chords recommended gauges for strings
ralph patt for and string guitars charts of scales chords and chord progressions string gauges
three other jazz guitar websites oberlin alexandre october
tuning your guitar in major thirds tune afresh and improvise
archived from the original on march retrieved december zemb patrick august
sommaire du site musical french summary of the musical site
archived from the original on june retrieved august corman tony august
free downloadable method book
archived from the original on august retrieved august guitar tunings database
tuner scales and chords for tunings most popular and for beginners
retrieved december video tutorial on major and minor chords in major thirds tuning on youtube bromley keith october
chord shapes for major thirds tuning on string guitar pdf
new standard tuning nst is an alternative tuning for the guitar that approximates all fifths tuning
the guitar strings are assigned the notes from lowest to highest the five lowest open strings are each tuned to an interval of perfect fifth the two highest strings are minor third apart
all fifths tuning is typically used for mandolins cellos violas and violins
on guitar tuning the strings in fifths would mean the first string would be high something that was impractical until recently
nst provides good approximation to all fifths tuning
like other regular tunings nst allows chord fingerings to be shifted from one set of strings to another
nst range is wider both lower and higher than the range of standard tuning in which the strings are tuned to the open notes
the greater range allows nst guitars to play repertoire that would be impractical if not impossible on standard tuned guitar
nst was developed by robert fripp the guitarist for king crimson
fripp taught the new standard tuning in guitar craft courses beginning in and thousands of guitar craft students continue to use the tuning
like other alternative tunings for guitar nst provides challenges and new opportunities to guitarists who have developed music especially suited to nst
nst places the guitar strings under greater tension than standard tuning
standard sets of guitar strings do not work well with the tuning as the lowest strings are too loose and the highest string may snap under the increased tension
special sets of nst strings have been available for decades and some guitarists have assembled nst sets from individual strings
history new standard tuning nst was invented by robert fripp of the band king crimson in september
was in the apple health spa on bleecker and thompson back in september in the sauna at half past in the morning almost asleep and the tuning flew over my head
at the time couldn understand what it was for
was asked to give guitar seminar at claymont court in december to raise funds for the running of the estate and the children school
there was click and realized the tuning was for the guitar class
fripp began using the tuning in before beginning his guitar craft seminars which have taught the tuning to three thousand guitarists
the tuning is from low to high
the original version of nst was all fifths tuning
however in the fripp never attained the all fifth high while he could attain the string lifetime distribution was too short
experimenting with string fripp succeeded
originally seen in ths all the way the top string would not go to so as on tenor banjo adopted an on the first string
these kept breaking so was adopted
in fripp suggested that guitar circle members experiment with an string from octave plus of gary goodman if successful the experiment could lead to the nst according to fripp
in fripp suggested renaming the tuning as guitar craft standard tuning or pentatonic tuning
properties the lowest five strings are tuned in perfect fifths from low the first string is minor third up from the to since the lowest five strings are tuned in fifths guitars with nst can be played with the fingerings for chords and scales used on the violin cello and mandolin the first five strings of nst have all fifths tuning and so its all fifths chords are movable around the fretboard
in contrast standard tuning has an irregular major third interjected among its perfect fourths which complicates the learning of chords by beginners the distinct open notes are the notes of the major pentatonic scale on which contains only consonant intervals
the pentatonic scale omits the open of standard tuning and all fifths tuning which forms dissonant second interval with with the king crimson fripp had used pentatonic harmony in discipline thela hun ginjeet and sartori in tangier
harmonics overtones with note of music one strikes the fundamental and in addition to the root note other notes are generated these are the harmonic series as one fundamental note contains within it other notes in the octave two fundamentals produce remarkable array of harmonics and the number of possible combinations between all the notes increases phenomenally
with triad affairs stand good chance of getting severely out of hand
new standard tuning lists four notes from the harmonic sequence overtones for the note when the low open note string is struck its harmonic sequence begins with the notes to strengthen given chord vincent persichetti twentieth century harmony recommends adding perfect fifths above the initial overtones rather than adding higher overtones such as and the higher persichetti book influenced fripp
in new standard tuning is the fundamental overtone as fifth reinforces as fifth reinforces as fifth reinforces both as fifth reinforces and as the fifth overtone reinforces and as the sixth overtone reinforces range like all fifths tuning nst has greater range than the standard tuning perfect fifth greater major third lower and minor third higher
chords perfect intervals rather than thirds asked whether nst facilitates new intervals or harmonies that aren readily available in standard tuning fripp responded yes that part of it
it more rational system but it also better sounding better for chords better for single notes
to build chords fripp uses perfect intervals in fourths fifths and octaves so avoiding minor and major thirds
quartal and quintal harmony was stressed from the beginning of fripp teaching of guitar craft
fripp began course with these directions now pick note from the following series it was series of fourths or fifths
when you are ready do not be in any hurry but when you are ready play your note then pick others and play them as the situation demands it
your first note will be the first intentional note you have played in week
it is challenge to adapt conventional guitar chords to new standard tuning
nst has wider intervals between consecutive strings than standard tuning
most songs that is music which has both words and instrumental accompaniments written in the nst have quality of walking on long stilts
there are rarely many intervals harmonic or melodic in these guitar accompaniments that are closer than major third except in the top of the voicing
close voicings from single guitar in nst are possible thanks to the minor third between the first and second string and this is often the only practical place where close voicings occur with any regularity
historical background modern quartal and quintal harmony revives the polyphonic traditions of medieval europe
before the common practice period european polyphony emphasized unison intervals and octaves and also perfect fifths
from the renaissance to western symphonic music was diatonic emphasizing the tertian harmony of major and minor scales keys and chords
much popular music especially rock retains diatonic harmony
string gauges with traditional guitar strings the low may be loose and the high may be too tight
special gauges are therefore more suitable for nst
for steel stringed acoustic guitars many guitar craft participants use either an inch set or an inch set string sets may be purchased as set from manufacturer or purchased singly and assembled by the guitarist
in inch gauge was being evaluated by fripp and other members of guitar circle who are considering replacing the first string note with an note the better to approximate the note of all fifths tuning
the inch gauge was produced by octave plus of gary goodman
robert fripp uses lighter strings for electric guitar
artists who use nst robert fripp has used the new standard tuning since fripp has taught nst in his guitar craft courses
in guitar craft and since in the successor guitar circles students use only new standard tuning
having to use new tuning the students are challenged to approach their playing with greater mindfulness putting to rest their habitual use of automatic chords or licks
with the new tuning guitarists have to find new ways of musical expression the tuning is used by students of guitar craft of which there have been three thousand
guitar craft alumni who continue to practice nst are called crafty guitarists or crafties
some crafty guitarists formed the league of crafty guitarists lcg which toured with robert fripp and released several albums
the guitar craft experience and the league of crafty guitarists trained guitarists who went on to form new bands such as the california guitar trio and trey gunn the california guitar trio and gunn toured with fripp as the robert fripp string quintet
other alumni of the league of crafty guitarists include members of los gauchos alemanes such as guitarist steve ball ball is associated with the seattle guitar circle along with lcg alumnus curt golden
the collection plague of crafty guitarists features the following guitar craft alumni who were listed in review by barry cleveland tobin buttram nigel gavin geary street quartet bill hibbits janssen and jensen alejandro miniaci and from power trio project sur pacifico playmovil and santos luminosos
nst has been adapted for instruments besides guitar
italian guitarist fabio mittino plays in nst
trey gunn crimson warr guitar player from to and markus reuter tuner with crimson drummer pat mastelotto have adapted nst for their and string instruments in reuter used tuning
finnish musician heikki malmberg uses string guitar tuned in nst with an additional low additionally british musician michael linden west plays the oud in nst
see also notes references cleveland barry august
the plague of crafty guitarists volume one
archived from the original on february retrieved march cleveland barry december
california guitar trio interview pdf
guitar player subscription required
retrieved march fripp robert
seven guitar craft themes definitive scores for guitar ensemble
original transcriptions by curt golden layout scores and tablatures ariel rzezak and theo morresi first limited ed
ismn dgm sku partitas
on the discipline of craft and art an interview with robert fripp
retrieved january persichetti vincent
twentieth century harmony creative aspects and practice
isbn oclc sethares william
madison wisconsin university of wisconsin department of electrical engineering
pdf version by bill sethares
retrieved may tamm eric robert fripp from crimson king to crafty master progressive ears ed
faber and faber isbn zipped microsoft word document archived from the original on october retrieved march zwerdling daniel september
all things considered npr weekend ed
washington dc national public radio
html transcription subscription required
archived from the original on october retrieved march
further reading drozdowski ted february
robert fripp plectral purist answers the dumb questions
external links courses in new standard tuning are offered by guitar circle the successor of guitar craft guitar circle of europe guitar circle of latin america guitar circle of north america the frakctured zone is king crimson fan website with notation and tabs to songs in nst with acknowledgment to trey gunn for permission
harmonization of diatonic major scale on progressions of chords triads and sevenths
new standard tuning of robert fripp guitar craft pdf
in statistics mediation model seeks to identify and explain the mechanism or process that underlies an observed relationship between an independent variable and dependent variable via the inclusion of third hypothetical variable known as mediator variable also mediating variable intermediary variable or intervening variable 
rather than direct causal relationship between the independent variable and the dependent variable mediation model proposes that the independent variable influences the mediator variable which in turn influences the dependent variable 
thus the mediator variable serves to clarify the nature of the relationship between the independent and dependent variables mediation analyses are employed to understand known relationship by exploring the underlying mechanism or process by which one variable influences another variable through mediator variable 
in particular mediation analysis can contribute to better understanding the relationship between an independent variable and dependent variable when these variables do not have an obvious direct connection 
baron and kenny steps for mediation analysis baron and kenny laid out several requirements that must be met to form true mediation relationship 
they are outlined below using real world example 
see the diagram above for visual representation of the overall mediating relationship to be explained 
note hayes critiqued baron and kenny mediation steps approach and as of david kenny on his website stated that mediation can exist in the absence of significant total effect and therefore step below may not be needed 
this situation is sometimes referred to as inconsistent mediation 
later publications by hayes also questioned the concepts of full or partial mediation and advocated for these terms along with the classical mediation steps approach outlined below to be abandoned 
step regress the dependent variable on the independent variable to confirm that the independent variable is significant predictor of the dependent variable independent variable dependent variable is significant step regress the mediator on the independent variable to confirm that the independent variable is significant predictor of the mediator 
if the mediator is not associated with the independent variable then it couldn possibly mediate anything independent variable mediator is significant step regress the dependent variable on both the mediator and independent variable to confirm that the mediator is significant predictor of the dependent variable and the strength of the coefficient of the previously significant independent variable in step is now greatly reduced if not rendered nonsignificant 
is significant should be smaller in absolute value than the original effect for the independent variable above example the following example drawn from howell explains each step of baron and kenny requirements to understand further how mediation effect is characterized 
step and step use simple regression analysis whereas step uses multiple regression analysis 
how you were parented independent variable predicts how confident you feel about parenting your own children dependent variable 
how you were parented independent variable predicts your feelings of competence and self esteem mediator 
your feelings of competence and self esteem mediator predict how confident you feel about parenting your own children dependent variable while controlling for how you were parented independent variable such findings would lead to the conclusion implying that your feelings of competence and self esteem mediate the relationship between how you were parented and how confident you feel about parenting your own children 
if step does not yield significant result one may still have grounds to move to step sometimes there is actually significant relationship between independent and dependent variables but because of small sample sizes or other extraneous factors there could not be enough power to predict the effect that actually exists 
direct versus indirect effects in the diagram shown above the indirect effect is the product of path coefficients and 
the direct effect is the coefficient 
the direct effect measures the extent to which the dependent variable changes when the independent variable increases by one unit and the mediator variable remains unaltered 
in contrast the indirect effect measures the extent to which the dependent variable changes when the independent variable is held constant and the mediator variable changes by the amount it would have changed had the independent variable increased by one unit 
in linear systems the total effect is equal to the sum of the direct and indirect ab in the model above 
in nonlinear models the total effect is not generally equal to the sum of the direct and indirect effects but to modified combination of the two 
full versus partial mediation mediator variable can either account for all or some of the observed relationship between two variables 
full mediation maximum evidence for mediation also called full mediation would occur if inclusion of the mediation variable drops the relationship between the independent variable and dependent variable see pathway in diagram above to zero 
partial mediation partial mediation maintains that the mediating variable accounts for some but not all of the relationship between the independent variable and dependent variable 
partial mediation implies that there is not only significant relationship between the mediator and the dependent variable but also some direct relationship between the independent and dependent variable 
in order for either full or partial mediation to be established the reduction in variance explained by the independent variable must be significant as determined by one of several tests such as the sobel test 
the effect of an independent variable on the dependent variable can become nonsignificant when the mediator is introduced simply because trivial amount of variance is explained not true mediation 
thus it is imperative to show significant reduction in variance explained by the independent variable before asserting either full or partial mediation 
it is possible to have statistically significant indirect effects in the absence of total effect 
this can be explained by the presence of several mediating paths that cancel each other out and become noticeable when one of the cancelling mediators is controlled for 
this implies that the terms partial and full mediation should always be interpreted relative to the set of variables that are present in the model 
in all cases the operation of fixing variable must be distinguished from that of controlling for variable which has been inappropriately used in the literature 
the former stands for physically fixing while the latter stands for conditioning on adjusting for or adding to the regression model 
the two notions coincide only when all error terms not shown in the diagram are statistically uncorrelated 
when errors are correlated adjustments must be made to neutralize those correlations before embarking on mediation analysis see bayesian networks 
sobel test sobel test is performed to determine if the relationship between the independent variable and dependent variable has been significantly reduced after inclusion of the mediator variable 
in other words this test assesses whether mediation effect is significant 
it examines the relationship between the independent variable and the dependent variable compared to the relationship between the independent variable and dependent variable including the mediation factor 
the sobel test is more accurate than the baron and kenny steps explained above however it does have low statistical power 
as such large sample sizes are required in order to have sufficient power to detect significant effects 
this is because the key assumption of sobel test is the assumption of normality 
because sobel test evaluates given sample on the normal distribution small sample sizes and skewness of the sampling distribution can be problematic see normal distribution for more details 
thus the rule of thumb as suggested by mackinnon et al is that sample size of is required to detect small effect sample size of is sufficient in detecting medium effect and sample size of is required to detect large effect 
the equation for sobel is preacher hayes bootstrap method the bootstrapping method provides some advantages to the sobel test primarily an increase in power 
the preacher and hayes bootstrapping method is non parametric test 
as such the bootstrap method does not violate assumptions of normality and is therefore recommended for small sample sizes 
bootstrapping involves repeatedly randomly sampling observations with replacement from the data set to compute the desired statistic in each resample 
computing over hundreds or thousands of bootstrap resamples provide an approximation of the sampling distribution of the statistic of interest 
the preacher hayes method provides point estimates and confidence intervals by which one can assess the significance or nonsignificance of mediation effect 
point estimates reveal the mean over the number of bootstrapped samples and if zero does not fall between the resulting confidence intervals of the bootstrapping method one can confidently conclude that there is significant mediation effect to report 
significance of mediation as outlined above there are few different options one can choose from to evaluate mediation model 
bootstrapping is becoming the most popular method of testing mediation because it does not require the normality assumption to be met and because it can be effectively utilized with smaller sample sizes 
however mediation continues to be most frequently determined using the logic of baron and kenny or the sobel test 
it is becoming increasingly more difficult to publish tests of mediation based purely on the baron and kenny method or tests that make distributional assumptions such as the sobel test 
thus it is important to consider your options when choosing which test to conduct 
approaches to mediation while the concept of mediation as defined within psychology is theoretically appealing the methods used to study mediation empirically have been challenged by statisticians and epidemiologists and interpreted formally 
experimental causal chain design an experimental causal chain design is used when the proposed mediator is experimentally manipulated 
such design implies that one manipulates some controlled third variable that they have reason to believe could be the underlying mechanism of given relationship 
measurement of mediation design measurement of mediation design can be conceptualized as statistical approach 
such design implies that one measures the proposed intervening variable and then uses statistical analyses to establish mediation 
this approach does not involve manipulation of the hypothesized mediating variable but only involves measurement 
criticisms of mediation measurement experimental approaches to mediation must be carried out with caution 
first it is important to have strong theoretical support for the exploratory investigation of potential mediating variable 
criticism of mediation approach rests on the ability to manipulate and measure mediating variable 
thus one must be able to manipulate the proposed mediator in an acceptable and ethical fashion 
as such one must be able to measure the intervening process without interfering with the outcome 
the mediator must also be able to establish construct validity of manipulation 
one of the most common criticisms of the measurement of mediation approach is that it is ultimately correlational design 
consequently it is possible that some other third variable independent from the proposed mediator could be responsible for the proposed effect 
however researchers have worked hard to provide counter evidence to this disparagement 
specifically the following counter arguments have been put forward temporal precedence for example if the independent variable precedes the dependent variable in time this would provide evidence suggesting directional and potentially causal link from the independent variable to the dependent variable 
nonspuriousness and or no confounds for example should one identify other third variables and prove that they do not alter the relationship between the independent variable and the dependent variable he she would have stronger argument for their mediation effect 
see other rd variables below mediation can be an extremely useful and powerful statistical test however it must be used properly 
it is important that the measures used to assess the mediator and the dependent variable are theoretically distinct and that the independent variable and mediator cannot interact 
should there be an interaction between the independent variable and the mediator one would have grounds to investigate moderation 
other third variables confounding another model that is often tested is one in which competing variables in the model are alternative potential mediators or an unmeasured cause of the dependent variable 
an additional variable in causal model may obscure or confound the relationship between the independent and dependent variables 
potential confounders are variables that may have causal impact on both the independent variable and dependent variable 
they include common sources of measurement error as discussed above as well as other influences shared by both the independent and dependent variables 
in experimental studies there is special concern about aspects of the experimental manipulation or setting that may account for study effects rather than the motivating theoretical factor 
any of these problems may produce spurious relationships between the independent and dependent variables as measured 
ignoring confounding variable may bias empirical estimates of the causal effect of the independent variable 
suppression suppressor variable increases the predictive validity of another variable when included in regression equation 
suppression can occur when single causal variable is related to an outcome variable through two separate mediator variables and when one of those mediated effects is positive and one is negative 
in such case each mediator variable suppresses or conceals the effect that is carried through the other mediator variable 
for example higher intelligence scores causal variable may cause an increase in error detection mediator variable which in turn may cause decrease in errors made at work on an assembly line an outcome variable at the same time intelligence could also cause an increase in boredom which in turn may cause an increase in errors 
thus in one causal path intelligence decreases errors and in the other it increases them 
when neither mediator is included in the analysis intelligence appears to have no effect or weak effect on errors 
however when boredom is controlled intelligence will appear to decrease errors and when error detection is controlled intelligence will appear to increase errors 
if intelligence could be increased while only boredom was held constant errors would decrease if intelligence could be increased while holding only error detection constant errors would increase 
in general the omission of suppressors or confounders will lead to either an underestimation or an overestimation of the effect of on thereby either reducing or artificially inflating the magnitude of relationship between two variables 
moderators other important third variables are moderators 
moderators are variables that can make the relationship between two variables either stronger or weaker 
such variables further characterize interactions in regression by affecting the direction and or strength of the relationship between and moderating relationship can be thought of as an interaction 
it occurs when the relationship between variables and depends on the level of see moderation for further discussion 
moderated mediation mediation and moderation can co occur in statistical models 
it is possible to mediate moderation and moderate mediation 
moderated mediation is when the effect of the treatment on the mediator and or the partial effect on the dependent variable depend in turn on levels of another variable moderator 
essentially in moderated mediation mediation is first established and then one investigates if the mediation effect that describes the relationship between the independent variable and dependent variable is moderated by different levels of another variable moderator 
this definition has been outlined by muller judd and yzerbyt and preacher rucker and hayes 
models of moderated mediation there are five possible models of moderated mediation as illustrated in the diagrams below 
in the first model the independent variable also moderates the relationship between the mediator and the dependent variable 
the second possible model of moderated mediation involves new variable which moderates the relationship between the independent variable and the mediator the path 
the third model of moderated mediation involves new moderator variable which moderates the relationship between the mediator and the dependent variable the path 
moderated mediation can also occur when one moderating variable affects both the relationship between the independent variable and the mediator the path and the relationship between the mediator and the dependent variable the path 
the fifth and final possible model of moderated mediation involves two new moderator variables one moderating the path and the other moderating the path 
in addition to the models mentioned above new variable can also exist which moderates the relationship between the independent variable and mediator the path while at the same time have the new variable moderate the relationship between the independent variable and dependent variable the path 
mediated moderation mediated moderation is variant of both moderation and mediation 
this is where there is initially overall moderation and the direct effect of the moderator variable on the outcome is mediated 
the main difference between mediated moderation and moderated mediation is that for the former there is initial overall moderation and this effect is mediated and for the latter there is no moderation but the effect of either the treatment on the mediator path is moderated or the effect of the mediator on the outcome path is moderated in order to establish mediated moderation one must first establish moderation meaning that the direction and or the strength of the relationship between the independent and dependent variables path differs depending on the level of third variable the moderator variable 
researchers next look for the presence of mediated moderation when they have theoretical reason to believe that there is fourth variable that acts as the mechanism or process that causes the relationship between the independent variable and the moderator path or between the moderator and the dependent variable path 
example the following is published example of mediated moderation in psychological research 
participants were presented with an initial stimulus prime that made them think of morality or made them think of might 
they then participated in the prisoner dilemma game pdg in which participants pretend that they and their partner in crime have been arrested and they must decide whether to remain loyal to their partner or to compete with their partner and cooperate with the authorities 
the researchers found that prosocial individuals were affected by the morality and might primes whereas proself individuals were not 
thus social value orientation proself vs prosocial moderated the relationship between the prime independent variable morality vs might and the behaviour chosen in the pdg dependent variable competitive vs cooperative 
the researchers next looked for the presence of mediated moderation effect 
regression analyses revealed that the type of prime morality vs might mediated the moderating relationship of participants social value orientation on pdg behaviour 
prosocial participants who experienced the morality prime expected their partner to cooperate with them so they chose to cooperate themselves 
prosocial participants who experienced the might prime expected their partner to compete with them which made them more likely to compete with their partner and cooperate with the authorities 
in contrast participants with pro self social value orientation always acted competitively 
regression equations for moderated mediation and mediated moderation muller judd and yzerbyt outline three fundamental models that underlie both moderated mediation and mediated moderation 
mo represents the moderator variable me represents the mediator variable and represents the measurement error of each regression equation 
step moderation of the relationship between the independent variable and the dependent variable also called the overall treatment effect path in the diagram 
to establish overall moderation the regression weight must be significant first step for establishing mediated moderation 
establishing moderated mediation requires that there be no moderation effect so the regression weight must not be significant 
step moderation of the relationship between the independent variable and the mediator path 
if the regression weight is significant the moderator affects the relationship between the independent variable and the mediator 
step moderation of both the relationship between the independent and dependent variables path and the relationship between the mediator and the dependent variable path 
if both in step and in step are significant the moderator affects the relationship between the independent variable and the mediator path 
if both in step and in step are significant the moderator affects the relationship between the mediator and the dependent variable path 
either or both of the conditions above may be true 
causal mediation analysis fixing versus conditioning mediation analysis quantifies the extent to which variable participates in the transmittance of change from cause to its effect 
it is inherently causal notion hence it cannot be defined in statistical terms 
traditionally however the bulk of mediation analysis has been conducted within the confines of linear regression with statistical terminology masking the causal character of the relationships involved 
this led to difficulties biases and limitations that have been alleviated by modern methods of causal analysis based on causal diagrams and counterfactual logic 
the source of these difficulties lies in defining mediation in terms of changes induced by adding third variables into regression equation 
such statistical changes are epiphenomena which sometimes accompany mediation but in general fail to capture the causal relationships that mediation analysis aims to quantify 
the basic premise of the causal approach is that it is not always appropriate to control for the mediator when we seek to estimate the direct effect of on see the figure above 
the classical rationale for controlling for is that if we succeed in preventing from changing then whatever changes we measure in are attributable solely to variations in and we are justified then in proclaiming the effect observed as direct effect of on 
unfortunately controlling for does not physically prevent from changing it merely narrows the analyst attention to cases of equal values 
moreover the language of probability theory does not possess the notation to express the idea of preventing from changing or physically holding constant 
the only operator probability provides is conditioning which is what we do when we control for or add as regressor in the equation for the result is that instead of physically holding constant say at and comparing for units under to those under we allow to vary but ignore all units except those in which achieves the value these two operations are fundamentally different and yield different results except in the case of no omitted variables 
to illustrate assume that the error terms of and are correlated 
under such conditions the structural coefficient and between and and between and can no longer be estimated by regressing on and in fact the regression slopes may both be nonzero even when is zero 
this has two consequences 
first new strategies must be devised for estimating the structural coefficients and second the basic definitions of direct and indirect effects must go beyond regression analysis and should invoke an operation that mimics fixing rather than conditioning on definitions such an operator denoted do was defined in pearl and it operates by removing the equation of and replacing it by constant for example if the basic mediation model consists of the equations then after applying the operator do the model becomes and after applying the operator do the model becomes where the functions and as well as the distributions of the error terms and remain unaltered 
if we further rename the variables and resulting from do as and respectively we obtain what came to be known as potential outcomes or structural counterfactuals 
these new variables provide convenient notation for defining direct and indirect effects 
in particular four types of effects have been defined for the transition from to total effect controlled direct effect natural direct effect natural indirect effect where stands for expectation taken over the error terms 
these effects have the following interpretations te measures the expected increase in the outcome as changes from to while the mediator is allowed to track the change in as dictated by the function 
cde measures the expected increase in the outcome as changes from to while the mediator is fixed at pre specified level uniformly over the entire population nde measures the expected increase in as changes from to while setting the mediator variable to whatever value it would have obtained under before the change 
nie measures the expected increase in when the is held constant at and changes to whatever value it would have attained for each individual under the difference te nde measures the extent to which mediation is necessary for explaining the effect while the nie measures the extent to which mediation is sufficient for sustaining it controlled version of the indirect effect does not exist because there is no way of disabling the direct effect by fixing variable to constant 
according to these definitions the total effect can be decomposed as sum where nier stands for the reverse transition from to it becomes additive in linear systems where reversal of transitions entails sign reversal 
the power of these definitions lies in their generality they are applicable to models with arbitrary nonlinear interactions arbitrary dependencies among the disturbances and both continuous and categorical variables 
the mediation formula in linear analysis all effects are determined by sums of products of structural coefficients giving independent of therefore all effects are estimable whenever the model is identified 
in non linear systems more stringent conditions are needed for estimating the direct and indirect effects 
for example if no confounding exists and are mutually independent the following formulas can be derived 
the last two equations are called mediation formulas and have become the target of estimation in many studies of mediation 
they give distribution free expressions for direct and indirect effects and demonstrate that despite the arbitrary nature of the error distributions and the functions and mediated effects can nevertheless be estimated from data using regression 
the analyses of moderated mediation and mediating moderators fall as special cases of the causal mediation analysis and the mediation formulas identify how various interactions coefficients contribute to the necessary and sufficient components of mediation 
example assume the model takes the form where the parameter quantifies the degree to which modifies the effect of on even when all parameters are estimated from data it is still not obvious what combinations of parameters measure the direct and indirect effect of on or more practically how to assess the fraction of the total effect that is explained by mediation and the fraction of that is owed to mediation 
in linear analysis the former fraction is captured by the product the latter by the difference and the two quantities coincide 
in the presence of interaction however each fraction demands separate analysis as dictated by the mediation formula which yields thus the fraction of output response for which mediation would be sufficient is while the fraction for which mediation would be necessary is 
these fractions involve non obvious combinations of the model parameters and can be constructed mechanically with the help of the mediation formula 
significantly due to interaction direct effect can be sustained even when the parameter vanishes and moreover total effect can be sustained even when both the direct and indirect effects vanish 
this illustrates that estimating parameters in isolation tells us little about the effect of mediation and more generally mediation and moderation are intertwined and cannot be assessed separately 
references as of june this article is derived in whole or in part from causal analysis in theory and practice 
the copyright holder has licensed the content in manner that permits reuse under cc by sa and gfdl 
all relevant terms must be followed 
notes bibliographypreacher kristopher hayes andrew 
spss and sas procedures for estimating indirect effects in simple mediation models 
behavior research methods instruments and computers 
pmid preacher kristopher hayes andrew 
asymptotic and resampling strategies for assessing and comparing indirect effects in multiple mediator models 
pmid preacher zyphur zhang 
general multilevel sem framework for assessing multilevel mediation 
pmid baron and kenny 
the moderator mediator variable distinction in social psychological research conceptual strategic and statistical considerations journal of personality and social psychology vol 
statistical power analysis for the behavioral sciences nd ed 
new york ny academic press 
beyond baron and kenny statistical mediation analysis in the new millennium 
statistical methods for psychology th ed 
belmot ca cengage learning 
advances in statistical methods for substance abuse prevention research 
doi pmc pmid preacher kelley 
effect sizes measures for mediation models quantitative strategies for communicating indirect effects 
pmid rucker preacher tormala 
mediation analysis in social psychology current practices and new recommendations 
social and personality psychology compass 
asymptotic confidence intervals for indirect effects in structural equation models 
jstor spencer zanna fong 
establishing causal chain why experiments are often more effective than mediational analyses in examining psychological processes 
journal of personality and social psychology 
the mediation formula guide to the assessment of causal pathways in nonlinear models 
in berzuini dawid bernardinelli 
causality statistical perspectives and applications 
chichester uk john wiley and sons ltd pp 
research methods in psychology th ed pp 
new york mcgraw hill 
the determiners of behavior at choice point 
degrees of hunger reward and nonreward and maze learning in rats 
university of california publications in psychology 
explanation in causal inference 
external links summary of mediation methods at psychwiki example of causal mediation using propensity scores the methodology center penn state university book on moderation and mediation analysis including an introduction to the process macro for spss and sas andrew hayes ohio state university online text of the determiner of behavior at choice point kenneth maccorquodale and paul meehl on distinction between hypothetical constructs and intervening variables classics in the history of psychology retr
in music closely related key or close key is one sharing many common tones with an original key as opposed to distantly related key or distant key 
in music harmony there are six of them five share all or all except one pitches with key with which it is being compared and is adjacent to it on the circle of fifths and its relative major or minor and one shares the same tonic 
such keys are the most commonly used destinations or transpositions in modulation because of their strong structural links with the home key 
distant keys may be reached sequentially through closely related keys by chain modulation for example to to for example one principle that every composer of haydn day classical music era kept in mind was over all unity of tonality 
no piece dared wander too far from its tonic key and no piece in four movement form dared to present tonality not closely related to the key of the whole series 
for example the first movement of mozart piano sonata no 
modulates only to closely related keys the dominant supertonic and submediant given major key tonic the related keys are ii supertonic the relative minor of the subdominant iii mediant the relative minor of the dominant iv subdominant one less sharp or one more flat around circle of fifths dominant one more sharp or one fewer flat around circle of fifths vi submediant or relative minor different tonic same key signature parallel minor same tonic different key signature specifically starting from minor key the closely related keys are the mediant or relative major iii the subdominant iv the minor dominant the submediant vi the subtonic vii and the parallel major 
in the key of minor when we translate them to keys we get major minor minor major major majoranother view of closely related keys is that there are six closely related keys based on the tonic and the remaining triads of the diatonic scale excluding the dissonant diminished triads 
four of the five differ by one accidental one has the same key signature and one uses the parallel modal form 
in the key of major these would be minor minor major major minor and minor 
despite being three sharps or flats away from the original key in the circle of fifths parallel keys are also considered as closely related keys as the tonal center is the same and this makes this key have an affinity with the original key 
in modern music the closeness of relation between any two keys or sets of pitches may be determined by the number of tones they share in common which allows one to consider modulations not occurring in standard major minor tonality 
for example in music based on the pentatonic scale containing pitches and modulating fifth higher gives the collection of pitches and having four of five tones in common 
however modulating up tritone would produce which shares no common tones with the original scale 
thus the scale fifth higher is very closely related while the scale tritone higher is not 
other modulations may be placed in order from closest to most distant depending upon the number of common tones 
another view in modern music notably in bart common tonic produces closely related keys the other scales being the six other modes 
this usage can be found in several of the mikrokosmos piano pieces 
when modulation causes the new key to traverse the bottom of the circle of fifths this may give rise to theoretical key containing eight or more sharps or flats in its notated key signature in such case notational conventions require recasting the new section in its enharmonically equivalent key 
andranik tangian suggests and visualizations of key chord proximity for both all major and all minor keys chords by locating them along single subdominant dominant axis which wraps torus that is then unfolded 
see also chromatic mediant common chord music monotonality parallel and counter parallel pitch space references further reading howard hanson harmonic materials of modern music 
appleton century crofts inc
in the field of multivariate statistics kernel principal component analysis kernel pca is an extension of principal component analysis pca using techniques of kernel methods 
using kernel the originally linear operations of pca are performed in reproducing kernel hilbert space 
background linear pca recall that conventional pca operates on zero centered data that is where is one of the multivariate observations 
it operates by diagonalizing the covariance matrix in other words it gives an eigendecomposition of the covariance matrix which can be rewritten as for see also covariance matrix as linear operator introduction of the kernel to pca to understand the utility of kernel pca particularly for clustering observe that while points cannot in general be linearly separated in dimensions they can almost always be linearly separated in dimensions 
that is given points if we map them to an dimensional space with where it is easy to construct hyperplane that divides the points into arbitrary clusters 
of course this creates linearly independent vectors so there is no covariance on which to perform eigendecomposition explicitly as we would in linear pca 
instead in kernel pca non trivial arbitrary function is chosen that is never calculated explicitly allowing the possibility to use very high dimensional if we never have to actually evaluate the data in that space 
since we generally try to avoid working in the space which we will call the feature space we can create the by kernel which represents the inner product space see gramian matrix of the otherwise intractable feature space 
the dual form that arises in the creation of kernel allows us to mathematically formulate version of pca in which we never actually solve the eigenvectors and eigenvalues of the covariance matrix in the space see kernel trick 
the elements in each column of represent the dot product of one point of the transformed data with respect to all the transformed points points 
some well known kernels are shown in the example below 
because we are never working directly in the feature space the kernel formulation of pca is restricted in that it computes not the principal components themselves but the projections of our data onto those components 
to evaluate the projection from point in the feature space onto the kth principal component where superscript means the component not powers of we note that denotes dot product which is simply the elements of the kernel it seems all that left is to calculate and normalize the which can be done by solving the eigenvector equation where is the number of data points in the set and and are the eigenvalues and eigenvectors of then to normalize the eigenvectors we require that care must be taken regarding the fact that whether or not has zero mean in its original space it is not guaranteed to be centered in the feature space which we never compute explicitly 
since centered data is required to perform an effective principal component analysis we centralize to become where denotes by matrix for which each element takes value we use to perform the kernel pca algorithm described above 
one caveat of kernel pca should be illustrated here 
in linear pca we can use the eigenvalues to rank the eigenvectors based on how much of the variation of the data is captured by each principal component 
this is useful for data dimensionality reduction and it could also be applied to kpca 
however in practice there are cases that all variations of the data are same 
this is typically caused by wrong choice of kernel scale 
large datasets in practice large data set leads to large and storing may become problem 
one way to deal with this is to perform clustering on the dataset and populate the kernel with the means of those clusters 
since even this method may yield relatively large it is common to compute only the top eigenvalues and eigenvectors of the eigenvalues are calculated in this way 
example consider three concentric clouds of points shown we wish to use kernel pca to identify these groups 
the color of the points does not represent information involved in the algorithm but only shows how the transformation relocates the data points 
first consider the kernel applying this to kernel pca yields the next image 
now consider gaussian kernel that is this kernel is measure of closeness equal to when the points coincide and equal to at infinity 
note in particular that the first principal component is enough to distinguish the three different groups which is impossible using only linear pca because linear pca operates only in the given in this case two dimensional space in which these concentric point clouds are not linearly separable 
applications kernel pca has been demonstrated to be useful for novelty detection and image de noising 
see also cluster analysis nonlinear dimensionality reduction spectral clustering references
guitar tunings are the assignment of pitches to the open strings of guitars including acoustic guitars electric guitars and classical guitars
tunings are described by the particular pitches that are made by notes in western music
by convention the notes are ordered and arranged from the lowest pitched string the deepest bass sounding note to the highest pitched string the highest sounding note or the thickest string to thinnest or the lowest frequency to the highest
this sometimes confuses beginner guitarists since the highest pitched string is referred to as the st string and the lowest pitched is the th string
standard tuning defines the string pitches as and from the lowest pitch low to the highest pitch high
standard tuning is used by most guitarists and frequently used tunings can be understood as variations on standard tuning
to aid in memorising these notes mnemonics are used for example elephants and donkeys grow big ears the term guitar tunings may refer to pitch sets other than standard tuning also called nonstandard alternative or alternate
there are hundreds of these tunings often with small variants of established tunings
communities of guitarists who share common musical tradition often use the same or similar tuning styles
standard and alternatives standard standard tuning is the tuning most frequently used on six string guitar and musicians assume this tuning by default if specific alternate or scordatura is not mentioned
in scientific pitch notation the guitar standard tuning consists of the following notes
the guitar is transposing instrument that is music for guitars is notated one octave higher than the true pitch
this is to reduce the need for ledger lines in music written for the instrument and thus simplify the reading of notes when playing the guitar standard tuning provides reasonably simple fingering fret hand movement for playing standard scales and basic chords in all major and minor keys
separation of the first high and second string as well as the separation between the third fourth fifth and sixth low strings by five semitone interval perfect fourth allows the guitarist to play chromatic scale with each of the four fingers of the fretting hand controlling one of the first four frets index finger on fret little finger on fret etc
only when the hand is in the first position the open notes of the second and third strings are separated by four semitones major third
this tuning pattern of low fourths one major third and one fourth was inherited by the guitar from its predecessor instrument the viol
the irregular major third breaks the fingering patterns of scales and chords so that guitarists have to memorize multiple chord shapes for each chord
scales and chords are simplified by major thirds tuning and all fourths tuning which are regular tunings maintaining the same musical interval between consecutive open string notes
alternative alternative alternate tuning refers to any open string note arrangement other than standard tuning
these offer different kinds of deep or ringing sounds chord voicings and fingerings on the guitar
alternative tunings are common in folk music where the guitar may be called upon to produce sustained note or chord known as drone
this often gives folk music its haunting and lamenting ambience due to the atmosphere and mood that the notes make
alternative tunings change the fingering of common chords when playing the guitar and this can ease the playing of certain chords while simultaneously increase the difficulty of playing other chords
some tunings are used for particular songs and may be named after the song title
there are hundreds of these tunings although many are slight variations of other alternate tunings
several alternative tunings are used regularly by communities of guitarists who share common musical tradition such as american folk or celtic folk music the various alternative tunings have been grouped into the following categories dropped open both major and minor cross note modal instrumental based on other stringed instruments miscellaneous special joni mitchell is known for developing shorthand descriptive method of noting guitar tuning where the first letter documents the note of the lowest string and is followed by the relative fret half step offsets required to obtain the pitch of the next higher string
this scheme highlights pitch relationships and simplifies the process of comparing different tuning schemes
string gauges string gauge refers to the thickness and diameter of guitar string which influences the overall sound and pitch of the guitar depending on the guitar string used
some alternative tunings are difficult or even impossible to achieve with conventional guitars due to the sets of guitar strings which have gauges optimized for standard tuning
with conventional sets of guitar strings some higher tunings increase the string tension until playing the guitar requires significantly more finger strength and stamina or even until string snaps or the guitar is warped
however with lower tunings the sets of guitar strings may be loose and buzz
the tone of the guitar strings is also negatively affected by using unsuitable string gauges on the guitar
generally alternative tunings benefit from re stringing of the guitar with string gauges purposefully chosen to optimize particular tunings by using lighter strings for higher pitched notes to lower the tension of the strings and heavier strings for lower pitched notes to prevent string buzz and vibration
dropped tunings dropped tuning is one of the categories of alternative tunings and the process starts with standard tuning and typically lowers the pitch of drops only single string almost always the lowest pitched string on the guitar
the drop tuning is common in electric guitar and heavy metal music
the low string is tuned down one whole step to and the rest of the strings remain in standard tuning
this creates an open power chord three note fifth with the low three strings dad
there also exists double drop tuning in which both strings are down tuned whole step to
the rest of the strings keep their original pitch
although the drop tuning was introduced and developed by blues and classical guitarists it is well known from its usage in contemporary heavy metal and hard rock bands
early hard rock songs tuned in drop include the beatles want you she so heavy and led zeppelin moby dick both first released in tuning the lowest string one tone down from to allowed these musicians to acquire heavier and darker sound than in standard tuning
without needing to tune all strings standard tuning they could tune just one in order to lower the key
drop is also convenient tuning because it expands the scale of an instrument by two semitones and
in the mid three alternative rock bands king soundgarden and melvins influenced by led zeppelin and black sabbath made extensive use of drop tuning
while playing power chords chord that includes the prime fifth and octave in standard tuning requires player to use two or three fingers drop tuning needs just one similar in technique to playing barre chords
it allowed them to use different methods of articulating power chords legato for example and more importantly it allowed guitarists to change chords faster
this new technique of playing power chords introduced by these early grunge bands was great influence on many artists such as rage against the machine and tool
the same drop tuning then became common practice among alternative metal acts such as the band helmet who used the tuning great deal throughout their career and would later influence many alternative metal and nu metal bands
open tunings an open tuning allows the guitarist to play chord by strumming the open strings no strings fretted
open tunings may be chordal or modal
in chordal open tunings the open chord consists of at least three different pitch classes
in given key these are the root note its rd and its th and may include all the strings or subset
the tuning is named for the base chord when played open typically major chord and all similar chords in the chromatic scale are played by barring all strings across single fret
open tunings are common in blues and folk music
these tunings are frequently used in the playing of slide and lap slide hawaiian guitars and hawaiian slack key music
musician who is well known for using open tuning in his music is ry cooder who uses open tunings when playing the slide guitar most modern music uses equal temperament because it facilitates the ability to play the guitar in any key as compared to just intonation which favors certain keys and makes the other keys sound less in tune repetitive open tunings are used for two classical non spanish guitars
for the english guitar the open chord is major for the russian guitar which has seven strings it is major when the open strings constitute minor chord the open tuning may sometimes be called cross note tuning
major key tunings major open tunings give major chord with the open strings
open tunings often tune the lowest open note to or and they often tune the highest open note to or tuning down the open string from to or avoids the risk of breaking strings which is associated with tuning up strings
open the open tuning also called vestapol tuning is common open tuning used by european and american western guitarists working with alternative tunings
the allman brothers instrumental little martha used an open tuning raised one half step giving an open tuning with the same intervallic relationships as open open the english guitar used repetitive open tuning with distinct open notes that approximated major thirds tuning
this tuning is evident in william ackerman song townsend shuffle as well as by john fahey for his tribute to mississippi john hurt the tuning uses some of the harmonic sequence overtones of the note this overtone series tuning was modified by mick ralphs who used high note rather than the high note for can get enough on bad company
ralphs said it needs the open to have that ring and it never really sounds right in standard tuning
open mick ralphs open tuning was originally an open tuning which listed the initial six overtones of the note namely ralphs used this open tuning for hey hey and while writing the demo of can get enough open tuning usually refers to
the open tuning variant was used by joni mitchell for electricity for the roses and hunter the good samaritan
truncating this tuning to for his five string guitar keith richards uses this overtones tuning on the rolling stones honky tonk women brown sugar and start me up the seven string russian guitar uses the open tuning which contains mostly major and minor thirds
creating any kind of open tuning any kind of chordal tuning can be achieved simply by using the notes in the chord and tuning the strings to those notes
for example asus has the notes by tuning the strings to only those notes it creates chordal asus tuning
bass players may omit the last two strings
minor or cross note tunings cross note tunings include minor third so giving minor chord with open strings
fretting the minor third string at the first fret produces major third so allowing one finger fretting of major chord
by contrast it is more difficult to fret minor chord using an open major chord tuning
bukka white and skip james are well known for using cross note minor in their music
other open chordal tunings some guitarists choose open tunings that use more complex chords which gives them more available intervals on the open strings
and other such tunings are common among lap steel players such as hawaiian slack key guitarists and country guitarists and are also sometimes applied to the regular guitar by bottleneck slide repurposed from glass bottle players striving to emulate these styles
common tuning for example is which provides open major and minor thirds open major and minor sixths fifths and octaves
by contrast most open major or open minor tunings provide only octaves fifths and either major third sixth or minor third sixth but not both
don helms of hank williams band favored tuning slack key artist henry kaleialoha allen uses modified tuning with on the bottom harmon davis favored tuning david gilmour has used an open tuning
modal tunings modal tunings are open tunings in which the open strings of the guitar do not produce tertian major or minor or variants thereof chord
the strings may be tuned to exclusively present single interval all fourths all fifths etc
or they may be tuned to non tertian chord unresolved suspensions such as for example
modal open tunings may use only one or two pitch classes across all strings as for example some metal guitarists who tune each string to either or forming power chords of ambiguous major minor tonality
popular modal tunings include modal and modal
lowered standard derived from standard eadgbe all the strings are tuned lower by the same interval thus providing the same chord positions transposed to lower key
lower tunings are popular among rock and heavy metal bands
the reason for tuning down below the standard pitch is usually either to accommodate singer vocal range or to get deeper heavier sound or pitch
common examples include tuning rock guitarists such as jimi hendrix on the songs voodoo child slight return and little wing occasionally tune all their strings down by one semitone to obtain tuning
this makes the strings easier to bend when playing and with standard fingering results in lower key
it also facilitates shape fingerings when playing with horn instruments
tuning tuning also called one step lower whole step down full step or standard is another alternative
each string is lowered by whole tone two semitones resulting in it is used mostly by heavy metal bands to achieve heavier deeper sound and by blues guitarists who use it to accommodate string bending and by string guitar players to reduce the mechanical load on their instrument
among musicians elliott smith was known to use tuning as his main tuning for his music
it was also used for several songs on the velvet underground album the velvet underground nico
regular tunings in standard tuning there is an interval of major third between the second and third strings and all the other intervals are fourths
the irregularity has price
chords cannot be shifted around the fretboard in the standard tuning which requires four chord shapes for the major chords
there are separate chord forms for chords having their root note on the third fourth fifth and sixth strings
these are called inversions
in contrast regular tunings have equal intervals between the strings and so they have symmetrical scales all along the fretboard
this makes it simpler to translate chords
for the regular tunings chords may be moved diagonally around the fretboard
the diagonal movement of chords is especially simple for the regular tunings that are repetitive in which case chords can be moved vertically chords can be moved three strings up or down in major thirds tuning and chords can be moved two strings up or down in augmented fourths tuning
regular tunings thus appeal to new guitarists and also to jazz guitarists whose improvisation is simplified by regular intervals
on the other hand five and six string open chords cowboy chords are more difficult to play in regular tuning than in standard tuning
instructional literature uses standard tuning
traditionally course begins with the hand in first position that is with the left hand covering frets
beginning players first learn open chords belonging to the major keys and guitarists who play mainly open chords in these three major keys and their relative minor keys am em bm may prefer standard tuning over many regular tunings on the other hand minor thirds tuning features many barre chords with repeated notes properties that appeal to acoustic guitarists and beginners
major thirds and perfect fourths standard tuning mixes major third with its perfect fourths
regular tunings that are based on either major thirds or perfect fourths are used for example in jazz
all fourths tuning keeps the lowest four strings of standard tuning changing the major third to perfect fourth
jazz musician stanley jordan stated that all fourths tuning simplifies the fingerboard making it logical major thirds tuning tuning is regular tuning in which the musical intervals between successive strings are each major thirds for example
unlike all fourths and all fifths tuning tuning repeats its octave after three strings which simplifies the learning of chords and improvisation
this repetition provides the guitarist with many possibilities for fingering chords
with six strings major thirds tuning has smaller range than standard tuning with seven strings the major thirds tuning covers the range of standard tuning on six strings major thirds tunings require less hand stretching than other tunings because each tuning packs the octave twelve notes into four consecutive frets
the major third intervals let the guitarist play major chords and minor chords with two three consecutive fingers on two consecutive frets chord inversion is especially simple in major thirds tuning
the guitarist can invert chords by raising one or two notes on three strings playing the raised notes with the same finger as the original notes
in contrast inverting triads in standard and all fourths tuning requires three fingers on span of four frets
in standard tuning the shape of an inversion depends on the involvement of the major third between the nd and rd strings
all fifths and new standard tuning all fifths tuning is tuning in intervals of perfect fifths like that of mandolin or violin other names include perfect fifths and fifths
it has wide range
its implementation has been impossible with nylon strings and has been difficult with conventional steel strings
the high makes the first string very taut and consequently conventionally gauged string easily breaks
jazz guitarist carl kress used variation of all fifths tuning with the bottom four strings in fifths and the top two strings in thirds resulting in
this facilitated tenor banjo chord shapes on the bottom four strings and plectrum banjo chord shapes on the top four strings
contemporary new york jazz guitarist marty grosz uses this tuning
all fifths tuning has been approximated by the so called new standard tuning nst of king crimson robert fripp which nst replaces all fifths high with high
to build chords fripp uses perfect intervals in fourths fifths and octaves so avoiding minor thirds and especially major thirds which are slightly sharp in equal temperament tuning in comparison to thirds in just intonation
it is challenge to adapt conventional guitar chords to new standard tuning which is based on all fifths tuning
some closely voiced jazz chords become impractical in nst and all fifths tuning
instrumental tunings these are tunings in which some or all strings are retuned to emulate the standard tuning of some other instrument such as lute banjo cittern mandolin etc
many of these tunings overlap other categories especially open and modal tunings
miscellaneous or special tunings this category includes everything that does not fit into any of the other categories for example but not limited to tunings designated only for particular piece non western intervals and modes micro or macro tones half sharps flats etc
and hybrid tunings combining features of major alternate tuning categories most commonly an open tuning with the lowest string dropped
see also bass guitar tuning list of guitar tunings mathematics and music open tuning stringed instrument tunings dadgad notes citation references references allen warren september december
wa encyclopedia of guitar tunings
recommended by marcus gary
guitar zero the science of learning to be musical
archived from the original on july retrieved june annala hannu tlik heiki
composers for other plucked instruments rudolf straube
handbook of guitar and lute composers
translated by katarina backman
the illustrated history of the guitar
playing the guitar how the guitar is tuned pp
and alternative tunings pp
special contributors isaac guillory and alastair crawford fully revised and updated ed
london and sydney pan books
in casabona helen belew adrian eds
new directions in modern guitar
guitar player basic library
isbn griewank andreas january tuning guitars and reading music in major thirds matheon preprints vol
berlin germany dfg research center matheon mathematics for key technologies berlin urn nbn de matheon
postscript file and pdf file archived from the original on november grossman stefan
the book of guitar tunings
new york amsco publishing company
isbn lccn persichetti vincent
twentieth century harmony creative aspects and practice
isbn oclc peterson jonathon
tuning in thirds new approach to playing leads to new kind of guitar
american lutherie the quarterly journal of the guild of american luthiers
tacoma wa the guild of american luthiers
issn archived from the original on october retrieved october roche eric
thinking outside the box
the acoustic guitar bible
london bobcat books limited smt
madison wisconsin university of wisconsin department of electrical engineering
retrieved may sethares bill
alternate tuning guide pdf
madison wisconsin university of wisconsin department of electrical engineering
retrieved may sethares william
madison wisconsin university of wisconsin department of electrical engineering
retrieved may tamm eric
chapter ten guitar craft
robert fripp from crimson king to crafty master
isbn archived from the original on october retrieved march via progressive ears
zipped microsoft word document weissman dick
guitar tunings comprehensive guide
alternate tunings guitar essentials
acoustic guitar magazine private lessons
hal leonard publishing corporation
isbn lccn hanson mark
the complete book of alternate tunings
alternate tunings picture chords
mastering alternate tunings revolutionary system of fretboard navigation for fingerstyle guitarists
alternate tuning chord dictionary
isbn lccn maloof richard
alternate tunings for guitar
cherry lane music company
isbn lccn shark mark
the tao of tunings map to the world of alternate tunings
external links allen warren september december
wa encyclopedia of guitar tunings
recommended by marcus gary
guitar zero the science of learning to be musical
retrieved june sethares william
alternate tuning guide interactive
uses wolfram cdf player
the circle of fifths text table shows the number of flats or sharps in each of the diatonic musical scales and keys 
both major and minor keys have no flats or sharps 
in the table minor keys are written with lowercase letters for brevity 
however in common guitar tabs notation minor key is designated with lowercase 
for example minor is am and sharp minor is 
the small interval between equivalent notes such as sharp and flat is the pythagorean comma 
minor scales start with major scales start with 
see also circle of fifths key signature musical notation notes
searchable symmetric encryption sse is form of encryption that allows one to efficiently search over collection of encrypted documents or files without the ability to decrypt them 
sse can be used to outsource files to an untrusted cloud storage server without ever revealing the files in the clear but while preserving the server ability to search over them 
description searchable symmetric encryption scheme is symmetric key encryption scheme that encrypts collection of documents where each document is viewed as set of keywords from keyword space given the encryption key and keyword one can generate search token with which the encrypted data collection can be searched for the result of the search is the subset of encrypted documents that contain the keyword static sse static sse scheme consists of three algorithms that work as follows takes as input security parameter and document collection and outputs symmetric key and an encrypted document collection takes as input the secret key and keyword and outputs search token takes as input the encrypted document collection and search token and outputs set of encrypted documents static sse scheme is used by client and an untrusted server as follows 
the client encrypts its data collection using the algorithm which returns secret key and an encrypted document collection the client keeps secret and sends to the untrusted server 
to search for keyword the client runs the algorithm on and to generate search token which it sends to the server 
the server runs search with and and returns the resulting encrypted documents back to the server 
dynamic sse dynamic sse scheme supports in addition to search the insertion and deletion of documents 
dynamic sse scheme consists of seven algorithms where and are as in the static case and the remaining algorithms work as follows takes as input the secret key and new document and outputs an insert token takes as input the encrypted document collection edc and an insert token and outputs an updated encrypted document collection takes as input the secret key and document identifier and outputs delete token takes as input the encrypted data collection and delete token and outputs an updated encrypted data collection to add new document the client runs on and to generate an insert token which it sends to the server 
the server runs with and and stores the updated encrypted document collection 
to delete document with identifier the client runs the algorithm with and to generate delete token which it sends to the server 
the server runs with and and stores the updated encrypted document collection 
an sse scheme that does not support and is called semi dynamic 
history of searchable symmetric encryption the problem of searching on encrypted data was considered by song wagner and perrig though previous work on oblivious ram by goldreich and ostrovsky could be used in theory to address the problem 
this work proposed an sse scheme with search algorithm that runs in time where 
goh and chang and mitzenmacher gave new sse constructions with search algorithms that run in time where is the number of documents 
curtmola garay kamara and ostrovsky later proposed two static constructions with search time where is the number of documents that contain which is optimal 
this work also proposed semi dynamic construction with log search time where is the number of updates 
an optimal dynamic sse construction was later proposed by kamara papamanthou and roeder goh and chang and mitzenmacher proposed security definitions for sse 
these were strengthened and extended by curtmola garay kamara and ostrovsky who proposed the notion of adaptive security for sse 
this work also was the first to observe leakage in sse and to formally capture it as part of the security definition 
leakage was further formalized and generalized by chase and kamara 
islam kuzu and kantarcioglu described the first leakage attack all the previously mentioned constructions support single keyword search 
cash jarecki jutla krawczyk rosu and steiner proposed an sse scheme that supports conjunctive search in sub linear time in the construction can also be extended to support disjunctive and boolean searches that can be expressed in searchable normal form snf in sub linear time 
at the same time pappas krell vo kolesnikov malkin choi george keromytis and bellovin described construction that supports conjunctive and all disjunctive and boolean searches in sub linear time 
security sse schemes are designed to guarantee that the untrusted server cannot learn any partial information about the documents or the search queries beyond some well defined and reasonable leakage 
the leakage of scheme is formally described using leakage profile which itself can consists of several leakage patterns 
sse constructions attempt to minimize leakage while achieving the best possible search efficiency 
sse security can be analyzed in several adversarial models but the most common are the persistent model where an adversary is given the encrypted data collection and transcript of all the operations executed on the collection the snapshot model where an adversary is only given the encrypted data collection but possibly after each operation 
security in the persistent model in the persistent model there are sse schemes that achieve wide variety of leakage profiles 
the most common leakage profile for static schemes that achieve single keyword search in optimal time is which reveals the number of documents in the collection the size of each document in the collection if and when query was repeated and which encrypted documents match the search query 
it is known however how to construct schemes that leak considerably less at an additional cost in search time and storage when considering dynamic sse schemes the state of the art constructions with optimal time search have leakage profiles that guarantee forward privacy which means that inserts cannot be correlated with past search queries 
security in the snapshot model in the snapshot model efficient dynamic sse schemes with no leakage beyond the number of documents and the size of the collection can be constructed 
when using an sse construction that is secure in the snapshot model one has to carefully consider how the scheme will be deployed because some systems might cache previous search queries 
cryptanalysis leakage profile only describes the leakage of an sse scheme but it says nothing about whether that leakage can be exploited or not 
cryptanalysis is therefore used to better understand the real world security of leakage profile 
there is wide variety of attacks working in different adversarial models based on variety of assumptions and attacking different leakage profiles 
see also homomorphic encryption oblivious ram structured encryption deterministic encryption references
in music guitar chord is set of notes played on guitar
chord notes are often played simultaneously but they can be played sequentially in an arpeggio
the implementation of guitar chords depends on the guitar tuning
most guitars used in popular music have six strings with the standard tuning of the spanish classical guitar namely from the lowest pitched string to the highest in standard tuning the intervals present among adjacent strings are perfect fourths except for the major third
standard tuning requires four chord shapes for the major triads
there are separate chord forms for chords having their root note on the third fourth fifth and sixth strings
for six string guitar in standard tuning it may be necessary to drop or omit one or more tones from the chord this is typically the root or fifth
the layout of notes on the fretboard in standard tuning often forces guitarists to permute the tonal order of notes in chord
the playing of conventional chords is simplified by open tunings which are especially popular in folk blues guitar and non spanish classical guitar such as english and russian guitar
for example the typical twelve bar blues uses only three chords each of which can be played in every open tuning by fretting six strings with one finger
open tunings are used especially for steel guitar and slide guitar
open tunings allow one finger chords to be played with greater consonance than do other tunings which use equal temperament at the cost of increasing the dissonance in other chords
the playing of to string guitar chords is simplified by the class of alternative tunings called regular tunings in which the musical intervals are the same for each pair of consecutive strings
regular tunings include major thirds tuning all fourths and all fifths tunings
for each regular tuning chord patterns may be diagonally shifted down the fretboard property that simplifies beginners learning of chords and that simplifies advanced players improvisation
on the other hand in regular tunings string chords in the keys of and are more difficult to play
conventionally guitarists double notes in chord to increase its volume an important technique for players without amplification doubling notes and changing the order of notes also changes the timbre of chords
it can make possible chord which is composed of the all same note on different strings
many chords can be played with the same notes in more than one place on the fretboard
musical fundamentals the theory of guitar chords respects harmonic conventions of western music
discussions of basic guitar chords rely on fundamental concepts in music theory the twelve notes of the octave musical intervals chords and chord progressions
intervals the octave consists of twelve notes
its natural notes constitute the major scale and
the intervals between the notes of chromatic scale are listed in table in which only the emboldened intervals are discussed in this article section on fundamental chords those intervals and other seventh intervals are discussed in the section on intermediate chords
the unison and octave intervals have perfect consonance
octave intervals were popularized by the jazz playing of wes montgomery
the perfect fifth interval is highly consonant which means that the successive playing of the two notes from the perfect fifth sounds harmonious
semitone is the distance between two adjacent notes on the chromatic circle which displays the twelve notes of an octave
as indicated by their having been emboldened in the table handful of intervals thirds minor and major perfect fifths and minor sevenths are used in the following discussion of fundamental guitar chords
as already stated the perfect fifths interval is the most harmonious after the unison and octave intervals
an explanation of human perception of harmony relates the mechanics of vibrating string to the musical acoustics of sound waves using the harmonic analysis of fourier series
when string is struck with finger or pick plectrum it vibrates according to its harmonic series
when an open note string is struck its harmonic series begins with the terms
the root note is associated with sequence of intervals beginning with the unison interval the octave interval the perfect fifth the perfect fourth and the major third
in particular this sequence of intervals contains the thirds of the major chord
with note of music one strikes the fundamental and in addition to the root note other notes are generated these are the harmonic series as one fundamental note contains within it other notes in the octave two fundamentals produce remarkable array of harmonics and the number of possible combinations between all the notes increases phenomenally
with triad affairs stand good chance of getting severely out of hand
perfect fifths the perfect fifth interval is featured in guitar playing and in sequences of chords
the sequence of fifth intervals built on the major scale is used in the construction of triads which is discussed below
cycle of fifths concatenating the perfect fifths yields the sequence of fifths this sequence of fifths displays all the notes of the octave
this sequence of fifths shall be used in the discussions of chord progressions below
power chord the perfect fifth interval is called power chord by guitarists who play them especially in blues and rock music
the who guitarist peter townshend performed power chords with theatrical windmill strum
power chords are often played with the notes repeated in higher octaves although established the term power chord is inconsistent with the usual definition of chord in musical theory which requires three or more distinct notes in each chord
chords in music theory brief overview the musical theory of chords is reviewed to provide terminology for discussion of guitar chords
three kinds of chords which are emphasized in introductions to guitar playing are discussed
these basic chords arise in chord triples that are conventional in western music triples that are called three chord progressions
after each type of chord is introduced its role in three chord progressions is noted
intermediate discussions of chords derive both chords and their progressions simultaneously from the harmonization of scales
the basic guitar chords can be constructed by stacking thirds that is by concatenating two or three third intervals where all of the lowest notes come from the scale
triads major both major and minor chords are examples of musical triads which contain three distinct notes
triads are often introduced as an ordered triplet the root the third which is above the root by either major third for major chord or minor third for minor chord the fifth which is perfect fifth above the root consequently the fifth is third above the third either minor third above major third or major third above minor third
the major triad has root major third and fifth
the major chord major third interval is replaced by minor third interval in the minor chord which shall be discussed in the next subsection
for example major triad consists of the root third fifth notes
the three notes of major triad have been introduced as an ordered triplet namely root third fifth where the major third is four semitones above the root and where the perfect fifth is seven semitones above the root
this type of triad is in closed position
triads are quite commonly played in open position for example the major triad is often played with the third and fifth an octave higher respectively sixteen and nineteen semitones above the root
another variation of the major triad changes the order of the notes for example the major triad is often played as where is perfect fifth and is raised an octave above the perfect third
alternative orderings of the notes in triad are discussed below in the discussions of chord inversions and drop chords
in popular music subset of triads is emphasized those with notes from the three major keys which also contain the notes of their relative minor keys am em bm
progressions the major chords are highlighted by the three chord theory of chord progressions which describes the three chord song that is archetypal in popular music
when played sequentially in any order the chords from three chord progression sound harmonious good together the most basic three chord progressions of western harmony have only major chords
in each key three chords are designated with the roman numerals of musical notation the tonic the subdominant iv and the dominant
while the chords of each three chord progression are numbered iv and they appear in other orders
in the the iv chord progression was used in hound dog elvis presley and in chantilly lace the big bopper major chord progressions are constructed in the harmonization of major scales in triads
for example stacking the major scale with thirds creates chord progression which is traditionally enumerated with the roman numerals ii iii iv vi viio its sub progression iv is used in popular music as already discussed
further chords are constructed by stacking additional thirds
stacking the dominant major triad with minor third creates the dominant seventh chord which shall be discussed after minor chords
minor minor chord has the root and the fifth of the corresponding major chord but its first interval is minor third rather than major third minor chords arise in the harmonization of the major scale in thirds which was already discussed the minor chords have the degree positions ii iii and vi
minor chords arise as the tonic notes of minor keys that share the same key signature with major keys
from the major key ii iii iv vi viio progression the secondary minor triads ii iii vi appear in the relative minor key corresponding chord progression as iv or iv or iv for example from vi ii iii progression am dm em the chord em is often played as or in minor chord progression
among basic chords the minor chords are the tonic chords of the relative minors of the three major keys the technique of changing among relative keys pairs of relative majors and relative minors is form of modulation
minor chords are constructed by the harmonization of minor scales in triads
seventh chords major minor chords with dominant function adding minor seventh to major triad creates dominant seventh denoted
in music theory the dominant seventh described here is called major minor seventh emphasizing the chord construction rather than its usual function
dominant sevenths are often the dominant chords in three chord progressions in which they increase the tension with the tonic already inherent in the dominant triad
the dominant seventh discussed is the most commonly played seventh chord
an major iv chord progression was used by paul mccartney in the song legs on his album ram these progressions with seventh chords arise in the harmonization of major scales in seventh chords
twelve bar blues be they in major key or minor key such iv chord progressions are extended over twelve bars in popular music especially in jazz blues and rock music
for example twelve bar blues progression of chords in the key of has three sets of four bars this progression is simplified by playing the sevenths as major chords
the twelve bar blues structure is used by mccartney legs which was noted earlier
playing chords open strings inversion and note doubling the implementation of musical chords on guitars depends on the tuning
since standard tuning is most commonly used expositions of guitar chords emphasize the implementation of musical chords on guitars with standard tuning
the implementation of chords using particular tunings is defining part of the literature on guitar chords which is omitted in the abstract musical theory of chords for all instruments
for example in the guitar like other stringed instruments but unlike the piano open string notes are not fretted and so require less hand motion
thus chords that contain open notes are more easily played and hence more frequently played in popular music such as folk music
many of the most popular tunings standard tuning open tunings and new standard tuning are rich in the open notes used by popular chords
open tunings allow major triads to be played by barring one fret with only one finger using the finger like capo
on guitars without zeroth fret after the nut the intonation of an open note may differ from then note when fretted on other strings consequently on some guitars the sound of an open note may be inferior to that of fretted note unlike the piano the guitar has the same notes on different strings
consequently guitar players often double notes in chord so increasing the volume of sound
doubled notes also changes the chordal timbre having different string widths tensions and tunings the doubled notes reinforce each other like the doubled strings of twelve string guitar add chorusing and depth
notes can be doubled at identical pitches or in different octaves
for triadic chords doubling the third interval which is either major third or minor third clarifies whether the chord is major or minor unlike piano or the voices of choir the guitar in standard tuning has difficulty playing the chords as stacks of thirds which would require the left hand to span too many frets particularly for dominant seventh chords as explained below
if in particular tuning chords cannot be played in closed position then they often can be played in open position similarly if in particular tuning chords cannot be played in root position they can often be played in inverted positions
chord is inverted when the bass note is not the root note
additional chords can be generated with drop or drop voicing which are discussed for standard tuning implementation of dominant seventh chords below
when providing harmony in accompanying melody guitarists may play chords all at once or as arpeggios
arpeggiation was the traditional method of playing chords for guitarists for example in the time of mozart
contemporary guitarists using arpeggios include johnny marr of the smiths
fundamental chords standard tuning six string guitar has five musical intervals between its consecutive strings
in standard tuning the intervals are four perfect fourths and one major third the comparatively irregular interval for the pair
consequently standard tuning requires four chord shapes for the major chords
there are separate chord forms for chords having their root note on the third fourth fifth and sixth strings
of course beginner learns guitar by learning notes and chords and irregularities make learning the guitar difficult even more difficult than learning the formation of plural nouns in german according to gary marcus
nonetheless most beginners use standard tuning another feature of standard tuning is that the ordering of notes often differs from root position
notes are often inverted or otherwise permuted particularly with seventh chords in standard tuning as discussed below
power chords fingerings as previously discussed each power chord has only one interval perfect fifth between the root note and the fifth
in standard tuning the following fingerings are conventional triads triads are usually played with doubled notes as the following examples illustrate
major commonly used major chords are convenient to play in standard tuning in which fundamental chords are available in open position that is the first three frets and additional open strings
for the major chord the conventional left hand fingering doubles the and notes in the next octave this fingering uses two open notes and on the first string on the second string on the third string on the fourth string on the fifth string sixth string is not played major chords guide for guitar chord charts xx movable remember that no sharps or flats are between bc and ef normal nashville style for the other commonly used chords the conventional fingerings also double notes and feature open string notes besides doubling the fifth note the conventional major chord features tripled bass note
the major and major chords are commonly played as barre chords with the first finger depressing five six strings
major chord has the same shape as the major chord but it is located two frets further up the fretboard
the major chord is the same shape as major but it is located one fret further up the fretboard
minor minor chords commonly notated as cm cmi or cmin are the same as major chords except that they have minor third instead of major third
this is difference of one semitone
to create minor from the major chord in major shape the second finger should be lifted so that the third string plays onto the barre
compare the major to minor the other shapes can be modified as well suspended movable suspended chords guide for chord charts in standard tuning sus sus sus sus sus sus sus sus sus these chords are used extensively by my bloody valentine on the album loveless
they are also used on the who song pinball wizard and many many more songs
dominant sevenths drop two as previously stated dominant seventh is four note chord combining major chord and minor seventh
for example the dominant seventh chord adds to the major chord
the naive chord spans six frets from fret to fret such seventh chords contain some pretty serious stretches in the left hand
an illustration shows naive chord which would be extremely difficult to play besides the open position chord that is conventional in standard tuning
the standard tuning implementation of chord is second inversion drop chord in which the second highest note in second inversion of the chord is lowered by an octave
drop two chords are used for sevenths chords besides the major minor seventh with dominant function which are discussed in the section on intermediate chords below
drop two chords are used particularly in jazz guitar
drop two second inversions are examples of openly voiced chords which are typical of standard tuning and other popular guitar tunings
alternatively voiced seventh chords are commonly played with standard tuning
list of fret number configurations for some common chords follows this requires no barre unlike the major
xx other chord inversions already in basic guitar playing inversion is important for sevenths chords in standard tuning
it is also important for playing major chords
in standard tuning chord inversion depends on the bass note string and so there are three different forms for the inversion of each major chord depending on the position of the irregular major thirds interval between the and strings
for example if the note the open sixth string is played over the minor chord then the chord would be
this has the note as its lowest tone instead of it is often written as am where the letter following the slash indicates the new bass note
however in popular music it is usual to play inverted chords on the guitar when they are not part of the harmony since the bass guitar can play the root pitch
alternate tunings there are many alternate tunings
these change the way chords are played making some chords easier to play and others harder
open tunings each allow chord to be played by strumming the strings when open or while fretting no strings
open tunings are common in blues and folk music and they are used in the playing of slide guitar
drop tunings are common in hard rock and heavy metal music
in drop tuning the standard tuning string is tuned down to note
with drop tuning the bottom three strings are tuned to root fifth octave tuning which simplifies the playing of power chords
regular tunings allow chord note forms to be shifted all around the fretboard on all six strings unlike standard or other non regular tunings
knowing few note patterns for example of the major minor and chords enables guitarist to play all such chords sethares learn handful of chord forms in regular tuning and you ll know hundreds of chords
ref open tunings an open tuning allows chord to be played by strumming the strings when open or while fretting no strings
the base chord consists of at least three notes and may include all the strings or subset
the tuning is named for the base chord when played open typically major triad and each major triad can be played by barring exactly one fret
open tunings are common in blues and folk music and they are used in the playing of slide and lap slide hawaiian guitars
ry cooder uses open tunings when he plays slide guitar open tunings improve the intonation of major chords by reducing the error of third intervals in equal temperaments
for example in the open overtones tuning the interval is major third and of course each successive pair of notes on the and strings is also major third similarly the open string minor third induces minor thirds among all the frets of the strings
the thirds of equal temperament have audible deviations from the thirds of just intonation equal temperaments is used in modern music because it facilitates music in all keys while on piano and other instruments just intonation provided better sounding major third intervals for only subset of keys
sonny landreth keith richards and other open masters often lower the second string slightly so the major third is in tune with the overtone series
this adjustment dials out the dissonance and makes those big one finger major chords come alive
repetitive open tunings are used for two non spanish classical guitars
for the english guitar the open chord is major for the russian guitar which has seven strings major
mixing perfect fourth and minor third along with major third these tunings are on average major thirds regular tunings
while on average major thirds tunings are conventional open tunings properly major thirds tunings are unconventional open tunings because they have augmented triads as their open chords
regular tunings guitar chords are dramatically simplified by the class of alternative tunings called regular tunings
in each regular tuning the musical intervals are the same for each pair of consecutive strings
regular tunings include major thirds all fourths augmented fourths and all fifths tunings
for each regular tuning chord patterns may be diagonally shifted down the fretboard property that simplifies beginners learning of chords and that simplifies advanced players improvisation
the diagonal shifting of major chord in tuning appears in diagram
further simplifications occur for the regular tunings that are repetitive that is which repeat their strings
for example the tuning repeats its octave after every two strings
such repetition further simplifies the learning of chords and improvisation this repetition results in two copies of the three open strings notes each in different octave
similarly the augmented fourths tuning repeats itself after one string
chord is inverted when the bass note is not the root note
chord inversion is especially simple in tuning
chords are inverted simply by raising one or two notes by three strings each raised note is played with the same finger as the original note
inverted major and minor chords can be played on two frets in tuning
in standard tuning the shape of inversions depends on the involvement of the irregular major third and can involve four frets
it is challenge to adapt conventional guitar chords to new standard tuning which is based on all fifths tuning
intermediate chords after major and minor triads are learned intermediate guitarists play seventh chords
tertian harmonization stacking of third intervals the fundamental guitar chords major and minor triads and dominant sevenths are tertian chords which concatenate third intervals with each such third being either major or minor
more triads diminished and augmented as discussed above major and minor triads are constructed by stacking thirds the major triad concatenates supplementing with perfect fifth interval and the minor triad concatenates supplementing with interval similar tertian harmonization yields the remaining two triads the diminished triad concatenates supplementing with diminished fifth interval and the augmented triad concatenates supplementing with an augmented fifth interval
more sevenths major minor and half diminished stacking thirds also constructs the most used seventh chords
the most important seventh chords concatenate major triad with third interval supplementing it with seventh interval the dominant major minor seventh concatenates major triad with another minor third supplementing it with minor seventh interval
the major seventh concatenates major triad with major third supplementing it with major seventh interval
the minor seventh concatenates minor triad with minor third supplementing it with minor seventh interval
the half diminished seventh concatenates diminished triad with major third supplementing it with diminished seventh interval
the fully diminished seventh concatenates diminished triad with minor third supplementing it with diminished seventh interval four of these five seventh chords all but the diminished seventh are constructed via the tertian harmonization of major scale
as already stated the major minor seventh has the dominant function
the major seventh plays the tonic and subdominant iv roles the minor seventh plays the ii iii and vi roles
the half diminished seventh plays the vii role while absent from the tertian harmonization of the major scale the diminished seventh plays the viio role in the tertian harmonization of the harmonic minor scale besides these five types there are many more seventh chords which are less used in the tonal harmony of the common practice period
when playing seventh chords guitarists often play only subset of notes from the chord
the fifth is often omitted
when guitar is accompanied by bass the guitarist may omit the bass note from chord
as discussed earlier the third of triad is doubled to emphasize its major or minor quality similarly the third of seventh is doubled to emphasize its major or minor quality
the most frequent seventh is the dominant seventh the minor half diminished and major sevenths are also popular
chord progression circle of fifths the previously discussed iv chord progressions of major triads is subsequence of the circle progression which ascends by perfect fourths and descends by perfect fifths perfect fifths and perfect fourths are inverse intervals because one reaches the same pitch class by either ascending by perfect fourth five semitones or descending by perfect fifth seven semitones
for example the jazz standard autumn leaves contains the iv vii vim ii circle of fifths chord progression its sevenths occur in the tertian harmonization in sevenths of the minor scale
other subsequences of the fifths circle chord progression are used in music
in particular the ii progression is the most important chord progression in jazz music
chord chart guide for major inversions major inversions for guitar in standard tuning
the low is on the left
the demonstrates three of the different movable shapes
xxx xxx xxx xxx xxx xxx xxx xxx xxx specific tunings standard tuning minor and major sevenths besides the dominant seventh chords discussed above other seventh chords especially minor seventh chords and major seventh chords are used in guitar music
minor seventh chords have the following fingerings in standard tuning dm xx em am bm or xx also an chord major seventh chords have the following fingerings in standard tuning cmaj dmaj xx emaj fmaj gmaj amaj major thirds tuning in major thirds tuning the chromatic scale is arranged on three consecutive strings in four consecutive frets
this four fret arrangement facilitates the left hand technique for classical spanish guitar for each hand position of four frets the hand is stationary and the fingers move each finger being responsible for exactly one fret
consequently three hand positions covering frets and partition the fingerboard of classical guitar which has exactly frets only two or three frets are needed for the guitar chords major minor and dominant sevenths which are emphasized in introductions to guitar playing and to the fundamentals of music
each major and minor chord can be played on exactly two successive frets on exactly three successive strings and therefore each needs only two fingers
other chords seconds fourths sevenths and ninths are played on only three successive frets
advanced chords and harmony sequences of thirds and seconds the circle of fifths was discussed in the section on intermediate guitar chords
other progressions are also based on sequences of third intervals progressions are occasionally based on sequences of second intervals
extended chords as their categorical name suggests extended chords indeed extend seventh chords by stacking one or more additional third intervals successively constructing ninth eleventh and finally thirteenth chords thirteenth chords contain all seven notes of the diatonic scale
in closed position extended chords contain dissonant intervals or may sound supersaturated particularly thirteenth chords with their seven notes
consequently extended chords are often played with the omission of one or more tones especially the fifth and often the third as already noted for seventh chords similarly eleventh chords often omit the ninth and thirteenth chords the ninth or eleventh
often the third is raised an octave mimicking its position in the root sequence of harmonics dominant ninth chords were used by beethoven and eleventh chords appeared in impressionist music
thirteenth chords appeared in the twentieth century
extended chords appear in many musical genres including jazz funk rhythm and blues and progressive rock
chord guide for major and minor chords standard tuning read from left to right low to high major am xx bbm xx bm xx cm xx xx dm xx em fm xx gm xx minor am bm cm dm em fm gm alternative harmonies scales and modes conventional music uses diatonic harmony the major and minor keys and major and minor scales as sketched above
jazz guitarists must be fluent with jazz chords and also with many scales and modes of all the forms of music jazz demands the highest level of musicianship in terms of both theory and technique whole tone scales were used by king crimson for the title track on its red album of whole tone scales were also used by king crimson guitarist robert fripp on fractured
beyond tertian harmony in popular music chords are often extended also with added tones especially added sixths
quartal and quintal harmony chords are also systematically constructed by stacking not only thirds but also fourths and fifths supplementing tertian major minor harmony with quartal and quintal harmonies
quartal and quintal harmonies are used by guitarists who play jazz folk and rock music
quartal harmony has been used in jazz by guitarists such as jim hall especially on sonny rollins the bridge george benson skydive kenny burrell so what and wes montgomery little sunflower harmonies based on fourths and fifths also appear in folk guitar
on her debut album song to seagull joni mitchell used both quartal and quintal harmony in dawntreader and she used quintal harmony in seagull quartal and quintal harmonies also appear in alternate tunings
it is easier to finger the chords that are based on perfect fifths in new standard tuning than in standard tuning
new standard tuning was invented by robert fripp guitarist for king crimson
preferring to base chords on perfect intervals especially octaves fifths and fourths fripp often avoids minor thirds and especially major thirds which are sharp in equal temperament tuning in comparison to thirds in just intonation
alternative harmonies can also be generated by stacking second intervals major or minor
see also chord diagram guitar mel bay deluxe encyclopedia of guitar chords voice leading references footnotes citations bibliography further reading bay william
deluxe guitar chord encyclopedia case size edition
isbn kirkeby ole august
welcome to guitar version
archived from the original on april retrieved june patt ralph
berklee college of music professors at the department of guitar at the berklee college of music wrote the following books which like their colleagues chapman and willmott are berklee course textbooks goodrick mick
the advancing guitarist applying guitar concepts and techniques
hal leonard corp isbn goodrick mick
mr goodchord almanac of guitar voice leading name that chord
mr goodchord almanac of guitar voice leading for the year and beyond
isbn goodrick mick miller tim
creative chordal harmony for guitar using generic modality compression
berklee jazz guitar dictionary
berklee college of music
berklee rock guitar dictionary
berklee college of music
external links guitar at curlie guitar lessons at curlie
in statistics the logistic model or logit model is statistical model that models the probability of an event taking place by having the log odds for the event be linear combination of one or more independent variables 
in regression analysis logistic regression or logit regression is estimating the parameters of logistic model the coefficients in the linear combination 
formally in binary logistic regression there is single binary dependent variable coded by an indicator variable where the two values are labeled and while the independent variables can each be binary variable two classes coded by an indicator variable or continuous variable any real value 
the corresponding probability of the value labeled can vary between certainly the value and certainly the value hence the labeling the function that converts log odds to probability is the logistic function hence the name 
the unit of measurement for the log odds scale is called logit from logistic unit hence the alternative names 
see background and definition for formal mathematics and example for worked example 
binary variables are widely used in statistics to model the probability of certain class or event taking place such as the probability of team winning of patient being healthy etc 
see applications and the logistic model has been the most commonly used model for binary regression since about binary variables can be generalized to categorical variables when there are more than two possible values 
whether an image is of cat dog lion etc 
and the binary logistic regression generalized to multinomial logistic regression 
if the multiple categories are ordered one can use the ordinal logistic regression for example the proportional odds ordinal logistic model 
see extensions for further extensions 
the logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification it is not classifier though it can be used to make classifier for instance by choosing cutoff value and classifying inputs with probability greater than the cutoff as one class below the cutoff as the other this is common way to make binary classifier 
analogous linear models for binary variables with different sigmoid function instead of the logistic function to convert the linear combination to probability can also be used most notably the probit model see alternatives 
the defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at constant rate with each independent variable having its own parameter for binary dependent variable this generalizes the odds ratio 
more abstractly the logistic function is the natural parameter for the bernoulli distribution and in this sense is the simplest way to convert real number to probability 
in particular it maximizes entropy minimizes added information and in this sense makes the fewest assumptions of the data being modeled see maximum entropy 
the parameters of logistic regression are most commonly estimated by maximum likelihood estimation mle 
this does not have closed form expression unlike linear least squares see model fitting 
logistic regression by mle plays similarly basic role for binary or categorical responses as linear regression by ordinary least squares ols plays for scalar responses it is simple well analyzed baseline model see comparison with linear regression for discussion 
the logistic regression as general statistical model was originally developed and popularized primarily by joseph berkson beginning in berkson where he coined logit see history 
applications logistic regression is used in various fields including machine learning most medical fields and social sciences 
for example the trauma and injury severity score triss which is widely used to predict mortality in injured patients was originally developed by boyd et al 
many other medical scales used to assess severity of patient have been developed using logistic regression 
logistic regression may be used to predict the risk of developing given disease 
diabetes coronary heart disease based on observed characteristics of the patient age sex body mass index results of various blood tests etc 
another example might be to predict whether nepalese voter will vote nepali congress or communist party of nepal or any other party based on age income sex race state of residence votes in previous elections etc 
the technique can also be used in engineering especially for predicting the probability of failure of given process system or product 
it is also used in marketing applications such as prediction of customer propensity to purchase product or halt subscription etc 
in economics it can be used to predict the likelihood of person ending up in the labor force and business application would be to predict the likelihood of homeowner defaulting on mortgage 
conditional random fields an extension of logistic regression to sequential data are used in natural language processing 
example problem as simple example we can use logistic regression with one explanatory variable and two categories to answer the following question group of students spends between and hours studying for an exam 
how does the number of hours spent studying affect the probability of the student passing the exam 
the reason for using logistic regression for this problem is that the values of the dependent variable pass and fail while represented by and are not cardinal numbers 
if the problem was changed so that pass fail was replaced with the grade cardinal numbers then simple regression analysis could be used 
the table shows the number of hours each student spent studying and whether they passed or failed 
we wish to fit logistic function to the data consisting of the hours studied xk and the outcome of the test yk for pass for fail 
the data points are indexed by the subscript which runs from to the variable is called the explanatory variable and the variable is called the categorical variable consisting of two categories pass or fail corresponding to the categorical values and respectively 
model the logistic function is of the form where is location parameter the midpoint of the curve where and is scale parameter 
this expression may be rewritten as where and is known as the intercept it is the vertical intercept or intercept of the line and inverse scale parameter or rate parameter these are the intercept and slope of the log odds as function of conversely and 
fit the usual measure of goodness of fit for logistic regression uses logistic loss or log loss the negative log likelihood 
for given xk and yk write 
the are the probabilities that the corresponding will be unity and are the probabilities that they will be zero see bernoulli distribution 
we wish to find the values of and which give the best fit to the data 
in the case of linear regression the sum of the squared deviations of the fit from the data points yk the squared error loss is taken as measure of the goodness of fit and the best fit is obtained when that function is minimized 
the log loss for the th point is ln if ln if the log loss can be interpreted as the surprisal of the actual outcome relative to the prediction and is measure of information content 
note that log loss is always greater than or equal to equals only in case of perfect prediction when and or and and approaches infinity as the prediction gets worse when and or and meaning the actual outcome is more surprising 
since the value of the logistic function is always strictly between zero and one the log loss is always greater than zero and less than infinity 
note that unlike in linear regression where the model can have zero loss at point by passing through data point and zero loss overall if all points are on line in logistic regression it is not possible to have zero loss at any points since is either or but these can be combined into single expression ln ln 
this expression is more formally known as the cross entropy of the predicted distribution from the actual distribution as probability distributions on the two element space of pass fail 
the sum of these the total loss is the overall negative log likelihood and the best fit is obtained for those choices of and for which is minimized 
alternatively instead of minimizing the loss one can maximize its inverse the positive log likelihood ln ln ln ln or equivalently maximize the likelihood function itself which is the probability that the given data set is produced by particular logistic function this method is known as maximum likelihood estimation 
parameter estimation since is nonlinear in and determining their optimum values will require numerical methods 
note that one method of maximizing is to require the derivatives of with respect to and to be zero and the maximization procedure can be accomplished by solving the above two equations for and which again will generally require the use of numerical methods 
the values of and which maximize and using the above data are found to be which yields value for and of predictions the and coefficients may be entered into the logistic regression equation to estimate the probability of passing the exam 
for example for student who studies hours entering the value into the equation gives the estimated probability of passing the exam of probability of passing exam similarly for student who studies hours the estimated probability of passing the exam is probability of passing exam this table shows the estimated probability of passing the exam for several values of hours studying 
model evaluation the logistic regression analysis gives the following output 
by the wald test the output indicates that hours studying is significantly associated with the probability of passing the exam 
rather than the wald method the recommended method to calculate the value for logistic regression is the likelihood ratio test lrt which for this data gives see error and significance of the fit below 
generalizations this simple model is an example of binary logistic regression and has one explanatory variable and binary categorical variable which can assume one of two categorical values 
multinomial logistic regression is the generalization of binary logistic regression to include any number of explanatory variables and any number of categories 
background definition of the logistic function an explanation of logistic regression can begin with an explanation of the standard logistic function 
the logistic function is sigmoid function which takes any real input and outputs value between zero and one 
for the logit this is interpreted as taking input log odds and having output probability 
the standard logistic function is defined as follows graph of the logistic function on the interval is shown in figure let us assume that is linear function of single explanatory variable the case where is linear combination of multiple explanatory variables is treated similarly 
we can then express as follows and the general logistic function can now be written as in the logistic model is interpreted as the probability of the dependent variable equaling success case rather than failure non case 
it clear that the response variables are not identically distributed differs from one data point to another though they are independent given design matrix and shared parameters 
definition of the inverse of the logistic function we can now define the logit log odds function as the inverse of the standard logistic function 
it is easy to see that it satisfies logit ln and equivalently after exponentiating both sides we have the odds 
interpretation of these terms in the above equations the terms are as follows is the logit function 
the equation for illustrates that the logit log odds or natural logarithm of the odds is equivalent to the linear regression expression 
ln denotes the natural logarithm 
is the probability that the dependent variable equals case given some linear combination of the predictors 
the formula for illustrates that the probability of the dependent variable equaling case is equal to the value of the logistic function of the linear regression expression 
this is important in that it shows that the value of the linear regression expression can vary from negative to positive infinity and yet after transformation the resulting expression for the probability ranges between and is the intercept from the linear regression equation the value of the criterion when the predictor is equal to zero 
is the regression coefficient multiplied by some value of the predictor 
base denotes the exponential function 
definition of the odds the odds of the dependent variable equaling case given some linear combination of the predictors is equivalent to the exponential function of the linear regression expression 
this illustrates how the logit serves as link function between the probability and the linear regression expression 
given that the logit ranges between negative and positive infinity it provides an adequate criterion upon which to conduct linear regression and the logit is easily converted back into the odds so we define odds of the dependent variable equaling case given some linear combination of the predictors as follows odds 
the odds ratio for continuous independent variable the odds ratio can be defined as odds odds this exponential relationship provides an interpretation for the odds multiply by for every unit increase in for binary independent variable the odds ratio is defined as where and are cells in contingency table 
multiple explanatory variables if there are multiple explanatory variables the above expression can be revised to then when this is used in the equation relating the log odds of success to the values of the predictors the linear regression will be multiple regression with explanators the parameters for all are all estimated 
again the more traditional equations are log and where usually definition the basic setup of logistic regression is as follows 
we are given dataset containing points 
each point consists of set of input variables xm also called independent variables explanatory variables predictor variables features or attributes and binary outcome variable yi also known as dependent variable response variable output variable or class 
it can assume only the two possible values often meaning no or failure or often meaning yes or success 
the goal of logistic regression is to use the dataset to create predictive model of the outcome variable 
as in linear regression the outcome variables yi are assumed to depend on the explanatory variables xm 
explanatory variablesthe explanatory variables may be of any type real valued binary categorical etc 
the main distinction is between continuous variables and discrete variables 
discrete variables referring to more than two possible choices are typically coded using dummy variables or indicator variables that is separate explanatory variables taking the value or are created for each possible value of the discrete variable with meaning variable does have the given value and meaning variable does not have that value 
outcome variablesformally the outcomes yi are described as being bernoulli distributed data where each outcome is determined by an unobserved probability pi that is specific to the outcome at hand but related to the explanatory variables 
this can be expressed in any of the following equivalent forms bernoulli pr if if pr the meanings of these four lines are the first line expresses the probability distribution of each yi conditioned on the explanatory variables it follows bernoulli distribution with parameters pi the probability of the outcome of for trial as noted above each separate trial has its own probability of success just as each trial has its own explanatory variables 
the probability of success pi is not observed only the outcome of an individual bernoulli trial using that probability 
the second line expresses the fact that the expected value of each yi is equal to the probability of success pi which is general property of the bernoulli distribution 
in other words if we run large number of bernoulli trials using the same probability of success pi then take the average of all the and outcomes then the result would be close to pi 
this is because doing an average this way simply computes the proportion of successes seen which we expect to converge to the underlying probability of success 
the third line writes out the probability mass function of the bernoulli distribution specifying the probability of seeing each of the two possible outcomes 
the fourth line is another way of writing the probability mass function which avoids having to write separate cases and is more convenient for certain types of calculations 
this relies on the fact that yi can take only the value or in each case one of the exponents will be choosing the value under it while the other is canceling out the value under it 
hence the outcome is either pi or pi as in the previous line linear predictor functionthe basic idea of logistic regression is to use the mechanism already developed for linear regression by modeling the probability pi using linear predictor function 
linear combination of the explanatory variables and set of regression coefficients that are specific to the model at hand but the same for all trials 
the linear predictor function for particular data point is written as where are regression coefficients indicating the relative effect of particular explanatory variable on the outcome 
the model is usually put into more compact form as follows the regression coefficients are grouped into single vector of size for each data point an additional explanatory pseudo variable is added with fixed value of corresponding to the intercept coefficient 
the resulting explanatory variables xm are then grouped into single vector xi of size this makes it possible to write the linear predictor function as follows using the notation for dot product between two vectors 
many explanatory variables two categories the above example of binary logistic regression on one explanatory variable can be generalized to binary logistic regression on any number of explanatory variables and any number of categorical values 
to begin with we may consider logistic model with explanatory variables xm and as in the example above two categorical values and 
for the simple binary logistic regression model we assumed linear relationship between the predictor variable and the log odds also called logit of the event that this linear relationship may be extended to the case of explanatory variables log where is the log odds and are parameters of the model 
an additional generalization has been introduced in which the base of the model is not restricted to the euler number in most applications the base of the logarithm is usually taken to be however in some cases it can be easier to communicate results by working in base or base for more compact notation we will specify the explanatory variables and the coefficients as dimensional vectors with an added explanatory variable 
the logit may now be written as solving for the probability that yields where is the sigmoid function with base the above formula shows that once the are fixed we can easily compute either the log odds that for given observation or the probability that for given observation 
the main use case of logistic model is to be given an observation and estimate the probability that the optimum beta coefficients may again be found by maximizing the log likelihood 
for measurements defining xk as the explanatory vector of the th measurement and yk as the categorical outcome of that measurement the log likelihood may be written in form very similar to the simple case above log log as in the simple example above finding the optimum parameters will require numerical methods 
one useful technique is to equate the derivatives of the log likelihood with respect to each of the parameters to zero yielding set of equations which will hold at the maximum of the log likelihood where xmk is the value of the xm explanatory variable from the th measurement 
consider an example with explanatory variables and coefficients and which have been determined by the above method 
to be concrete the model is log where is the probability of the event that this can be interpreted as follows is the intercept 
it is the log odds of the event that when the predictors by exponentiating we can see that when the odds of the event that are to or similarly the probability of the event that when can be computed as means that increasing by increases the log odds by so if increases by the odds that increase by factor of note that the probability of has also increased but it has not increased by as much as the odds have increased 
means that increasing by increases the log odds by so if increases by the odds that increase by factor of note how the effect of on the log odds is twice as great as the effect of but the effect on the odds is times greater 
but the effect on the probability of is not as much as times greater it only the effect on the odds that is times greater 
multinomial logistic regression many explanatory variables and many categories in the above cases of two categories binomial logistic regression the categories were indexed by and and we had two probability distributions the probability that the outcome was in category was given by and the probability that the outcome was in category was given by 
the sum of both probabilities is equal to unity as they must be 
in general if we have explanatory variables including and categories we will need separate probability distributions one for each category indexed by which describe the probability that the categorical outcome for explanatory vector will be in category 
it will also be required that the sum of these probabilities over all categories be equal to unity 
using the mathematically convenient base these probabilities are for each of the probabilities except will have their own set of regression coefficients it can be seen that as required the sum of the over all categories is unity 
note that the selection of to be defined in terms of the other probabilities is artificial 
any of the probabilities could have been selected to be so defined 
this special value of is termed the pivot index and the log odds tn are expressed in terms of the pivot probability and are again expressed as linear combination of the explanatory variables ln note also that for the simple case of the two category case is recovered with and 
the log likelihood that particular set of measurements or data points will be generated by the above probabilities can now be calculated 
indexing each measurement by let the th set of measured explanatory variables be denoted by and their categorical outcomes be denoted by which can be equal to any integer in 
the log likelihood is then ln where is an indicator function which is equal to unity if yk and zero otherwise 
in the case of two explanatory variables this indicator function was defined as yk when and yk when this was convenient but not necessary 
again the optimum beta coefficients may be found by maximizing the log likelihood function generally using numerical methods 
possible method of solution is to set the derivatives of the log likelihood with respect to each beta coefficient equal to zero and solve for the beta coefficients where is the th coefficient of the vector and is the th explanatory variable of the th measurement 
once the beta coefficients have been estimated from the data we will be able to estimate the probability that any subsequent set of explanatory variables will result in any of the possible outcome categories 
interpretations there are various equivalent specifications and interpretations of logistic regression which fit into different types of more general models and allow different generalizations 
as generalized linear model the particular model used by logistic regression which distinguishes it from standard linear regression and from other types of regression analysis used for binary valued outcomes is the way the probability of particular outcome is linked to the linear predictor function logit logit ln written using the more compact notation described above this is logit logit ln this formulation expresses logistic regression as type of generalized linear model which predicts variables with various types of probability distributions by fitting linear predictor function of the above form to some sort of arbitrary transformation of the expected value of the variable 
the intuition for transforming using the logit function the natural log of the odds was explained above 
it also has the practical effect of converting the probability which is bounded to be between and to variable that ranges over thereby matching the potential range of the linear prediction function on the right side of the equation 
note that both the probabilities pi and the regression coefficients are unobserved and the means of determining them is not part of the model itself 
they are typically determined by some sort of optimization procedure 
maximum likelihood estimation that finds values that best fit the observed data 
that give the most accurate predictions for the data already observed usually subject to regularization conditions that seek to exclude unlikely values 
extremely large values for any of the regression coefficients 
the use of regularization condition is equivalent to doing maximum posteriori map estimation an extension of maximum likelihood 
regularization is most commonly done using squared regularizing function which is equivalent to placing zero mean gaussian prior distribution on the coefficients but other regularizers are also possible 
whether or not regularization is used it is usually not possible to find closed form solution instead an iterative numerical method must be used such as iteratively reweighted least squares irls or more commonly these days quasi newton method such as the bfgs method the interpretation of the parameter estimates is as the additive effect on the log of the odds for unit change in the the explanatory variable 
in the case of dichotomous explanatory variable for instance gender is the estimate of the odds of having the outcome for say males compared with females 
an equivalent formula uses the inverse of the logit function which is the logistic function 
logit the formula can also be written as probability distribution specifically using probability mass function pr as latent variable model the logistic model has an equivalent formulation as latent variable model 
this formulation is common in the theory of discrete choice models and makes it easier to extend to certain more complicated models with multiple correlated choices as well as to compare logistic regression to the closely related probit model 
imagine that for each trial there is continuous latent variable yi 
an unobserved random variable that is distributed as follows where logistic 
the latent variable can be written directly in terms of the linear predictor function and an additive random error variable that is distributed according to standard logistic distribution 
then yi can be viewed as an indicator for whether this latent variable is positive if 
the choice of modeling the error variable specifically with standard logistic distribution rather than general logistic distribution with the location and scale set to arbitrary values seems restrictive but in fact it is not 
it must be kept in mind that we can choose the regression coefficients ourselves and very often can use them to offset changes in the parameters of the error variable distribution 
for example logistic error variable distribution with non zero location parameter which sets the mean is equivalent to distribution with zero location parameter where has been added to the intercept coefficient 
both situations produce the same value for yi regardless of settings of explanatory variables 
similarly an arbitrary scale parameter is equivalent to setting the scale parameter to and then dividing all regression coefficients by in the latter case the resulting value of yi will be smaller by factor of than in the former case for all sets of explanatory variables but critically it will always remain on the same side of and hence lead to the same yi choice 
note that this predicts that the irrelevancy of the scale parameter may not carry over into more complex models where more than two choices are available 
it turns out that this formulation is exactly equivalent to the preceding one phrased in terms of the generalized linear model and without any latent variables 
this can be shown as follows using the fact that the cumulative distribution function cdf of the standard logistic distribution is the logistic function which is the inverse of the logit function 
pr logit then pr pr pr pr pr because the logistic distribution is symmetric logit see above this formulation which is standard in discrete choice models makes clear the relationship between logistic regression the logit model and the probit model which uses an error variable distributed according to standard normal distribution instead of standard logistic distribution 
both the logistic and normal distributions are symmetric with basic unimodal bell curve shape 
the only difference is that the logistic distribution has somewhat heavier tails which means that it is less sensitive to outlying data and hence somewhat more robust to model mis specifications or erroneous data 
two way latent variable model yet another formulation uses two separate latent variables where ev ev where ev is standard type extreme value distribution 
pr pr then if otherwise 
this model has separate latent variable and separate set of regression coefficients for each possible outcome of the dependent variable 
the reason for this separation is that it makes it easy to extend logistic regression to multi outcome categorical variables as in the multinomial logit model 
in such model it is natural to model each possible outcome using different set of regression coefficients 
it is also possible to motivate each of the separate latent variables as the theoretical utility associated with making the associated choice and thus motivate logistic regression in terms of utility theory 
in terms of utility theory rational actor always chooses the choice with the greatest associated utility 
this is the approach taken by economists when formulating discrete choice models because it both provides theoretically strong foundation and facilitates intuitions about the model which in turn makes it easy to consider various sorts of extensions 
see the example below 
the choice of the type extreme value distribution seems fairly arbitrary but it makes the mathematics work out and it may be possible to justify its use through rational choice theory 
it turns out that this model is equivalent to the previous model although this seems non obvious since there are now two sets of regression coefficients and error variables and the error variables have different distribution 
in fact this model reduces directly to the previous one with the following substitutions an intuition for this comes from the fact that since we choose based on the maximum of two values only their difference matters not the exact values and this effectively removes one degree of freedom 
another critical fact is that the difference of two type extreme value distributed variables is logistic distribution 
we can demonstrate the equivalent as follows pr pr pr pr pr pr pr substitute as above pr substitute as above pr now same as above model pr logit example as an example consider province level election where the choice is between right of center party left of center party and secessionist party 
the parti qu cois which wants quebec to secede from canada 
we would then use three latent variables one for each choice 
then in accordance with utility theory we can then interpret the latent variables as expressing the utility that results from making each of the choices 
we can also interpret the regression coefficients as indicating the strength that the associated factor 
explanatory variable has in contributing to the utility or more correctly the amount by which unit change in an explanatory variable changes the utility of given choice 
voter might expect that the right of center party would lower taxes especially on rich people 
this would give low income people no benefit 
no change in utility since they usually don pay taxes would cause moderate benefit 
somewhat more money or moderate utility increase for middle incoming people would cause significant benefits for high income people 
on the other hand the left of center party might be expected to raise taxes and offset it with increased welfare and other assistance for the lower and middle classes 
this would cause significant positive benefit to low income people perhaps weak benefit to middle income people and significant negative benefit to high income people 
finally the secessionist party would take no direct actions on the economy but simply secede 
low income or middle income voter might expect basically no clear utility gain or loss from this but high income voter might expect negative utility since he she is likely to own companies which will have harder time doing business in such an environment and probably lose money 
these intuitions can be expressed as follows this clearly shows that separate sets of regression coefficients need to exist for each choice 
when phrased in terms of utility this can be seen very easily 
different choices have different effects on net utility furthermore the effects vary in complex ways that depend on the characteristics of each individual so there need to be separate sets of coefficients for each characteristic not simply single extra per choice characteristic 
even though income is continuous variable its effect on utility is too complex for it to be treated as single variable 
either it needs to be directly split up into ranges or higher powers of income need to be added so that polynomial regression on income is effectively done 
as log linear model yet another formulation combines the two way latent variable formulation above with the original formulation higher up without latent variables and in the process provides link to one of the standard formulations of the multinomial logit 
here instead of writing the logit of the probabilities pi as linear predictor we separate the linear predictor into two one for each of the two outcomes ln pr ln ln pr ln two separate sets of regression coefficients have been introduced just as in the two way latent variable model and the two equations appear form that writes the logarithm of the associated probability as linear predictor with an extra term ln at the end 
this term as it turns out serves as the normalizing factor ensuring that the result is distribution 
this can be seen by exponentiating both sides pr pr in this form it is clear that the purpose of is to ensure that the resulting distribution over yi is in fact probability distribution 
it sums to this means that is simply the sum of all un normalized probabilities and by dividing each probability by the probabilities become normalized 
that is and the resulting equations are pr pr or generally pr this shows clearly how to generalize this formulation to more than two outcomes as in multinomial logit 
note that this general formulation is exactly the softmax function as in pr softmax 
in order to prove that this is equivalent to the previous model note that the above model is overspecified in that pr and pr cannot be independently specified rather pr pr so knowing one automatically determines the other 
as result the model is nonidentifiable in that multiple combinations of and will produce the same probabilities for all possible explanatory variables 
in fact it can be seen that adding any constant vector to both of them will produce the same probabilities pr as result we can simplify matters and restore identifiability by picking an arbitrary value for one of the two vectors 
we choose to set then and so pr which shows that this formulation is indeed equivalent to the previous formulation 
as in the two way latent variable formulation any settings where will produce equivalent results 
note that most treatments of the multinomial logit model start out either by extending the log linear formulation presented here or the two way latent variable formulation presented above since both clearly show the way that the model could be extended to multi way outcomes 
in general the presentation with latent variables is more common in econometrics and political science where discrete choice models and utility theory reign while the log linear formulation here is more common in computer science 
machine learning and natural language processing 
as single layer perceptron the model has an equivalent formulation 
this functional form is commonly called single layer perceptron or single layer artificial neural network 
single layer neural network computes continuous output instead of step function 
the derivative of pi with respect to xk is computed from the general form where is an analytic function in with this choice the single layer neural network is identical to the logistic regression model 
this function has continuous derivative which allows it to be used in backpropagation 
this function is also preferred because its derivative is easily calculated 
in terms of binomial data closely related model assumes that each is associated not with single bernoulli trial but with ni independent identically distributed trials where the observation yi is the number of successes observed the sum of the individual bernoulli distributed random variables and hence follows binomial distribution bin for an example of this distribution is the fraction of seeds pi that germinate after ni are planted 
in terms of expected values this model is expressed as follows so that logit logit ln or equivalently pr this model can be fit using the same sorts of methods as the above more basic model 
model fitting maximum likelihood estimation mle the regression coefficients are usually estimated using maximum likelihood estimation 
unlike linear regression with normally distributed residuals it is not possible to find closed form expression for the coefficient values that maximize the likelihood function so that an iterative process must be used instead for example newton method 
this process begins with tentative solution revises it slightly to see if it can be improved and repeats this revision until no more improvement is made at which point the process is said to have converged in some instances the model may not reach convergence 
non convergence of model indicates that the coefficients are not meaningful because the iterative process was unable to find appropriate solutions 
failure to converge may occur for number of reasons having large ratio of predictors to cases multicollinearity sparseness or complete separation 
having large ratio of variables to cases results in an overly conservative wald statistic discussed below and can lead to non convergence 
regularized logistic regression is specifically intended to be used in this situation 
multicollinearity refers to unacceptably high correlations between predictors 
as multicollinearity increases coefficients remain unbiased but standard errors increase and the likelihood of model convergence decreases 
to detect multicollinearity amongst the predictors one can conduct linear regression analysis with the predictors of interest for the sole purpose of examining the tolerance statistic used to assess whether multicollinearity is unacceptably high 
sparseness in the data refers to having large proportion of empty cells cells with zero counts 
zero cell counts are particularly problematic with categorical predictors 
with continuous predictors the model can infer values for the zero cell counts but this is not the case with categorical predictors 
the model will not converge with zero cell counts for categorical predictors because the natural logarithm of zero is an undefined value so that the final solution to the model cannot be reached 
to remedy this problem researchers may collapse categories in theoretically meaningful way or add constant to all cells 
another numerical problem that may lead to lack of convergence is complete separation which refers to the instance in which the predictors perfectly predict the criterion all cases are accurately classified and the likelihood maximized with infinite coefficients 
in such instances one should re examine the data as there may be some kind of error 
one can also take semi parametric or non parametric approaches via local likelihood or nonparametric quasi likelihood methods which avoid assumptions of parametric form for the index function and is robust to the choice of the link function probit or logit 
iteratively reweighted least squares irls binary logistic regression or can for example be calculated using iteratively reweighted least squares irls which is equivalent to maximizing the log likelihood of bernoulli distributed process using newton method 
if the problem is written in vector matrix form with parameters explanatory variables and expected value of the bernoulli distribution the parameters can be found using the following iterative algorithm where diag is diagonal weighting matrix the vector of expected values the regressor matrix and the vector of response variables 
more details can be found in the literature 
bayesian in bayesian statistics context prior distributions are normally placed on the regression coefficients for example in the form of gaussian distributions 
there is no conjugate prior of the likelihood function in logistic regression 
when bayesian inference was performed analytically this made the posterior distribution difficult to calculate except in very low dimensions 
now though automatic software such as openbugs jags pymc stan or turing jl allows these posteriors to be computed using simulation so lack of conjugacy is not concern 
however when the sample size or the number of parameters is large full bayesian simulation can be slow and people often use approximate methods such as variational bayesian methods and expectation propagation 
rule of ten widely used rule of thumb the one in ten rule states that logistic regression models give stable values for the explanatory variables if based on minimum of about events per explanatory variable epv where event denotes the cases belonging to the less frequent category in the dependent variable 
thus study designed to use explanatory variables for an event 
myocardial infarction expected to occur in proportion of participants in the study will require total of participants 
however there is considerable debate about the reliability of this rule which is based on simulation studies and lacks secure theoretical underpinning 
according to some authors the rule is overly conservative in some circumstances with the authors stating if we somewhat subjectively regard confidence interval coverage less than percent type error greater than percent or relative bias greater than percent as problematic our results indicate that problems are fairly frequent with epv uncommon with epv and still observed with epv 
the worst instances of each problem were not severe with epv and usually comparable to those with epv others have found results that are not consistent with the above using different criteria 
useful criterion is whether the fitted model will be expected to achieve the same predictive discrimination in new sample as it appeared to achieve in the model development sample 
for that criterion events per candidate variable may be required 
also one can argue that observations are needed only to estimate the model intercept precisely enough that the margin of error in predicted probabilities is with confidence level 
error and significance of fit deviance and likelihood ratio test simple case in any fitting procedure the addition of another fitting parameter to model 
the beta parameters in logistic regression model will almost always improve the ability of the model to predict the measured outcomes 
this will be true even if the additional term has no predictive value since the model will simply be overfitting to the noise in the data 
the question arises as to whether the improvement gained by the addition of another fitting parameter is significant enough to recommend the inclusion of the additional term or whether the improvement is simply that which may be expected from overfitting 
in short for logistic regression statistic known as the deviance is defined which is measure of the error between the logistic model fit and the outcome data 
in the limit of large number of data points the deviance is chi squared distributed which allows chi squared test to be implemented in order to determine the significance of the explanatory variables 
linear regression and logistic regression have many similarities 
for example in simple linear regression set of data points xk yk are fitted to proposed model function of the form the fit is obtained by choosing the parameters which minimize the sum of the squares of the residuals the squared error term for each data point the minimum value which constitutes the fit will be denoted by the idea of null model may be introduced in which it is assumed that the variable is of no use in predicting the yk outcomes the data points are fitted to null model function of the form with squared error term the fitting process consists of choosing value of which minimizes of the fit to the null model denoted by where the subscript denotes the null model 
it is seen that the null model is optimized by where is the mean of the yk values and the optimized is which is proportional to the square of the uncorrected sample standard deviation of the yk data points 
we can imagine case where the yk data points are randomly assigned to the various xk and then fitted using the proposed model 
specifically we can consider the fits of the proposed model to every permutation of the yk outcomes 
it can be shown that the optimized error of any of these fits will never be less than the optimum error of the null model and that the difference between these minimum error will follow chi squared distribution distribution with degrees of freedom equal those of the proposed model minus those of the null model which in this case will be 
using the chi squared test we may then estimate how many of these permuted sets of yk will yield an minimum error less than or equal to the minimum error using the original yk and so we can estimate how significant an improvement is given by the inclusion of the variable in the proposed model 
for logistic regression the measure of goodness of fit is the likelihood function or its logarithm the log likelihood the likelihood function is analogous to the in the linear regression case except that the likelihood is maximized rather than minimized 
denote the maximized log likelihood of the proposed model by 
in the case of simple binary logistic regression the set of data points are fitted in probabilistic sense to function of the form where is the probability that the log odds are given by and the log likelihood is ln ln for the null model the probability that is given by the log odds for the null model are given by and the log likelihood is ln ln since we have at the maximum of the maximum log likelihood for the null model is ln ln the optimum is ln where is again the mean of the yk values 
again we can conceptually consider the fit of the proposed model to every permutation of the yk and it can be shown that the maximum log likelihood of these permutation fits will never be smaller than that of the null model also as an analog to the error of the linear regression case we may define the deviance of logistic regression fit as ln which will always be positive or zero 
the reason for this choice is that not only is the deviance good measure of the goodness of fit it is also approximately chi squared distributed with the approximation improving as the number of data points increases becoming exactly chi square distributed in the limit of an infinite number of data points 
as in the case of linear regression we may use this fact to estimate the probability that random set of data points will give better fit than the fit obtained by the proposed model and so have an estimate how significantly the model is improved by including the xk data points in the proposed model 
for the simple model of student test scores described above the maximum value of the log likelihood of the null model is the maximum value of the log likelihood for the simple model is so that the deviance is using the chi squared test of significance the integral of the chi squared distribution with one degree of freedom from to infinity is equal to this effectively means that about out of fits to random yk can be expected to have better fit smaller deviance than the given yk and so we can conclude that the inclusion of the variable and data in the proposed model is very significant improvement over the null model 
in other words we reject the null hypothesis with confidence 
goodness of fit summary goodness of fit in linear regression models is generally measured using 
since this has no direct analog in logistic regression various methods ch including the following can be used instead 
deviance and likelihood ratio tests in linear regression analysis one is concerned with partitioning variance via the sum of squares calculations variance in the criterion is essentially divided into variance accounted for by the predictors and residual variance 
in logistic regression analysis deviance is used in lieu of sum of squares calculations 
deviance is analogous to the sum of squares calculations in linear regression and is measure of the lack of fit to the data in logistic regression model 
when saturated model is available model with theoretically perfect fit deviance is calculated by comparing given model with the saturated model 
this computation gives the likelihood ratio test ln likelihood of the fitted model likelihood of the saturated model 
in the above equation represents the deviance and ln represents the natural logarithm 
the log of this likelihood ratio the ratio of the fitted model to the saturated model will produce negative value hence the need for negative sign 
can be shown to follow an approximate chi squared distribution 
smaller values indicate better fit as the fitted model deviates less from the saturated model 
when assessed upon chi square distribution nonsignificant chi square values indicate very little unexplained variance and thus good model fit 
conversely significant chi square value indicates that significant amount of the variance is unexplained 
when the saturated model is not available common case deviance is calculated simply as log likelihood of the fitted model and the reference to the saturated model log likelihood can be removed from all that follows without harm 
two measures of deviance are particularly important in logistic regression null deviance and model deviance 
the null deviance represents the difference between model with only the intercept which means no predictors and the saturated model 
the model deviance represents the difference between model with at least one predictor and the saturated model 
in this respect the null model provides baseline upon which to compare predictor models 
given that deviance is measure of the difference between given model and the saturated model smaller values indicate better fit 
thus to assess the contribution of predictor or set of predictors one can subtract the model deviance from the null deviance and assess the difference on chi square distribution with degrees of freedom equal to the difference in the number of parameters estimated 
let null ln likelihood of null model likelihood of the saturated model fitted ln likelihood of fitted model likelihood of the saturated model 
then the difference of both is null fitted ln likelihood of null model likelihood of the saturated model ln likelihood of fitted model likelihood of the saturated model ln likelihood of null model likelihood of the saturated model likelihood of fitted model likelihood of the saturated model ln likelihood of the null model likelihood of fitted model 
if the model deviance is significantly smaller than the null deviance then one can conclude that the predictor or set of predictors significantly improve the model fit 
this is analogous to the test used in linear regression analysis to assess the significance of prediction 
pseudo squared in linear regression the squared multiple correlation is used to assess goodness of fit as it represents the proportion of variance in the criterion that is explained by the predictors 
in logistic regression analysis there is no agreed upon analogous measure but there are several competing measures each with limitations four of the most commonly used indices and one less commonly used one are examined on this page likelihood ratio cox and snell cs nagelkerke mcfadden mcf tjur hosmer lemeshow test the hosmer lemeshow test uses test statistic that asymptotically follows distribution to assess whether or not the observed event rates match expected event rates in subgroups of the model population 
this test is considered to be obsolete by some statisticians because of its dependence on arbitrary binning of predicted probabilities and relative low power 
coefficient significance after fitting the model it is likely that researchers will want to examine the contribution of individual predictors 
to do so they will want to examine the regression coefficients 
in linear regression the regression coefficients represent the change in the criterion for each unit change in the predictor 
in logistic regression however the regression coefficients represent the change in the logit for each unit change in the predictor 
given that the logit is not intuitive researchers are likely to focus on predictor effect on the exponential function of the regression coefficient the odds ratio see definition 
in linear regression the significance of regression coefficient is assessed by computing test 
in logistic regression there are several different tests designed to assess the significance of an individual predictor most notably the likelihood ratio test and the wald statistic 
likelihood ratio test the likelihood ratio test discussed above to assess model fit is also the recommended procedure to assess the contribution of individual predictors to given model 
in the case of single predictor model one simply compares the deviance of the predictor model with that of the null model on chi square distribution with single degree of freedom 
if the predictor model has significantly smaller deviance 
chi square using the difference in degrees of freedom of the two models then one can conclude that there is significant association between the predictor and the outcome 
although some common statistical packages 
spss do provide likelihood ratio test statistics without this computationally intensive test it would be more difficult to assess the contribution of individual predictors in the multiple logistic regression case 
to assess the contribution of individual predictors one can enter the predictors hierarchically comparing each new model with the previous to determine the contribution of each predictor 
there is some debate among statisticians about the appropriateness of so called stepwise procedures 
the fear is that they may not preserve nominal statistical properties and may become misleading 
wald statistic alternatively when assessing the contribution of individual predictors in given model one may examine the significance of the wald statistic 
the wald statistic analogous to the test in linear regression is used to assess the significance of coefficients 
the wald statistic is the ratio of the square of the regression coefficient to the square of the standard error of the coefficient and is asymptotically distributed as chi square distribution 
although several statistical packages spss sas report the wald statistic to assess the contribution of individual predictors the wald statistic has limitations 
when the regression coefficient is large the standard error of the regression coefficient also tends to be larger increasing the probability of type ii error 
the wald statistic also tends to be biased when data are sparse 
case control sampling suppose cases are rare 
then we might wish to sample them more frequently than their prevalence in the population 
for example suppose there is disease that affects person in and to collect our data we need to do complete physical 
it may be too expensive to do thousands of physicals of healthy people in order to obtain data for only few diseased individuals 
thus we may evaluate more diseased individuals perhaps all of the rare outcomes 
this is also retrospective sampling or equivalently it is called unbalanced data 
as rule of thumb sampling controls at rate of five times the number of cases will produce sufficient control data logistic regression is unique in that it may be estimated on unbalanced data rather than randomly sampled data and still yield correct coefficient estimates of the effects of each independent variable on the outcome 
that is to say if we form logistic model from such data if the model is correct in the general population the parameters are all correct except for we can correct if we know the true prevalence as follows log log where is the true prevalence and is the prevalence in the sample 
discussion like other forms of regression analysis logistic regression makes use of one or more predictor variables that may be either continuous or categorical 
unlike ordinary linear regression however logistic regression is used for predicting dependent variables that take membership in one of limited number of categories treating the dependent variable in the binomial case as the outcome of bernoulli trial rather than continuous outcome 
given this difference the assumptions of linear regression are violated 
in particular the residuals cannot be normally distributed 
in addition linear regression may make nonsensical predictions for binary dependent variable 
what is needed is way to convert binary variable into continuous one that can take on any real value negative or positive 
to do that binomial logistic regression first calculates the odds of the event happening for different levels of each independent variable and then takes its logarithm to create continuous criterion as transformed version of the dependent variable 
the logarithm of the odds is the logit of the probability the logit is defined as follows although the dependent variable in logistic regression is bernoulli the logit is on an unrestricted scale 
the logit function is the link function in this kind of generalized linear model 
is the bernoulli distributed response variable and is the predictor variable the values are the linear parameters 
the logit of the probability of success is then fitted to the predictors 
the predicted value of the logit is converted back into predicted odds via the inverse of the natural logarithm the exponential function 
thus although the observed dependent variable in binary logistic regression is or variable the logistic regression estimates the odds as continuous variable that the dependent variable is success 
in some applications the odds are all that is needed 
in others specific yes or no prediction is needed for whether the dependent variable is or is not success this categorical prediction can be based on the computed odds of success with predicted odds above some chosen cutoff value being translated into prediction of success 
maximum entropy of all the functional forms used for estimating the probabilities of particular categorical outcome which optimize the fit by maximizing the likelihood function 
probit regression poisson regression etc 
the logistic regression solution is unique in that it is maximum entropy solution 
this is case of general property an exponential family of distributions maximizes entropy given an expected value 
in the case of the logistic model the logistic function is the natural parameter of the bernoulli distribution it is in canonical form and the logistic function is the canonical link function while other sigmoid functions are non canonical link functions this underlies its mathematical elegance and ease of optimization 
see exponential family maximum entropy derivation for details 
proof in order to show this we use the method of lagrange multipliers 
the lagrangian is equal to the entropy plus the sum of the products of lagrange multipliers times various constraint expressions 
the general multinomial case will be considered since the proof is not made that much simpler by considering simpler cases 
equating the derivative of the lagrangian with respect to the various probabilities to zero yields functional form for those probabilities which corresponds to those used in logistic regression as in the above section on multinomial logistic regression we will consider explanatory variables denoted and which include there will be total of data points indexed by and the data points are given by and the xmk will also be represented as an dimensional vector 
there will be possible values of the categorical variable ranging from to let pn be the probability given explanatory variable vector that the outcome will be define which is the probability that for the th measurement the categorical outcome is the lagrangian will be expressed as function of the probabilities pnk and will minimized by equating the derivatives of the lagrangian with respect to these probabilities to zero 
an important point is that the probabilities are treated equally and the fact that they sum to unity is part of the lagrangian formulation rather than being assumed from the beginning 
the first contribution to the lagrangian is the entropy ln the log likelihood is ln assuming the multinomial logistic function the derivative of the log likelihood with respect the beta coefficients was found to be very important point here is that this expression is remarkably not an explicit function of the beta coefficients 
it is only function of the probabilities pnk and the data 
rather than being specific to the assumed multinomial logistic case it is taken to be general statement of the condition at which the log likelihood is maximized and makes no reference to the functional form of pnk 
there are then fitting constraints and the fitting constraint term in the lagrangian is then where the nm are the appropriate lagrange multipliers 
there are normalization constraints which may be written so that the normalization term in the lagrangian is where the are the appropriate lagrange multipliers 
the lagrangian is then the sum of the above three terms setting the derivative of the lagrangian with respect to one of the probabilities to zero yields ln using the more condensed vector notation and dropping the primes on the and indices and then solving for yields where imposing the normalization constraint we can solve for the zk and write the probabilities as the are not all independent 
we can add any constant dimensional vector to each of the without changing the value of the probabilities so that there are only rather than independent in the multinomial logistic regression section above the was subtracted from each which set the exponential term involving to unity and the beta coefficients were given by 
other approaches in machine learning applications where logistic regression is used for binary classification the mle minimises the cross entropy loss function 
logistic regression is an important machine learning algorithm 
the goal is to model the probability of random variable being or given experimental data consider generalized linear model function parameterized by pr therefore pr and since we see that pr is given by pr 
we now calculate the likelihood function assuming that all the observations in the sample are independently bernoulli distributed pr pr typically the log likelihood is maximized log log pr which is maximized using optimization techniques such as gradient descent 
assuming the pairs are drawn uniformly from the underlying distribution then in the limit of large lim log pr pr log pr pr log pr pr log pr kl where is the conditional entropy and kl is the kullback leibler divergence 
this leads to the intuition that by maximizing the log likelihood of model you are minimizing the kl divergence of your model from the maximal entropy distribution 
intuitively searching for the model that makes the fewest assumptions in its parameters 
comparison with linear regression logistic regression can be seen as special case of the generalized linear model and thus analogous to linear regression 
the model of logistic regression however is based on quite different assumptions about the relationship between the dependent and independent variables from those of linear regression 
in particular the key differences between these two models can be seen in the following two features of logistic regression 
first the conditional distribution is bernoulli distribution rather than gaussian distribution because the dependent variable is binary 
second the predicted values are probabilities and are therefore restricted to through the logistic distribution function because logistic regression predicts the probability of particular outcomes rather than the outcomes themselves 
alternatives common alternative to the logistic model logit model is the probit model as the related names suggest 
from the perspective of generalized linear models these differ in the choice of link function the logistic model uses the logit function inverse logistic function while the probit model uses the probit function inverse error function 
equivalently in the latent variable interpretations of these two methods the first assumes standard logistic distribution of errors and the second standard normal distribution of errors 
other sigmoid functions or error distributions can be used instead 
logistic regression is an alternative to fisher method linear discriminant analysis 
if the assumptions of linear discriminant analysis hold the conditioning can be reversed to produce logistic regression 
the converse is not true however because logistic regression does not require the multivariate normal assumption of discriminant analysis the assumption of linear predictor effects can easily be relaxed using techniques such as spline functions 
history detailed history of the logistic regression is given in cramer 
the logistic function was developed as model of population growth and named logistic by pierre fran ois verhulst in the and under the guidance of adolphe quetelet see logistic function history for details 
in his earliest paper verhulst did not specify how he fit the curves to the data 
in his more detailed paper verhulst determined the three parameters of the model by making the curve pass through three observed points which yielded poor predictions the logistic function was independently developed in chemistry as model of autocatalysis wilhelm ostwald 
an autocatalytic reaction is one in which one of the products is itself catalyst for the same reaction while the supply of one of the reactants is fixed 
this naturally gives rise to the logistic equation for the same reason as population growth the reaction is self reinforcing but constrained 
the logistic function was independently rediscovered as model of population growth in by raymond pearl and lowell reed published as pearl reed which led to its use in modern statistics 
they were initially unaware of verhulst work and presumably learned about it from gustave du pasquier but they gave him little credit and did not adopt his terminology 
verhulst priority was acknowledged and the term logistic revived by udny yule in and has been followed since 
pearl and reed first applied the model to the population of the united states and also initially fitted the curve by making it pass through three points as with verhulst this again yielded poor results in the the probit model was developed and systematized by chester ittner bliss who coined the term probit in bliss and by john gaddum in gaddum and the model fit by maximum likelihood estimation by ronald fisher in fisher as an addendum to bliss work 
the probit model was principally used in bioassay and had been preceded by earlier work dating to see probit model history 
the probit model influenced the subsequent development of the logit model and these models competed with each other the logistic model was likely first used as an alternative to the probit model in bioassay by edwin bidwell wilson and his student jane worcester in wilson worcester 
however the development of the logistic model as general alternative to the probit model was principally due to the work of joseph berkson over many decades beginning in berkson where he coined logit by analogy with probit and continuing through berkson and following years 
the logit model was initially dismissed as inferior to the probit model but gradually achieved an equal footing with the logit particularly between and by the logit model achieved parity with the probit model in use in statistics journals and thereafter surpassed it 
this relative popularity was due to the adoption of the logit outside of bioassay rather than displacing the probit within bioassay and its informal use in practice the logit popularity is credited to the logit model computational simplicity mathematical properties and generality allowing its use in varied fields various refinements occurred during that time notably by david cox as in cox the multinomial logit model was introduced independently in cox and thiel which greatly increased the scope of application and the popularity of the logit model 
in daniel mcfadden linked the multinomial logit to the theory of discrete choice specifically luce choice axiom showing that the multinomial logit followed from the assumption of independence of irrelevant alternatives and interpreting odds of alternatives as relative preferences this gave theoretical foundation for the logistic regression 
extensions there are large numbers of extensions multinomial logistic regression or multinomial logit handles the case of multi way categorical dependent variable with unordered values also called classification 
note that the general case of having dependent variables with more than two values is termed polytomous regression 
ordered logistic regression or ordered logit handles ordinal dependent variables ordered values 
mixed logit is an extension of multinomial logit that allows for correlations among the choices of the dependent variable 
an extension of the logistic model to sets of interdependent variables is the conditional random field 
conditional logistic regression handles matched or stratified data when the strata are small 
it is mostly used in the analysis of observational studies 
software most statistical software can do binary logistic regression 
spss for basic logistic regression 
stata sas proc logistic for basic logistic regression 
proc catmod when all the variables are categorical 
proc glimmix for multilevel model logistic regression 
glm in the stats package using family binomial lrm in the rms package glmnet package for an efficient implementation regularized logistic regression lmer for mixed effects logistic regression rfast package command gm logistic for fast and heavy calculations involving large scale data 
arm package for bayesian logistic regression python logit in the statsmodels module 
logisticregression in the scikit learn module 
logisticregressor in the tensorflow module 
full example of logistic regression in the theano tutorial bayesian logistic regression with ard prior code tutorial variational bayes logistic regression with ard prior code tutorial bayesian logistic regression code tutorial ncss logistic regression in ncss matlab mnrfit in the statistics and machine learning toolbox with incorrect coded as instead of fminunc fmincon fitglm mnrfit fitclinear mle can all do logistic regression 
java jvm liblinear apache flink apache spark sparkml supports logistic regression fpga logistic regresesion ip core in hls for fpga notably microsoft excel statistics extension package does not include it 
see also logistic function discrete choice jarrow turnbull model limited dependent variable multinomial logit model ordered logit hosmer lemeshow test brier score mlpack contains implementation of logistic regression local case control sampling logistic model tree references further reading external links media related to logistic regression at wikimedia commons econometrics lecture topic logit model on youtube by mark thoma logistic regression tutorial mlelr software in for teaching purposes
batch normalization also known as batch norm is method used to make training of artificial neural networks faster and more stable through normalization of the layers inputs by re centering and re scaling
it was proposed by sergey ioffe and christian szegedy in while the effect of batch normalization is evident the reasons behind its effectiveness remain under discussion
it was believed that it can mitigate the problem of internal covariate shift where parameter initialization and changes in the distribution of the inputs of each layer affect the learning rate of the network
recently some scholars have argued that batch normalization does not reduce internal covariate shift but rather smooths the objective function which in turn improves the performance
however at initialization batch normalization in fact induces severe gradient explosion in deep networks which is only alleviated by skip connections in residual networks
others maintain that batch normalization achieves length direction decoupling and thereby accelerates neural networks
more recently normalize gradient clipping technique and smart hyperparameter tuning has been introduced in normalizer free nets so called nf nets which mitigates the need for batch normalization
internal covariate shift each layer of neural network has inputs with corresponding distribution which is affected during the training process by the randomness in the parameter initialization and the randomness in the input data
the effect of these sources of randomness on the distribution of the inputs to internal layers during training is described as internal covariate shift
although clear cut precise definition seems to be missing the phenomenon observed in experiments is the change on means and variances of the inputs to internal layers during training
batch normalization was initially proposed to mitigate internal covariate shift
during the training stage of networks as the parameters of the preceding layers change the distribution of inputs to the current layer changes accordingly such that the current layer needs to constantly readjust to new distributions
this problem is especially severe for deep networks because small changes in shallower hidden layers will be amplified as they propagate within the network resulting in significant shift in deeper hidden layers
therefore the method of batch normalization is proposed to reduce these unwanted shifts to speed up training and to produce more reliable models
besides reducing internal covariate shift batch normalization is believed to introduce many other benefits
with this additional operation the network can use higher learning rate without vanishing or exploding gradients
furthermore batch normalization seems to have regularizing effect such that the network improves its generalization properties and it is thus unnecessary to use dropout to mitigate overfitting
it has been observed also that with batch norm the network becomes more robust to different initialization schemes and learning rates
procedures transformation in neural network batch normalization is achieved through normalization step that fixes the means and variances of each layer inputs
ideally the normalization would be conducted over the entire training set but to use this step jointly with stochastic optimization methods it is impractical to use the global information
thus normalization is restrained to each mini batch in the training process
let us use to denote mini batch of size of the entire training set
the empirical mean and variance of could thus be denoted as and for layer of the network with dimensional input
each dimension of its input is then normalized
re centered and re scaled separately where and and are the per dimension mean and standard deviation respectively
is added in the denominator for numerical stability and is an arbitrarily small constant
the resulting normalized activation have zero mean and unit variance if is not taken into account
to restore the representation power of the network transformation step then follows as where the parameters and are subsequently learned in the optimization process
formally the operation that implements batch normalization is transform called the batch normalizing transform
the output of the bn transform is then passed to other network layers while the normalized output remains internal to the current layer
backpropagation the described bn transform is differentiable operation and the gradient of the loss with respect to the different parameters can be computed directly with the chain rule
specifically depends on the choice of activation function and the gradient against other parameters could be expressed as function of and inference during the training stage the normalization steps depend on the mini batches to ensure efficient and reliable training
however in the inference stage this dependence is not useful any more
instead the normalization step in this stage is computed with the population statistics such that the output could depend on the input in deterministic manner
the population mean and variance var are computed as and var
the population statistics thus is complete representation of the mini batches
the bn transform in the inference step thus becomes inf var where is passed on to future layers instead of
since the parameters are fixed in this transformation the batch normalization procedure is essentially applying linear transform to the activation
theoretical understanding although batch normalization has become popular due to its strong empirical performance the working mechanism of the method is not yet well understood
the explanation made in the original paper was that batch norm works by reducing internal covariate shift but this has been challenged by more recent work
one experiment trained vgg network under different training regimes standard no batch norm batch norm and batch norm with noise added to each layer during training
in the third model the noise has non zero mean and non unit variance
it explicitly introduces covariate shift
despite this it showed similar accuracy to the second model and both performed better than the first suggesting that covariate shift is not the reason that batch norm improves performance
smoothness one alternative explanation is that the improvement with batch normalization is instead due to it producing smoother parameter space and smoother gradients as formalized by smaller lipschitz constant
consider two identical networks one contains batch normalization layers and the other doesn the behaviors of these two networks are then compared
denote the loss functions as and respectively
let the input to both networks be and the output be for which where is the layer weights
for the second network additionally goes through batch normalization layer
denote the normalized activation as which has zero mean and unit variance
let the transformed activation be and suppose and are constants
finally denote the standard deviation over mini batch as first it can be shown that the gradient magnitude of batch normalized network is bounded with the bound expressed as
since the gradient magnitude represents the lipschitzness of the loss this relationship indicates that batch normalized network could achieve greater lipschitzness comparatively
notice that the bound gets tighter when the gradient correlates with the activation which is common phenomena
the scaling of is also significant since the variance is often large
secondly the quadratic form of the loss hessian with respect to activation in the gradient direction can be bounded as the scaling of indicates that the loss hessian is resilient to the mini batch variance whereas the second term on the right hand side suggests that it becomes smoother when the hessian and the inner product are non negative
if the loss is locally convex then the hessian is positive semi definite while the inner product is positive if is in the direction towards the minimum of the loss
it could thus be concluded from this inequality that the gradient generally becomes more predictive with the batch normalization layer
it then follows to translate the bounds related to the loss with respect to the normalized activation to bound on the loss with respect to the network weights where and in addition to the smoother landscape it is further shown that batch normalization could result in better initialization with the following inequality where and are the local optimal weights for the two networks respectively
some scholars argue that the above analysis cannot fully capture the performance of batch normalization because the proof only concerns the largest eigenvalue or equivalently one direction in the landscape at all points
it is suggested that the complete eigenspectrum needs to be taken into account to make conclusive analysis
measure since it is hypothesized that batch normalization layers could reduce internal covariate shift an experiment is set up to measure quantitatively how much covariate shift is reduced
first the notion of internal covariate shift needs to be defined mathematically
specifically to quantify the adjustment that layer parameters make in response to updates in previous layers the correlation between the gradients of the loss before and after all previous layers are updated is measured since gradients could capture the shifts from the first order training method
if the shift introduced by the changes in previous layers is small then the correlation between the gradients would be close to the correlation between the gradients are computed for four models standard vgg network vgg network with batch normalization layers layer deep linear network dln trained with full batch gradient descent and dln network with batch normalization layers
interestingly it is shown that the standard vgg and dln models both have higher correlations of gradients compared with their counterparts indicating that the additional batch normalization layers are not reducing internal covariate shift
vanishing exploding gradients even though batchnorm was originally introduced to alleviate gradient vanishing or explosion problems deep batchnorm network in fact suffers from gradient explosion at initialization time no matter what it uses for nonlinearity
thus the optimization landscape is very far from smooth for randomly initialized deep batchnorm network
more precisely if the network has layers then the gradient of the first layer weights has norm for some depending only on the nonlinearity
for any fixed nonlinearity decreases as the batch size increases
for example for relu decreases to as the batch size tends to infinity
practically this means deep batchnorm networks are untrainable
this is only relieved by skip connections in the fashion of residual networks this gradient explosion on the surface contradicts the smoothness property explained in the previous section but in fact they are consistent
the previous section studies the effect of inserting single batchnorm in network while the gradient explosion depends on stacking batchnorms typical of modern deep neural networks
decoupling another possible reason for the success of batch normalization is that it decouples the length and direction of the weight vectors and thus facilitates better training
by interpreting batch norm as reparametrization of weight space it can be shown that the length and the direction of the weights are separated and can thus be trained separately
for particular neural network unit with input and weight vector denote its output as where is the activation function and denote
assume that and that the spectrum of the matrix is bounded as such that is symmetric positive definite
adding batch normalization to this unit thus results in by definition
the variance term can be simplified such that assume that has zero mean and can be omitted then it follows that where is the induced norm of hence it could be concluded that where and and accounts for its length and direction separately
this property could then be used to prove the faster convergence of problems with batch normalization
linear convergence least square problem with the reparametrization interpretation it could then be proved that applying batch normalization to the ordinary least squares problem achieves linear convergence rate in gradient descent which is faster than the regular gradient descent with only sub linear convergence
denote the objective of minimizing an ordinary least squares problem as where
since the objective thus becomes where is excluded to avoid in the denominator
since the objective is convex with respect to its optimal value could be calculated by setting the partial derivative of the objective against to the objective could be further simplified to be
note that this objective is form of the generalized rayleigh quotient where is symmetric matrix and is symmetric positive definite matrix
it is proven that the gradient descent convergence rate of the generalized rayleigh quotient is where is the largest eigenvalue of is the second largest eigenvalue of and is the smallest eigenvalue of in our case is rank one matrix and the convergence result can be simplified accordingly
specifically consider gradient descent steps of the form with step size and starting from then
learning halfspace problem the problem of learning halfspaces refers to the training of the perceptron which is the simplest form of neural network
the optimization problem in this case is where and is an arbitrary loss function
suppose that is infinitely differentiable and has bounded derivative
assume that the objective function is smooth and that solution exists and is bounded such that
also assume is multivariate normal random variable
with the gaussian assumption it can be shown that all critical points lie on the same line for any choice of loss function specifically the gradient of could be represented as where and is the th derivative of by setting the gradient to it thus follows that the bounded critical points can be expressed as where depends on and combining this global property with length direction decoupling it could thus be proved that this optimization problem converges linearly
first variation of gradient descent with batch normalization gradient descent in normalized parameterization gdnp is designed for the objective function such that the direction and length of the weights are updated separately
denote the stopping criterion of gdnp as let the step size be
for each step if then update the direction as
then update the length according to where is the classical bisection algorithm and is the total iterations ran in the bisection step
denote the total number of iterations as then the final output of gdnp is the gdnp algorithm thus slightly modifies the batch normalization step for the ease of mathematical analysis
it can be shown that in gdnp the partial derivative of against the length component converges to zero at linear rate such that where and are the two starting points of the bisection algorithm on the left and on the right correspondingly
further for each iteration the norm of the gradient of with respect to converges linearly such that
combining these two inequalities bound could thus be obtained for the gradient with respect to such that the algorithm is guaranteed to converge linearly
although the proof stands on the assumption of gaussian input it is also shown in experiments that gdnp could accelerate optimization without this constraint
neural networks consider multilayer perceptron mlp with one hidden layer and hidden units with mapping from input to scalar output described as where and are the input and output weights of unit correspondingly and is the activation function and is assumed to be tanh function
the input and output weights could then be optimized with where is loss function
consider fixed and optimizing only it can be shown that the critical points of of particular hidden unit all align along one line depending on incoming information into the hidden layer such that where is scalar
this result could be proved by setting the gradient of to zero and solving the system of equations
apply the gdnp algorithm to this optimization problem by alternating optimization over the different hidden units
specifically for each hidden unit run gdnp to find the optimal and with the same choice of stopping criterion and stepsize it follows that since the parameters of each hidden unit converge linearly the whole optimization problem has linear rate of convergence
references ioffe sergey szegedy christian
batch normalization accelerating deep network training by reducing internal covariate shift icml proceedings of the nd international conference on international conference on machine learning volume july pages simonyan karen zisserman andrew
very deep convolutional networks for large scale image recognition
linear combination of atomic orbitals or lcao is quantum superposition of atomic orbitals and technique for calculating molecular orbitals in quantum chemistry 
in quantum mechanics electron configurations of atoms are described as wavefunctions 
in mathematical sense these wave functions are the basis set of functions the basis functions which describe the electrons of given atom 
in chemical reactions orbital wavefunctions are modified 
the electron cloud shape is changed according to the type of atoms participating in the chemical bond 
it was introduced in by sir john lennard jones with the description of bonding in the diatomic molecules of the first main row of the periodic table but had been used earlier by linus pauling for 
mathematical description an initial assumption is that the number of molecular orbitals is equal to the number of atomic orbitals included in the linear expansion 
in sense atomic orbitals combine to form molecular orbitals which can be numbered to and which may not all be the same 
the expression linear expansion for the th molecular orbital would be or where is molecular orbital represented as the sum of atomic orbitals each multiplied by corresponding coefficient and numbered to represents which atomic orbital is combined in the term 
the coefficients are the weights of the contributions of the atomic orbitals to the molecular orbital 
the hartree fock method is used to obtain the coefficients of the expansion 
the orbitals are thus expressed as linear combinations of basis functions and the basis functions are single electron functions which may or may not be centered on the nuclei of the component atoms of the molecule 
in either case the basis functions are usually also referred to as atomic orbitals even though only in the former case this name seems to be adequate 
the atomic orbitals used are typically those of hydrogen like atoms since these are known analytically 
slater type orbitals but other choices are possible such as the gaussian functions from standard basis sets or the pseudo atomic orbitals from plane wave pseudopotentials 
by minimizing the total energy of the system an appropriate set of coefficients of the linear combinations is determined 
this quantitative approach is now known as the hartree fock method 
however since the development of computational chemistry the lcao method often refers not to an actual optimization of the wave function but to qualitative discussion which is very useful for predicting and rationalizing results obtained via more modern methods 
in this case the shape of the molecular orbitals and their respective energies are deduced approximately from comparing the energies of the atomic orbitals of the individual atoms or molecular fragments and applying some recipes known as level repulsion and the like 
the graphs that are plotted to make this discussion clearer are called correlation diagrams 
the required atomic orbital energies can come from calculations or directly from experiment via koopmans theorem 
this is done by using the symmetry of the molecules and orbitals involved in bonding and thus is sometimes called symmetry adapted linear combination salc 
the first step in this process is assigning point group to the molecule 
each operation in the point group is performed upon the molecule 
the number of bonds that are unmoved is the character of that operation 
this reducible representation is decomposed into the sum of irreducible representations 
these irreducible representations correspond to the symmetry of the orbitals involved 
molecular orbital diagrams provide simple qualitative lcao treatment 
the ckel method the extended ckel method and the pariser parr pople method provide some quantitative theories 
see also quantum chemistry computer programs hartree fock method basis set chemistry tight binding holstein herring method external links lcao chemistry umeche maine edu link references
in cryptography related key attack is any form of cryptanalysis where the attacker can observe the operation of cipher under several different keys whose values are initially unknown but where some mathematical relationship connecting the keys is known to the attacker 
for example the attacker might know that the last bits of the keys are always the same even though they don know at first what the bits are 
this appears at first glance to be an unrealistic model it would certainly be unlikely that an attacker could persuade human cryptographer to encrypt plaintexts under numerous secret keys related in some way 
kasumi kasumi is an eight round bit block cipher with bit key 
it is based upon misty and was designed to form the basis of the confidentiality and integrity algorithms 
mark blunden and adrian escott described differential related key attacks on five and six rounds of kasumi 
differential attacks were introduced by biham and shamir 
related key attacks were first introduced by biham 
differential related key attacks are discussed in kelsey et al 
wep an important example of cryptographic protocol that failed because of related key attack is wired equivalent privacy wep used in wi fi wireless networks 
each client wi fi network adapter and wireless access point in wep protected network shares the same wep key 
encryption uses the rc algorithm stream cipher 
it is essential that the same key never be used twice with stream cipher 
to prevent this from happening wep includes bit initialization vector iv in each message packet 
the rc key for that packet is the iv concatenated with the wep key 
wep keys have to be changed manually and this typically happens infrequently 
an attacker therefore can assume that all the keys used to encrypt packets share single wep key 
this fact opened up wep to series of attacks which proved devastating 
the simplest to understand uses the fact that the bit iv only allows little under million possibilities 
because of the birthday paradox it is likely that for every packets two will share the same iv and hence the same rc key allowing the packets to be attacked 
more devastating attacks take advantage of certain weak keys in rc and eventually allow the wep key itself to be recovered 
in agents from the federal bureau of investigation publicly demonstrated the ability to do this with widely available software tools in about three minutes 
preventing related key attacks one approach to preventing related key attacks is to design protocols and applications so that encryption keys will never have simple relationship with each other 
for example each encryption key can be generated from the underlying key material using key derivation function 
for example replacement for wep wi fi protected access wpa uses three levels of keys master key working key and rc key 
the master wpa key is shared with each client and access point and is used in protocol called temporal key integrity protocol tkip to create new working keys frequently enough to thwart known attack methods 
the working keys are then combined with longer bit iv to form the rc key for each packet 
this design mimics the wep approach enough to allow wpa to be used with first generation wi fi network cards some of which implemented portions of wep in hardware 
however not all first generation access points can run wpa 
another more conservative approach is to employ cipher designed to prevent related key attacks altogether usually by incorporating strong key schedule 
newer version of wi fi protected access wpa uses the aes block cipher instead of rc in part for this reason 
there are related key attacks against aes but unlike those against rc they re far from practical to implement and wpa key generation functions may provide some security against them 
many older network cards cannot run wpa
in statistics ordinary least squares ols is type of linear least squares method for choosing the unknown parameters in linear regression model with fixed level one effects of linear function of set of explanatory variables by the principle of least squares minimizing the sum of the squares of the differences between the observed dependent variable values of the variable being observed in the input dataset and the output of the linear function of the independent variable 
geometrically this is seen as the sum of the squared distances parallel to the axis of the dependent variable between each data point in the set and the corresponding point on the regression surface the smaller the differences the better the model fits the data 
the resulting estimator can be expressed by simple formula especially in the case of simple linear regression in which there is single regressor on the right side of the regression equation 
the ols estimator is consistent for the level one fixed effects when the regressors are exogenous and forms perfect colinearity rank condition consistent for the variance estimate of the residuals when regressors have finite fourth moments and by the gauss markov theorem optimal in the class of linear unbiased estimators when the errors are homoscedastic and serially uncorrelated 
under these conditions the method of ols provides minimum variance mean unbiased estimation when the errors have finite variances 
under the additional assumption that the errors are normally distributed with zero mean ols is the maximum likelihood estimator that outperforms any non linear unbiased estimator 
linear model suppose the data consists of observations each observation includes scalar response and column vector of parameters regressors in linear regression model the response variable is linear function of the regressors or in vector form where as introduced previously is column vector of the th observation of all the explanatory variables is vector of unknown parameters and the scalar represents unobserved random variables errors of the th observation 
accounts for the influences upon the responses from sources other than the explanators this model can also be written in matrix notation as where and are vectors of the response variables and the errors of the observations and is an matrix of regressors also sometimes called the design matrix whose row is and contains the th observations on all the explanatory variables 
as rule the constant term is always included in the set of regressors say by taking for all the coefficient corresponding to this regressor is called the intercept 
regressors do not have to be independent there can be any desired relationship between the regressors so long as it is not linear relationship 
for instance we might suspect the response depends linearly both on value and its square in which case we would include one regressor whose value is just the square of another regressor 
in that case the model would be quadratic in the second regressor but none the less is still considered linear model because the model is still linear in the parameters 
matrix vector formulation consider an overdetermined system of linear equations in unknown coefficients with note for linear model as above not all elements in contains information on the data points 
the first column is populated with ones only the other columns contain actual data 
so here is equal to the number of regressors plus one 
this can be written in matrix form as where 
such system usually has no exact solution so the goal is instead to find the coefficients which fit the equations best in the sense of solving the quadratic minimization problem where the objective function is given by justification for choosing this criterion is given in properties below 
this minimization problem has unique solution provided that the columns of the matrix are linearly independent given by solving the so called normal equations the matrix is known as the normal matrix or gram matrix and the matrix is known as the moment matrix of regressand by regressors 
finally is the coefficient vector of the least squares hyperplane expressed as or 
estimation suppose is candidate value for the parameter vector the quantity yi xitb called the residual for the th observation measures the vertical distance between the data point xi yi and the hyperplane xtb and thus assesses the degree of fit between the actual data and the model 
the sum of squared residuals ssr also called the error sum of squares ess or residual sum of squares rss is measure of the overall model fit where denotes the matrix transpose and the rows of denoting the values of all the independent variables associated with particular value of the dependent variable are xi xit 
the value of which minimizes this sum is called the ols estimator for the function is quadratic in with positive definite hessian and therefore this function possesses unique global minimum at which can be given by the explicit formula proof argmin the product xt is gram matrix and its inverse is the cofactor matrix of closely related to its covariance matrix 
the matrix xt xt xt is called the moore penrose pseudoinverse matrix of this formulation highlights the point that estimation can be carried out if and only if there is no perfect multicollinearity between the explanatory variables which would cause the gram matrix to have no inverse 
after we have estimated the fitted values or predicted values from the regression will be where xtx xt is the projection matrix onto the space spanned by the columns of this matrix is also sometimes called the hat matrix because it puts hat onto the variable another matrix closely related to is the annihilator matrix in this is projection matrix onto the space orthogonal to both matrices and are symmetric and idempotent meaning that and and relate to the data matrix via identities px and mx matrix creates the residuals from the regression using these residuals we can estimate the value of using the reduced chi squared statistic the denominator is the statistical degrees of freedom 
the first quantity is the ols estimate for whereas the second is the mle estimate for 
the two estimators are quite similar in large samples the first estimator is always unbiased while the second estimator is biased but has smaller mean squared error 
in practice is used more often since it is more convenient for the hypothesis testing 
the square root of is called the regression standard error standard error of the regression or standard error of the equation it is common to assess the goodness of fit of the ols regression by comparing how much the initial variation in the sample can be reduced by regressing onto the coefficient of determination is defined as ratio of explained variance to the total variance of the dependent variable in the cases where the regression sum of squares equals the sum of squares of residuals where tss is the total sum of squares for the dependent variable and is an matrix of ones 
is centering matrix which is equivalent to regression on constant it simply subtracts the mean from variable 
in order for to be meaningful the matrix of data on regressors must contain column vector of ones to represent the constant whose coefficient is the regression intercept 
in that case will always be number between and with values close to indicating good degree of fit 
the variance in the prediction of the independent variable as function of the dependent variable is given in the article polynomial least squares 
simple linear regression model if the data matrix contains only two variables constant and scalar regressor xi then this is called the simple regression model 
this case is often considered in the beginner statistics classes as it provides much simpler formulas even suitable for manual calculation 
the parameters are commonly denoted as the least squares estimates in this case are given by simple formulas alternative derivations in the previous section the least squares estimator was obtained as value that minimizes the sum of squared residuals of the model 
however it is also possible to derive the same estimator from other approaches 
in all cases the formula for ols estimator remains the same xtx xty the only difference is in how we interpret this result 
projection for mathematicians ols is an approximate solution to an overdetermined system of linear equations where is the unknown 
assuming the system cannot be solved exactly the number of equations is much larger than the number of unknowns we are looking for solution that could provide the smallest discrepancy between the right and left hand sides 
in other words we are looking for the solution that satisfies min where is the standard norm in the dimensional euclidean space rn 
the predicted quantity is just certain linear combination of the vectors of regressors 
thus the residual vector will have the smallest length when is projected orthogonally onto the linear subspace spanned by the columns of the ols estimator in this case can be interpreted as the coefficients of vector decomposition of py along the basis of in other words the gradient equations at the minimum can be written as geometrical interpretation of these equations is that the vector of residuals is orthogonal to the column space of since the dot product is equal to zero for any conformal vector this means that is the shortest of all possible vectors that is the variance of the residuals is the minimum possible 
this is illustrated at the right 
introducing and matrix with the assumption that matrix is non singular and kt cf 
orthogonal projections the residual vector should satisfy the following equation 
the equation and solution of linear least squares are thus described as follows another way of looking at it is to consider the regression line to be weighted average of the lines passing through the combination of any two points in the dataset 
although this way of calculation is more computationally expensive it provides better intuition on ols 
maximum likelihood the ols estimator is identical to the maximum likelihood estimator mle under the normality assumption for the error terms 
proof this normality assumption has historical importance as it provided the basis for the early work in linear regression analysis by yule and pearson 
from the properties of mle we can infer that the ols estimator is asymptotically efficient in the sense of attaining the cram rao bound for variance if the normality assumption is satisfied 
generalized method of moments in iid case the ols estimator can also be viewed as gmm estimator arising from the moment conditions these moment conditions state that the regressors should be uncorrelated with the errors 
since xi is vector the number of moment conditions is equal to the dimension of the parameter vector and thus the system is exactly identified 
this is the so called classical gmm case when the estimator does not depend on the choice of the weighting matrix 
note that the original strict exogeneity assumption xi implies far richer set of moment conditions than stated above 
in particular this assumption implies that for any vector function the moment condition xi will hold 
however it can be shown using the gauss markov theorem that the optimal choice of function is to take which results in the moment equation posted above 
properties assumptions there are several different frameworks in which the linear regression model can be cast in order to make the ols technique applicable 
each of these settings produces the same formulas and same results 
the only difference is the interpretation and the assumptions which have to be imposed in order for the method to give meaningful results 
the choice of the applicable framework depends mostly on the nature of data in hand and on the inference task which has to be performed 
one of the lines of difference in interpretation is whether to treat the regressors as random variables or as predefined constants 
in the first case random design the regressors xi are random and sampled together with the yi from some population as in an observational study 
this approach allows for more natural study of the asymptotic properties of the estimators 
in the other interpretation fixed design the regressors are treated as known constants set by design and is sampled conditionally on the values of as in an experiment 
for practical purposes this distinction is often unimportant since estimation and inference is carried out while conditioning on all results stated in this article are within the random design framework 
classical linear regression model the classical model focuses on the finite sample estimation and inference meaning that the number of observations is fixed 
this contrasts with the other approaches which study the asymptotic behavior of ols and in which the number of observations is allowed to grow to infinity 
the linear functional form must coincide with the form of the actual data generating process 
the errors in the regression should have conditional mean zero the immediate consequence of the exogeneity assumption is that the errors have mean zero and that the regressors are uncorrelated with the errors xt the exogeneity assumption is critical for the ols theory 
if it holds then the regressor variables are called exogenous 
if it doesn then those regressors that are correlated with the error term are called endogenous and the ols estimator becomes biased 
in such case the method of instrumental variables may be used to carry out inference no linear dependence 
the regressors in must all be linearly independent 
mathematically this means that the matrix must have full column rank almost surely pr rank usually it is also assumed that the regressors have finite moments up to at least the second moment 
then the matrix qxx xtx is finite and positive semi definite 
when this assumption is violated the regressors are called linearly dependent or perfectly multicollinear 
in such case the value of the regression coefficient cannot be learned although prediction of values is still possible for new values of the regressors that lie in the same linearly dependent subspace spherical errors var where in is the identity matrix in dimension and is parameter which determines the variance of each observation 
this is considered nuisance parameter in the model although usually it is also estimated 
if this assumption is violated then the ols estimates are still valid but no longer efficient 
it is customary to split this assumption into two parts homoscedasticity which means that the error term has the same variance in each observation 
when this requirement is violated this is called heteroscedasticity in such case more efficient estimator would be weighted least squares 
if the errors have infinite variance then the ols estimates will also have infinite variance although by the law of large numbers they will nonetheless tend toward the true values so long as the errors have zero mean 
in this case robust estimation techniques are recommended 
no autocorrelation the errors are uncorrelated between observations for this assumption may be violated in the context of time series data panel data cluster samples hierarchical data repeated measures data longitudinal data and other data with dependencies 
in such cases generalized least squares provides better alternative than the ols 
another expression for autocorrelation is serial correlation normality 
it is sometimes additionally assumed that the errors have normal distribution conditional on the regressors 
this assumption is not needed for the validity of the ols method although certain additional finite sample properties can be established in case when it does especially in the area of hypotheses testing 
also when the errors are normal the ols estimator is equivalent to the maximum likelihood estimator mle and therefore it is asymptotically efficient in the class of all regular estimators 
importantly the normality assumption applies only to the error terms contrary to popular misconception the response dependent variable is not required to be normally distributed 
independent and identically distributed iid in some applications especially with cross sectional data an additional assumption is imposed that all observations are independent and identically distributed 
this means that all observations are taken from random sample which makes all the assumptions listed earlier simpler and easier to interpret 
also this framework allows one to state asymptotic results as the sample size which are understood as theoretical possibility of fetching new independent observations from the data generating process 
the list of assumptions in this case is iid observations xi yi is independent from and has the same distribution as xj yj for all no perfect multicollinearity qxx xi xit is positive definite matrix exogeneity xi homoscedasticity var xi 
time series model the stochastic process is co integrating 
the regressors are predetermined xi for all the matrix qxx xi xit is of full rank and hence positive definite is martingale difference sequence with finite matrix of second moments qxx xi xit 
finite sample properties first of all under the strict exogeneity assumption the ols estimators and are unbiased meaning that their expected values coincide with the true values of the parameters proof if the strict exogeneity does not hold as is the case with many time series models where exogeneity is assumed only with respect to the past shocks but not the future ones then these estimators will be biased in finite samples 
the variance covariance matrix or simply covariance matrix of is equal to var in particular the standard error of each coefficient is equal to square root of the th diagonal element of this matrix 
the estimate of this standard error is obtained by replacing the unknown quantity with its estimate 
thus it can also be easily shown that the estimator is uncorrelated with the residuals from the model cov the gauss markov theorem states that under the spherical errors assumption that is the errors should be uncorrelated and homoscedastic the estimator is efficient in the class of linear unbiased estimators 
this is called the best linear unbiased estimator blue 
efficiency should be understood as if we were to find some other estimator which would be linear in and unbiased then var var in the sense that this is nonnegative definite matrix 
this theorem establishes optimality only in the class of linear unbiased estimators which is quite restrictive 
depending on the distribution of the error terms other non linear estimators may provide better results than ols 
assuming normality the properties listed so far are all valid regardless of the underlying distribution of the error terms 
however if you are willing to assume that the normality assumption holds that is that in then additional properties of the ols estimators can be stated 
the estimator is normally distributed with mean and variance as given before 
this estimator reaches the cram rao bound for the model and thus is optimal in the class of all unbiased estimators 
note that unlike the gauss markov theorem this result establishes optimality among both linear and non linear estimators but only in the case of normally distributed error terms 
the estimator will be proportional to the chi squared distribution the variance of this estimator is equal to which does not attain the cram rao bound of 
however it was shown that there are no unbiased estimators of with variance smaller than that of the estimator 
if we are willing to allow biased estimators and consider the class of estimators that are proportional to the sum of squared residuals ssr of the model then the best in the sense of the mean squared error estimator in this class will be ssr which even beats the cram rao bound in case when there is only one regressor moreover the estimators and are independent the fact which comes in useful when constructing the and tests for the regression 
influential observations as was mentioned before the estimator is linear in meaning that it represents linear combination of the dependent variables yi 
the weights in this linear combination are functions of the regressors and generally are unequal 
the observations with high weights are called influential because they have more pronounced effect on the value of the estimator 
to analyze which observations are influential we remove specific th observation and consider how much the estimated quantities are going to change similarly to the jackknife method 
it can be shown that the change in the ols estimator for will be equal to where hj xjt xtx xj is the th diagonal element of the hat matrix and xj is the vector of regressors corresponding to the th observation 
similarly the change in the predicted value for th observation resulting from omitting that observation from the dataset will be equal to from the properties of the hat matrix hj and they sum up to so that on average hj 
these quantities hj are called the leverages and observations with high hj are called leverage points 
usually the observations with high leverage ought to be scrutinized more carefully in case they are erroneous or outliers or in some other way atypical of the rest of the dataset 
partitioned regression sometimes the variables and corresponding parameters in the regression can be logically split into two groups so that the regression takes form where and have dimensions and are and vectors with the frisch waugh lovell theorem states that in this regression the residuals and the ols estimate will be numerically identical to the residuals and the ols estimate for in the following regression where is the annihilator matrix for regressors 
the theorem can be used to establish number of theoretical results 
for example having regression with constant and another regressor is equivalent to subtracting the means from the dependent variable and the regressor and then running the regression for the de meaned variables but without the constant term 
constrained estimation suppose it is known that the coefficients in the regression satisfy system of linear equations where is matrix of full rank and is vector of known constants where in this case least squares estimation is equivalent to minimizing the sum of squared residuals of the model subject to the constraint the constrained least squares cls estimator can be given by an explicit formula 
this expression for the constrained estimator is valid as long as the matrix xtx is invertible 
it was assumed from the beginning of this article that this matrix is of full rank and it was noted that when the rank condition fails will not be identifiable 
however it may happen that adding the restriction makes identifiable in which case one would like to find the formula for the estimator 
the estimator is equal to where is matrix such that the matrix is non singular and rtq such matrix can always be found although generally it is not unique 
the second formula coincides with the first in case when xtx is invertible 
large sample properties the least squares estimators are point estimates of the linear regression model parameters however generally we also want to know how close those estimates might be to the true values of parameters 
in other words we want to construct the interval estimates 
since we haven made any assumption about the distribution of error term it is impossible to infer the distribution of the estimators and nevertheless we can apply the central limit theorem to derive their asymptotic properties as sample size goes to infinity 
while the sample size is necessarily finite it is customary to assume that is large enough so that the true distribution of the ols estimator is close to its asymptotic limit 
we can show that under the model assumptions the least squares estimator for is consistent that is converges in probability to and asymptotically normal proof where 
intervals using this asymptotic distribution approximate two sided confidence intervals for the th component of the vector can be constructed as at the confidence level where denotes the quantile function of standard normal distribution and jj is the th diagonal element of matrix 
similarly the least squares estimator for is also consistent and asymptotically normal provided that the fourth moment of exists with limiting distribution 
these asymptotic distributions can be used for prediction testing hypotheses constructing other estimators etc as an example consider the problem of prediction 
suppose is some point within the domain of distribution of the regressors and one wants to know what the response variable would have been at that point 
the mean response is the quantity whereas the predicted response is 
clearly the predicted response is random variable its distribution can be derived from that of which allows construct confidence intervals for mean response to be constructed at the confidence level 
hypothesis testing two hypothesis tests are particularly widely used 
first one wants to know if the estimated regression equation is any better than simply predicting that all values of the response variable equal its sample mean if not it is said to have no explanatory power 
the null hypothesis of no explanatory value of the estimated regression is tested using an test 
if the calculated value is found to be large enough to exceed its critical value for the pre chosen level of significance the null hypothesis is rejected and the alternative hypothesis that the regression has explanatory power is accepted 
otherwise the null hypothesis of no explanatory power is accepted 
second for each explanatory variable of interest one wants to know whether its estimated coefficient differs significantly from zero that is whether this particular explanatory variable in fact has explanatory power in predicting the response variable 
here the null hypothesis is that the true coefficient is zero 
this hypothesis is tested by computing the coefficient statistic as the ratio of the coefficient estimate to its standard error 
if the statistic is larger than predetermined value the null hypothesis is rejected and the variable is found to have explanatory power with its coefficient significantly different from zero 
otherwise the null hypothesis of zero value of the true coefficient is accepted 
in addition the chow test is used to test whether two subsamples both have the same underlying true coefficient values 
the sum of squared residuals of regressions on each of the subsets and on the combined data set are compared by computing an statistic if this exceeds critical value the null hypothesis of no difference between the two subsets is rejected otherwise it is accepted 
example with real data the following data set gives average heights and weights for american women aged source the world almanac and book of facts 
when only one dependent variable is being modeled scatterplot will suggest the form and strength of the relationship between the dependent variable and regressors 
it might also reveal outliers heteroscedasticity and other aspects of the data that may complicate the interpretation of fitted regression model 
the scatterplot suggests that the relationship is strong and can be approximated as quadratic function 
ols can handle non linear relationships by introducing the regressor height 
the regression model then becomes multiple linear model the output from most popular statistical packages will look similar to this in this table the value column gives the least squares estimates of parameters the std error column shows standard errors of each coefficient estimate the statistic and value columns are testing whether any of the coefficients might be equal to zero 
the statistic is calculated simply as if the errors follow normal distribution follows student distribution 
under weaker conditions is asymptotically normal 
large values of indicate that the null hypothesis can be rejected and that the corresponding coefficient is not zero 
the second column value expresses the results of the hypothesis test as significance level 
conventionally values smaller than are taken as evidence that the population coefficient is nonzero 
squared is the coefficient of determination indicating goodness of fit of the regression 
this statistic will be equal to one if fit is perfect and to zero when regressors have no explanatory power whatsoever 
this is biased estimate of the population squared and will never decrease if additional regressors are added even if they are irrelevant 
adjusted squared is slightly modified version of designed to penalize for the excess number of regressors which do not add to the explanatory power of the regression 
this statistic is always smaller than can decrease as new regressors are added and even be negative for poorly fitting models log likelihood is calculated under the assumption that errors follow normal distribution 
even though the assumption is not very reasonable this statistic may still find its use in conducting lr tests 
durbin watson statistic tests whether there is any evidence of serial correlation between the residuals 
as rule of thumb the value smaller than will be an evidence of positive correlation 
akaike information criterion and schwarz criterion are both used for model selection 
generally when comparing two alternative models smaller values of one of these criteria will indicate better model 
standard error of regression is an estimate of standard error of the error term 
total sum of squares model sum of squared and residual sum of squares tell us how much of the initial variation in the sample were explained by the regression 
statistic tries to test the hypothesis that all coefficients except the intercept are equal to zero 
this statistic has distribution under the null hypothesis and normality assumption and its value indicates probability that the hypothesis is indeed true 
note that when errors are not normal this statistic becomes invalid and other tests such as wald test or lr test should be used 
ordinary least squares analysis often includes the use of diagnostic plots designed to detect departures of the data from the assumed form of the model 
these are some of the common diagnostic plots residuals against the explanatory variables in the model 
non linear relation between these variables suggests that the linearity of the conditional mean function may not hold 
different levels of variability in the residuals for different levels of the explanatory variables suggests possible heteroscedasticity 
residuals against explanatory variables not in the model 
any relation of the residuals to these variables would suggest considering these variables for inclusion in the model 
residuals against the fitted values 
residuals against the preceding residual 
this plot may identify serial correlations in the residuals an important consideration when carrying out statistical inference using regression models is how the data were sampled 
in this example the data are averages rather than measurements on individual women 
the fit of the model is very good but this does not imply that the weight of an individual woman can be predicted with high accuracy based only on her height 
sensitivity to rounding this example also demonstrates that coefficients determined by these calculations are sensitive to how the data is prepared 
the heights were originally given rounded to the nearest inch and have been converted and rounded to the nearest centimetre 
since the conversion factor is one inch to cm this is not an exact conversion 
the original inches can be recovered by round and then re converted to metric without rounding 
if this is done the results become using either of these equations to predict the weight of woman gives similar values kg with rounding vs kg without rounding 
thus seemingly small variation in the data has real effect on the coefficients but small effect on the results of the equation 
while this may look innocuous in the middle of the data range it could become significant at the extremes or in the case where the fitted model is used to project outside the data range extrapolation 
this highlights common error this example is an abuse of ols which inherently requires that the errors in the independent variable in this case height are zero or at least negligible 
the initial rounding to nearest inch plus any actual measurement errors constitute finite and non negligible error 
as result the fitted parameters are not the best estimates they are presumed to be 
though not totally spurious the error in the estimation will depend upon relative size of the and errors 
another example with less real data problem statement we can use the least square mechanism to figure out the equation of two body orbit in polar base co ordinates 
the equation typically used is cos where is the radius of how far the object is from one of the bodies 
in the equation the parameters and are used to determine the path of the orbit 
we have measured the following data 
we need to find the least squares approximation of and for the given data 
solution first we need to represent and in linear form 
so we are going to rewrite the equation as cos 
now we can use this form to represent our observational data as where is and is and is constructed by the first column being the coefficient of and the second column being the coefficient of and is the values for the respective so and 
on solving we get so and see also bayesian least squares fama macbeth regression nonlinear least squares numerical methods for linear least squares nonlinear system identification references further reading dougherty christopher 
introduction to econometrics nd ed 
new york oxford university press 
isbn gujarati damodar porter dawn 
basic econometics fifth ed 
boston mcgraw hill irwin 
isbn heij christiaan boer paul franses philip kloek teun van dijk herman 
econometric methods with applications in business and economics st ed 
oxford oxford university press 
isbn hill carter griffiths william lim guay 
principles of econometrics rd ed 
hoboken nj john wiley sons 
the simple regression model 
introductory econometrics modern approach th ed 
mason oh cengage learning
virginity is the state of person who has never engaged in sexual intercourse
the term virgin originally only referred to sexually inexperienced women but has evolved to encompass range of definitions as found in traditional modern and ethical concepts
heterosexual individuals may or may not consider loss of virginity to occur only through penile vaginal penetration while people of other sexual orientations often include oral sex anal sex or mutual masturbation in their definitions of losing one virginity there are cultural and religious traditions that place special value and significance on this state predominantly towards unmarried females associated with notions of personal purity honor and worth
like chastity the concept of virginity has traditionally involved sexual abstinence
the concept of virginity usually involves moral or religious issues and can have consequences in terms of social status and in interpersonal relationships
although virginity has social implications and had significant legal implications in some societies in the past it has no legal consequences in most societies today
the social implications of virginity still remain in many societies and can have varying effects on an individual social agency
etymology the word virgin comes via old french virgine from the root form of latin virgo genitive virginis meaning literally maiden or virgin sexually intact young woman or sexually inexperienced woman
as in latin the english word is also often used with wider reference by relaxing the age gender or sexual criteria
in this case more mature women can be virgins the virgin queen men can be virgins and potential initiates into many fields can be colloquially termed virgins for example skydiving virgin
in the latter usage virgin means uninitiated
the latin word likely arose by analogy with suit of lexemes based on vireo meaning to be green fresh or flourishing mostly with botanic reference in particular virga meaning strip of wood the first known use of virgin in english is found in middle english manuscript held at trinity college cambridge of about ar haue martirs and confessors and uirgines maked faier bode inne to women
in this and many later contexts the reference is specifically christian alluding to members of the ordo virginum order of virgins which applies to the consecrated virgins known to have existed since the early church from the writings of the church fathers by about the word was expanded to apply also to mary the mother of jesus hence to sexual virginity explicitly conceiud hali gast born virgine marie
further expansion of the word to include virtuous or na ve young women irrespective of religious connection occurred over about another century until by about we find voide vacand of vices as virgyns it ware
these are three of the eighteen definitions of virgin from the first edition of the oxford english dictionary oed pages
most of the oed definitions however are similar
the german word for virgin is jungfrau
jungfrau literally means young woman but is not used in this sense anymore
instead junge frau can be used
jungfrau is the word reserved specifically for sexual inexperience
as frau means woman it suggests female referent
unlike english german also has specific word for male virgin ngling youngling
it is however dated and rarely used
jungfrau with some masculine modifier is more typical as evidenced by the film the year old virgin about year old male virgin titled in german jungfrau nnlich sucht
german also distinguishes between young women and girls who are denoted by the word dchen
the english cognate maid was often used to imply virginity especially in poetry
maid marian the love interest of the legendary outlaw robin hood in english folklore
german is not the only language to have specific name for male virginity in french male virgins are called puceau
the greek word for virgin is parthenos see parthenon
although typically applied to women like english it is also applied to men in both cases specifically denoting absence of sexual experience
when used of men it does not carry strong association of never married status
however in reference to women historically it was sometimes used to refer to an engaged woman parthenos autou his virgin his fianc as opposed to gun autou his woman his wife
this distinction is necessary due to there being no specific word for wife or husband in greek
by extension from its primary sense the idea that virgin has sexual blank slate unchanged by any past intimate connection or experience can imply that the person is of unadulterated purity
culture concept the concept of virginity has significance only in particular social cultural or moral context
according to hanne blank virginity reflects no known biological imperative and grants no demonstrable evolutionary advantage
medieval bestiaries stated that the only way to capture or tame unicorn was by way of using virgin as lure due to her implied purity
the topic is popular in renaissance paintings
although virginity has historically been correlated with purity and worth many feminist scholars believe that virginity itself is myth
they argue that no standardized medical definition of virginity exists that there is no scientifically verifiable proof of virginity loss and that sexual intercourse results in no change in personality
jessica valenti feminist writer and author of the purity myth reasons that the concept of virginity is also dubious because of the many individual definitions of virginity loss and that valuing virginity has placed woman morality between her legs
she critiques the notion that sexual activity has any influence on morality or ethics the urge of wanting one spouse or partner to have never engaged in sexual activities is called virgin complex
person may also have virgin complex directed towards oneself
definitions of virginity loss there are varying understandings as to which types of sexual activities result in loss of virginity
the traditional view is that virginity is only lost through vaginal penetration by the penis consensual or non consensual and that acts of oral sex anal sex mutual masturbation or other forms of non penetrative sex do not result in loss of virginity
person who engages in such acts without having engaged in vaginal intercourse is often regarded among heterosexuals and researchers as technically virgin
by contrast gay or lesbian individuals often describe such acts as resulting in loss of virginity
some gay males regard penile anal penetration as resulting in loss of virginity but not oral sex or non penetrative sex and lesbians may regard oral sex or fingering as loss of virginity
some lesbians who debate the traditional definition consider whether or not non penile forms of vaginal penetration constitute virginity loss while other gay men and lesbians assert that the term virginity is meaningless to them because of the prevalence of the traditional definition whether person can lose their virginity through rape is also subject to debate with the belief that virginity can only be lost through consensual sex being prevalent in some studies
in study by researcher and author laura carpenter many men and women discussed how they felt virginity could not be taken through rape
they described losing their virginities in one of three ways as gift stigma or part of the process
carpenter states that despite perceptions of what determines virginity loss being as varied among gay men and lesbians as they are among heterosexuals and in some cases more varied among the former that the matter has been described to her as people viewing sexual acts relating to virginity loss as acts that correspond to your sexual orientation which suggests the following so if you re gay male you re supposed to have anal sex because that what gay men do
and if you re gay woman then you re supposed to have oral sex because that what gay women do
and so those become like markers for when virginity is lost
the concept of technical virginity or sexual abstinence through oral sex is popular among teenagers
for example oral sex is common among adolescent girls who fellate their boyfriends not only to preserve their virginity but also to create and maintain intimacy or to avoid pregnancy
in study published in jama the journal of the american medical association the definition of sex was examined based on random sample of college students from us states it found that said oral genital contact like fellatio cunnilingus did not constitute having sex
stephanie sanders of the kinsey institute co author of the study stated that the technical virginity thing that going on
she and other researchers titled their findings would you say you had sex if
by contrast in study released in by the guttmacher institute author of the findings laura lindberg stated that there is widespread belief that teens engage in nonvaginal forms of sex especially oral sex as way to be sexually active while still claiming that technically they are virgins but that her study drew the conclusion that research shows that this supposed substitution of oral sex for vaginal sex is largely myth study published in the canadian journal of human sexuality focusing on definitions of having sex and noting studies concerning university students from the united states the united kingdom and australia reported that hile the vast majority of respondents more than in these three studies included penile vaginal intercourse in their definition of sex fewer between and respondents considered penile anal intercourse to constitute having sex and that oral genital behaviours were defined as sex by between and of respondents
different study by the kinsey institute sampled people ranging in ages
nearly percent of people in the study agreed that penile vaginal intercourse meant had sex
but the numbers changed as the questions got more specific
percent of respondents based had sex on whether the man had achieved an orgasm concluding that absence of an orgasm does not constitute having had sex
about percent of respondents said penile anal intercourse meant had sex
about percent of people believed oral sex was sex
virginity pledges or abstinence pledges made by heterosexual teenagers and young adults may also include the practice of technical virginity
in peer reviewed study by sociologists peter bearman and hannah brueckner which looked at virginity pledgers five years after their pledge they found that the pledgers have similar proportions of sexually transmitted diseases stds and at least as high proportions of anal and oral sex as those who have not made virginity pledge and deduced that there was substitution of oral and anal sex for vaginal sex among the pledgers
however the data for anal sex without vaginal sex reported by males did not reflect this directly
early loss of virginity early loss of virginity has been shown to be linked to factors such as level of education independence biological factors like age and gender and social factors such as parental supervision or religious affiliation with the most common being sociodemographic variables
along with this sexual abuse has also been shown to have link to later risky sexual behaviors and younger age of voluntary sexual intercourse
sexual initiation at an earlier age has been associated with less frequency of condom use less satisfaction and more frequency of non autonomous reasons for that first sexual encounter
adverse effects for losing virginity at an early age include lower chance of economic stability lower level of education social isolation marital disruption and greater medical consequences
these medical consequences consist of an increase in stds cervical cancer pelvic inflammatory disease fertility and unwanted pregnancies
female virginity cultural value the first act of sexual intercourse by female is commonly considered within many cultures to be an important personal milestone
its significance is reflected in expressions such as saving oneself losing one virginity taking someone virginity and sometimes as deflowering
the occasion is at times seen as the end of innocence integrity or purity and the sexualization of the individual traditionally there was cultural expectation that female would not engage in premarital sex and would come to her wedding virgin and that she would give up her virginity to her new husband in the act of consummation of the marriage
feminine sexual practices have revolved around the idea of females waiting to have sex until they are married some females who have been previously sexually active or their hymen has been otherwise damaged may undergo surgical procedure called hymenorrhaphy or hymenoplasty to repair or replace her hymen and cause vaginal bleeding on the next intercourse as proof of virginity see below
in some cultures an unmarried female who is found not to be virgin whether by choice or as result of rape can be subject to shame ostracism or even an honor killing
in those cultures female virginity is closely interwoven with personal or even family honor especially those known as shame societies in which the loss of virginity before marriage is matter of deep shame
in some parts of africa the myth that sex with virgin can cure hiv aids continues to prevail leading to girls and women being raped
in other societies such as many modern day western cultures lack of sexual abstinence before marriage is not as socially stigmatized as it may be in the formerly mentioned cultures virginity is regarded as valuable commodity in some cultures
in the past within most societies woman options for marriage were largely dependent upon her status as virgin
those women who were not virgins experienced dramatic decrease in opportunities for socially advantageous marriage and in some instances the premarital loss of virginity eliminated their chances of marriage entirely
modern virginity auctions like that of natalie dylan are discussed in the documentary how to lose your virginity
the bible required man who seduced or raped virgin to pay her bride price to her father and marry the girl
in some countries until the late th century woman could sue man who had taken her virginity but did not marry her
in some languages the compensation for these damages are called wreath money
proof of virginity some cultures require proof of bride virginity before her marriage
this has traditionally been tested by the presence of an intact hymen which was verified by either physical examination usually by physician who provided certificate of virginity or by proof of blood which refers to vaginal bleeding that results from the tearing of the hymen after the first sanctioned sexual contact
in some cultures the nuptial blood spotted bed sheet would be displayed as proof of both consummation of marriage and that the bride had been virgin
coerced medical virginity tests are practiced in many regions of the world but are today condemned as form of abuse of women
according to the world health organization who sexual violence encompasses wide range of acts including violent acts against the sexual integrity of women including female genital mutilation and obligatory inspections for virginity researchers stress that the presence or absence of hymen is not reliable indicator of whether or not female has been vaginally penetrated
the hymen is thin film of membrane situated just inside the vulva which can partially occlude the entrance to the vaginal canal
it is flexible and can be stretched or torn during first engagement in vaginal intercourse
however hymen may also be broken during physical activity
many women possess such thin fragile hymens easily stretched and already perforated at birth that the hymen can be broken in childhood without the girl even being aware of it often through athletic activities
for example slip while riding bicycle may on occasion result in the bicycle saddle horn entering the introitus just far enough to break the hymen
further there is the case of women with damaged hymens undergoing hymenorrhaphy or hymenoplasty to repair or replace their hymens and cause vaginal bleeding on the next intercourse as proof of virginity
others consider the practice to be virginity fraud or unnecessary
some call themselves born again virgins
there is common belief that some women are born without hymen but some doubt has been cast on this by recent study
it is likely that almost all women are born with hymen but not necessarily ones that will experience measurable change during first experience of vaginal intercourse
some medical procedures occasionally may require woman hymen to be opened hymenotomy
male virginity historically and in modern times female virginity has been regarded as more significant than male virginity the perception that sexual prowess is fundamental to masculinity has lowered the expectation of male virginity without lowering the social status
for example in some islamic cultures unmarried women who have been sexually active or raped may be subject to name calling shunning or family shame while unmarried men who have lost their virginities are not though premarital sex is forbidden in the quran with regard to both men and women
among various countries or cultures males are expected or encouraged to want to engage in sexual activity and to be more sexually experienced
not following these standards often leads to teasing and other such ridicule from their male peers
study by the guttmacher institute showed that in the countries surveyed most men have experienced sexual intercourse by their th birthdays male sexuality is seen as something that is innate and competitive and displays different set of cultural values and stigmas from female sexuality and virginity
in one study scholars wenger and berger found that male virginity is understood to be real by society but it has been ignored by sociological studies
within british and american culture in particular male virginity has been made an object of embarrassment and ridicule in films such as summer of american pie the inbetweeners movie and the year old virgin with the male virgin typically being presented as socially inept
such attitudes have resulted in some men keeping their status as virgin secret
prevalence of virginity the prevalence of virginity varies from culture to culture
in cultures which place importance on female virginity at marriage the age at which virginity is lost is in effect determined by the age at which marriages would normally take place in those cultures as well as the minimum marriage age set by the laws of the country where the marriage takes place in cross cultural study at what age do women and men have their first sexual intercourse
michael bozon of the french institut national tudes mographiques found that contemporary cultures fall into three broad categories
in the first group the data indicated families arranging marriage for daughters as close to puberty as possible with significantly older men
age of men at sexual initiation in these societies is at later ages than that of women but is often extra marital
this group included sub saharan africa the study listed mali senegal and ethiopia
the study considered the indian subcontinent to also fall into this group although data was only available from nepal in the second group the data indicated families encouraged daughters to delay marriage and to abstain from sexual activity before that time
however sons are encouraged to gain experience with older women or prostitutes before marriage
age of men at sexual initiation in these societies is at lower ages than that of women
this group includes latin cultures both from southern europe portugal greece and romania are noted and from latin america brazil chile and the dominican republic
the study considered many asian societies to also fall into this group although matching data was only available from thailand in the third group age of men and women at sexual initiation was more closely matched
there were two sub groups however
in non latin catholic countries poland and lithuania are mentioned age at sexual initiation was higher suggesting later marriage and reciprocal valuing of male and female virginity
the same pattern of late marriage and reciprocal valuing of virginity was reflected in singapore and sri lanka
the study considered china and vietnam to also fall into this group although data were not available finally in northern and eastern european countries age at sexual initiation was lower with both men and women involved in sexual activity before any union formation
the study listed switzerland germany and the czech republic as members of this group according to unicef survey in out of developed nations with available data more than two thirds of young people have had sexual intercourse while still in their teens
in australia the united kingdom and the united states approximately of year olds and of year olds have had sex
international survey sought to study the sexual behavior of teenagers
students aged from countries completed self administered anonymous classroom survey consisting of standard questionnaire developed by the hbsc health behaviour in school aged children international research network
the survey revealed that the majority of the students were still virgins they had no experience of sexual intercourse and among those who were sexually active the majority used contraception
in kaiser family foundation study of us teenagers of teens reported feeling pressure to have sex of sexually active teens reported being in relationship where they felt things were moving too fast sexually and had done something sexual they didn really want to do
several polls have indicated peer pressure as factor in encouraging both girls and boys to have sex some studies suggest that people commence sexual activity at an earlier age than previous generations
the durex global sex survey found that people worldwide are having sex for the first time at an average age of ranging from in iceland to in india though evidence has shown that the average age is not good indicator of sexual initiation and that percentages of sexually initiated youth at each age are preferred
survey of uk teenagers between the ages of and conducted by yougov for channel showed that only of these teenagers intended to wait until marriage before having sex
according to cdc study in the to year old age group percent of males and percent of females in the united states reported never having an opposite sex partner the rates of teenage pregnancy vary and range from per girls in some sub saharan african countries to per in south korea
the rate for the united states is per the highest in the developed world and about four times the european union average
the teenage pregnancy rates between countries must take into account the level of general sex education available and access to contraceptive options
many western countries have instituted sex education programs the main objective of which is to reduce such pregnancies and stds
in the united states federal government shifted the objective of sex education towards abstinence only sex education programs promoting sexual abstinence before marriage virginity and prohibiting information on birth control and contraception
in president george bush announced five year global hiv aids strategy also known as the president emergency plan for aids relief pepfar which committed the to provide billion over five years toward aids relief in countries in africa and the caribbean and in vietnam
part of the funding was earmarked specifically for abstinence only until marriage programs
in one study about virginity pledges male pledgers were times more likely to remain virgins by age than those who did not pledge vs and estimated that female pledgers were times more likely to remain virgins by age than those who did not pledge vs
social psychology some cultural anthropologists argue that romantic love and sexual jealousy are universal features of human relationships
social values related to virginity reflect both sexual jealousy and ideals of romantic love and appear to be deeply embedded in human nature psychology explores the connection between thought and behavior
seeking understanding of social or anti social behaviors includes sexual behavior
joan kahn and kathryn london studied women married between and to see if virginity at marriage influenced risk of divorce
in this study women who were virgins at the time of marriage were shown to have less marital upset
it was shown that when observable characteristics were controlled women who were non virgins at the time of marriage had higher risk for divorce
however it was also shown that the link between premarital sex and the risk of divorce were attributed to prior unobserved differences such as deviating from norms study conducted by smith and schaffer found that someone first sexual experience has been linked to their sexual performance for years to come
participants whose first intercourse was pleasant showed more satisfaction in their current sex lives
different study showed that when compared with virgins nonvirgins have been shown to have higher levels of independence less desire for achievement more criticism from society and greater level of deviance
ethics social norms and legal implications human sexual activity like many other kinds of activity engaged in by humans is generally influenced by social rules that are culturally specific and vary widely
these social rules are referred to as sexual morality what can and can not be done by society rules and sexual norms what is and is not expected
there are number of groups within societies promoting their views of sexual morality in variety of ways including through sex education religious teachings seeking commitments or virginity pledges and other means
most countries have laws which set minimum marriage age with the most common age being years reduced to in special circumstances typically when the female partner is pregnant but the actual age at first marriage can be considerably higher
laws also prescribe the minimum age at which person is permitted to engage in sex commonly called the age of consent
social and legal attitudes toward the appropriate age of consent have drifted upwards in modern times
for example while ages from to were typically acceptable in western countries during the mid th century the end of the th century and the beginning of the th century were marked by changing attitudes resulting in raising the ages of consent to ages generally ranging from to today the age of consent varies from years or onset of puberty to but is the most common age of consent though some jurisdictions also having close in age exception allowing two adolescents as young as years of age to have sex with each other provided their ages are not more than specified number of years apart typical no more then to years age difference depending on the jurisdiction
some countries outlaw any sex outside marriage entirely
historically and still in many countries and jurisdictions today female sexual experience is sometimes considered relevant factor in the prosecution of perpetrator of rape
also historically man who took female virginity could be forced to marry her
in addition children born as result of premarital sex were subject to various legal and social disabilities such as being considered illegitimate and thus barred from inheriting from the putative father estate from bearing the father surname or title and support from the putative father
many of these legal disabilities on children born from extramarital relationships have been abolished by law in most western countries though social ostracism may still apply
religious views all major religions have moral codes covering issues of sexuality morality and ethics
though these moral codes do not address issues of sexuality directly they seek to regulate the situations which can give rise to sexual interest and to influence people sexual activities and practices
however the impact of religious teaching has at times been limited
for example though most religions disapprove of premarital sexual relations it has always been widely practiced
nevertheless these religious codes have always had strong influence on peoples attitudes to sexual issues
ancient greece and rome virginity was often considered virtue denoting purity and physical self restraint and is an important characteristic in greek mythology
in ancient greek literature such as the homeric hymns there are references to the parthenon goddesses artemis athena and hestia proclaiming pledges to eternal virginity greek
however it has been argued maiden state of parthenia greek as invoked by these deities carries slightly different meaning from what is normally understood as virginity in modern western religions
rather parthenia focused more on marriageability and abstract concepts without strict physical requirements which would be adversely affected but not entirely relinquished by pre marital sexual intercourse
for these reasons other goddesses not eternally committed to parthenia within the homeric hymns are able to renew theirs through ritual such as hera or choose an appearance which implies the possession of it such as aphrodite
although accounts vary the goddess of witchcraft known as hecate has been portrayed as virgin as well in roman times the vestal virgins were the highly respected strictly celibate although not necessarily virginal priestesses of vesta and keepers of the sacred fire of vesta
the vestals were committed to the priesthood before puberty when years old and sworn to celibacy for period of years
the chastity of the vestals was considered to have direct bearing on the health of the roman state
allowing the sacred fire of vesta to die out suggesting that the goddess had withdrawn her protection from the city was serious offence and was punishable by scourging
because vestal chastity was thought to be directly correlated to the sacred burning of the fire if the fire were extinguished it might be assumed that vestal had been unchaste
the penalty for vestal virgin found to have had sexual relations while in office was being buried alive
buddhism the most common formulation of buddhist ethics for lay followers are the five precepts and the eightfold path
these precepts take the form of voluntary personal undertakings not divine mandate or instruction
the third of the five precepts is to refrain from committing sensual misconduct
sensual misconduct is defined in the pali canon as follows abandoning sensual misconduct man abstains from sensual misconduct
he does not get sexually involved with those who are protected by their mothers their fathers their brothers their sisters their relatives or their dhamma those with husbands those who entail punishments or even those crowned with flowers by another man
virginity specifically is not mentioned in the canon
on the other hand buddhist monks and nuns of most traditions are expected to refrain from all sexual activity and the buddha is said to have admonished his followers to avoid unchastity as if it were pit of burning cinders
the rd of the precepts in buddhism warns against any sensual misconduct though the exact definition of it is unclear
buddhists have been more open compared to other religions about the subject of sex and that has expanded over time
as with christianity although traditionalist would assume that one should not have sex before marriage many buddhists do
there are different branches of buddhism like tantric and puritan and they have very different views on the subject of sex yet managed to get along
tantric is sanskrit word it is typically translated as two things or person being bound together
in the time of gotama the man who came to be known as buddha sex was not taboo
the world the prince lived in was filled with earthly pleasures
women naked from the waist above were in the court solely to serve the prince
gotama father even constructed chamber of love
prince gotama and founded the beginnings of buddhism which included the denial of earthly pleasures in order to follow the middle way
the stark contrast between the way buddha lived his life before and after rejecting the material world may arguably be one of the reasons buddhism evolved the way it did
in the present the mother of buddha does not have to be virgin she must have never had child however
hinduism in hinduism premarital virginity on the part of the bride is considered ideal
the prevailing hindu marriage ceremony or the vedic wedding centers around the kanyadan ritual which literally means gift of virgin by father of the maiden through which the hindus believe they gain greatest spiritual merit and marriages of the daughters are considered spiritual obligation
the purity of women is especially valued in south asia where hinduism is most commonly practiced
sex had never been taboo in ancient india and intactness of the hymen had nothing to do with virginity
judaism premarital sex is forbidden in judaism
in fact the precedent for the mitzvot which are related in deuteronomy which regard what happens when man rapes virgin may well have been set at shechem after the rape of dinah cf
there are other references in the torah to virginity
in the first reference in genesis lot offers his virgin daughters to the people of sodom for sexual purposes in an attempt to protect his guests cf
genesis with the implication that the people of sodom would be more likely to accept the offer in view of the girls virginity than they would otherwise
this also sets the precedent for israelites to avoid homosexual activity cf
the next reference is at genesis where eliezer is seeking wife for his master abraham son
he meets rebecca and the narrative tells us the damsel was very fair to look upon virgin neither had any man known her in biblical terms to know is euphemism for sexual relations
children born to single woman are not regarded as illegitimate mamzer or subject to social or religious disabilities perez and zerach for example and although their mother was widow who was willingly impregnated by her father in law were not counted as mamzerim cf
halakhah also contains rules related to protecting female virgins and rules regarding pre marital sex rape and the effects of each
in torah damsel who has not the sign of virginity in the early marriage shall be punished by death penalty since the unvirgin woman among israel is equal with defiled whore in her father house
christianity paul the apostle expressed the view that person body belongs to god and is god temple corinthians and that premarital sex is immoral corinthians on an equal level as adultery
corinthians paul also expressed the view in corinthians that sexual abstinence is the preferred state for both men and women
however he stated that sexual relations are expected between married couple
according to classicist evelyn stagg and new testament scholar frank stagg the new testament holds that sex is reserved for marriage
they maintain that the new testament teaches that sex outside of marriage is sin of adultery if either of the participants is married otherwise the sin of fornication if neither of the participants are married
an imperative given in corinthians says flee from sexual immorality
all other sins people commit are outside their bodies but those who sin sexually sin against their own bodies
cor those who are sexually immoral or adulterers are listed in corinthians in list of wrongdoers who will not inherit the kingdom of god
galatians and corinthians also address fornication
the apostolic decree of the council of jerusalem also includes prohibition on fornication
aquinas went further emphasizing that acts other than copulation destroy virginity and clarifying that involuntary sexual pleasure does not destroy virginity
from his summa theologica pleasure resulting from resolution of semen may arise in two ways
if this be the result of the mind purpose it destroys virginity whether copulation takes place or not
augustine however mentions copulation because such like resolution is the ordinary and natural result thereof
on another way this may happen beside the purpose of the mind either during sleep or through violence and without the mind consent although the flesh derives pleasure from it or again through weakness of nature as in the case of those who are subject to flow of semen
on such cases virginity is not forfeit because such like pollution is not the result of impurity which excludes virginity
some have theorized that the new testament was not against sex before marriage
the discussion turns on two greek words moicheia adultery and porneia fornication see also pornography
the first word is restricted to contexts involving sexual betrayal of spouse however the second word is used as generic term for illegitimate sexual activity
elsewhere in corinthians incest homosexual intercourse according to some interpretations and prostitution are all explicitly forbidden by name however the septuagint uses porneia to refer to male temple prostitution
paul is preaching about activities based on sexual prohibitions in leviticus in the context of achieving holiness
the theory suggests it is these and only these behaviors that are intended by paul prohibition in chapter seven
the strongest argument against this theory is that the modern interpretation of the new testament outside corinthians speaks against premarital sex christian orthodoxy accepts that mary the mother of jesus was virgin at the time jesus was conceived based on the accounts in the gospel of matthew and the gospel of luke
roman catholics eastern orthodox and oriental orthodox as well as many lutherans and anglicans hold to the dogma of the perpetual virginity of mary
however other christians reject the dogma citing sources such as mark isn this the carpenter the son of mary and the brother of james joses judas and simon
and aren his sisters here with us
the catholic church holds that in semitic usage the terms brother sister are applied not only to children of the same parents but to nephews nieces cousins half brothers and half sisters
catholics orthodox christians lutherans and other groups such as high church anglicans may refer to mary as the virgin mary or the blessed virgin mary the catholic encyclopedia says there are two elements in virginity the material element that is to say the absence in the past and in the present of all complete and voluntary delectation whether from lust or from the lawful use of marriage and the formal element that is the firm resolution to abstain forever from sexual pleasure and that virginity is irreparably lost by sexual pleasure voluntarily and completely experienced
however for the purposes of consecrated virgins it is canonically enough that they have never been married or lived in open violation of chastity
islam islam considers extramarital sex to be sinful and forbidden
though islamic law prescribes punishments for muslim men and women for the act of zin though in western cultures premarital sex and loss of virginity may be considered shameful to the individual in some muslim societies an act of premarital sex even if not falling within the legal standards of proof may result in personal shame and loss of family honor in some modern day largely muslim societies such as turkey vaginal examinations for verifying woman virginity are clinical practice which are at times state enforced
these types of examinations are typically ordered for women who go against traditional societal notions of public morality and rules of modesty though in the turkish penal code was altered to require woman consent prior to performing such an examination
sikhism in sikhism sexual activity is supposed to occur only between married individuals
sikhism advises against premarital sex as it has high potential of being an indulgence of lust kaam or extreme sexual desire
sikhism teaches that young women must have decent modesty sharam because the honor izzat of her family could be jeopardized
sexual activity and even living together prior to marriage is not allowed in sikhism
virginity is an important aspect of spirituality and it has to be preserved before marriage or when one is ready to move into another sacred state of being with their significant other
see also almah artificial hymen brahmacharya confraternity of the cord of saint thomas purity ball purity ring references further reading journal articlesarmour stacy and dana haynie
adolescent sexual debut and later delinquency
journal of youth and adolescence
abstract only cooksey elizabeth mott frank neubauer stefanie
friendships and early relationships links to sexual initiation among american adolescents born to young mothers pdf
perspectives on sexual and reproductive health
jstor pmid goodson evans edmundson
female adolescents and onset of sexual intercourse theory based review of research from to
journal of adolescent health
doi pmid rich lauren kim sun bin
employment and the sexual and reproductive behavior of female adolescents
perspectives on sexual and reproductive health
jstor pmid archived from the original on rosenberg
age at first sex and human papillomavirus infection linked through behavioral factors and partner traits
perspectives on sexual and reproductive health
jstor archived from the original on monographsbently thomas
the monument of matrones conteining seven severall lamps of virginitie
thomas dawson carpenter laura
virginity lost an intimate portrait of first sexual experiences
new york university press isbn external links
in mathematics the linear span also called the linear hull or just span of set of vectors from vector space denoted span is defined as the set of all linear combinations of the vectors in it can be characterized either as the intersection of all linear subspaces that contain or as the smallest subspace containing the linear span of set of vectors is therefore vector space itself 
spans can be generalized to matroids and modules 
to express that vector space is linear span of subset one commonly uses the following phrases either spans is spanning set of is spanned generated by or is generator or generator set of definition given vector space over field the span of set of vectors not necessarily infinite is defined to be the intersection of all subspaces of that contain is referred to as the subspace spanned by or by the vectors in conversely is called spanning set of and we say that spans alternatively the span of may be defined as the set of all finite linear combinations of elements vectors of which follows from the above definition 
in the case of infinite infinite linear combinations 
where combination may involve an infinite sum assuming that such sums are defined somehow as in say banach space are excluded by the definition generalization that allows these is not equivalent 
examples the real vector space has as spanning set 
this particular spanning set is also basis 
if were replaced by it would also form the canonical basis of another spanning set for the same space is given by but this set is not basis because it is linearly dependent 
the set is not spanning set of since its span is the space of all vectors in whose last component is zero 
that space is also spanned by the set as is linear combination of and 
it does however span 
when interpreted as subset of 
the empty set is spanning set of since the empty set is subset of all possible vector spaces in and is the intersection of all of these vector spaces 
the set of functions xn where is non negative integer spans the space of polynomials 
theorems equivalence of definitions the set of all linear combinations of subset of vector space over is the smallest linear subspace of containing proof 
we first prove that span is subspace of since is subset of we only need to prove the existence of zero vector in span that span is closed under addition and that span is closed under scalar multiplication 
letting it is trivial that the zero vector of exists in span since adding together two linear combinations of also produces linear combination of where all and multiplying linear combination of by scalar will produce another linear combination of thus is subspace of suppose that is linear subspace of containing it follows that span since every vi is linear combination of trivially 
since is closed under addition and scalar multiplication then every linear combination must be contained in thus span is contained in every subspace of containing and the intersection of all such subspaces or the smallest such subspace is equal to the set of all linear combinations of size of spanning set is at least size of linearly independent set every spanning set of vector space must contain at least as many elements as any linearly independent set of vectors from proof 
let be spanning set and be linearly independent set of vectors from we want to show that since spans then must also span and must be linear combination of thus is linearly dependent and we can remove one vector from that is linear combination of the other elements 
this vector cannot be any of the wi since is linearly indepedent 
the resulting set is which is spanning set of we repeat this step times where the resulting set after the pth step is the union of and vectors of it is ensured until the nth step that there will always be some to remove out of for every adjoint of and thus there are at least as many vi as there are wi 
to verify this we assume by way of contradiction that then at the mth step we have the set and we can adjoin another vector but since is spanning set of is linear combination of 
this is contradiction since is linearly independent 
spanning set can be reduced to basis let be finite dimensional vector space 
any set of vectors that spans can be reduced to basis for by discarding vectors if necessary 
if there are linearly dependent vectors in the set 
if the axiom of choice holds this is true without the assumption that has finite dimension 
this also indicates that basis is minimal spanning set when is finite dimensional 
generalizations generalizing the definition of the span of points in space subset of the ground set of matroid is called spanning set if the rank of equals the rank of the entire ground set 
the vector space definition can also be generalized to modules 
given an module and collection of elements an of the submodule of spanned by an is the sum of cyclic modules consisting of all linear combinations of the elements ai 
as with the case of vector spaces the submodule of spanned by any subset of is the intersection of all submodules containing that subset 
closed linear span functional analysis in functional analysis closed linear span of set of vectors is the minimal closed set which contains the linear span of that set 
suppose that is normed vector space and let be any non empty subset of the closed linear span of denoted by sp or span is the intersection of all the closed linear subspaces of which contain one mathematical formulation of this is sp sp 
the closed linear span of the set of functions xn on the interval where is non negative integer depends on the norm used 
if the norm is used then the closed linear span is the hilbert space of square integrable functions on the interval 
but if the maximum norm is used the closed linear span will be the space of continuous functions on the interval 
in either case the closed linear span contains functions that are not polynomials and so are not in the linear span itself 
however the cardinality of the set of functions in the closed linear span is the cardinality of the continuum which is the same cardinality as for the set of polynomials 
notes the linear span of set is dense in the closed linear span 
moreover as stated in the lemma below the closed linear span is indeed the closure of the linear span 
closed linear spans are important when dealing with closed linear subspaces which are themselves highly important see riesz lemma 
useful lemma let be normed space and let be any non empty subset of then so the usual way to find the closed linear span is to find the linear span first and then the closure of that linear span 
see also affine hull conical combination convex hull citations sources textbooks axler sheldon jay 
linear algebra done right rd ed 
linear algebra th ed 
isbn lane saunders mac birkhoff garrett 
advanced linear algebra nd ed 
isbn rynne brian youngson martin 
isbn lay david linear algebra and its applications th edition 
web lankham isaiah nachtergaele bruno schilling anne february 
linear algebra as an introduction to abstract mathematics pdf 
university of california davis 
retrieved september weisstein eric wolfgang 
retrieved feb cs maint url status link linear hull 
april retrieved feb cs maint url status link external links linear combinations and span understanding linear combinations and spans of vectors khanacademy org 
linear combinations span and basis vectors 
essence of linear algebra 
archived from the original on via youtube
in statistics principal component regression pcr is regression analysis technique that is based on principal component analysis pca 
more specifically pcr is used for estimating the unknown regression coefficients in standard linear regression model 
in pcr instead of regressing the dependent variable on the explanatory variables directly the principal components of the explanatory variables are used as regressors 
one typically uses only subset of all the principal components for regression making pcr kind of regularized procedure and also type of shrinkage estimator 
often the principal components with higher variances the ones based on eigenvectors corresponding to the higher eigenvalues of the sample variance covariance matrix of the explanatory variables are selected as regressors 
however for the purpose of predicting the outcome the principal components with low variances may also be important in some cases even more important one major use of pcr lies in overcoming the multicollinearity problem which arises when two or more of the explanatory variables are close to being collinear 
pcr can aptly deal with such situations by excluding some of the low variance principal components in the regression step 
in addition by usually regressing on only subset of all the principal components pcr can result in dimension reduction through substantially lowering the effective number of parameters characterizing the underlying model 
this can be particularly useful in settings with high dimensional covariates 
also through appropriate selection of the principal components to be used for regression pcr can lead to efficient prediction of the outcome based on the assumed model 
the principle the pcr method may be broadly divided into three major steps perform pca on the observed data matrix for the explanatory variables to obtain the principal components and then usually select subset based on some appropriate criteria of the principal components so obtained for further use 
now regress the observed vector of outcomes on the selected principal components as covariates using ordinary least squares regression linear regression to get vector of estimated regression coefficients with dimension equal to the number of selected principal components now transform this vector back to the scale of the actual covariates using the selected pca loadings the eigenvectors corresponding to the selected principal components to get the final pcr estimator with dimension equal to the total number of covariates for estimating the regression coefficients characterizing the original model 
details of the method data representation let denote the vector of observed outcomes and denote the corresponding data matrix of observed covariates where and denote the size of the observed sample and the number of covariates respectively with each of the rows of denotes one set of observations for the dimensional covariate and the respective entry of denotes the corresponding observed outcome 
data pre processing assume that and each of the columns of have already been centered so that all of them have zero empirical means 
this centering step is crucial at least for the columns of since pcr involves the use of pca on and pca is sensitive to centering of the data 
underlying model following centering the standard gauss markov linear regression model for on can be represented as where denotes the unknown parameter vector of regression coefficients and denotes the vector of random errors with and var for some unknown variance parameter objective the primary goal is to obtain an efficient estimator for the parameter based on the data 
one frequently used approach for this is ordinary least squares regression which assuming is full column rank gives the unbiased estimator of pcr is another technique that may be used for the same purpose of estimating pca step pcr starts by performing pca on the centered data matrix for this let denote the singular value decomposition of where diag with denoting the non negative singular values of while the columns of and are both orthonormal sets of vectors denoting the left and right singular vectors of respectively 
the principal components gives spectral decomposition of where diag diag with denoting the non negative eigenvalues also known as the principal values of while the columns of denote the corresponding orthonormal set of eigenvectors 
then and respectively denote the principal component and the principal component direction or pca loading corresponding to the th largest principal value for each 
derived covariates for any let denote the matrix with orthonormal columns consisting of the first columns of let denote the matrix having the first principal components as its columns 
may be viewed as the data matrix obtained by using the transformed covariates instead of using the original covariates the pcr estimator let denote the vector of estimated regression coefficients obtained by ordinary least squares regression of the response vector on the data matrix then for any the final pcr estimator of based on using the first principal components is given by fundamental characteristics and applications of the pcr estimator two basic properties the fitting process for obtaining the pcr estimator involves regressing the response vector on the derived data matrix which has orthogonal columns for any since the principal components are mutually orthogonal to each other 
thus in the regression step performing multiple linear regression jointly on the selected principal components as covariates is equivalent to carrying out independent simple linear regressions or univariate regressions separately on each of the selected principal components as covariate 
when all the principal components are selected for regression so that then the pcr estimator is equivalent to the ordinary least squares estimator 
thus this is easily seen from the fact that and also observing that is an orthogonal matrix 
variance reduction for any the variance of is given by var diag in particular var var hence for all we have var var thus for all we have var var where indicates that square symmetric matrix is non negative definite 
consequently any given linear form of the pcr estimator has lower variance compared to that of the same linear form of the ordinary least squares estimator 
addressing multicollinearity under multicollinearity two or more of the covariates are highly correlated so that one can be linearly predicted from the others with non trivial degree of accuracy 
consequently the columns of the data matrix that correspond to the observations for these covariates tend to become linearly dependent and therefore tends to become rank deficient losing its full column rank structure 
more quantitatively one or more of the smaller eigenvalues of get very close or become exactly equal to under such situations 
the variance expressions above indicate that these small eigenvalues have the maximum inflation effect on the variance of the least squares estimator thereby destabilizing the estimator significantly when they are close to this issue can be effectively addressed through using pcr estimator obtained by excluding the principal components corresponding to these small eigenvalues 
dimension reduction pcr may also be used for performing dimension reduction 
to see this let denote any matrix having orthonormal columns for any 
suppose now that we want to approximate each of the covariate observations through the rank linear transformation for some 
then it can be shown that is minimized at the matrix with the first principal component directions as columns and the corresponding dimensional derived covariates 
thus the dimensional principal components provide the best linear approximation of rank to the observed data matrix the corresponding reconstruction error is given by thus any potential dimension reduction may be achieved by choosing the number of principal components to be used through appropriate thresholding on the cumulative sum of the eigenvalues of since the smaller eigenvalues do not contribute significantly to the cumulative sum the corresponding principal components may be continued to be dropped as long as the desired threshold limit is not exceeded 
the same criteria may also be used for addressing the multicollinearity issue whereby the principal components corresponding to the smaller eigenvalues may be ignored as long as the threshold limit is maintained 
regularization effect since the pcr estimator typically uses only subset of all the principal components for regression it can be viewed as some sort of regularized procedure 
more specifically for any the pcr estimator denotes the regularized solution to the following constrained minimization problem min subject to 
the constraint may be equivalently written as where 
thus when only proper subset of all the principal components are selected for regression the pcr estimator so obtained is based on hard form of regularization that constrains the resulting solution to the column space of the selected principal component directions and consequently restricts it to be orthogonal to the excluded directions 
optimality of pcr among class of regularized estimators given the constrained minimization problem as defined above consider the following generalized version of it min subject to where denotes any full column rank matrix of order with let denote the corresponding solution 
thus arg min subject to then the optimal choice of the restriction matrix for which the corresponding estimator achieves the minimum prediction error is given by where diag 
quite clearly the resulting optimal estimator is then simply given by the pcr estimator based on the first principal components 
efficiency since the ordinary least squares estimator is unbiased for we have var mse where mse denotes the mean squared error 
now if for some we additionally have then the corresponding is also unbiased for and therefore var mse 
we have already seen that var var which then implies mse mse for that particular thus in that case the corresponding would be more efficient estimator of compared to based on using the mean squared error as the performance criteria 
in addition any given linear form of the corresponding would also have lower mean squared error compared to that of the same linear form of now suppose that for given then the corresponding is biased for however since var var it is still possible that mse mse especially if is such that the excluded principal components correspond to the smaller eigenvalues thereby resulting in lower bias 
in order to ensure efficient estimation and prediction performance of pcr as an estimator of park proposes the following guideline for selecting the principal components to be used for regression drop the principal component if and only if practical implementation of this guideline of course requires estimates for the unknown model parameters and in general they may be estimated using the unrestricted least squares estimates obtained from the original full model 
park however provides slightly modified set of estimates that may be better suited for this purpose unlike the criteria based on the cumulative sum of the eigenvalues of which is probably more suited for addressing the multicollinearity problem and for performing dimension reduction the above criteria actually attempts to improve the prediction and estimation efficiency of the pcr estimator by involving both the outcome as well as the covariates in the process of selecting the principal components to be used in the regression step 
alternative approaches with similar goals include selection of the principal components based on cross validation or the mallow cp criteria 
often the principal components are also selected based on their degree of association with the outcome 
shrinkage effect of pcr in general pcr is essentially shrinkage estimator that usually retains the high variance principal components corresponding to the higher eigenvalues of as covariates in the model and discards the remaining low variance components corresponding to the lower eigenvalues of 
thus it exerts discrete shrinkage effect on the low variance components nullifying their contribution completely in the original model 
in contrast the ridge regression estimator exerts smooth shrinkage effect through the regularization parameter or the tuning parameter inherently involved in its construction 
while it does not completely discard any of the components it exerts shrinkage effect over all of them in continuous manner so that the extent of shrinkage is higher for the low variance components and lower for the high variance components 
frank and friedman conclude that for the purpose of prediction itself the ridge estimator owing to its smooth shrinkage effect is perhaps better choice compared to the pcr estimator having discrete shrinkage effect 
in addition the principal components are obtained from the eigen decomposition of that involves the observations for the explanatory variables only 
therefore the resulting pcr estimator obtained from using these principal components as covariates need not necessarily have satisfactory predictive performance for the outcome 
somewhat similar estimator that tries to address this issue through its very construction is the partial least squares pls estimator 
similar to pcr pls also uses derived covariates of lower dimensions 
however unlike pcr the derived covariates for pls are obtained based on using both the outcome as well as the covariates 
while pcr seeks the high variance directions in the space of the covariates pls seeks the directions in the covariate space that are most useful for the prediction of the outcome 
variant of the classical pcr known as the supervised pcr was proposed 
in spirit similar to that of pls it attempts at obtaining derived covariates of lower dimensions based on criterion that involves both the outcome as well as the covariates 
the method starts by performing set of simple linear regressions or univariate regressions wherein the outcome vector is regressed separately on each of the covariates taken one at time 
then for some the first covariates that turn out to be the most correlated with the outcome based on the degree of significance of the corresponding estimated regression coefficients are selected for further use 
conventional pcr as described earlier is then performed but now it is based on only the data matrix corresponding to the observations for the selected covariates 
the number of covariates used and the subsequent number of principal components used are usually selected by cross validation 
generalization to kernel settings the classical pcr method as described above is based on classical pca and considers linear regression model for predicting the outcome based on the covariates 
however it can be easily generalized to kernel machine setting whereby the regression function need not necessarily be linear in the covariates but instead it can belong to the reproducing kernel hilbert space associated with any arbitrary possibly non linear symmetric positive definite kernel 
the linear regression model turns out to be special case of this setting when the kernel function is chosen to be the linear kernel 
in general under the kernel machine setting the vector of covariates is first mapped into high dimensional potentially infinite dimensional feature space characterized by the kernel function chosen 
the mapping so obtained is known as the feature map and each of its coordinates also known as the feature elements corresponds to one feature may be linear or non linear of the covariates 
the regression function is then assumed to be linear combination of these feature elements 
thus the underlying regression model in the kernel machine setting is essentially linear regression model with the understanding that instead of the original set of covariates the predictors are now given by the vector potentially infinite dimensional of feature elements obtained by transforming the actual covariates using the feature map 
however the kernel trick actually enables us to operate in the feature space without ever explicitly computing the feature map 
it turns out that it is only sufficient to compute the pairwise inner products among the feature maps for the observed covariate vectors and these inner products are simply given by the values of the kernel function evaluated at the corresponding pairs of covariate vectors 
the pairwise inner products so obtained may therefore be represented in the form of symmetric non negative definite matrix also known as the kernel matrix 
pcr in the kernel machine setting can now be implemented by first appropriately centering this kernel matrix say with respect to the feature space and then performing kernel pca on the centered kernel matrix say whereby an eigendecomposition of is obtained 
kernel pcr then proceeds by usually selecting subset of all the eigenvectors so obtained and then performing standard linear regression of the outcome vector on these selected eigenvectors 
the eigenvectors to be used for regression are usually selected using cross validation 
the estimated regression coefficients having the same dimension as the number of selected eigenvectors along with the corresponding selected eigenvectors are then used for predicting the outcome for future observation 
in machine learning this technique is also known as spectral regression 
clearly kernel pcr has discrete shrinkage effect on the eigenvectors of quite similar to the discrete shrinkage effect of classical pcr on the principal components as discussed earlier 
however the feature map associated with the chosen kernel could potentially be infinite dimensional and hence the corresponding principal components and principal component directions could be infinite dimensional as well 
therefore these quantities are often practically intractable under the kernel machine setting 
kernel pcr essentially works around this problem by considering an equivalent dual formulation based on using the spectral decomposition of the associated kernel matrix 
under the linear regression model which corresponds to choosing the kernel function as the linear kernel this amounts to considering spectral decomposition of the corresponding kernel matrix and then regressing the outcome vector on selected subset of the eigenvectors of so obtained 
it can be easily shown that this is the same as regressing the outcome vector on the corresponding principal components which are finite dimensional in this case as defined in the context of the classical pcr 
thus for the linear kernel the kernel pcr based on dual formulation is exactly equivalent to the classical pcr based on primal formulation 
however for arbitrary and possibly non linear kernels this primal formulation may become intractable owing to the infinite dimensionality of the associated feature map 
thus classical pcr becomes practically infeasible in that case but kernel pcr based on the dual formulation still remains valid and computationally scalable 
see also principal component analysis partial least squares regression ridge regression canonical correlation deming regression total sum of squares references further reading amemiya takeshi
in probability theory and statistics collection of random variables is independent and identically distributed if each random variable has the same probability distribution as the others and all are mutually independent 
this property is usually abbreviated as iid or iid 
iid was first defined in statistics and finds application in different fields such as data mining and signal processing 
introduction in statistics we commonly deal with random samples 
random sample can be thought of as set of objects that are chosen randomly 
or more formally it sequence of independent identically distributed iid random variables 
in other words the terms random sample and iid are basically one and the same 
in statistics we usually say random sample but in probability it more common to say iid identically distributed means that there are no overall trends the distribution doesn fluctuate and all items in the sample are taken from the same probability distribution 
independent means that the sample items are all independent events 
in other words they aren connected to each other in any way 
in other words knowledge of the value of one variable gives no information about the value of the other and vice versa 
application independent and identically distributed random variables are often used as an assumption which tends to simplify the underlying mathematics 
in practical applications of statistical modeling however the assumption may or may not be realistic the 
assumption is also used in central limit theorem which states that the probability distribution of the sum or average of 
variables with finite variance approaches normal distribution often the 
assumption arises in the context of sequences of random variables 
then independent and identically distributed implies that an element in the sequence is independent of the random variables that came before it 
in this way an 
sequence is different from markov sequence where the probability distribution for the nth random variable is function of the previous random variable in the sequence for first order markov sequence 
sequence does not imply the probabilities for all elements of the sample space or event space must be the same 
for example repeated throws of loaded dice will produce sequence that is despite the outcomes being biased 
definition definition for two random variables suppose that the random variables and are defined to assume values in let and be the cumulative distribution functions of and respectively and denote their joint cumulative distribution function by 
two random variables and are identically distributed if and only if two random variables and are independent if and only if 
see further independence probability theory two random variables 
two random variables and are 
if they are independent and identically distributed 
if and only if definition for more than two random variables the definition extends naturally to more than two random variables 
we say that random variables are 
if they are independent see further independence probability theory more than two random variables and identically distributed 
if and only if where denotes the joint cumulative distribution function of definition for independence in probability theory two events are called independent if and only if and 
in the following ab is short for and 
suppose there are the two events of the experiment if there is possibility 
generally the occurrence of has an effect on the probability of which is called conditional probability and only when the occurrence of has no effect on the occurrence of there is 
note if then are mutually independent which cannot be established with mutually incompatible at the same time that is independence must be compatible and mutual exclusion must be related 
suppose are three events 
if ab bc ac abc are satisfied then the events are independent of each other 
more general definition is there are events an 
if the probabilities of the product events for any events are equal to the product of the probabilities of each event then the events an are independent of each other 
examples example sequence of outcomes of spins of fair or unfair roulette wheel is 
one implication of this is that if the roulette ball lands on red for example times in row the next spin is no more or less likely to be black than on any other spin see the gambler fallacy 
sequence of fair or loaded dice rolls is 
sequence of fair or unfair coin flips is 
in signal processing and image processing the notion of transformation to 
implies two specifications the 
the signal level must be balanced on the time axis 
the signal spectrum must be flattened 
transformed by filtering such as deconvolution to white noise signal 
signal where all frequencies are equally present 
example toss coin times and record how many times does the coin lands on head 
independent each outcome of landing will not affect the other outcome which means the results are independent from each other 
identically distributed if the coin is homogeneous material each time the probability for head is which means the probability is identical for each time 
example roll dice times and record how many time the result is independent each outcome of the dice will not affect the next one which means the results are independent from each other 
identically distributed if the dice is homogeneous material each time the probability for the number is which means the probability is identical for each time 
example choose card from standard deck of cards containing cards then place the card back in the deck 
repeat it for times 
record the number of king appears independent each outcome of the card will not affect the next one which means the results are independent from each other 
identically distributed after drawing one card from it each time the probability for king is which means the probability is identical for each time 
generalizations many results that were first proven under the assumption that the random variables are 
have been shown to be true even under weaker distributional assumption 
exchangeable random variables the most general notion which shares the main properties of 
variables are exchangeable random variables introduced by bruno de finetti 
exchangeability means that while variables may not be independent future ones behave like past ones formally any value of finite sequence is as likely as any permutation of those values the joint probability distribution is invariant under the symmetric group 
this provides useful generalization for example sampling without replacement is not independent but is exchangeable 
vy process in stochastic calculus 
variables are thought of as discrete time vy process each variable gives how much one changes from one time to another 
for example sequence of bernoulli trials is interpreted as the bernoulli process 
one may generalize this to include continuous time vy processes and many vy processes can be seen as limits of 
variables for instance the wiener process is the limit of the bernoulli process 
in machine learning why assume the data in machine learning are independent and identically distributed 
machine learning uses currently acquired massive quantities of data to deliver faster more accurate results 
therefore we need to use historical data with overall representativeness 
if the data obtained is not representative of the overall situation then the rules will be summarized badly or wrongly 
hypothesis the number of individual cases in the training sample can be greatly reduced 
this assumption makes maximization very easy to calculate mathematically 
observing the assumption of independent and identical distribution in mathematics simplifies the calculation of the likelihood function in optimization problems 
because of the assumption of independence the likelihood function can be written like this 
in order to maximize the probability of the observed event take the log function and maximize the parameter that is to say to compute log where log log log log 
log the computer is very efficient to calculate multiple additions but it is not efficient to calculate the multiplication 
this simplification is the core reason for the increase in computational efficiency 
and this log transformation is also in the process of maximizing turning many exponential functions into linear functions 
for two reasons this hypothesis is easy to use the central limit theorem in practical applications 
even if the sample comes from more complex non gaussian distribution it can also approximate well 
because it can be simplified from the central limit theorem to gaussian distribution 
for large number of observable samples the sum of many random variables will have an approximately normal distribution 
the second reason is that the accuracy of the model depends on the simplicity and representative power of the model unit as well as the data quality 
because the simplicity of the unit makes it easy to interpret and scale and the representative power scale out of the unit improves the model accuracy 
like in deep neural network each neuron is very simple but has strong representative power layer by layer to represent more complex features to improve model accuracy 
see also de finetti theorem pairwise independent variables central limit theorem references further reading
in music theory the circle of fifths is way of organizing the chromatic pitches as sequence of perfect fifths 
this is strictly true in the standard tone equal temperament system using different system requires one interval of diminished sixth to be treated as fifth 
if is chosen as starting point the sequence is continuing the pattern from returns the sequence to its starting point of this order places the most closely related key signatures adjacent to one another 
it is usually illustrated in the form of circle 
definition the circle of fifths organizes pitches in sequence of perfect fifths generally shown as circle with the pitches and their corresponding keys in clockwise progression 
musicians and composers often use the circle of fifths to describe the musical relationships between pitches 
its design is helpful in composing and harmonizing melodies building chords and modulating to different keys within composition using the system of just intonation perfect fifth consists of two pitches with frequency ratio of but generating twelve successive perfect fifths in this way does not result in return to the pitch class of the starting note 
to adjust for this instruments are generally tuned with the equal temperament system 
twelve equal temperament fifths lead to note exactly seven octaves above the initial tone this results in perfect fifth that is equivalent to seven equal temperament semitones 
the top of the circle shows the key of major with no sharps or flats 
proceeding clockwise the pitches ascend by fifths 
the key signatures associated with those pitches also change the key of has one sharp the key of has sharps and so on 
similarly proceeding counterclockwise from the top of the circle the notes change by descending fifths and the key signatures change accordingly the key of has one flat the key of has flats and so on 
some keys at the bottom of the circle can be notated either in sharps or in flats 
starting at any pitch and ascending by fifth generates all twelve tones before returning to the beginning pitch class pitch class consists of all of the notes indicated by given letter regardless of octave all for example belong to the same pitch class 
moving counterclockwise the pitches descend by fifth but ascending by perfect fourth will lead to the same note an octave higher therefore in the same pitch class 
moving counter clockwise from could be thought of as descending by fifth to or ascending by fourth to structure and use diatonic key signatures each of the twelve pitches can serve as the tonic of major or minor key and each of these keys will have diatonic scale associated with it 
the circle diagram shows the number of sharps or flats in each key signature with the major key indicated by capital letter and the minor key indicated by lower case letter 
major and minor keys that have the same key signature are referred to as relative major and relative minor of one another 
modulation and chord progression tonal music often modulates to new tonal center whose key signature differs from the original by only one flat or sharp 
these closely related keys are fifth apart from each other and are therefore adjacent in the circle of fifths 
chord progressions also often move between chords whose roots are related by perfect fifth making the circle of fifths useful in illustrating the harmonic distance between chords 
the circle of fifths is used to organize and describe the harmonic function of chords 
chords can progress in pattern of ascending perfect fourths alternately viewed as descending perfect fifths in functional succession 
this can be shown by the circle of fifths in which therefore scale degree ii is closer to the dominant than scale degree iv 
in this view the tonic is considered the end point of chord progression derived from the circle of fifths 
according to richard franko goldman harmony in western music the iv chord is in the simplest mechanisms of diatonic relationships at the greatest distance from in terms of the descending circle of fifths it leads away from rather than toward it 
he states that the progression ii an authentic cadence would feel more final or resolved than iv plagal cadence 
goldman concurs with nattiez who argues that the chord on the fourth degree appears long before the chord on ii and the subsequent final in the progression iv viio iii vi ii and is farther from the tonic there as well 
in this and related articles upper case roman numerals indicate major triads while lower case roman numerals indicate minor triads 
circle closure in non equal tuning systems using the exact ratio of frequencies to define perfect fifth just intonation does not quite result in return to the pitch class of the starting note after going around the circle of fifths 
equal temperament tuning produces fifths that return to tone exactly seven octaves above the initial tone and makes the frequency ratio of each half step the same 
an equal tempered fifth has frequency ratio of or about approximately two cents narrower than justly tuned fifth at ratio of ascending by justly tuned fifths fails to close the circle by an excess of approximately cents roughly quarter of semitone an interval known as the pythagorean comma 
in pythagorean tuning this problem is solved by markedly shortening the width of one of the twelve fifths which makes it severely dissonant 
this anomalous fifth is called the wolf fifth humorous reference to wolf howling an off pitch note 
the quarter comma meantone tuning system uses eleven fifths slightly narrower than the equally tempered fifth and requires much wider and even more dissonant wolf fifth to close the circle 
more complex tuning systems based on just intonation such as limit tuning use at most eight justly tuned fifths and at least three non just fifths some slightly narrower and some slightly wider than the just fifth to close the circle 
other tuning systems use up to tones the original tones and more between them in order to close the circle of fifths 
history the circle of fifths developed in the late and early to theorize the modulation of the baroque era see baroque era 
the first circle of fifths diagram appears in the grammatika of the composer and theorist nikolay diletsky who intended to present music theory as tool for composition 
it was the first of its kind aimed at teaching russian audience how to write western style polyphonic compositions 
circle of fifths diagram was independently created by german composer and theorist johann david heinichen in his neu erfundene und gr ndliche anweisung which he called the musical circle german musicalischer circul 
this was also published in his der general bass in der composition 
heinichen placed the relative minor key next to the major key which did not reflect the actual proximity of keys 
johann mattheson and others attempted to improve this david kellner proposed having the major keys on one circle and the relative minor keys on second inner circle 
this was later developed into chordal space incorporating the parallel minor as well some sources imply that the circle of fifths was known in antiquity by pythagoras 
this is misunderstanding and an anachronism 
tuning by fifths so called pythagorean tuning dates to ancient mesopotamia see music of mesopotamia music theory though they did not extend this to twelve note scale stopping at seven 
the pythagorean comma was calculated by euclid and by chinese mathematicians in the huainanzi see pythagorean comma history 
thus it was known in antiquity that cycle of twelve fifths was almost exactly seven octaves more practically alternating ascending fifths and descending fourths was almost exactly an octave 
however this was theoretical knowledge and was not used to construct repeating twelve tone scale nor to modulate 
this was done later in meantone temperament and twelve tone equal temperament which allowed modulation while still being in tune but did not develop in europe until about 
use in musical pieces from the baroque music era and the classical era of music and in western popular music traditional music and folk music when pieces or songs modulate to new key these modulations are often associated with the circle of fifths 
in practice compositions rarely make use of the entire circle of fifths 
more commonly composers make use of the compositional idea of the cycle of ths when music moves consistently through smaller or larger segment of the tonal structural resources which the circle abstractly represents 
the usual practice is to derive the circle of fifths progression from the seven tones of the diatonic scale rather from the full range of twelve tones present in the chromatic scale 
in this diatonic version of the circle one of the fifths is not true fifth it is tritone or diminished fifth 
between and in the natural diatonic scale 
without sharps or flats 
here is how the circle of fifths derives through permutation from the diatonic major scale and from the natural minor scale the following is the basic sequence of chords that can be built over the major bass line and over the minor adding sevenths to the chords creates greater sense of forward momentum to the harmony baroque era according to richard taruskin arcangelo corelli was the most influential composer to establish the pattern as standard harmonic trope it was precisely in corelli time the late seventeenth century that the circle of fifths was being theorized as the main propellor of harmonic motion and it was corelli more than any one composer who put that new idea into telling practice 
the circle of fifths progression occurs frequently in the music of bach 
in the following from jauchzet gott in allen landen bwv even when the solo bass line implies rather than states the chords involved handel uses circle of fifths progression as the basis for the passacaglia movement from his harpsichord suite no 
baroque composers learnt to enhance the propulsive force of the harmony engendered by the circle of fifths by adding sevenths to most of the constituent chords 
these sevenths being dissonances create the need for resolution thus turning each progression of the circle into simultaneous reliever and re stimulator of harmonic tension hence harnessed for expressive purposes 
striking passages that illustrate the use of sevenths occur in the aria pena tiranna in handel opera amadigi di gaula and in bach keyboard arrangement of alessandro marcello concerto for oboe and strings 
nineteenth century during the nineteenth century composers made use of the circle of fifths to enhance the expressive character of their music 
franz schubert poignant impromptu in flat major contains such passage as does the intermezzo movement from mendelssohn string quartet no robert schumann evocative child falling asleep from his kinderszenen springs surprise at the end of the progression the piece ends on an minor chord instead of the expected tonic minor 
in wagner opera tterd mmerung cycle of fifths progression occurs in the music which transitions from the end of the prologue into the first scene of act set in the imposing hall of the wealthy gibichungs 
status and reputation are written all over the motifs assigned to gunther chief of the gibichung clan jazz and popular music the enduring popularity of the circle of fifths as both form building device and as an expressive musical trope is evident in the number of standard popular songs composed during the twentieth century 
it is also favored as vehicle for improvisation by jazz musicians 
bart howard fly me to the moon the song opens with pattern of descending phrases in essence the hook of the song presented with soothing predictability almost as if the future direction of the melody is dictated by the opening five notes 
the harmonic progression for its part rarely departs from the circle of fifths 
jerome kern all the things you are ray noble cherokee 
many jazz musicians have found this particularly challenging as the middle eight progresses so rapidly through the circle creating series of ii progressions that temporarily pass through several tonalities 
kosmo prevert and mercer autumn leaves the beatles you never give me your money mike oldfield incantations carlos santana europa earth cry heaven smile gloria gaynor will survive pet shop boys it sin donna summer love to love you baby related concepts diatonic circle of fifths the diatonic circle of fifths is the circle of fifths encompassing only members of the diatonic scale 
therefore it contains diminished fifth in major between and see structure implies multiplicity 
the circle progression is commonly circle of fifths through the diatonic chords including one diminished chord 
circle progression in major with chords iv viio iii vi ii is shown below 
chromatic circle the circle of fifths is closely related to the chromatic circle which also arranges the twelve equal tempered pitch classes in circular ordering 
key difference between the two circles is that the chromatic circle can be understood as continuous space where every point on the circle corresponds to conceivable pitch class and every conceivable pitch class corresponds to point on the circle 
by contrast the circle of fifths is fundamentally discrete structure and there is no obvious way to assign pitch classes to each of its points 
in this sense the two circles are mathematically quite different 
however the twelve equal tempered pitch classes can be represented by the cyclic group of order twelve or equivalently the residue classes modulo twelve the group has four generators which can be identified with the ascending and descending semitones and the ascending and descending perfect fifths 
the semitonal generator gives rise to the chromatic circle while the perfect fifth gives rise to the circle of fifths 
relation with chromatic scale the circle of fifths or fourths may be mapped from the chromatic scale by multiplication and vice versa 
to map between the circle of fifths and the chromatic scale in integer notation multiply by and for the circle of fourths multiply by 
here is demonstration of this procedure 
start off with an ordered tuple tone row of integers representing the notes of the chromatic scale 
now multiply the entire tuple by and then apply modulo reduction to each of the numbers subtract from each number as many times as necessary until the number becomes smaller than which is equivalent to which is the circle of fifths 
note that this is enharmonically equivalent to 
enharmonic equivalents theoretical keys and the spiral of fifths equal temperament tuning does not use the exact ratio of frequencies that defines perfect fifth wheras the system of just intonation uses this exact ratio 
ascending by fifths in equal temperament leads to return to the starting pitch class starting with and ascending by fifths leads to another after twelve iterations 
this does not occur if an exact ratio is used just intonation 
the adjustment made in equal temperament tuning is called the pythagorean comma 
because of this difference pitches that are enharmonically equivalent in equal temperament tuning and are not equivalent when using just intonation 
in just intonation the sequence of fifths can therefore be visualized as spiral not circle sequence of twelve fifths results in comma pump by the pythagorean comma visualized as going up level in the spiral 
see also circle closure in non equal tuning systems 
without enharmonic equivalence continuing sequence of fifths results in notes with double accidentals double sharps or double flats 
when using equal temperament these can be replaced by an enharmonically equivalent note 
keys with double sharps or flats in the key signatures are called theoretical keys their use is extremely rare 
notation in these cases is not standardized 
the default behaviour of lilypond pictured above writes single sharps or flats in the circle of fifths order before proceeding to double sharps or flats 
this is the format used in john foulds world requiem op 
which ends with the key signature of major as displayed above 
the sharps in the key signature of major here proceed single sharps or flats in the key signature are sometimes repeated as courtesy 
max reger supplement to the theory of modulation which contains minor key signatures on pp 
these have at the start and also at the end with double flat symbol going the convention of lilypond and foulds would suppress the initial 
sometimes the double signs are written at the beginning of the key signature followed by the single signs 
for example the key signature is notated as 
this convention is used by victor ewald by the program finale software and by some theoretical works 
see also approach chord sonata form well temperament circle of fifths text table pitch constellation multiplicative group of integers modulo notes references barnett gregory 
tonal organization in seventeenth century music theory 
in thomas christensen ed 
the cambridge history of western music theory 
cambridge cambridge university press 
the jazz standards guide to the repertoire 
isbn goldman richard franko 
harmony in western music 
theoretical work of late seventeenth century muscovy nikolai diletskii grammatika and the earliest circle of fifths 
journal of the american musicological society 
between modes and keys german theory 
prelude to musical geometry 
the college mathematics journal 
jstor archived from the original on retrieved nattiez jean jacques 
music and discourse toward semiology of music translated by carolyn abbate 
princeton new jersey princeton university press 
originally published in french as musicologie rale et miologie 
the oxford history of western music music in the seventeenth and eighteenth centuries 
further reading indy vincent 
cours de composition musicale 
paris durand et fils 
between modes and keys german theory 
the complete idiot guide to music theory nd ed 
indianapolis in alpha isbn purwins hendrik 
profiles of pitch classes circularity of relative pitch and key experiments models computational music analysis and perspectives 
berlin technische universit berlin 
purwins hendrik benjamin blankertz and klaus obermayer 
toroidal models in tonal theory and pitch class analysis 
in computing in musicology tonal theory for the digital age 
external links decoding the circle of vths interactive circle of fifths interactive circle of fifths for guitarists
principal component analysis pca is popular technique for analyzing large datasets containing high number of dimensions features per observation increasing the interpretability of data while preserving the maximum amount of information and enabling the visualization of multidimensional data 
formally pca is statistical technique for reducing the dimensionality of dataset 
this is accomplished by linearly transforming the data into new coordinate system where most of the variation in the data can be described with fewer dimensions than the initial data 
many studies use the first two principal components in order to plot the data in two dimensions and to visually identify clusters of closely related data points 
principal component analysis has applications in many fields such as population genetics microbiome studies and atmospheric science 
the principal components of collection of points in real coordinate space are sequence of unit vectors where the th vector is the direction of line that best fits the data while being orthogonal to the first vectors 
here best fitting line is defined as one that minimizes the average squared perpendicular distance from the points to the line 
these directions constitute an orthonormal basis in which different individual dimensions of the data are linearly uncorrelated 
principal component analysis pca is the process of computing the principal components and using them to perform change of basis on the data sometimes using only the first few principal components and ignoring the rest 
in data analysis the first principal component of set of variables presumed to be jointly normally distributed is the derived variable formed as linear combination of the original variables that explains the most variance 
the second principal component explains the most variance in what is left once the effect of the first component is removed and we may proceed through iterations until all the variance is explained 
pca is most commonly used when many of the variables are highly correlated with each other and it is desirable to reduce their number to an independent set 
pca is used in exploratory data analysis and for making predictive models 
it is commonly used for dimensionality reduction by projecting each data point onto only the first few principal components to obtain lower dimensional data while preserving as much of the data variation as possible 
the first principal component can equivalently be defined as direction that maximizes the variance of the projected data 
the th principal component can be taken as direction orthogonal to the first principal components that maximizes the variance of the projected data 
for either objective it can be shown that the principal components are eigenvectors of the data covariance matrix 
thus the principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix 
pca is the simplest of the true eigenvector based multivariate analyses and is closely related to factor analysis 
factor analysis typically incorporates more domain specific assumptions about the underlying structure and solves eigenvectors of slightly different matrix 
pca is also related to canonical correlation analysis cca 
cca defines coordinate systems that optimally describe the cross covariance between two datasets while pca defines new orthogonal coordinate system that optimally describes variance in single dataset 
robust and norm based variants of standard pca have also been proposed 
history pca was invented in by karl pearson as an analogue of the principal axis theorem in mechanics it was later independently developed and named by harold hotelling in the 
depending on the field of application it is also named the discrete karhunen lo ve transform klt in signal processing the hotelling transform in multivariate quality control proper orthogonal decomposition pod in mechanical engineering singular value decomposition svd of invented in the last quarter of the th century eigenvalue decomposition evd of xtx in linear algebra factor analysis for discussion of the differences between pca and factor analysis see ch 
of jolliffe principal component analysis eckart young theorem harman or empirical orthogonal functions eof in meteorological science empirical eigenfunction decomposition sirovich empirical component analysis lorenz quasiharmonic modes brooks et al spectral decomposition in noise and vibration and empirical modal analysis in structural dynamics 
intuition pca can be thought of as fitting dimensional ellipsoid to the data where each axis of the ellipsoid represents principal component 
if some axis of the ellipsoid is small then the variance along that axis is also small 
to find the axes of the ellipsoid we must first center the values of each variable in the dataset on by subtracting the mean of the variable observed values from each of those values 
these transformed values are used instead of the original observed values for each of the variables 
then we compute the covariance matrix of the data and calculate the eigenvalues and corresponding eigenvectors of this covariance matrix 
then we must normalize each of the orthogonal eigenvectors to turn them into unit vectors 
once this is done each of the mutually orthogonal unit eigenvectors can be interpreted as an axis of the ellipsoid fitted to the data 
this choice of basis will transform the covariance matrix into diagonalized form in which the diagonal elements represent the variance of each axis 
the proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues 
biplots and scree plots degree of explained variance are used to explain findings of the pca 
details pca is defined as an orthogonal linear transformation that transforms the data to new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate called the first principal component the second greatest variance on the second coordinate and so on consider an data matrix with column wise zero empirical mean the sample mean of each column has been shifted to zero where each of the rows represents different repetition of the experiment and each of the columns gives particular kind of feature say the results from particular sensor 
mathematically the transformation is defined by set of size of dimensional vectors of weights or coefficients that map each row vector of to new vector of principal component scores given by in such way that the individual variables of considered over the data set successively inherit the maximum possible variance from with each coefficient vector constrained to be unit vector where is usually selected to be strictly less than to reduce dimensionality 
first component in order to maximize variance the first weight vector thus has to satisfy arg max arg max equivalently writing this in matrix form gives arg max arg max since has been defined to be unit vector it equivalently also satisfies arg max the quantity to be maximised can be recognised as rayleigh quotient 
standard result for positive semidefinite matrix such as xtx is that the quotient maximum possible value is the largest eigenvalue of the matrix which occurs when is the corresponding eigenvector 
with found the first principal component of data vector can then be given as score in the transformed co ordinates or as the corresponding vector in the original variables 
further components the th component can be found by subtracting the first principal components from and then finding the weight vector which extracts the maximum variance from this new data matrix arg max it turns out that this gives the remaining eigenvectors of xtx with the maximum values for the quantity in brackets given by their corresponding eigenvalues 
thus the weight vectors are eigenvectors of xtx 
the th principal component of data vector can therefore be given as score tk in the transformed coordinates or as the corresponding vector in the space of the original variables where is the kth eigenvector of xtx 
the full principal components decomposition of can therefore be given as where is by matrix of weights whose columns are the eigenvectors of xtx 
the transpose of is sometimes called the whitening or sphering transformation 
columns of multiplied by the square root of corresponding eigenvalues that is eigenvectors scaled up by the variances are called loadings in pca or in factor analysis 
covariances xtx itself can be recognized as proportional to the empirical sample covariance matrix of the dataset xt 
the sample covariance between two of the different principal components over the dataset is given by where the eigenvalue property of has been used to move from line to line however eigenvectors and corresponding to eigenvalues of symmetric matrix are orthogonal if the eigenvalues are different or can be orthogonalised if the vectors happen to share an equal repeated value 
the product in the final line is therefore zero there is no sample covariance between different principal components over the dataset 
another way to characterise the principal components transformation is therefore as the transformation to coordinates which diagonalise the empirical sample covariance matrix 
in matrix form the empirical covariance matrix for the original variables can be written the empirical covariance matrix between the principal components becomes where is the diagonal matrix of eigenvalues of xtx 
is equal to the sum of the squares over the dataset associated with each component that is tk 
dimensionality reduction the transformation maps data vector from an original space of variables to new space of variables which are uncorrelated over the dataset 
however not all the principal components need to be kept 
keeping only the first principal components produced by using only the first eigenvectors gives the truncated transformation where the matrix tl now has rows but only columns 
in other words pca learns linear transformation where the columns of matrix form an orthogonal basis for the features the components of representation that are decorrelated 
by construction of all the transformed data matrices with only columns this score matrix maximises the variance in the original data that has been preserved while minimising the total squared reconstruction error or such dimensionality reduction can be very useful step for visualising and processing high dimensional datasets while still retaining as much of the variance in the dataset as possible 
for example selecting and keeping only the first two principal components finds the two dimensional plane through the high dimensional dataset in which the data is most spread out so if the data contains clusters these too may be most spread out and therefore most visible to be plotted out in two dimensional diagram whereas if two directions through the data or two of the original variables are chosen at random the clusters may be much less spread apart from each other and may in fact be much more likely to substantially overlay each other making them indistinguishable 
similarly in regression analysis the larger the number of explanatory variables allowed the greater is the chance of overfitting the model producing conclusions that fail to generalise to other datasets 
one approach especially when there are strong correlations between different possible explanatory variables is to reduce them to few principal components and then run the regression against them method called principal component regression 
dimensionality reduction may also be appropriate when the variables in dataset are noisy 
if each column of the dataset contains independent identically distributed gaussian noise then the columns of will also contain similarly identically distributed gaussian noise such distribution is invariant under the effects of the matrix which can be thought of as high dimensional rotation of the co ordinate axes 
however with more of the total variance concentrated in the first few principal components compared to the same noise variance the proportionate effect of the noise is less the first few components achieve higher signal to noise ratio 
pca thus can have the effect of concentrating much of the signal into the first few principal components which can usefully be captured by dimensionality reduction while the later principal components may be dominated by noise and so disposed of without great loss 
if the dataset is not too large the significance of the principal components can be tested using parametric bootstrap as an aid in determining how many principal components to retain 
singular value decomposition the principal components transformation can also be associated with another matrix factorization the singular value decomposition svd of here is an by rectangular diagonal matrix of positive numbers called the singular values of is an by matrix the columns of which are orthogonal unit vectors of length called the left singular vectors of and is by whose columns are orthogonal unit vectors of length and called the right singular vectors of in terms of this factorization the matrix xtx can be written where is the square diagonal matrix with the singular values of and the excess zeros chopped off that satisfies comparison with the eigenvector factorization of xtx establishes that the right singular vectors of are equivalent to the eigenvectors of xtx while the singular values of are equal to the square root of the eigenvalues of xtx 
using the singular value decomposition the score matrix can be written so each column of is given by one of the left singular vectors of multiplied by the corresponding singular value 
this form is also the polar decomposition of efficient algorithms exist to calculate the svd of without having to form the matrix xtx so computing the svd is now the standard way to calculate principal components analysis from data matrix unless only handful of components are required 
as with the eigen decomposition truncated score matrix tl can be obtained by considering only the first largest singular values and their singular vectors the truncation of matrix or using truncated singular value decomposition in this way produces truncated matrix that is the nearest possible matrix of rank to the original matrix in the sense of the difference between the two having the smallest possible frobenius norm result known as the eckart young theorem 
further considerations the singular values in are the square roots of the eigenvalues of the matrix xtx 
each eigenvalue is proportional to the portion of the variance more correctly of the sum of the squared distances of the points from their multidimensional mean that is associated with each eigenvector 
the sum of all the eigenvalues is equal to the sum of the squared distances of the points from their multidimensional mean 
pca essentially rotates the set of points around their mean in order to align with the principal components 
this moves as much of the variance as possible using an orthogonal transformation into the first few dimensions 
the values in the remaining dimensions therefore tend to be small and may be dropped with minimal loss of information see below 
pca is often used in this manner for dimensionality reduction 
pca has the distinction of being the optimal orthogonal transformation for keeping the subspace that has largest variance as defined above 
this advantage however comes at the price of greater computational requirements if compared for example and when applicable to the discrete cosine transform and in particular to the dct ii which is simply known as the dct 
nonlinear dimensionality reduction techniques tend to be more computationally demanding than pca 
pca is sensitive to the scaling of the variables 
if we have just two variables and they have the same sample variance and are completely correlated then the pca will entail rotation by and the weights they are the cosines of rotation for the two variables with respect to the principal component will be equal 
but if we multiply all values of the first variable by then the first principal component will be almost the same as that variable with small contribution from the other variable whereas the second component will be almost aligned with the second original variable 
this means that whenever the different variables have different units like temperature and mass pca is somewhat arbitrary method of analysis 
different results would be obtained if one used fahrenheit rather than celsius for example 
pearson original paper was entitled on lines and planes of closest fit to systems of points in space in space implies physical euclidean space where such concerns do not arise 
one way of making the pca less arbitrary is to use variables scaled so as to have unit variance by standardizing the data and hence use the autocorrelation matrix instead of the autocovariance matrix as basis for pca 
however this compresses or expands the fluctuations in all dimensions of the signal space to unit variance 
mean centering is necessary for performing classical pca to ensure that the first principal component describes the direction of maximum variance 
if mean subtraction is not performed the first principal component might instead correspond more or less to the mean of the data 
mean of zero is needed for finding basis that minimizes the mean square error of the approximation of the data mean centering is unnecessary if performing principal components analysis on correlation matrix as the data are already centered after calculating correlations 
correlations are derived from the cross product of two standard scores scores or statistical moments hence the name pearson product moment correlation 
also see the article by kromrey foster johnson on mean centering in moderated regression much ado about nothing 
since covariances are correlations of normalized variables or standard scores pca based on the correlation matrix of is equal to pca based on the covariance matrix of the standardized version of pca is popular primary technique in pattern recognition 
it is not however optimized for class separability 
however it has been used to quantify the distance between two or more classes by calculating center of mass for each class in principal component space and reporting euclidean distance between center of mass of two or more classes 
the linear discriminant analysis is an alternative which is optimized for class separability 
table of symbols and abbreviations properties and limitations of pca properties some properties of pca include property for any integer consider the orthogonal linear transformation where is element vector and is matrix and let be the variance covariance matrix for then the trace of denoted tr is maximized by taking where consists of the first columns of is the transpose of property consider again the orthonormal transformation with and defined as before 
then tr is minimized by taking where consists of the last columns of the statistical implication of this property is that the last few pcs are not simply unstructured left overs after removing the important pcs 
because these last pcs have variances as small as possible they are useful in their own right 
they can help to detect unsuspected near constant linear relationships between the elements of and they may also be useful in regression in selecting subset of variables from and in outlier detection 
property spectral decomposition of before we look at its usage we first look at diagonal elements var then perhaps the main statistical implication of the result is that not only can we decompose the combined variances of all the elements of into decreasing contributions due to each pc but we can also decompose the whole covariance matrix into contributions from each pc 
although not strictly decreasing the elements of will tend to become smaller as increases as is nonincreasing for increasing whereas the elements of tend to stay about the same size because of the normalization constraints limitations as noted above the results of pca depend on the scaling of the variables 
this can be cured by scaling each feature by its standard deviation so that one ends up with dimensionless features with unital variance the applicability of pca as described above is limited by certain tacit assumptions made in its derivation 
in particular pca can capture linear correlations between the features but fails when this assumption is violated see figure in the reference 
in some cases coordinate transformations can restore the linearity assumption and pca can then be applied see kernel pca 
another limitation is the mean removal process before constructing the covariance matrix for pca 
in fields such as astronomy all the signals are non negative and the mean removal process will force the mean of some astrophysical exposures to be zero which consequently creates unphysical negative fluxes and forward modeling has to be performed to recover the true magnitude of the signals 
as an alternative method non negative matrix factorization focusing only on the non negative elements in the matrices which is well suited for astrophysical observations 
see more at relation between pca and non negative matrix factorization 
pca is at disadvantage if the data has not been standardized before applying the algorithm to it 
pca transforms original data into data that is relevant to the principal components of that data which means that the new data variables cannot be interpreted in the same ways that the originals were 
they are linear interpretations of the original variables 
also if pca is not performed properly there is high likelihood of information loss pca relies on linear model 
if dataset has pattern hidden inside it that is nonlinear then pca can actually steer the analysis in the complete opposite direction of progress 
researchers at kansas state university discovered that the sampling error in their experiments impacted the bias of pca results 
if the number of subjects or blocks is smaller than and or the researcher is interested in pc beyond the first it may be better to first correct for the serial correlation before pca is conducted 
the researchers at kansas state also found that pca could be seriously biased if the autocorrelation structure of the data is not correctly handled 
pca and information theory dimensionality reduction results in loss of information in general 
pca based dimensionality reduction tends to minimize that information loss under certain signal and noise models 
under the assumption that that is that the data vector is the sum of the desired information bearing signal and noise signal one can show that pca can be optimal for dimensionality reduction from an information theoretic point of view 
in particular linsker showed that if is gaussian and is gaussian noise with covariance matrix proportional to the identity matrix the pca maximizes the mutual information between the desired information and the dimensionality reduced output if the noise is still gaussian and has covariance matrix proportional to the identity matrix that is the components of the vector are iid but the information bearing signal is non gaussian which is common scenario pca at least minimizes an upper bound on the information loss which is defined as 
the optimality of pca is also preserved if the noise is iid and at least more gaussian in terms of the kullback leibler divergence than the information bearing signal in general even if the above signal model holds pca loses its information theoretic optimality as soon as the noise becomes dependent 
computing pca using the covariance method the following is detailed description of pca using the covariance method see also here as opposed to the correlation method the goal is to transform given data set of dimension to an alternative data set of smaller dimension equivalently we are seeking to find the matrix where is the karhunen lo ve transform klt of matrix organize the data setsuppose you have data comprising set of observations of variables and you want to reduce the data so that each observation can be described with only variables suppose further that the data are arranged as set of data vectors with each representing single grouped observation of the variables 
write as row vectors each with elements 
place the row vectors into single matrix of dimensions calculate the empirical meanfind the empirical mean along each column place the calculated mean values into an empirical mean vector of dimensions calculate the deviations from the meanmean subtraction is an integral part of the solution towards finding principal component basis that minimizes the mean square error of approximating the data 
hence we proceed by centering the data as follows subtract the empirical mean vector from each row of the data matrix store mean subtracted data in the matrix where is an column vector of all for in some applications each variable column of may also be scaled to have variance equal to see score 
this step affects the calculated principal components but makes them independent of the units used to measure the different variables 
find the covariance matrixfind the empirical covariance matrix from matrix where is the conjugate transpose operator 
if consists entirely of real numbers which is the case in many applications the conjugate transpose is the same as the regular transpose 
the reasoning behind using instead of to calculate the covariance is bessel correction find the eigenvectors and eigenvalues of the covariance matrixcompute the matrix of eigenvectors which diagonalizes the covariance matrix where is the diagonal matrix of eigenvalues of this step will typically involve the use of computer based algorithm for computing eigenvectors and eigenvalues 
these algorithms are readily available as sub components of most matrix algebra systems such as sas matlab mathematica scipy idl interactive data language or gnu octave as well as opencv 
matrix will take the form of an diagonal matrix where is the jth eigenvalue of the covariance matrix and matrix also of dimension contains column vectors each of length which represent the eigenvectors of the covariance matrix the eigenvalues and eigenvectors are ordered and paired 
the jth eigenvalue corresponds to the jth eigenvector 
matrix denotes the matrix of right eigenvectors as opposed to left eigenvectors 
in general the matrix of right eigenvectors need not be the conjugate transpose of the matrix of left eigenvectors rearrange the eigenvectors and eigenvaluessort the columns of the eigenvector matrix and eigenvalue matrix in order of decreasing eigenvalue 
make sure to maintain the correct pairings between the columns in each matrix compute the cumulative energy content for each eigenvectorthe eigenvalues represent the distribution of the source data energy among each of the eigenvectors where the eigenvectors form basis for the data 
the cumulative energy content for the jth eigenvector is the sum of the energy content across all of the eigenvalues from through for select subset of the eigenvectors as basis vectorssave the first columns of as the matrix where use the vector as guide in choosing an appropriate value for the goal is to choose value of as small as possible while achieving reasonably high value of on percentage basis 
for example you may want to choose so that the cumulative energy is above certain threshold like percent 
in this case choose the smallest value of such that project the data onto the new basisthe projected data points are the rows of the matrix that is the first column of is the projection of the data points onto the first principal component the second column is the projection onto the second principal component etc 
derivation of pca using the covariance method let be dimensional random vector expressed as column vector 
without loss of generality assume has zero mean 
we want to find orthonormal transformation matrix so that px has diagonal covariance matrix that is px is random vector with all its distinct components pairwise uncorrelated 
quick computation assuming were unitary yields cov cov hence holds if and only if cov were diagonalisable by this is very constructive as cov is guaranteed to be non negative definite matrix and thus is guaranteed to be diagonalisable by some unitary matrix 
covariance free computation in practical implementations especially with high dimensional data large the naive covariance method is rarely used because it is not efficient due to high computational and memory costs of explicitly determining the covariance matrix 
the covariance free approach avoids the np operations of explicitly calculating and storing the covariance matrix xtx instead utilizing one of matrix free methods for example based on the function evaluating the product xt at the cost of np operations 
iterative computation one way to compute the first principal component efficiently is shown in the following pseudo code for data matrix with zero mean without ever computing its covariance matrix 
random vector of length norm do times vector of length for each row in rts is the eigenvalue error norm exit if error tolerance return this power iteration algorithm simply calculates the vector xt normalizes and places the result back in the eigenvalue is approximated by rt xtx which is the rayleigh quotient on the unit vector for the covariance matrix xtx 
if the largest singular value is well separated from the next largest one the vector gets close to the first principal component of within the number of iterations which is small relative to at the total cost cnp 
the power iteration convergence can be accelerated without noticeably sacrificing the small cost per iteration using more advanced matrix free methods such as the lanczos algorithm or the locally optimal block preconditioned conjugate gradient lobpcg method 
subsequent principal components can be computed one by one via deflation or simultaneously as block 
in the former approach imprecisions in already computed approximate principal components additively affect the accuracy of the subsequently computed principal components thus increasing the error with every new computation 
the latter approach in the block power method replaces single vectors and with block vectors matrices and every column of approximates one of the leading principal components while all columns are iterated simultaneously 
the main calculation is evaluation of the product xt 
implemented for example in lobpcg efficient blocking eliminates the accumulation of the errors allows using high level blas matrix matrix product functions and typically leads to faster convergence compared to the single vector one by one technique 
the nipals method non linear iterative partial least squares nipals is variant the classical power iteration with matrix deflation by subtraction implemented for computing the first few components in principal component or partial least squares analysis 
for very high dimensional datasets such as those generated in the omics sciences for example genomics metabolomics it is usually only necessary to compute the first few pcs 
the non linear iterative partial least squares nipals algorithm updates iterative approximations to the leading scores and loadings and by the power iteration multiplying on every iteration by on the left and on the right that is calculation of the covariance matrix is avoided just as in the matrix free implementation of the power iterations to xtx based on the function evaluating the product xt tx the matrix deflation by subtraction is performed by subtracting the outer product from leaving the deflated residual matrix used to calculate the subsequent leading pcs 
for large data matrices or matrices that have high degree of column collinearity nipals suffers from loss of orthogonality of pcs due to machine precision round off errors accumulated in each iteration and matrix deflation by subtraction 
gram schmidt re orthogonalization algorithm is applied to both the scores and the loadings at each iteration step to eliminate this loss of orthogonality 
nipals reliance on single vector multiplications cannot take advantage of high level blas and results in slow convergence for clustered leading singular values both these deficiencies are resolved in more sophisticated matrix free block solvers such as the locally optimal block preconditioned conjugate gradient lobpcg method 
online sequential estimation in an online or streaming situation with data arriving piece by piece rather than being stored in single batch it is useful to make an estimate of the pca projection that can be updated sequentially 
this can be done efficiently but requires different algorithms 
pca and qualitative variables in pca it is common that we want to introduce qualitative variables as supplementary elements 
for example many quantitative variables have been measured on plants 
for these plants some qualitative variables are available as for example the species to which the plant belongs 
these data were subjected to pca for quantitative variables 
when analyzing the results it is natural to connect the principal components to the qualitative variable species 
for this the following results are produced 
identification on the factorial planes of the different species for example using different colors 
representation on the factorial planes of the centers of gravity of plants belonging to the same species 
for each center of gravity and each axis value to judge the significance of the difference between the center of gravity and origin these results are what is called introducing qualitative variable as supplementary element 
this procedure is detailed in and husson pag and pag few software offer this option in an automatic way 
this is the case of spad that historically following the work of ludovic lebart was the first to propose this option and the package factominer 
applications intelligence the earliest application of factor analysis was in locating and measuring components of human intelligence 
it was believed that intelligence had various uncorrelated components such as spatial intelligence verbal intelligence induction deduction etc and that scores on these could be adduced by factor analysis from results on various tests to give single index known as the intelligence quotient iq 
the pioneering statistical psychologist spearman actually developed factor analysis in for his two factor theory of intelligence adding formal technique to the science of psychometrics 
in thurstone looked for factors of intelligence developing the notion of mental age 
standard iq tests today are based on this early work 
residential differentiation in shevky and williams introduced the theory of factorial ecology which dominated studies of residential differentiation from the to the 
neighbourhoods in city were recognizable or could be distinguished from one another by various characteristics which could be reduced to three by factor analysis 
these were known as social rank an index of occupational status familism or family size and ethnicity cluster analysis could then be applied to divide the city into clusters or precincts according to values of the three key factor variables 
an extensive literature developed around factorial ecology in urban geography but the approach went out of fashion after as being methodologically primitive and having little place in postmodern geographical paradigms 
one of the problems with factor analysis has always been finding convincing names for the various artificial factors 
in flood revived the factorial ecology approach to show that principal components analysis actually gave meaningful answers directly without resorting to factor rotation 
the principal components were actually dual variables or shadow prices of forces pushing people together or apart in cities 
the first component was accessibility the classic trade off between demand for travel and demand for space around which classical urban economics is based 
the next two components were disadvantage which keeps people of similar status in separate neighbourhoods mediated by planning and ethnicity where people of similar ethnic backgrounds try to co locate about the same time the australian bureau of statistics defined distinct indexes of advantage and disadvantage taking the first principal component of sets of key variables that were thought to be important 
these seifa indexes are regularly published for various jurisdictions and are used frequently in spatial analysis 
development indexes pca has been the only formal method available for the development of indexes which are otherwise hit or miss ad hoc undertaking 
the city development index was developed by pca from about indicators of city outcomes in survey of global cities 
the first principal component was subject to iterative regression adding the original variables singly until about of its variation was accounted for 
the index ultimately used about indicators but was good predictor of many more variables 
its comparative value agreed very well with subjective assessment of the condition of each city 
the coefficients on items of infrastructure were roughly proportional to the average costs of providing the underlying services suggesting the index was actually measure of effective physical and social investment in the city 
the country level human development index hdi from undp which has been published since and is very extensively used in development studies has very similar coefficients on similar indicators strongly suggesting it was originally constructed using pca 
population genetics in cavalli sforza and others pioneered the use of principal components analysis pca to summarise data on variation in human gene frequencies across regions 
the components showed distinctive patterns including gradients and sinusoidal waves 
they interpreted these patterns as resulting from specific ancient migration events 
since then pca has been ubiquitous in population genetics with thousands of papers using pca as display mechanism 
genetics varies largely according to proximity so the first two principal components actually show spatial distribution and may be used to map the relative geographical location of different population groups thereby showing individuals who have wandered from their original locations pca in genetics has been technically controversial in that the technique has been performed on discrete non normal variables and often on binary allele markers 
the lack of any measures of standard error in pca are also an impediment to more consistent usage 
in august the molecular biologist eran elhaik published theoretical paper in scientific reports analyzing pca applications 
he concluded that it was easy to manipulate the method which in his view generated results that were erroneous contradictory and absurd 
specifically he argued the results achieved in population genetics were characterized by cherry picking and circular reasoning 
market research and indexes of attitude market research has been an extensive user of pca 
it is used to develop customer satisfaction or customer loyalty scores for products and with clustering to develop market segments that may be targeted with advertising campaigns in much the same way as factorial ecology will locate geographical areas with similar characteristics pca rapidly transforms large amounts of data into smaller easier to digest variables that can be more rapidly and readily analyzed 
in any consumer questionnaire there are series of questions designed to elicit consumer attitudes and principal components seek out latent variables underlying these attitudes 
for example the oxford internet survey in asked people about their attitudes and beliefs and from these analysts extracted four principal component dimensions which they identified as escape social networking efficiency and problem creating another example from joe flood in extracted an attitudinal index toward housing from attitude questions in national survey of households in australia 
the first principal component represented general attitude toward property and home ownership 
the index or the attitude questions it embodied could be fed into general linear model of tenure choice 
the strongest determinant of private renting by far was the attitude index rather than income marital status or household type 
quantitative finance in quantitative finance principal component analysis can be directly applied to the risk management of interest rate derivative portfolios 
trading multiple swap instruments which are usually function of other market quotable swap instruments is sought to be reduced to usually or principal components representing the path of interest rates on macro basis 
converting risks to be represented as those to factor loadings or multipliers provides assessments and understanding beyond that available to simply collectively viewing risks to individual buckets 
pca has also been applied to equity portfolios in similar fashion both to portfolio risk and to risk return 
one application is to reduce portfolio risk where allocation strategies are applied to the principal portfolios instead of the underlying stocks 
second is to enhance portfolio return using the principal components to select stocks with upside potential 
neuroscience variant of principal components analysis is used in neuroscience to identify the specific properties of stimulus that increases neuron probability of generating an action potential 
this technique is known as spike triggered covariance analysis 
in typical application an experimenter presents white noise process as stimulus usually either as sensory input to test subject or as current injected directly into the neuron and records train of action potentials or spikes produced by the neuron as result 
presumably certain features of the stimulus make the neuron more likely to spike 
in order to extract these features the experimenter calculates the covariance matrix of the spike triggered ensemble the set of all stimuli defined and discretized over finite time window typically on the order of ms that immediately preceded spike 
the eigenvectors of the difference between the spike triggered covariance matrix and the covariance matrix of the prior stimulus ensemble the set of all stimuli defined over the same length time window then indicate the directions in the space of stimuli along which the variance of the spike triggered ensemble differed the most from that of the prior stimulus ensemble 
specifically the eigenvectors with the largest positive eigenvalues correspond to the directions along which the variance of the spike triggered ensemble showed the largest positive change compared to the varince of the prior 
since these were the directions in which varying the stimulus led to spike they are often good approximations of the sought after relevant stimulus features 
in neuroscience pca is also used to discern the identity of neuron from the shape of its action potential 
spike sorting is an important procedure because extracellular recording techniques often pick up signals from more than one neuron 
in spike sorting one first uses pca to reduce the dimensionality of the space of action potential waveforms and then performs clustering analysis to associate specific action potentials with individual neurons 
pca as dimension reduction technique is particularly suited to detect coordinated activities of large neuronal ensembles 
it has been used in determining collective variables that is order parameters during phase transitions in the brain 
relation with other methods correspondence analysis correspondence analysis ca was developed by jean paul benz cri and is conceptually similar to pca but scales the data which should be non negative so that rows and columns are treated equivalently 
it is traditionally applied to contingency tables 
ca decomposes the chi squared statistic associated to this table into orthogonal factors 
because ca is descriptive technique it can be applied to tables for which the chi squared statistic is appropriate or not 
several variants of ca are available including detrended correspondence analysis and canonical correspondence analysis 
one special extension is multiple correspondence analysis which may be seen as the counterpart of principal component analysis for categorical data 
factor analysis principal component analysis creates variables that are linear combinations of the original variables 
the new variables have the property that the variables are all orthogonal 
the pca transformation can be helpful as pre processing step before clustering 
pca is variance focused approach seeking to reproduce the total variable variance in which components reflect both common and unique variance of the variable 
pca is generally preferred for purposes of data reduction that is translating variable space into optimal factor space but not when the goal is to detect the latent construct or factors 
factor analysis is similar to principal component analysis in that factor analysis also involves linear combinations of variables 
different from pca factor analysis is correlation focused approach seeking to reproduce the inter correlations among variables in which the factors represent the common variance of variables excluding unique variance 
in terms of the correlation matrix this corresponds with focusing on explaining the off diagonal terms that is shared co variance while pca focuses on explaining the terms that sit on the diagonal 
however as side result when trying to reproduce the on diagonal terms pca also tends to fit relatively well the off diagonal correlations 
results given by pca and factor analysis are very similar in most situations but this is not always the case and there are some problems where the results are significantly different 
factor analysis is generally used when the research purpose is detecting data structure that is latent constructs or factors or causal modeling 
if the factor model is incorrectly formulated or the assumptions are not met then factor analysis will give erroneous results 
means clustering it has been asserted that the relaxed solution of means clustering specified by the cluster indicators is given by the principal components and the pca subspace spanned by the principal directions is identical to the cluster centroid subspace 
however that pca is useful relaxation of means clustering was not new result and it is straightforward to uncover counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions 
non negative matrix factorization non negative matrix factorization nmf is dimension reduction method where only non negative elements in the matrices are used which is therefore promising method in astronomy in the sense that astrophysical signals are non negative 
the pca components are orthogonal to each other while the nmf components are all non negative and therefore constructs non orthogonal basis 
in pca the contribution of each component is ranked based on the magnitude of its corresponding eigenvalue which is equivalent to the fractional residual variance frv in analyzing empirical data 
for nmf its components are ranked based only on the empirical frv curves 
the residual fractional eigenvalue plots that is as function of component number given total of components for pca has flat plateau where no data is captured to remove the quasi static noise then the curves dropped quickly as an indication of over fitting and captures random noise 
the frv curves for nmf is decreasing continuously when the nmf components are constructed sequentially indicating the continuous capturing of quasi static noise then converge to higher levels than pca indicating the less over fitting property of nmf 
iconography of correlations it is often difficult to interpret the principal components when the data include many variables of various origins or when some variables are qualitative 
this leads the pca user to delicate elimination of several variables 
if observations or variables have an excessive impact on the direction of the axes they should be removed and then projected as supplementary elements 
in addition it is necessary to avoid interpreting the proximities between the points close to the center of the factorial plane 
the iconography of correlations on the contrary which is not projection on system of axes does not have these drawbacks 
we can therefore keep all the variables 
the principle of the diagram is to underline the remarkable correlations of the correlation matrix by solid line positive correlation or dotted line negative correlation 
strong correlation is not remarkable if it is not direct but caused by the effect of third variable 
conversely weak correlations can be remarkable 
for example if variable depends on several independent variables the correlations of with each of them are weak and yet remarkable 
generalizations sparse pca particular disadvantage of pca is that the principal components are usually linear combinations of all input variables 
sparse pca overcomes this disadvantage by finding linear combinations that contain just few input variables 
it extends the classic method of principal component analysis pca for the reduction of dimensionality of data by adding sparsity constraint on the input variables 
several approaches have been proposed including regression framework convex relaxation semidefinite programming framework generalized power method framework an alternating maximization framework forward backward greedy search and exact methods using branch and bound techniques bayesian formulation framework the methodological and theoretical developments of sparse pca as well as its applications in scientific studies were recently reviewed in survey paper 
nonlinear pca most of the modern methods for nonlinear dimensionality reduction find their theoretical and algorithmic roots in pca or means 
pearson original idea was to take straight line or plane which will be the best fit to set of data points 
trevor hastie expanded on this concept by proposing principal curves as the natural extension for the geometric interpretation of pca which explicitly constructs manifold for data approximation followed by projecting the points onto it as is illustrated by fig 
see also the elastic map algorithm and principal geodesic analysis 
another popular generalization is kernel pca which corresponds to pca performed in reproducing kernel hilbert space associated with positive definite kernel 
in multilinear subspace learning pca is generalized to multilinear pca mpca that extracts features directly from tensor representations 
mpca is solved by performing pca in each mode of the tensor iteratively 
mpca has been applied to face recognition gait recognition etc 
mpca is further extended to uncorrelated mpca non negative mpca and robust mpca 
way principal component analysis may be performed with models such as tucker decomposition parafac multiple factor analysis co inertia analysis statis and distatis 
robust pca while pca finds the mathematically optimal method as in minimizing the squared error it is still sensitive to outliers in the data that produce large errors something that the method tries to avoid in the first place 
it is therefore common practice to remove outliers before computing pca 
however in some contexts outliers can be difficult to identify 
for example in data mining algorithms like correlation clustering the assignment of points to clusters and outliers is not known beforehand 
recently proposed generalization of pca based on weighted pca increases robustness by assigning different weights to data objects based on their estimated relevancy 
outlier resistant variants of pca have also been proposed based on norm formulations pca robust principal component analysis rpca via decomposition in low rank and sparse matrices is modification of pca that works well with respect to grossly corrupted observations 
similar techniques independent component analysis independent component analysis ica is directed to similar problems as principal component analysis but finds additively separable components rather than successive approximations 
network component analysis given matrix it tries to decompose it into two matrices such that key difference from techniques such as pca and ica is that some of the entries of are constrained to be here is termed the regulatory layer 
while in general such decomposition can have multiple solutions they prove that if the following conditions are satisfied has full column rank each column of must have at least zeroes where is the number of columns of or alternatively the number of rows of 
the justification for this criterion is that if node is removed from the regulatory layer along with all the output nodes connected to it the result must still be characterized by connectivity matrix with full column rank 
must have full row rank then the decomposition is unique up to multiplication by scalar 
discriminant analysis of principal components discriminant analysis of principal components dapc is multivariate method used to identify and describe clusters of genetically related individuals 
genetic variation is partitioned into two components variation between groups and within groups and it maximizes the former 
linear discriminants are linear combinations of alleles which best separate the clusters 
alleles that most contribute to this discrimination are therefore those that are the most markedly different across groups 
the contributions of alleles to the groupings identified by dapc can allow identifying regions of the genome driving the genetic divergence among groups in dapc data is first transformed using principal components analysis pca and subsequently clusters are identified using discriminant analysis da 
dapc can be realized on using the package adegenet 
more info adegenet on the web software source code alglib and library that implements pca and truncated pca analytica the built in eigendecomp function computes principal components 
elki includes pca for projection including robust variants of pca as well as pca based clustering algorithms 
gretl principal component analysis can be performed either via the pca command or via the princomp function 
julia supports pca with the pca function in the multivariatestats package knime java based nodal arranging software for analysis in this the nodes called pca pca compute pca apply pca inverse make it easily 
mathematica implements principal component analysis with the principalcomponents command using both covariance and correlation methods 
mathphp php mathematics library with support for pca 
matlab the svd function is part of the basic system 
in the statistics toolbox the functions princomp and pca give the principal components while the function pcares gives the residuals and reconstructed matrix for low rank pca approximation 
matplotlib python library have pca package in the mlab module 
mlpack provides an implementation of principal component analysis in 
nag library principal components analysis is implemented via the aa routine available in both the fortran versions of the library 
nmath proprietary numerical library containing pca for the net framework 
gnu octave free software computational environment mostly compatible with matlab the function princomp gives the principal component 
opencv oracle database implemented via dbms data mining svds scoring mode by specifying setting value svds scoring pca orange software integrates pca in its visual programming environment 
pca displays scree plot degree of explained variance where user can interactively select the number of principal components 
origin contains pca in its pro version 
qlucore commercial software for analyzing multivariate data with instant response using pca 
free statistical package the functions princomp and prcomp can be used for principal component analysis prcomp uses singular value decomposition which generally gives better numerical accuracy 
some packages that implement pca in include but are not limited to ade vegan exposition dimred and factominer 
sas proprietary software for example see scikit learn python library for machine learning which contains pca probabilistic pca kernel pca sparse pca and other techniques in the decomposition module 
spss proprietary software most commonly used by social scientists for pca factor analysis and associated cluster analysis 
weka java library for machine learning which contains modules for computing principal components 
see also references further reading jackson 
user guide to principal components wiley 
springer series in statistics 
springer series in statistics 
new york springer verlag 
isbn husson fran ois bastien pag me 
exploratory multivariate analysis by example using chapman hall crc the series london 
multiple factor analysis by example using chapman hall crc the series london external links university of copenhagen video by rasmus bro on youtube stanford university video by andrew ng on youtube tutorial on principal component analysis layman introduction to principal component analysis on youtube video of less than seconds 
statquest principal component analysis pca clearly explained on youtube see also the list of software implementations
in statistical modeling regression analysis is set of statistical processes for estimating the relationships between dependent variable often called the outcome or response variable or label in machine learning parlance and one or more independent variables often called predictors covariates explanatory variables or features 
the most common form of regression analysis is linear regression in which one finds the line or more complex linear combination that most closely fits the data according to specific mathematical criterion 
for example the method of ordinary least squares computes the unique line or hyperplane that minimizes the sum of squared differences between the true data and that line or hyperplane 
for specific mathematical reasons see linear regression this allows the researcher to estimate the conditional expectation or population average value of the dependent variable when the independent variables take on given set of values 
less common forms of regression use slightly different procedures to estimate alternative location parameters quantile regression or necessary condition analysis or estimate the conditional expectation across broader collection of non linear models nonparametric regression 
regression analysis is primarily used for two conceptually distinct purposes 
first regression analysis is widely used for prediction and forecasting where its use has substantial overlap with the field of machine learning 
second in some situations regression analysis can be used to infer causal relationships between the independent and dependent variables 
importantly regressions by themselves only reveal relationships between dependent variable and collection of independent variables in fixed dataset 
to use regressions for prediction or to infer causal relationships respectively researcher must carefully justify why existing relationships have predictive power for new context or why relationship between two variables has causal interpretation 
the latter is especially important when researchers hope to estimate causal relationships using observational data 
history the earliest form of regression was the method of least squares which was published by legendre in and by gauss in legendre and gauss both applied the method to the problem of determining from astronomical observations the orbits of bodies about the sun mostly comets but also later the then newly discovered minor planets 
gauss published further development of the theory of least squares in including version of the gauss markov theorem 
the term regression was coined by francis galton in the th century to describe biological phenomenon 
the phenomenon was that the heights of descendants of tall ancestors tend to regress down towards normal average phenomenon also known as regression toward the mean 
for galton regression had only this biological meaning but his work was later extended by udny yule and karl pearson to more general statistical context 
in the work of yule and pearson the joint distribution of the response and explanatory variables is assumed to be gaussian 
this assumption was weakened by fisher in his works of and fisher assumed that the conditional distribution of the response variable is gaussian but the joint distribution need not be 
in this respect fisher assumption is closer to gauss formulation of in the and economists used electromechanical desk calculators to calculate regressions 
before it sometimes took up to hours to receive the result from one regression regression methods continue to be an area of active research 
in recent decades new methods have been developed for robust regression regression involving correlated responses such as time series and growth curves regression in which the predictor independent variable or response variables are curves images graphs or other complex data objects regression methods accommodating various types of missing data nonparametric regression bayesian methods for regression regression in which the predictor variables are measured with error regression with more predictor variables than observations and causal inference with regression 
regression model in practice researchers first select model they would like to estimate and then use their chosen method ordinary least squares to estimate the parameters of that model 
regression models involve the following components the unknown parameters often denoted as scalar or vector the independent variables which are observed in data and are often denoted as vector where denotes row of data 
the dependent variable which are observed in data and often denoted using the scalar the error terms which are not directly observed in data and are often denoted using the scalar in various fields of application different terminologies are used in place of dependent and independent variables 
most regression models propose that is function of and with representing an additive error term that may stand in for un modeled determinants of or random statistical noise the researchers goal is to estimate the function that most closely fits the data 
to carry out regression analysis the form of the function must be specified 
sometimes the form of this function is based on knowledge about the relationship between and that does not rely on the data 
if no such knowledge is available flexible or convenient form for is chosen 
for example simple univariate regression may propose suggesting that the researcher believes to be reasonable approximation for the statistical process generating the data 
once researchers determine their preferred statistical model different forms of regression analysis provide tools to estimate the parameters for example least squares including its most common variant ordinary least squares finds the value of that minimizes the sum of squared errors given regression method will ultimately provide an estimate of usually denoted to distinguish the estimate from the true unknown parameter value that generated the data 
using this estimate the researcher can then use the fitted value for prediction or to assess the accuracy of the model in explaining the data 
whether the researcher is intrinsically interested in the estimate or the predicted value will depend on context and their goals 
as described in ordinary least squares least squares is widely used because the estimated function approximates the conditional expectation 
however alternative variants least absolute deviations or quantile regression are useful when researchers want to model other functions 
it is important to note that there must be sufficient data to estimate regression model 
for example suppose that researcher has access to rows of data with one dependent and two independent variables 
suppose further that the researcher wants to estimate bivariate linear model via least squares if the researcher only has access to data points then they could find infinitely many combinations that explain the data equally well any combination can be chosen that satisfies all of which lead to and are therefore valid solutions that minimize the sum of squared residuals 
to understand why there are infinitely many options note that the system of equations is to be solved for unknowns which makes the system underdetermined 
alternatively one can visualize infinitely many dimensional planes that go through fixed points 
more generally to estimate least squares model with distinct parameters one must have distinct data points 
if then there does not generally exist set of parameters that will perfectly fit the data 
the quantity appears often in regression analysis and is referred to as the degrees of freedom in the model 
moreover to estimate least squares model the independent variables 
must be linearly independent one must not be able to reconstruct any of the independent variables by adding and multiplying the remaining independent variables 
as discussed in ordinary least squares this condition ensures that is an invertible matrix and therefore that unique solution exists 
underlying assumptions by itself regression is simply calculation using the data 
in order to interpret the output of regression as meaningful statistical quantity that measures real world relationships researchers often rely on number of classical assumptions 
these assumptions often include the sample is representative of the population at large 
the independent variables are measured with no error 
deviations from the model have an expected value of zero conditional on covariates the variance of the residuals is constant across observations homoscedasticity 
the residuals are uncorrelated with one another 
mathematically the variance covariance matrix of the errors is diagonal handful of conditions are sufficient for the least squares estimator to possess desirable properties in particular the gauss markov assumptions imply that the parameter estimates will be unbiased consistent and efficient in the class of linear unbiased estimators 
practitioners have developed variety of methods to maintain some or all of these desirable properties in real world settings because these classical assumptions are unlikely to hold exactly 
for example modeling errors in variables can lead to reasonable estimates independent variables are measured with errors 
heteroscedasticity consistent standard errors allow the variance of to change across values of correlated errors that exist within subsets of the data or follow specific patterns can be handled using clustered standard errors geographic weighted regression or newey west standard errors among other techniques 
when rows of data correspond to locations in space the choice of how to model within geographic units can have important consequences 
the subfield of econometrics is largely focused on developing techniques that allow researchers to make reasonable real world conclusions in real world settings where classical assumptions do not hold exactly 
linear regression in linear regression the model specification is that the dependent variable is linear combination of the parameters but need not be linear in the independent variables 
for example in simple linear regression for modeling data points there is one independent variable and two parameters and straight line in multiple linear regression there are several independent variables or functions of independent variables 
adding term in to the preceding regression gives parabola this is still linear regression although the expression on the right hand side is quadratic in the independent variable it is linear in the parameters and in both cases is an error term and the subscript indexes particular observation 
returning our attention to the straight line case given random sample from the population we estimate the population parameters and obtain the sample linear regression model the residual is the difference between the value of the dependent variable predicted by the model and the true value of the dependent variable one method of estimation is ordinary least squares 
this method obtains parameter estimates that minimize the sum of squared residuals ssr minimization of this function results in set of normal equations set of simultaneous linear equations in the parameters which are solved to yield the parameter estimators in the case of simple regression the formulas for the least squares estimates are where is the mean average of the values and is the mean of the values 
under the assumption that the population error term has constant variance the estimate of that variance is given by this is called the mean square error mse of the regression 
the denominator is the sample size reduced by the number of model parameters estimated from the same data for regressors or if an intercept is used 
in this case so the denominator is the standard errors of the parameter estimates are given by under the further assumption that the population error term is normally distributed the researcher can use these estimated standard errors to create confidence intervals and conduct hypothesis tests about the population parameters 
general linear model in the more general multiple regression model there are independent variables where is the th observation on the th independent variable 
if the first independent variable takes the value for all then is called the regression intercept 
the least squares parameter estimates are obtained from normal equations 
the residual can be written as the normal equations are in matrix notation the normal equations are written as where the element of is the element of the column vector is and the element of is thus is is and is the solution is 
diagnostics once regression model has been constructed it may be important to confirm the goodness of fit of the model and the statistical significance of the estimated parameters 
commonly used checks of goodness of fit include the squared analyses of the pattern of residuals and hypothesis testing 
statistical significance can be checked by an test of the overall fit followed by tests of individual parameters 
interpretations of these diagnostic tests rest heavily on the model assumptions 
although examination of the residuals can be used to invalidate model the results of test or test are sometimes more difficult to interpret if the model assumptions are violated 
for example if the error term does not have normal distribution in small samples the estimated parameters will not follow normal distributions and complicate inference 
with relatively large samples however central limit theorem can be invoked such that hypothesis testing may proceed using asymptotic approximations 
limited dependent variables limited dependent variables which are response variables that are categorical variables or are variables constrained to fall only in certain range often arise in econometrics 
the response variable may be non continuous limited to lie on some subset of the real line 
for binary zero or one variables if analysis proceeds with least squares linear regression the model is called the linear probability model 
nonlinear models for binary dependent variables include the probit and logit model 
the multivariate probit model is standard method of estimating joint relationship between several binary dependent variables and some independent variables 
for categorical variables with more than two values there is the multinomial logit 
for ordinal variables with more than two values there are the ordered logit and ordered probit models 
censored regression models may be used when the dependent variable is only sometimes observed and heckman correction type models may be used when the sample is not randomly selected from the population of interest 
an alternative to such procedures is linear regression based on polychoric correlation or polyserial correlations between the categorical variables 
such procedures differ in the assumptions made about the distribution of the variables in the population 
if the variable is positive with low values and represents the repetition of the occurrence of an event then count models like the poisson regression or the negative binomial model may be used 
nonlinear regression when the model function is not linear in the parameters the sum of squares must be minimized by an iterative procedure 
this introduces many complications which are summarized in differences between linear and non linear least squares 
interpolation and extrapolation regression models predict value of the variable given known values of the variables 
prediction within the range of values in the dataset used for model fitting is known informally as interpolation 
prediction outside this range of the data is known as extrapolation 
performing extrapolation relies strongly on the regression assumptions 
the further the extrapolation goes outside the data the more room there is for the model to fail due to differences between the assumptions and the sample data or the true values 
it is generally advised that when performing extrapolation one should accompany the estimated value of the dependent variable with prediction interval that represents the uncertainty 
such intervals tend to expand rapidly as the values of the independent variable moved outside the range covered by the observed data 
for such reasons and others some tend to say that it might be unwise to undertake extrapolation however this does not cover the full set of modeling errors that may be made in particular the assumption of particular form for the relation between and properly conducted regression analysis will include an assessment of how well the assumed form is matched by the observed data but it can only do so within the range of values of the independent variables actually available 
this means that any extrapolation is particularly reliant on the assumptions being made about the structural form of the regression relationship 
best practice advice here is that linear in variables and linear in parameters relationship should not be chosen simply for computational convenience but that all available knowledge should be deployed in constructing regression model 
if this knowledge includes the fact that the dependent variable cannot go outside certain range of values this can be made use of in selecting the model even if the observed dataset has no values particularly near such bounds 
the implications of this step of choosing an appropriate functional form for the regression can be great when extrapolation is considered 
at minimum it can ensure that any extrapolation arising from fitted model is realistic or in accord with what is known 
power and sample size calculations there are no generally agreed methods for relating the number of observations versus the number of independent variables in the model 
one method conjectured by good and hardin is where is the sample size is the number of independent variables and is the number of observations needed to reach the desired precision if the model had only one independent variable 
for example researcher is building linear regression model using dataset that contains patients 
if the researcher decides that five observations are needed to precisely define straight line then the maximum number of independent variables the model can support is because log log 
other methods although the parameters of regression model are usually estimated using the method of least squares other methods which have been used include bayesian methods 
bayesian linear regression percentage regression for situations where reducing percentage errors is deemed more appropriate 
least absolute deviations which is more robust in the presence of outliers leading to quantile regression nonparametric regression requires large number of observations and is computationally intensive scenario optimization leading to interval predictor models distance metric learning which is learned by the search of meaningful distance metric in given input space 
software all major statistical software packages perform least squares regression analysis and inference 
simple linear regression and multiple regression using least squares can be done in some spreadsheet applications and on some calculators 
while many statistical software packages can perform various types of nonparametric and robust regression these methods are less standardized 
different software packages implement different methods and method with given name may be implemented differently in different packages 
specialized regression software has been developed for use in fields such as survey analysis and neuroimaging 
see also references further reading william kruskal and judith tanur ed 
linear hypotheses international encyclopedia of statistics 
free press evan williams regression pp 
analysis of variance pp 
regression and correlation analysis new palgrave dictionary of economics pp 
birkes david and dodge alternative methods of regression 
isbn chatfield calculating interval forecasts journal of business and economic statistics pp 
applied regression analysis rd ed 
applied regression analysis linear models and related methods 
sage hardle applied nonparametric regression isbn meade nigel islam towhidul 
prediction intervals for growth curve forecasts 
sen srivastava regression analysis theory methods and applications springer verlag berlin th printing 
strutz data fitting and uncertainty practical introduction to weighted least squares and beyond 
vieweg teubner isbn stulp freek and olivier sigaud 
many regression algorithms one unified model review 
https doi org neunet 
operations and production systems with multiple objectives 
external links regression analysis encyclopedia of mathematics ems press earliest uses regression basic history and references what is multiple regression used for 
multiple regression regression of weakly correlated data how linear regression mistakes can appear when range is much smaller than range
in machine learning hyperparameter is parameter whose value is used to control the learning process 
by contrast the values of other parameters typically node weights are derived via training 
hyperparameters can be classified as model hyperparameters that cannot be inferred while fitting the machine to the training set because they refer to the model selection task or algorithm hyperparameters that in principle have no influence on the performance of the model but affect the speed and quality of the learning process 
an example of model hyperparameter is the topology and size of neural network 
examples of algorithm hyperparameters are learning rate and batch size as well as mini batch size 
batch size can refer to the full data sample where mini batch size would be smaller sample set 
different model training algorithms require different hyperparameters some simple algorithms such as ordinary least squares regression require none 
given these hyperparameters the training algorithm learns the parameters from the data 
for instance lasso is an algorithm that adds regularization hyperparameter to ordinary least squares regression which has to be set before estimating the parameters through the training algorithm 
considerations the time required to train and test model can depend upon the choice of its hyperparameters 
hyperparameter is usually of continuous or integer type leading to mixed type optimization problems 
the existence of some hyperparameters is conditional upon the value of others 
the size of each hidden layer in neural network can be conditional upon the number of layers 
difficulty learnable parameters usually but not always hyperparameters cannot be learned using well known gradient based methods such as gradient descent lbfgs which are commonly employed to learn parameters 
these hyperparameters are those parameters describing model representation that cannot be learned by common optimization methods but nonetheless affect the loss function 
an example would be the tolerance hyperparameter for errors in support vector machines 
untrainable parameters sometimes hyperparameters cannot be learned from the training data because they aggressively increase the capacity of model and can push the loss function to an undesired minimum overfitting to and picking up noise in the data as opposed to correctly mapping the richness of the structure in the data 
for example if we treat the degree of polynomial equation fitting regression model as trainable parameter the degree would increase until the model perfectly fit the data yielding low training error but poor generalization performance 
tunability most performance variation can be attributed to just few hyperparameters 
the tunability of an algorithm hyperparameter or interacting hyperparameters is measure of how much performance can be gained by tuning it 
for an lstm while the learning rate followed by the network size are its most crucial hyperparameters batching and momentum have no significant effect on its performance although some research has advocated the use of mini batch sizes in the thousands other work has found the best performance with mini batch sizes between and 
robustness an inherent stochasticity in learning directly implies that the empirical hyperparameter performance is not necessarily its true performance 
methods that are not robust to simple changes in hyperparameters random seeds or even different implementations of the same algorithm cannot be integrated into mission critical control systems without significant simplification and robustification reinforcement learning algorithms in particular require measuring their performance over large number of random seeds and also measuring their sensitivity to choices of hyperparameters 
their evaluation with small number of random seeds does not capture performance adequately due to high variance 
some reinforcement learning methods 
ddpg deep deterministic policy gradient are more sensitive to hyperparameter choices than others 
optimization hyperparameter optimization finds tuple of hyperparameters that yields an optimal model which minimizes predefined loss function on given test data 
the objective function takes tuple of hyperparameters and returns the associated loss 
reproducibility apart from tuning hyperparameters machine learning involves storing and organizing the parameters and results and making sure they are reproducible 
in the absence of robust infrastructure for this purpose research code often evolves quickly and compromises essential aspects like bookkeeping and reproducibility 
online collaboration platforms for machine learning go further by allowing scientists to automatically share organize and discuss experiments data and algorithms 
reproducibility can be particularly difficult for deep learning models 
see also hyper heuristic replication crisis references
computerized batch processing is method of running software programs called jobs in batches automatically
while users are required to submit the jobs no other interaction by the user is required to process the batch
batches may automatically be run at scheduled times as well as being run contingent on the availability of computer resources
history the term batch processing originates in the traditional classification of methods of production as job production one off production batch production production of batch of multiple items at once one stage at time and flow production mass production all stages in process at once
early history early computers were capable of running only one program at time
each user had sole control of the machine for scheduled period of time
they would arrive at the computer with program and data often on punched paper cards and magnetic or paper tape and would load their program run and debug it and carry off their output when done
as computers became faster the setup and takedown time became larger percentage of available computer time
programs called monitors the forerunners of operating systems were developed which could process series or batch of programs often from magnetic tape prepared offline
the monitor would be loaded into the computer and run the first job of the batch
at the end of the job it would regain control and load and run the next until the batch was complete
often the output of the batch would be written to magnetic tape and printed or punched offline
examples of monitors were ibm fortran monitor system sos share operating system and finally ibsys for ibm systems in
third generation systems third generation computers capable of multiprogramming began to appear in the
instead of running one batch job at time these systems can have multiple batch programs running at the same time in order to keep the system as busy as possible
one or more programs might be awaiting input one actively running on the cpu and others generating output
instead of offline input and output programs called spoolers read jobs from cards disk or remote terminals and place them in job queue to be run
in order to prevent deadlocks the job scheduler needs to know each job resource requirements memory magnetic tapes mountable disks etc so various scripting languages were developed to supply this information in structured way
probably the most well known is ibm job control language jcl
job schedulers select jobs to run according to variety of criteria including priority memory size etc
remote batch is procedure for submitting batch jobs from remote terminals often equipped with punch card reader and line printer
sometimes asymmetric multiprocessing is used to spool batch input and output for one or more large computers using an attached smaller and less expensive system as in the ibm system attached support processor
later history the first general purpose time sharing system compatible time sharing system ctss was compatible with batch processing
this facilitated transitioning from batch processing to interactive computing from the late onwards interactive computing such as via text based computer terminal interfaces as in unix shells or read eval print loops and later graphical user interfaces became common
non interactive computation both one off jobs such as compilation and processing of multiple items in batches became retrospectively referred to as batch processing and the term batch job in early use often batch of jobs became common
early use is particularly found at the university of michigan around the michigan terminal system mts
although timesharing did exist its use was not robust enough for corporate data processing none of this was related to the earlier unit record equipment which was human operated
ongoing non interactive computation remains pervasive in computing both for general data processing and for system housekeeping tasks using system software
high level program executing multiple programs with some additional glue logic is today most often called script and written in scripting languages particularly shell scripts for system tasks in ibm pc dos and ms dos this is instead known as batch file
that includes unix based computers microsoft windows macos whose foundation is the bsd unix kernel and even smartphones
running script particularly one executed from an interactive login session is often known as job but that term is used very ambiguously
there is no direct counterpart to os batch processing in pc or unix systems
batch jobs are typically executed at scheduled time or on an as needed basis
perhaps the closest comparison is with processes run by an at or cron command in unix although the differences are significant
modern systems batch applications are still critical in most organizations in large part because many common business processes are amenable to batch processing
while online systems can also function when manual intervention is not desired they are not typically optimized to perform high volume repetitive tasks
therefore even new systems usually contain one or more batch applications for updating information at the end of the day generating reports printing documents and other non interactive tasks that must complete reliably within certain business deadlines
some applications are amenable to flow processing namely those that only need data from single input at once not totals for instance start the next step for each input as it completes the previous step
in this case flow processing lowers latency for individual inputs allowing them to be completed without waiting for the entire batch to finish
however many applications require data from all records notably computations such as totals
in this case the entire batch must be completed before one has usable result partial results are not usable
modern batch applications make use of modern batch frameworks such as jem the bee spring batch or implementations of jsr written for java and other frameworks for other programming languages to provide the fault tolerance and scalability required for high volume processing
in order to ensure high speed processing batch applications are often integrated with grid computing solutions to partition batch job over large number of processors although there are significant programming challenges in doing so
high volume batch processing places particularly heavy demands on system and application architectures as well
architectures that feature strong input output performance and vertical scalability including modern mainframe computers tend to provide better batch performance than alternatives
scripting languages became popular as they evolved along with batch processing
batch window batch window is period of less intensive online activity when the computer system is able to run batch jobs without interference from or with interactive online systems
bank end of day eod jobs require the concept of cutover where transaction and data are cut off for particular day batch activity deposits after pm will be processed the next day
as requirements for online systems uptime expanded to support globalization the internet and other business needs the batch window shrank and increasing emphasis was placed on techniques that would require online data to be available for maximum amount of time
batch size the batch size refers to the number of work units to be processed within one batch operation
some examples are the number of lines from file to load into database before committing the transaction
the number of messages to dequeue from queue
the number of requests to send within one payload
common batch processing usage efficient bulk database updates and automated transaction processing as contrasted to interactive online transaction processing oltp applications
the extract transform load etl step in populating data warehouses is inherently batch process in most implementations
performing bulk operations on digital images such as resizing conversion watermarking or otherwise editing group of image files
converting computer files from one format to another
for example batch job may convert proprietary and legacy files to common standard formats for end user queries and display
notable batch scheduling and execution environments the ibm mainframe os operating system or platform has arguably the most highly refined and evolved set of batch processing facilities owing to its origins long history and continuing evolution
today such systems commonly support hundreds or even thousands of concurrent online and batch tasks within single operating system image
technologies that aid concurrent batch and online processing include job control language jcl scripting languages such as rexx job entry subsystem jes and jes workload manager wlm automatic restart manager arm resource recovery services rrs ibm db data sharing parallel sysplex unique performance optimizations such as hiperdispatch channel architecture and several others
the unix programs cron at and batch today batch is variant of at allow for complex scheduling of jobs
windows has job scheduler
most high performance computing clusters use batch processing to maximize cluster usage
see also background process batch file batch renaming to rename lots of files automatically without human intervention in order to save time and effort batchpipes for utility that increases batch performance processing modes production support for batch job schedule stream support high throughput computing notes references
among alternative guitar tunings regular tunings have equal musical intervals between the paired notes of their successive open strings
guitar tunings assign pitches to the open strings of guitars
tunings can be described by the particular pitches that are denoted by notes in western music
by convention the notes are ordered from lowest to highest
the standard tuning defines the string pitches as and between the open strings of the standard tuning are three perfect fourths then the major third and the fourth perfect fourth
in contrast regular tunings have constant intervals between their successive open strings semitones minor third minor thirds or diminished tuning semitones major third major thirds or augmented tuning semitones perfect fourth all fourths tuning semitones augmented fourth tritone or diminished fifth augmented fourths tuning semitones perfect fifth all fifths tuningfor the regular tunings chords may be moved diagonally around the fretboard as well as vertically for the repetitive regular tunings minor thirds major thirds and augmented fourths
regular tunings thus often appeal to new guitarists and also to jazz guitarists as they facilitate key transpositions without requiring completely new set of fingerings for the new key
on the other hand some conventional major minor system chords are easier to play in standard tuning than in regular tuning
left handed guitarists may use the chord charts from one class of regular tunings for its left handed tuning for example the chord charts for all fifths tuning may be used for guitars strung with left handed all fourths tuning
the class of regular tunings has been named and described by professor william sethares
sethares chapter regular tunings in his revised alternate tuning guide is the leading source for this article
this article descriptions of particular regular tunings use other sources also
standard and alternative guitar tunings review this summary of standard tuning also introduces the terms for discussing alternative tunings
standard standard tuning has the following open string notes in standard tuning the separation of the second and third string is by major third interval which has width of four semitones
the irregularity has price
chords cannot be shifted around the fretboard in the standard tuning which requires four chord shapes for the major chords
there are separate chord forms for chords having their root note on the third fourth fifth and sixth strings
alternative alternative alternate tuning refers to any open string note arrangement other than standard tuning
such alternative tuning arrangements offer different chord voicing and sonorities
alternative tunings necessarily change the chord shapes associated with standard tuning which eases the playing of some often non standard chords at the cost of increasing the difficulty of some traditionally voiced chords
as with other scordatura tuning regular tunings may require re stringing the guitar with different string gauges
for example all fifths tuning has been difficult to implement on conventional guitars due to the extreme high pitch required from the top string
even common approximation to all fifths tuning new standard tuning requires special set of strings
properties with standard tuning and with all tunings chord patterns can be moved twelve frets down where the notes repeat in higher octave
for the standard tuning there is exactly one interval of third between the second and third strings and all the other intervals are fourths
working around the irregular third of standard tuning guitarists have to memorize chord patterns for at least three regions the first four strings tuned in perfect fourths two or more fourths and the third and one or more initial fourths the third and the last fourth
in contrast regular tunings have constant intervals between their successive open strings
in fact the class of each regular tuning is characterized by its musical interval as shown by the following list semitones minor third minor thirds tuning semitones major third major thirds tuning semitones perfect fourth all fourths tuning semitones augmented fourth tritone or diminished fifth augmented fourths tuning semitones perfect fifth all fifths tuningthe regular tunings whose number of semitones divides the number of notes in the octave repeat their open string notes raised one octave after strings for example having three semitones in its interval minor thirds tuning repeats its open notes after four strings having four semitones in its interval major thirds tuning repeats its open notes after three strings having six semitones in its interval augmented fourths tuning repeats its notes after two strings regular tunings have symmetrical scales all along the fretboard
this makes it simpler to translate chords into new keys
for the regular tunings chords may be moved diagonally around the fretboard
the shifting of chords is especially simple for the regular tunings that repeat their open strings in which case chords can be moved vertically chords can be moved three strings up or down in major thirds tuning and chords can be moved two strings up or down in augmented fourths tuning
regular tunings thus appeal to new guitarists and also to jazz guitarists whose improvisation is simplified by regular intervals
particular conventional chords are more difficult to play on the other hand particular traditional chords may be more difficult to play in regular tuning than in standard tuning
it can be difficult to play conventional chords especially in augmented fourths tuning and all fifths tuning in which the wide tritone and perfect fifth intervals require hand stretching
some chords that are conventional in folk music are difficult to play even in all fourths and major thirds tunings which do not require more hand stretching than standard tuning
on the other hand minor thirds tuning features many barre chords with repeated notes properties that appeal to beginners
frets covered by the hand the chromatic scale climbs from one string to the next after number of frets that is specific to each regular tuning
the chromatic scale climbs after exactly four frets in major thirds tuning so reducing the extensions of the little and index fingers hand stretching
for other regular tunings the successive strings have intervals that are minor thirds perfect fourths augmented fourths or perfect fifths thus the fretting hand covers three five six or seven frets respectively to play chromatic scale
of course the lowest chromatic scale uses the open strings and so requires one less fret to be covered
examples the following regular tunings are discussed by sethares who also mentions other regular tunings that are difficult to play or have had little musical interest to date
minor thirds or in each minor thirds tuning every interval between successive strings is minor third
thus each repeats its open notes after four strings
in the minor thirds tuning beginning with the open strings contain the notes gb of the diminished triad minor thirds tuning features many barre chords with repeated notes properties that appeal to acoustic guitarists and to beginners
doubled notes have different sounds because of differing string widths tensions and tunings and they reinforce each other like the doubled strings of twelve string guitar add chorusing and depth according to william sethares achieving the same range as standard tuned guitar using minor thirds tuning would require nine string guitar
major thirds major thirds tuning is regular tuning in which the musical intervals between successive strings are each major thirds
like minor thirds tuning and unlike all fourths and all fifths tuning major thirds tuning is repetitive tuning it repeats its octave after three strings which again simplifies the learning of chords and improvisation similarly minor thirds tuning repeats itself after four strings while augmented fourths tuning repeats itself after two strings
neighboring the standard tuning is the all thirds tuning that has the open strings or with six strings major thirds tuning has smaller range than standard tuning with seven strings the major thirds tuning covers the range of standard tuning on six strings
with the repetition of three open string notes each major thirds tuning provides the guitarist with many options for fingering chords
indeed the fingering of two successive frets suffices to play pure major and minor chords while the fingering of three successive frets suffices to play seconds fourths sevenths and ninths for the standard western guitar which has six strings major thirds tuning has smaller range than standard tuning on guitar with seven strings the major thirds tuning covers the range of standard tuning on six strings
even greater range is possible with guitars with eight strings major thirds tuning was heavily used in by the american jazz guitarist ralph patt to facilitate his style of improvisation
all fourths this tuning is like that of the lowest four strings in standard tuning
consequently of all the regular tunings it is the closest approximation to standard tuning and thus it best allows the transfer of knowledge of chords from standard tuning to regular tuning
jazz musician stanley jordan plays guitar in all fourths tuning he has stated that all fourths tuning simplifies the fingerboard making it logical for all fourths tuning all twelve major chords in the first or open positions are generated by two chords the open major chord and the major chord
the regularity of chord patterns reduces the number of finger positions that need to be memorized the left handed involute of an all fourths tuning is an all fifths tuning
all fourths tuning is based on the perfect fourth five semitones and all fifths tuning is based on the perfect fifth seven semitones
consequently chord charts for all fifths tunings may be used for left handed all fourths tuning
augmented fourths and etc
between the all fifths and all fourths tunings are augmented fourth tunings which are also called diminished fifths or tritone tunings
it is repetitive tuning that repeats its notes after two strings
with augmented fourths tunings the fretboard has greatest symmetry
in fact every augmented fourths tuning lists the notes of all the other augmented fourths tunings on the frets of its fretboard
professor sethares wrote that the augmented fourth interval is the only interval whose inverse is the same as itself
the augmented fourths tuning is the only tuning other than the trivial tuning for which all chords forms remain unchanged when the strings are reversed
thus the augmented fourths tuning is its own lefty tuning
of all the augmented fourths tunings the tuning is the closest approximation to the standard tuning and its fretboard is displayed next an augmented fourths tuning makes it very easy for playing half whole scales diminished licks and whole tone scales stated guitarist ron jarzombek
all fifths mandoguitar all fifths tuning is tuning in intervals of perfect fifths like that of mandolin cello or violin other names include perfect fifths and fifths
consequently classical compositions written for violin or guitar may be adapted to all fifths tuning more easily than to standard tuning
when he was asked whether tuning in fifths facilitates new intervals or harmonies that aren readily available in standard tuning robert fripp responded it more rational system but it also better sounding better for chords better for single notes
to build chords fripp uses perfect intervals in fourths fifths and octaves so avoiding minor thirds and especially major thirds which are sharp in equal temperament tuning in comparison to thirds in just intonation
it is challenge to adapt conventional guitar chords to new standard tuning which is based on all fifths tuning
some closely voiced jazz chords become impractical in nst and all fifths tuning it has wide range thus its implementation can be difficult
the high requires taut thin string and consequently is prone to breaking
this can be ameliorated by using shorter scale length guitar by shifting to different key or by shifting down fifth
all fifths tuning was used by the jazz guitarist carl kress
the left handed involute of an all fifths tuning is an all fourths tuning
all fifths tuning is based on the perfect fifth seven semitones and all fourths tuning is based on the perfect fourth five semitones
consequently chord charts for all fifths tunings are used for left handed all fourths tuning all fifths tuning has been approximated with tunings in the through the looking glass guitar of kei nakano which has been played by him since this new tuning is like mirror to all kinds of string instruments including guitar
also it can adapt to any other tunings of guitar
if tuned to normal guitar for the right handed person it is able to use for lefty guitar in general and vice versa
new standard tuning all fifths tuning has been approximated with tunings that avoid the high or the low
the has been replaced with in the new standard tuning nst of king crimson robert fripp
the original version of nst was all fifths tuning
however in the fripp never attained the all fifth high
while he could attain the string life time distribution was too short
experimenting with string fripp succeeded
originally seen in ths
all the way the top string would not go to so as on tenor banjo adopted an on the first string
these kept breaking so was adopted
in fripp experimented with string if successful the experiment could lead to the nst cgdae according to fripp
fripp nst has been taught in guitar craft courses
guitar craft and its successor guitar circle have taught fripp tuning to three thousand students
extreme intervals for regular tunings intervals wider than perfect fifth or narrower than minor third have thus far had limited interest
wide intervals two regular tunings based on sixths having intervals of minor sixths eight semitones and of major sixths nine semitones have received scholarly discussion
the chord charts for minor sixths tuning are useful for left handed guitarists playing in major thirds tuning the chord charts for major sixths tuning for left handed guitarists playing in minor thirds tuning the regular tunings with minor seventh ten semitones or major seventh eleven semitones intervals would make conventional major minor chord playing very difficult as would octave intervals
narrow intervals there are regular tunings that have as their intervals either zero semi tones unison one semi tone minor second or two semi tones major second
these tunings tend to increase the difficulty in playing the major minor system chords of conventionally tuned guitars the trivial class of unison tunings such as are each their own left handed tuning
unison tunings are briefly discussed in the article on ostrich tunings
having exactly one note unison tunings are also ostrich tunings which have exactly one pitch class but may have two or more octaves for example and non unison ostrich tunings are not regular
left handed involution the class of regular tunings is preserved under the involution from right handed to left handed tunings as observed by william sethares
the present discussion of left handed tunings is of interest to musical theorists mathematicians and left handed persons but may be skipped by other readers
for left handed guitars the ordering of the strings reverses the ordering of the strings for right handed guitars
for example the left handed involute of the standard tuning is the lefty tuning
similarly the left handed involute of the lefty tuning is the standard righty tuning the reordering of open strings in left handed tunings has an important consequence
the chord fingerings for the right handed tunings must be changed for left handed tunings
however the left handed involute of regular tuning is easily recognized it is another regular tuning
thus the chords for the involuted regular tuning may be used for the left handed involute of regular tuning
for example the left handed version of all fourths tuning is all fifths tuning and the left handed version of all fifths tuning is all fourths tuning
in general the left handed involute of the regular tuning based on the interval with semitones is the regular tuning based on its involuted interval with semitones all fourths tuning is based on the perfect fourth five semitones and all fifths tuning is based on the perfect fifth seven semitones as mentioned previously
the following table summarizes the lefty righty pairings discussed by sethares
the left handed involute of left handed involute is the original right handed tuning
the left handed version of the trivial tuning is also
among non trivial tunings only the class of augmented fourths tunings is fixed under the lefty involution
summary the principal regular tunings have their properties summarized in the following table notes references denyer ralph
playing the guitar how the guitar is tuned pp
and alternative tunings pp
special contributors isaac guillory and alastair crawford fully revised and updated ed
london and sydney pan books
griewank andreas january tuning guitars and reading music in major thirds matheon preprints vol
berlin germany dfg research center matheon mathematics for key technologies berlin msc classification arts
postscript file and pdf file archived from the original on november sethares bill
alternate tuning guide pdf
madison wisconsin university of wisconsin department of electrical engineering
retrieved may sethares bill january
alternate tuning guide pdf
madison wisconsin university of wisconsin department of electrical engineering
retrieved may sethares william
madison wisconsin university of wisconsin department of electrical engineering
further reading allen warren september december
wa encyclopedia of guitar tunings
archived from the original on july retrieved june
recommended by marcus gary
guitar zero the science of learning to be musical
alternate tuning guide interactive
retrieved june uses wolfram cdf player
guitar tunings comprehensive guide
external links major thirds professors andreas griewank and william sethares each recommend discussions of major thirds tuning by two jazz guitarists sethares regular tunings and griewank ole kirkeby for and string guitars charts of intervals major chords and minor chords and recommended gauges for strings
ralph patt for and string guitars charts of scales chords and chord progressions
all fourths yahoo group for all fourths tuning new standard tuning courses in new standard tuning are offered by guitar circle the successor of guitar craft guitar circle of europe guitar circle of latin america guitar circle of north america
in music theory major chord is chord that has root major third and perfect fifth
when chord comprises only these three notes it is called major triad
for example the major triad built on called major triad has pitches in harmonic analysis and on lead sheets major chord can be notated as cm or cmaj
major triad is represented by the integer notation
major triad can also be described by its intervals the interval between the bottom and middle notes is major third and the interval between the middle and top notes is minor third
by contrast minor triad has minor third interval on the bottom and major third interval on top
they both contain fifths because major third four semitones plus minor third three semitones equals perfect fifth seven semitones
chords that are constructed of consecutive or stacked thirds are called tertian
in western classical music from to and in western pop folk and rock music major chord is usually played as triad
along with the minor triad the major triad is one of the basic building blocks of tonal music in the western common practice period and western pop folk and rock music
it is considered consonant stable or not requiring resolution
in western music minor chord sounds darker than major chord giving off sense of sadness or somber feeling some major chords with additional notes such as the major seventh chord are also called major chords
major seventh chords are used in jazz and occasionally in rock music
in jazz major chords may also have other chord tones added such as the ninth and the thirteenth scale degrees
inversions given major chord may be voiced in many ways
for example the notes of major triad may be arranged in many different vertical orders and the chord will still be major triad
however if the lowest note
the bass note is not the root of the chord then the chord is said to be in an inversion it is in root position if the lowest note is the root of the chord it is in first inversion if the lowest note is its third and it is in second inversion if the lowest note is its fifth
these inversions of major triad are shown below
the additional notes above the bass note can be in any order and the chord still retains its inversion identity
for example major chord is considered to be in first inversion if its lowest note is regardless of how the notes above it are arranged or even doubled
major chord table in this table the chord names are in the leftmost column
the chords are given in root position
for given chord name the following three columns indicate the individual notes that make up this chord
thus in the first row the chord is major which is made up of the individual pitches and just intonation most western keyboard instruments are tuned to equal temperament
in equal temperament each semitone is the same distance apart and there are four semitones between the root and third three between the third and fifth and seven between the root and fifth
another tuning system that is used is just intonation
in just intonation major chord is tuned to the frequency ratio this may be found on iv vi iii and vi
in equal temperament the fifth is only two cents narrower than the just perfect fifth but the major third is noticeably different at about cents wider
see also major and minor musical tuning minor chord otonality and utonality references external links media related to major chords at wikimedia commons major triads explained on virtual piano major chords explained on virtual piano