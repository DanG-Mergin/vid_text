
principal component analysis pca is popular technique for analyzing large datasets containing high number of dimensions features per observation increasing the interpretability of data while preserving the maximum amount of information and enabling the visualization of multidimensional data 
formally pca is statistical technique for reducing the dimensionality of dataset 
this is accomplished by linearly transforming the data into new coordinate system where most of the variation in the data can be described with fewer dimensions than the initial data 
many studies use the first two principal components in order to plot the data in two dimensions and to visually identify clusters of closely related data points 
principal component analysis has applications in many fields such as population genetics microbiome studies and atmospheric science 
the principal components of collection of points in real coordinate space are sequence of unit vectors where the th vector is the direction of line that best fits the data while being orthogonal to the first vectors 
here best fitting line is defined as one that minimizes the average squared perpendicular distance from the points to the line 
these directions constitute an orthonormal basis in which different individual dimensions of the data are linearly uncorrelated 
principal component analysis pca is the process of computing the principal components and using them to perform change of basis on the data sometimes using only the first few principal components and ignoring the rest 
in data analysis the first principal component of set of variables presumed to be jointly normally distributed is the derived variable formed as linear combination of the original variables that explains the most variance 
the second principal component explains the most variance in what is left once the effect of the first component is removed and we may proceed through iterations until all the variance is explained 
pca is most commonly used when many of the variables are highly correlated with each other and it is desirable to reduce their number to an independent set 
pca is used in exploratory data analysis and for making predictive models 
it is commonly used for dimensionality reduction by projecting each data point onto only the first few principal components to obtain lower dimensional data while preserving as much of the data variation as possible 
the first principal component can equivalently be defined as direction that maximizes the variance of the projected data 
the th principal component can be taken as direction orthogonal to the first principal components that maximizes the variance of the projected data 
for either objective it can be shown that the principal components are eigenvectors of the data covariance matrix 
thus the principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix 
pca is the simplest of the true eigenvector based multivariate analyses and is closely related to factor analysis 
factor analysis typically incorporates more domain specific assumptions about the underlying structure and solves eigenvectors of slightly different matrix 
pca is also related to canonical correlation analysis cca 
cca defines coordinate systems that optimally describe the cross covariance between two datasets while pca defines new orthogonal coordinate system that optimally describes variance in single dataset 
robust and norm based variants of standard pca have also been proposed 
history pca was invented in by karl pearson as an analogue of the principal axis theorem in mechanics it was later independently developed and named by harold hotelling in the 
depending on the field of application it is also named the discrete karhunen lo ve transform klt in signal processing the hotelling transform in multivariate quality control proper orthogonal decomposition pod in mechanical engineering singular value decomposition svd of invented in the last quarter of the th century eigenvalue decomposition evd of xtx in linear algebra factor analysis for discussion of the differences between pca and factor analysis see ch 
of jolliffe principal component analysis eckart young theorem harman or empirical orthogonal functions eof in meteorological science empirical eigenfunction decomposition sirovich empirical component analysis lorenz quasiharmonic modes brooks et al spectral decomposition in noise and vibration and empirical modal analysis in structural dynamics 
intuition pca can be thought of as fitting dimensional ellipsoid to the data where each axis of the ellipsoid represents principal component 
if some axis of the ellipsoid is small then the variance along that axis is also small 
to find the axes of the ellipsoid we must first center the values of each variable in the dataset on by subtracting the mean of the variable observed values from each of those values 
these transformed values are used instead of the original observed values for each of the variables 
then we compute the covariance matrix of the data and calculate the eigenvalues and corresponding eigenvectors of this covariance matrix 
then we must normalize each of the orthogonal eigenvectors to turn them into unit vectors 
once this is done each of the mutually orthogonal unit eigenvectors can be interpreted as an axis of the ellipsoid fitted to the data 
this choice of basis will transform the covariance matrix into diagonalized form in which the diagonal elements represent the variance of each axis 
the proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues 
biplots and scree plots degree of explained variance are used to explain findings of the pca 
details pca is defined as an orthogonal linear transformation that transforms the data to new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate called the first principal component the second greatest variance on the second coordinate and so on consider an data matrix with column wise zero empirical mean the sample mean of each column has been shifted to zero where each of the rows represents different repetition of the experiment and each of the columns gives particular kind of feature say the results from particular sensor 
mathematically the transformation is defined by set of size of dimensional vectors of weights or coefficients that map each row vector of to new vector of principal component scores given by in such way that the individual variables of considered over the data set successively inherit the maximum possible variance from with each coefficient vector constrained to be unit vector where is usually selected to be strictly less than to reduce dimensionality 
first component in order to maximize variance the first weight vector thus has to satisfy arg max arg max equivalently writing this in matrix form gives arg max arg max since has been defined to be unit vector it equivalently also satisfies arg max the quantity to be maximised can be recognised as rayleigh quotient 
standard result for positive semidefinite matrix such as xtx is that the quotient maximum possible value is the largest eigenvalue of the matrix which occurs when is the corresponding eigenvector 
with found the first principal component of data vector can then be given as score in the transformed co ordinates or as the corresponding vector in the original variables 
further components the th component can be found by subtracting the first principal components from and then finding the weight vector which extracts the maximum variance from this new data matrix arg max it turns out that this gives the remaining eigenvectors of xtx with the maximum values for the quantity in brackets given by their corresponding eigenvalues 
thus the weight vectors are eigenvectors of xtx 
the th principal component of data vector can therefore be given as score tk in the transformed coordinates or as the corresponding vector in the space of the original variables where is the kth eigenvector of xtx 
the full principal components decomposition of can therefore be given as where is by matrix of weights whose columns are the eigenvectors of xtx 
the transpose of is sometimes called the whitening or sphering transformation 
columns of multiplied by the square root of corresponding eigenvalues that is eigenvectors scaled up by the variances are called loadings in pca or in factor analysis 
covariances xtx itself can be recognized as proportional to the empirical sample covariance matrix of the dataset xt 
the sample covariance between two of the different principal components over the dataset is given by where the eigenvalue property of has been used to move from line to line however eigenvectors and corresponding to eigenvalues of symmetric matrix are orthogonal if the eigenvalues are different or can be orthogonalised if the vectors happen to share an equal repeated value 
the product in the final line is therefore zero there is no sample covariance between different principal components over the dataset 
another way to characterise the principal components transformation is therefore as the transformation to coordinates which diagonalise the empirical sample covariance matrix 
in matrix form the empirical covariance matrix for the original variables can be written the empirical covariance matrix between the principal components becomes where is the diagonal matrix of eigenvalues of xtx 
is equal to the sum of the squares over the dataset associated with each component that is tk 
dimensionality reduction the transformation maps data vector from an original space of variables to new space of variables which are uncorrelated over the dataset 
however not all the principal components need to be kept 
keeping only the first principal components produced by using only the first eigenvectors gives the truncated transformation where the matrix tl now has rows but only columns 
in other words pca learns linear transformation where the columns of matrix form an orthogonal basis for the features the components of representation that are decorrelated 
by construction of all the transformed data matrices with only columns this score matrix maximises the variance in the original data that has been preserved while minimising the total squared reconstruction error or such dimensionality reduction can be very useful step for visualising and processing high dimensional datasets while still retaining as much of the variance in the dataset as possible 
for example selecting and keeping only the first two principal components finds the two dimensional plane through the high dimensional dataset in which the data is most spread out so if the data contains clusters these too may be most spread out and therefore most visible to be plotted out in two dimensional diagram whereas if two directions through the data or two of the original variables are chosen at random the clusters may be much less spread apart from each other and may in fact be much more likely to substantially overlay each other making them indistinguishable 
similarly in regression analysis the larger the number of explanatory variables allowed the greater is the chance of overfitting the model producing conclusions that fail to generalise to other datasets 
one approach especially when there are strong correlations between different possible explanatory variables is to reduce them to few principal components and then run the regression against them method called principal component regression 
dimensionality reduction may also be appropriate when the variables in dataset are noisy 
if each column of the dataset contains independent identically distributed gaussian noise then the columns of will also contain similarly identically distributed gaussian noise such distribution is invariant under the effects of the matrix which can be thought of as high dimensional rotation of the co ordinate axes 
however with more of the total variance concentrated in the first few principal components compared to the same noise variance the proportionate effect of the noise is less the first few components achieve higher signal to noise ratio 
pca thus can have the effect of concentrating much of the signal into the first few principal components which can usefully be captured by dimensionality reduction while the later principal components may be dominated by noise and so disposed of without great loss 
if the dataset is not too large the significance of the principal components can be tested using parametric bootstrap as an aid in determining how many principal components to retain 
singular value decomposition the principal components transformation can also be associated with another matrix factorization the singular value decomposition svd of here is an by rectangular diagonal matrix of positive numbers called the singular values of is an by matrix the columns of which are orthogonal unit vectors of length called the left singular vectors of and is by whose columns are orthogonal unit vectors of length and called the right singular vectors of in terms of this factorization the matrix xtx can be written where is the square diagonal matrix with the singular values of and the excess zeros chopped off that satisfies comparison with the eigenvector factorization of xtx establishes that the right singular vectors of are equivalent to the eigenvectors of xtx while the singular values of are equal to the square root of the eigenvalues of xtx 
using the singular value decomposition the score matrix can be written so each column of is given by one of the left singular vectors of multiplied by the corresponding singular value 
this form is also the polar decomposition of efficient algorithms exist to calculate the svd of without having to form the matrix xtx so computing the svd is now the standard way to calculate principal components analysis from data matrix unless only handful of components are required 
as with the eigen decomposition truncated score matrix tl can be obtained by considering only the first largest singular values and their singular vectors the truncation of matrix or using truncated singular value decomposition in this way produces truncated matrix that is the nearest possible matrix of rank to the original matrix in the sense of the difference between the two having the smallest possible frobenius norm result known as the eckart young theorem 
further considerations the singular values in are the square roots of the eigenvalues of the matrix xtx 
each eigenvalue is proportional to the portion of the variance more correctly of the sum of the squared distances of the points from their multidimensional mean that is associated with each eigenvector 
the sum of all the eigenvalues is equal to the sum of the squared distances of the points from their multidimensional mean 
pca essentially rotates the set of points around their mean in order to align with the principal components 
this moves as much of the variance as possible using an orthogonal transformation into the first few dimensions 
the values in the remaining dimensions therefore tend to be small and may be dropped with minimal loss of information see below 
pca is often used in this manner for dimensionality reduction 
pca has the distinction of being the optimal orthogonal transformation for keeping the subspace that has largest variance as defined above 
this advantage however comes at the price of greater computational requirements if compared for example and when applicable to the discrete cosine transform and in particular to the dct ii which is simply known as the dct 
nonlinear dimensionality reduction techniques tend to be more computationally demanding than pca 
pca is sensitive to the scaling of the variables 
if we have just two variables and they have the same sample variance and are completely correlated then the pca will entail rotation by and the weights they are the cosines of rotation for the two variables with respect to the principal component will be equal 
but if we multiply all values of the first variable by then the first principal component will be almost the same as that variable with small contribution from the other variable whereas the second component will be almost aligned with the second original variable 
this means that whenever the different variables have different units like temperature and mass pca is somewhat arbitrary method of analysis 
different results would be obtained if one used fahrenheit rather than celsius for example 
pearson original paper was entitled on lines and planes of closest fit to systems of points in space in space implies physical euclidean space where such concerns do not arise 
one way of making the pca less arbitrary is to use variables scaled so as to have unit variance by standardizing the data and hence use the autocorrelation matrix instead of the autocovariance matrix as basis for pca 
however this compresses or expands the fluctuations in all dimensions of the signal space to unit variance 
mean centering is necessary for performing classical pca to ensure that the first principal component describes the direction of maximum variance 
if mean subtraction is not performed the first principal component might instead correspond more or less to the mean of the data 
mean of zero is needed for finding basis that minimizes the mean square error of the approximation of the data mean centering is unnecessary if performing principal components analysis on correlation matrix as the data are already centered after calculating correlations 
correlations are derived from the cross product of two standard scores scores or statistical moments hence the name pearson product moment correlation 
also see the article by kromrey foster johnson on mean centering in moderated regression much ado about nothing 
since covariances are correlations of normalized variables or standard scores pca based on the correlation matrix of is equal to pca based on the covariance matrix of the standardized version of pca is popular primary technique in pattern recognition 
it is not however optimized for class separability 
however it has been used to quantify the distance between two or more classes by calculating center of mass for each class in principal component space and reporting euclidean distance between center of mass of two or more classes 
the linear discriminant analysis is an alternative which is optimized for class separability 
table of symbols and abbreviations properties and limitations of pca properties some properties of pca include property for any integer consider the orthogonal linear transformation where is element vector and is matrix and let be the variance covariance matrix for then the trace of denoted tr is maximized by taking where consists of the first columns of is the transpose of property consider again the orthonormal transformation with and defined as before 
then tr is minimized by taking where consists of the last columns of the statistical implication of this property is that the last few pcs are not simply unstructured left overs after removing the important pcs 
because these last pcs have variances as small as possible they are useful in their own right 
they can help to detect unsuspected near constant linear relationships between the elements of and they may also be useful in regression in selecting subset of variables from and in outlier detection 
property spectral decomposition of before we look at its usage we first look at diagonal elements var then perhaps the main statistical implication of the result is that not only can we decompose the combined variances of all the elements of into decreasing contributions due to each pc but we can also decompose the whole covariance matrix into contributions from each pc 
although not strictly decreasing the elements of will tend to become smaller as increases as is nonincreasing for increasing whereas the elements of tend to stay about the same size because of the normalization constraints limitations as noted above the results of pca depend on the scaling of the variables 
this can be cured by scaling each feature by its standard deviation so that one ends up with dimensionless features with unital variance the applicability of pca as described above is limited by certain tacit assumptions made in its derivation 
in particular pca can capture linear correlations between the features but fails when this assumption is violated see figure in the reference 
in some cases coordinate transformations can restore the linearity assumption and pca can then be applied see kernel pca 
another limitation is the mean removal process before constructing the covariance matrix for pca 
in fields such as astronomy all the signals are non negative and the mean removal process will force the mean of some astrophysical exposures to be zero which consequently creates unphysical negative fluxes and forward modeling has to be performed to recover the true magnitude of the signals 
as an alternative method non negative matrix factorization focusing only on the non negative elements in the matrices which is well suited for astrophysical observations 
see more at relation between pca and non negative matrix factorization 
pca is at disadvantage if the data has not been standardized before applying the algorithm to it 
pca transforms original data into data that is relevant to the principal components of that data which means that the new data variables cannot be interpreted in the same ways that the originals were 
they are linear interpretations of the original variables 
also if pca is not performed properly there is high likelihood of information loss pca relies on linear model 
if dataset has pattern hidden inside it that is nonlinear then pca can actually steer the analysis in the complete opposite direction of progress 
researchers at kansas state university discovered that the sampling error in their experiments impacted the bias of pca results 
if the number of subjects or blocks is smaller than and or the researcher is interested in pc beyond the first it may be better to first correct for the serial correlation before pca is conducted 
the researchers at kansas state also found that pca could be seriously biased if the autocorrelation structure of the data is not correctly handled 
pca and information theory dimensionality reduction results in loss of information in general 
pca based dimensionality reduction tends to minimize that information loss under certain signal and noise models 
under the assumption that that is that the data vector is the sum of the desired information bearing signal and noise signal one can show that pca can be optimal for dimensionality reduction from an information theoretic point of view 
in particular linsker showed that if is gaussian and is gaussian noise with covariance matrix proportional to the identity matrix the pca maximizes the mutual information between the desired information and the dimensionality reduced output if the noise is still gaussian and has covariance matrix proportional to the identity matrix that is the components of the vector are iid but the information bearing signal is non gaussian which is common scenario pca at least minimizes an upper bound on the information loss which is defined as 
the optimality of pca is also preserved if the noise is iid and at least more gaussian in terms of the kullback leibler divergence than the information bearing signal in general even if the above signal model holds pca loses its information theoretic optimality as soon as the noise becomes dependent 
computing pca using the covariance method the following is detailed description of pca using the covariance method see also here as opposed to the correlation method the goal is to transform given data set of dimension to an alternative data set of smaller dimension equivalently we are seeking to find the matrix where is the karhunen lo ve transform klt of matrix organize the data setsuppose you have data comprising set of observations of variables and you want to reduce the data so that each observation can be described with only variables suppose further that the data are arranged as set of data vectors with each representing single grouped observation of the variables 
write as row vectors each with elements 
place the row vectors into single matrix of dimensions calculate the empirical meanfind the empirical mean along each column place the calculated mean values into an empirical mean vector of dimensions calculate the deviations from the meanmean subtraction is an integral part of the solution towards finding principal component basis that minimizes the mean square error of approximating the data 
hence we proceed by centering the data as follows subtract the empirical mean vector from each row of the data matrix store mean subtracted data in the matrix where is an column vector of all for in some applications each variable column of may also be scaled to have variance equal to see score 
this step affects the calculated principal components but makes them independent of the units used to measure the different variables 
find the covariance matrixfind the empirical covariance matrix from matrix where is the conjugate transpose operator 
if consists entirely of real numbers which is the case in many applications the conjugate transpose is the same as the regular transpose 
the reasoning behind using instead of to calculate the covariance is bessel correction find the eigenvectors and eigenvalues of the covariance matrixcompute the matrix of eigenvectors which diagonalizes the covariance matrix where is the diagonal matrix of eigenvalues of this step will typically involve the use of computer based algorithm for computing eigenvectors and eigenvalues 
these algorithms are readily available as sub components of most matrix algebra systems such as sas matlab mathematica scipy idl interactive data language or gnu octave as well as opencv 
matrix will take the form of an diagonal matrix where is the jth eigenvalue of the covariance matrix and matrix also of dimension contains column vectors each of length which represent the eigenvectors of the covariance matrix the eigenvalues and eigenvectors are ordered and paired 
the jth eigenvalue corresponds to the jth eigenvector 
matrix denotes the matrix of right eigenvectors as opposed to left eigenvectors 
in general the matrix of right eigenvectors need not be the conjugate transpose of the matrix of left eigenvectors rearrange the eigenvectors and eigenvaluessort the columns of the eigenvector matrix and eigenvalue matrix in order of decreasing eigenvalue 
make sure to maintain the correct pairings between the columns in each matrix compute the cumulative energy content for each eigenvectorthe eigenvalues represent the distribution of the source data energy among each of the eigenvectors where the eigenvectors form basis for the data 
the cumulative energy content for the jth eigenvector is the sum of the energy content across all of the eigenvalues from through for select subset of the eigenvectors as basis vectorssave the first columns of as the matrix where use the vector as guide in choosing an appropriate value for the goal is to choose value of as small as possible while achieving reasonably high value of on percentage basis 
for example you may want to choose so that the cumulative energy is above certain threshold like percent 
in this case choose the smallest value of such that project the data onto the new basisthe projected data points are the rows of the matrix that is the first column of is the projection of the data points onto the first principal component the second column is the projection onto the second principal component etc 
derivation of pca using the covariance method let be dimensional random vector expressed as column vector 
without loss of generality assume has zero mean 
we want to find orthonormal transformation matrix so that px has diagonal covariance matrix that is px is random vector with all its distinct components pairwise uncorrelated 
quick computation assuming were unitary yields cov cov hence holds if and only if cov were diagonalisable by this is very constructive as cov is guaranteed to be non negative definite matrix and thus is guaranteed to be diagonalisable by some unitary matrix 
covariance free computation in practical implementations especially with high dimensional data large the naive covariance method is rarely used because it is not efficient due to high computational and memory costs of explicitly determining the covariance matrix 
the covariance free approach avoids the np operations of explicitly calculating and storing the covariance matrix xtx instead utilizing one of matrix free methods for example based on the function evaluating the product xt at the cost of np operations 
iterative computation one way to compute the first principal component efficiently is shown in the following pseudo code for data matrix with zero mean without ever computing its covariance matrix 
random vector of length norm do times vector of length for each row in rts is the eigenvalue error norm exit if error tolerance return this power iteration algorithm simply calculates the vector xt normalizes and places the result back in the eigenvalue is approximated by rt xtx which is the rayleigh quotient on the unit vector for the covariance matrix xtx 
if the largest singular value is well separated from the next largest one the vector gets close to the first principal component of within the number of iterations which is small relative to at the total cost cnp 
the power iteration convergence can be accelerated without noticeably sacrificing the small cost per iteration using more advanced matrix free methods such as the lanczos algorithm or the locally optimal block preconditioned conjugate gradient lobpcg method 
subsequent principal components can be computed one by one via deflation or simultaneously as block 
in the former approach imprecisions in already computed approximate principal components additively affect the accuracy of the subsequently computed principal components thus increasing the error with every new computation 
the latter approach in the block power method replaces single vectors and with block vectors matrices and every column of approximates one of the leading principal components while all columns are iterated simultaneously 
the main calculation is evaluation of the product xt 
implemented for example in lobpcg efficient blocking eliminates the accumulation of the errors allows using high level blas matrix matrix product functions and typically leads to faster convergence compared to the single vector one by one technique 
the nipals method non linear iterative partial least squares nipals is variant the classical power iteration with matrix deflation by subtraction implemented for computing the first few components in principal component or partial least squares analysis 
for very high dimensional datasets such as those generated in the omics sciences for example genomics metabolomics it is usually only necessary to compute the first few pcs 
the non linear iterative partial least squares nipals algorithm updates iterative approximations to the leading scores and loadings and by the power iteration multiplying on every iteration by on the left and on the right that is calculation of the covariance matrix is avoided just as in the matrix free implementation of the power iterations to xtx based on the function evaluating the product xt tx the matrix deflation by subtraction is performed by subtracting the outer product from leaving the deflated residual matrix used to calculate the subsequent leading pcs 
for large data matrices or matrices that have high degree of column collinearity nipals suffers from loss of orthogonality of pcs due to machine precision round off errors accumulated in each iteration and matrix deflation by subtraction 
gram schmidt re orthogonalization algorithm is applied to both the scores and the loadings at each iteration step to eliminate this loss of orthogonality 
nipals reliance on single vector multiplications cannot take advantage of high level blas and results in slow convergence for clustered leading singular values both these deficiencies are resolved in more sophisticated matrix free block solvers such as the locally optimal block preconditioned conjugate gradient lobpcg method 
online sequential estimation in an online or streaming situation with data arriving piece by piece rather than being stored in single batch it is useful to make an estimate of the pca projection that can be updated sequentially 
this can be done efficiently but requires different algorithms 
pca and qualitative variables in pca it is common that we want to introduce qualitative variables as supplementary elements 
for example many quantitative variables have been measured on plants 
for these plants some qualitative variables are available as for example the species to which the plant belongs 
these data were subjected to pca for quantitative variables 
when analyzing the results it is natural to connect the principal components to the qualitative variable species 
for this the following results are produced 
identification on the factorial planes of the different species for example using different colors 
representation on the factorial planes of the centers of gravity of plants belonging to the same species 
for each center of gravity and each axis value to judge the significance of the difference between the center of gravity and origin these results are what is called introducing qualitative variable as supplementary element 
this procedure is detailed in and husson pag and pag few software offer this option in an automatic way 
this is the case of spad that historically following the work of ludovic lebart was the first to propose this option and the package factominer 
applications intelligence the earliest application of factor analysis was in locating and measuring components of human intelligence 
it was believed that intelligence had various uncorrelated components such as spatial intelligence verbal intelligence induction deduction etc and that scores on these could be adduced by factor analysis from results on various tests to give single index known as the intelligence quotient iq 
the pioneering statistical psychologist spearman actually developed factor analysis in for his two factor theory of intelligence adding formal technique to the science of psychometrics 
in thurstone looked for factors of intelligence developing the notion of mental age 
standard iq tests today are based on this early work 
residential differentiation in shevky and williams introduced the theory of factorial ecology which dominated studies of residential differentiation from the to the 
neighbourhoods in city were recognizable or could be distinguished from one another by various characteristics which could be reduced to three by factor analysis 
these were known as social rank an index of occupational status familism or family size and ethnicity cluster analysis could then be applied to divide the city into clusters or precincts according to values of the three key factor variables 
an extensive literature developed around factorial ecology in urban geography but the approach went out of fashion after as being methodologically primitive and having little place in postmodern geographical paradigms 
one of the problems with factor analysis has always been finding convincing names for the various artificial factors 
in flood revived the factorial ecology approach to show that principal components analysis actually gave meaningful answers directly without resorting to factor rotation 
the principal components were actually dual variables or shadow prices of forces pushing people together or apart in cities 
the first component was accessibility the classic trade off between demand for travel and demand for space around which classical urban economics is based 
the next two components were disadvantage which keeps people of similar status in separate neighbourhoods mediated by planning and ethnicity where people of similar ethnic backgrounds try to co locate about the same time the australian bureau of statistics defined distinct indexes of advantage and disadvantage taking the first principal component of sets of key variables that were thought to be important 
these seifa indexes are regularly published for various jurisdictions and are used frequently in spatial analysis 
development indexes pca has been the only formal method available for the development of indexes which are otherwise hit or miss ad hoc undertaking 
the city development index was developed by pca from about indicators of city outcomes in survey of global cities 
the first principal component was subject to iterative regression adding the original variables singly until about of its variation was accounted for 
the index ultimately used about indicators but was good predictor of many more variables 
its comparative value agreed very well with subjective assessment of the condition of each city 
the coefficients on items of infrastructure were roughly proportional to the average costs of providing the underlying services suggesting the index was actually measure of effective physical and social investment in the city 
the country level human development index hdi from undp which has been published since and is very extensively used in development studies has very similar coefficients on similar indicators strongly suggesting it was originally constructed using pca 
population genetics in cavalli sforza and others pioneered the use of principal components analysis pca to summarise data on variation in human gene frequencies across regions 
the components showed distinctive patterns including gradients and sinusoidal waves 
they interpreted these patterns as resulting from specific ancient migration events 
since then pca has been ubiquitous in population genetics with thousands of papers using pca as display mechanism 
genetics varies largely according to proximity so the first two principal components actually show spatial distribution and may be used to map the relative geographical location of different population groups thereby showing individuals who have wandered from their original locations pca in genetics has been technically controversial in that the technique has been performed on discrete non normal variables and often on binary allele markers 
the lack of any measures of standard error in pca are also an impediment to more consistent usage 
in august the molecular biologist eran elhaik published theoretical paper in scientific reports analyzing pca applications 
he concluded that it was easy to manipulate the method which in his view generated results that were erroneous contradictory and absurd 
specifically he argued the results achieved in population genetics were characterized by cherry picking and circular reasoning 
market research and indexes of attitude market research has been an extensive user of pca 
it is used to develop customer satisfaction or customer loyalty scores for products and with clustering to develop market segments that may be targeted with advertising campaigns in much the same way as factorial ecology will locate geographical areas with similar characteristics pca rapidly transforms large amounts of data into smaller easier to digest variables that can be more rapidly and readily analyzed 
in any consumer questionnaire there are series of questions designed to elicit consumer attitudes and principal components seek out latent variables underlying these attitudes 
for example the oxford internet survey in asked people about their attitudes and beliefs and from these analysts extracted four principal component dimensions which they identified as escape social networking efficiency and problem creating another example from joe flood in extracted an attitudinal index toward housing from attitude questions in national survey of households in australia 
the first principal component represented general attitude toward property and home ownership 
the index or the attitude questions it embodied could be fed into general linear model of tenure choice 
the strongest determinant of private renting by far was the attitude index rather than income marital status or household type 
quantitative finance in quantitative finance principal component analysis can be directly applied to the risk management of interest rate derivative portfolios 
trading multiple swap instruments which are usually function of other market quotable swap instruments is sought to be reduced to usually or principal components representing the path of interest rates on macro basis 
converting risks to be represented as those to factor loadings or multipliers provides assessments and understanding beyond that available to simply collectively viewing risks to individual buckets 
pca has also been applied to equity portfolios in similar fashion both to portfolio risk and to risk return 
one application is to reduce portfolio risk where allocation strategies are applied to the principal portfolios instead of the underlying stocks 
second is to enhance portfolio return using the principal components to select stocks with upside potential 
neuroscience variant of principal components analysis is used in neuroscience to identify the specific properties of stimulus that increases neuron probability of generating an action potential 
this technique is known as spike triggered covariance analysis 
in typical application an experimenter presents white noise process as stimulus usually either as sensory input to test subject or as current injected directly into the neuron and records train of action potentials or spikes produced by the neuron as result 
presumably certain features of the stimulus make the neuron more likely to spike 
in order to extract these features the experimenter calculates the covariance matrix of the spike triggered ensemble the set of all stimuli defined and discretized over finite time window typically on the order of ms that immediately preceded spike 
the eigenvectors of the difference between the spike triggered covariance matrix and the covariance matrix of the prior stimulus ensemble the set of all stimuli defined over the same length time window then indicate the directions in the space of stimuli along which the variance of the spike triggered ensemble differed the most from that of the prior stimulus ensemble 
specifically the eigenvectors with the largest positive eigenvalues correspond to the directions along which the variance of the spike triggered ensemble showed the largest positive change compared to the varince of the prior 
since these were the directions in which varying the stimulus led to spike they are often good approximations of the sought after relevant stimulus features 
in neuroscience pca is also used to discern the identity of neuron from the shape of its action potential 
spike sorting is an important procedure because extracellular recording techniques often pick up signals from more than one neuron 
in spike sorting one first uses pca to reduce the dimensionality of the space of action potential waveforms and then performs clustering analysis to associate specific action potentials with individual neurons 
pca as dimension reduction technique is particularly suited to detect coordinated activities of large neuronal ensembles 
it has been used in determining collective variables that is order parameters during phase transitions in the brain 
relation with other methods correspondence analysis correspondence analysis ca was developed by jean paul benz cri and is conceptually similar to pca but scales the data which should be non negative so that rows and columns are treated equivalently 
it is traditionally applied to contingency tables 
ca decomposes the chi squared statistic associated to this table into orthogonal factors 
because ca is descriptive technique it can be applied to tables for which the chi squared statistic is appropriate or not 
several variants of ca are available including detrended correspondence analysis and canonical correspondence analysis 
one special extension is multiple correspondence analysis which may be seen as the counterpart of principal component analysis for categorical data 
factor analysis principal component analysis creates variables that are linear combinations of the original variables 
the new variables have the property that the variables are all orthogonal 
the pca transformation can be helpful as pre processing step before clustering 
pca is variance focused approach seeking to reproduce the total variable variance in which components reflect both common and unique variance of the variable 
pca is generally preferred for purposes of data reduction that is translating variable space into optimal factor space but not when the goal is to detect the latent construct or factors 
factor analysis is similar to principal component analysis in that factor analysis also involves linear combinations of variables 
different from pca factor analysis is correlation focused approach seeking to reproduce the inter correlations among variables in which the factors represent the common variance of variables excluding unique variance 
in terms of the correlation matrix this corresponds with focusing on explaining the off diagonal terms that is shared co variance while pca focuses on explaining the terms that sit on the diagonal 
however as side result when trying to reproduce the on diagonal terms pca also tends to fit relatively well the off diagonal correlations 
results given by pca and factor analysis are very similar in most situations but this is not always the case and there are some problems where the results are significantly different 
factor analysis is generally used when the research purpose is detecting data structure that is latent constructs or factors or causal modeling 
if the factor model is incorrectly formulated or the assumptions are not met then factor analysis will give erroneous results 
means clustering it has been asserted that the relaxed solution of means clustering specified by the cluster indicators is given by the principal components and the pca subspace spanned by the principal directions is identical to the cluster centroid subspace 
however that pca is useful relaxation of means clustering was not new result and it is straightforward to uncover counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions 
non negative matrix factorization non negative matrix factorization nmf is dimension reduction method where only non negative elements in the matrices are used which is therefore promising method in astronomy in the sense that astrophysical signals are non negative 
the pca components are orthogonal to each other while the nmf components are all non negative and therefore constructs non orthogonal basis 
in pca the contribution of each component is ranked based on the magnitude of its corresponding eigenvalue which is equivalent to the fractional residual variance frv in analyzing empirical data 
for nmf its components are ranked based only on the empirical frv curves 
the residual fractional eigenvalue plots that is as function of component number given total of components for pca has flat plateau where no data is captured to remove the quasi static noise then the curves dropped quickly as an indication of over fitting and captures random noise 
the frv curves for nmf is decreasing continuously when the nmf components are constructed sequentially indicating the continuous capturing of quasi static noise then converge to higher levels than pca indicating the less over fitting property of nmf 
iconography of correlations it is often difficult to interpret the principal components when the data include many variables of various origins or when some variables are qualitative 
this leads the pca user to delicate elimination of several variables 
if observations or variables have an excessive impact on the direction of the axes they should be removed and then projected as supplementary elements 
in addition it is necessary to avoid interpreting the proximities between the points close to the center of the factorial plane 
the iconography of correlations on the contrary which is not projection on system of axes does not have these drawbacks 
we can therefore keep all the variables 
the principle of the diagram is to underline the remarkable correlations of the correlation matrix by solid line positive correlation or dotted line negative correlation 
strong correlation is not remarkable if it is not direct but caused by the effect of third variable 
conversely weak correlations can be remarkable 
for example if variable depends on several independent variables the correlations of with each of them are weak and yet remarkable 
generalizations sparse pca particular disadvantage of pca is that the principal components are usually linear combinations of all input variables 
sparse pca overcomes this disadvantage by finding linear combinations that contain just few input variables 
it extends the classic method of principal component analysis pca for the reduction of dimensionality of data by adding sparsity constraint on the input variables 
several approaches have been proposed including regression framework convex relaxation semidefinite programming framework generalized power method framework an alternating maximization framework forward backward greedy search and exact methods using branch and bound techniques bayesian formulation framework the methodological and theoretical developments of sparse pca as well as its applications in scientific studies were recently reviewed in survey paper 
nonlinear pca most of the modern methods for nonlinear dimensionality reduction find their theoretical and algorithmic roots in pca or means 
pearson original idea was to take straight line or plane which will be the best fit to set of data points 
trevor hastie expanded on this concept by proposing principal curves as the natural extension for the geometric interpretation of pca which explicitly constructs manifold for data approximation followed by projecting the points onto it as is illustrated by fig 
see also the elastic map algorithm and principal geodesic analysis 
another popular generalization is kernel pca which corresponds to pca performed in reproducing kernel hilbert space associated with positive definite kernel 
in multilinear subspace learning pca is generalized to multilinear pca mpca that extracts features directly from tensor representations 
mpca is solved by performing pca in each mode of the tensor iteratively 
mpca has been applied to face recognition gait recognition etc 
mpca is further extended to uncorrelated mpca non negative mpca and robust mpca 
way principal component analysis may be performed with models such as tucker decomposition parafac multiple factor analysis co inertia analysis statis and distatis 
robust pca while pca finds the mathematically optimal method as in minimizing the squared error it is still sensitive to outliers in the data that produce large errors something that the method tries to avoid in the first place 
it is therefore common practice to remove outliers before computing pca 
however in some contexts outliers can be difficult to identify 
for example in data mining algorithms like correlation clustering the assignment of points to clusters and outliers is not known beforehand 
recently proposed generalization of pca based on weighted pca increases robustness by assigning different weights to data objects based on their estimated relevancy 
outlier resistant variants of pca have also been proposed based on norm formulations pca robust principal component analysis rpca via decomposition in low rank and sparse matrices is modification of pca that works well with respect to grossly corrupted observations 
similar techniques independent component analysis independent component analysis ica is directed to similar problems as principal component analysis but finds additively separable components rather than successive approximations 
network component analysis given matrix it tries to decompose it into two matrices such that key difference from techniques such as pca and ica is that some of the entries of are constrained to be here is termed the regulatory layer 
while in general such decomposition can have multiple solutions they prove that if the following conditions are satisfied has full column rank each column of must have at least zeroes where is the number of columns of or alternatively the number of rows of 
the justification for this criterion is that if node is removed from the regulatory layer along with all the output nodes connected to it the result must still be characterized by connectivity matrix with full column rank 
must have full row rank then the decomposition is unique up to multiplication by scalar 
discriminant analysis of principal components discriminant analysis of principal components dapc is multivariate method used to identify and describe clusters of genetically related individuals 
genetic variation is partitioned into two components variation between groups and within groups and it maximizes the former 
linear discriminants are linear combinations of alleles which best separate the clusters 
alleles that most contribute to this discrimination are therefore those that are the most markedly different across groups 
the contributions of alleles to the groupings identified by dapc can allow identifying regions of the genome driving the genetic divergence among groups in dapc data is first transformed using principal components analysis pca and subsequently clusters are identified using discriminant analysis da 
dapc can be realized on using the package adegenet 
more info adegenet on the web software source code alglib and library that implements pca and truncated pca analytica the built in eigendecomp function computes principal components 
elki includes pca for projection including robust variants of pca as well as pca based clustering algorithms 
gretl principal component analysis can be performed either via the pca command or via the princomp function 
julia supports pca with the pca function in the multivariatestats package knime java based nodal arranging software for analysis in this the nodes called pca pca compute pca apply pca inverse make it easily 
mathematica implements principal component analysis with the principalcomponents command using both covariance and correlation methods 
mathphp php mathematics library with support for pca 
matlab the svd function is part of the basic system 
in the statistics toolbox the functions princomp and pca give the principal components while the function pcares gives the residuals and reconstructed matrix for low rank pca approximation 
matplotlib python library have pca package in the mlab module 
mlpack provides an implementation of principal component analysis in 
nag library principal components analysis is implemented via the aa routine available in both the fortran versions of the library 
nmath proprietary numerical library containing pca for the net framework 
gnu octave free software computational environment mostly compatible with matlab the function princomp gives the principal component 
opencv oracle database implemented via dbms data mining svds scoring mode by specifying setting value svds scoring pca orange software integrates pca in its visual programming environment 
pca displays scree plot degree of explained variance where user can interactively select the number of principal components 
origin contains pca in its pro version 
qlucore commercial software for analyzing multivariate data with instant response using pca 
free statistical package the functions princomp and prcomp can be used for principal component analysis prcomp uses singular value decomposition which generally gives better numerical accuracy 
some packages that implement pca in include but are not limited to ade vegan exposition dimred and factominer 
sas proprietary software for example see scikit learn python library for machine learning which contains pca probabilistic pca kernel pca sparse pca and other techniques in the decomposition module 
spss proprietary software most commonly used by social scientists for pca factor analysis and associated cluster analysis 
weka java library for machine learning which contains modules for computing principal components 
see also references further reading jackson 
user guide to principal components wiley 
springer series in statistics 
springer series in statistics 
new york springer verlag 
isbn husson fran ois bastien pag me 
exploratory multivariate analysis by example using chapman hall crc the series london 
multiple factor analysis by example using chapman hall crc the series london external links university of copenhagen video by rasmus bro on youtube stanford university video by andrew ng on youtube tutorial on principal component analysis layman introduction to principal component analysis on youtube video of less than seconds 
statquest principal component analysis pca clearly explained on youtube see also the list of software implementations
in statistics the logistic model or logit model is statistical model that models the probability of an event taking place by having the log odds for the event be linear combination of one or more independent variables 
in regression analysis logistic regression or logit regression is estimating the parameters of logistic model the coefficients in the linear combination 
formally in binary logistic regression there is single binary dependent variable coded by an indicator variable where the two values are labeled and while the independent variables can each be binary variable two classes coded by an indicator variable or continuous variable any real value 
the corresponding probability of the value labeled can vary between certainly the value and certainly the value hence the labeling the function that converts log odds to probability is the logistic function hence the name 
the unit of measurement for the log odds scale is called logit from logistic unit hence the alternative names 
see background and definition for formal mathematics and example for worked example 
binary variables are widely used in statistics to model the probability of certain class or event taking place such as the probability of team winning of patient being healthy etc 
see applications and the logistic model has been the most commonly used model for binary regression since about binary variables can be generalized to categorical variables when there are more than two possible values 
whether an image is of cat dog lion etc 
and the binary logistic regression generalized to multinomial logistic regression 
if the multiple categories are ordered one can use the ordinal logistic regression for example the proportional odds ordinal logistic model 
see extensions for further extensions 
the logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification it is not classifier though it can be used to make classifier for instance by choosing cutoff value and classifying inputs with probability greater than the cutoff as one class below the cutoff as the other this is common way to make binary classifier 
analogous linear models for binary variables with different sigmoid function instead of the logistic function to convert the linear combination to probability can also be used most notably the probit model see alternatives 
the defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at constant rate with each independent variable having its own parameter for binary dependent variable this generalizes the odds ratio 
more abstractly the logistic function is the natural parameter for the bernoulli distribution and in this sense is the simplest way to convert real number to probability 
in particular it maximizes entropy minimizes added information and in this sense makes the fewest assumptions of the data being modeled see maximum entropy 
the parameters of logistic regression are most commonly estimated by maximum likelihood estimation mle 
this does not have closed form expression unlike linear least squares see model fitting 
logistic regression by mle plays similarly basic role for binary or categorical responses as linear regression by ordinary least squares ols plays for scalar responses it is simple well analyzed baseline model see comparison with linear regression for discussion 
the logistic regression as general statistical model was originally developed and popularized primarily by joseph berkson beginning in berkson where he coined logit see history 
applications logistic regression is used in various fields including machine learning most medical fields and social sciences 
for example the trauma and injury severity score triss which is widely used to predict mortality in injured patients was originally developed by boyd et al 
many other medical scales used to assess severity of patient have been developed using logistic regression 
logistic regression may be used to predict the risk of developing given disease 
diabetes coronary heart disease based on observed characteristics of the patient age sex body mass index results of various blood tests etc 
another example might be to predict whether nepalese voter will vote nepali congress or communist party of nepal or any other party based on age income sex race state of residence votes in previous elections etc 
the technique can also be used in engineering especially for predicting the probability of failure of given process system or product 
it is also used in marketing applications such as prediction of customer propensity to purchase product or halt subscription etc 
in economics it can be used to predict the likelihood of person ending up in the labor force and business application would be to predict the likelihood of homeowner defaulting on mortgage 
conditional random fields an extension of logistic regression to sequential data are used in natural language processing 
example problem as simple example we can use logistic regression with one explanatory variable and two categories to answer the following question group of students spends between and hours studying for an exam 
how does the number of hours spent studying affect the probability of the student passing the exam 
the reason for using logistic regression for this problem is that the values of the dependent variable pass and fail while represented by and are not cardinal numbers 
if the problem was changed so that pass fail was replaced with the grade cardinal numbers then simple regression analysis could be used 
the table shows the number of hours each student spent studying and whether they passed or failed 
we wish to fit logistic function to the data consisting of the hours studied xk and the outcome of the test yk for pass for fail 
the data points are indexed by the subscript which runs from to the variable is called the explanatory variable and the variable is called the categorical variable consisting of two categories pass or fail corresponding to the categorical values and respectively 
model the logistic function is of the form where is location parameter the midpoint of the curve where and is scale parameter 
this expression may be rewritten as where and is known as the intercept it is the vertical intercept or intercept of the line and inverse scale parameter or rate parameter these are the intercept and slope of the log odds as function of conversely and 
fit the usual measure of goodness of fit for logistic regression uses logistic loss or log loss the negative log likelihood 
for given xk and yk write 
the are the probabilities that the corresponding will be unity and are the probabilities that they will be zero see bernoulli distribution 
we wish to find the values of and which give the best fit to the data 
in the case of linear regression the sum of the squared deviations of the fit from the data points yk the squared error loss is taken as measure of the goodness of fit and the best fit is obtained when that function is minimized 
the log loss for the th point is ln if ln if the log loss can be interpreted as the surprisal of the actual outcome relative to the prediction and is measure of information content 
note that log loss is always greater than or equal to equals only in case of perfect prediction when and or and and approaches infinity as the prediction gets worse when and or and meaning the actual outcome is more surprising 
since the value of the logistic function is always strictly between zero and one the log loss is always greater than zero and less than infinity 
note that unlike in linear regression where the model can have zero loss at point by passing through data point and zero loss overall if all points are on line in logistic regression it is not possible to have zero loss at any points since is either or but these can be combined into single expression ln ln 
this expression is more formally known as the cross entropy of the predicted distribution from the actual distribution as probability distributions on the two element space of pass fail 
the sum of these the total loss is the overall negative log likelihood and the best fit is obtained for those choices of and for which is minimized 
alternatively instead of minimizing the loss one can maximize its inverse the positive log likelihood ln ln ln ln or equivalently maximize the likelihood function itself which is the probability that the given data set is produced by particular logistic function this method is known as maximum likelihood estimation 
parameter estimation since is nonlinear in and determining their optimum values will require numerical methods 
note that one method of maximizing is to require the derivatives of with respect to and to be zero and the maximization procedure can be accomplished by solving the above two equations for and which again will generally require the use of numerical methods 
the values of and which maximize and using the above data are found to be which yields value for and of predictions the and coefficients may be entered into the logistic regression equation to estimate the probability of passing the exam 
for example for student who studies hours entering the value into the equation gives the estimated probability of passing the exam of probability of passing exam similarly for student who studies hours the estimated probability of passing the exam is probability of passing exam this table shows the estimated probability of passing the exam for several values of hours studying 
model evaluation the logistic regression analysis gives the following output 
by the wald test the output indicates that hours studying is significantly associated with the probability of passing the exam 
rather than the wald method the recommended method to calculate the value for logistic regression is the likelihood ratio test lrt which for this data gives see error and significance of the fit below 
generalizations this simple model is an example of binary logistic regression and has one explanatory variable and binary categorical variable which can assume one of two categorical values 
multinomial logistic regression is the generalization of binary logistic regression to include any number of explanatory variables and any number of categories 
background definition of the logistic function an explanation of logistic regression can begin with an explanation of the standard logistic function 
the logistic function is sigmoid function which takes any real input and outputs value between zero and one 
for the logit this is interpreted as taking input log odds and having output probability 
the standard logistic function is defined as follows graph of the logistic function on the interval is shown in figure let us assume that is linear function of single explanatory variable the case where is linear combination of multiple explanatory variables is treated similarly 
we can then express as follows and the general logistic function can now be written as in the logistic model is interpreted as the probability of the dependent variable equaling success case rather than failure non case 
it clear that the response variables are not identically distributed differs from one data point to another though they are independent given design matrix and shared parameters 
definition of the inverse of the logistic function we can now define the logit log odds function as the inverse of the standard logistic function 
it is easy to see that it satisfies logit ln and equivalently after exponentiating both sides we have the odds 
interpretation of these terms in the above equations the terms are as follows is the logit function 
the equation for illustrates that the logit log odds or natural logarithm of the odds is equivalent to the linear regression expression 
ln denotes the natural logarithm 
is the probability that the dependent variable equals case given some linear combination of the predictors 
the formula for illustrates that the probability of the dependent variable equaling case is equal to the value of the logistic function of the linear regression expression 
this is important in that it shows that the value of the linear regression expression can vary from negative to positive infinity and yet after transformation the resulting expression for the probability ranges between and is the intercept from the linear regression equation the value of the criterion when the predictor is equal to zero 
is the regression coefficient multiplied by some value of the predictor 
base denotes the exponential function 
definition of the odds the odds of the dependent variable equaling case given some linear combination of the predictors is equivalent to the exponential function of the linear regression expression 
this illustrates how the logit serves as link function between the probability and the linear regression expression 
given that the logit ranges between negative and positive infinity it provides an adequate criterion upon which to conduct linear regression and the logit is easily converted back into the odds so we define odds of the dependent variable equaling case given some linear combination of the predictors as follows odds 
the odds ratio for continuous independent variable the odds ratio can be defined as odds odds this exponential relationship provides an interpretation for the odds multiply by for every unit increase in for binary independent variable the odds ratio is defined as where and are cells in contingency table 
multiple explanatory variables if there are multiple explanatory variables the above expression can be revised to then when this is used in the equation relating the log odds of success to the values of the predictors the linear regression will be multiple regression with explanators the parameters for all are all estimated 
again the more traditional equations are log and where usually definition the basic setup of logistic regression is as follows 
we are given dataset containing points 
each point consists of set of input variables xm also called independent variables explanatory variables predictor variables features or attributes and binary outcome variable yi also known as dependent variable response variable output variable or class 
it can assume only the two possible values often meaning no or failure or often meaning yes or success 
the goal of logistic regression is to use the dataset to create predictive model of the outcome variable 
as in linear regression the outcome variables yi are assumed to depend on the explanatory variables xm 
explanatory variablesthe explanatory variables may be of any type real valued binary categorical etc 
the main distinction is between continuous variables and discrete variables 
discrete variables referring to more than two possible choices are typically coded using dummy variables or indicator variables that is separate explanatory variables taking the value or are created for each possible value of the discrete variable with meaning variable does have the given value and meaning variable does not have that value 
outcome variablesformally the outcomes yi are described as being bernoulli distributed data where each outcome is determined by an unobserved probability pi that is specific to the outcome at hand but related to the explanatory variables 
this can be expressed in any of the following equivalent forms bernoulli pr if if pr the meanings of these four lines are the first line expresses the probability distribution of each yi conditioned on the explanatory variables it follows bernoulli distribution with parameters pi the probability of the outcome of for trial as noted above each separate trial has its own probability of success just as each trial has its own explanatory variables 
the probability of success pi is not observed only the outcome of an individual bernoulli trial using that probability 
the second line expresses the fact that the expected value of each yi is equal to the probability of success pi which is general property of the bernoulli distribution 
in other words if we run large number of bernoulli trials using the same probability of success pi then take the average of all the and outcomes then the result would be close to pi 
this is because doing an average this way simply computes the proportion of successes seen which we expect to converge to the underlying probability of success 
the third line writes out the probability mass function of the bernoulli distribution specifying the probability of seeing each of the two possible outcomes 
the fourth line is another way of writing the probability mass function which avoids having to write separate cases and is more convenient for certain types of calculations 
this relies on the fact that yi can take only the value or in each case one of the exponents will be choosing the value under it while the other is canceling out the value under it 
hence the outcome is either pi or pi as in the previous line linear predictor functionthe basic idea of logistic regression is to use the mechanism already developed for linear regression by modeling the probability pi using linear predictor function 
linear combination of the explanatory variables and set of regression coefficients that are specific to the model at hand but the same for all trials 
the linear predictor function for particular data point is written as where are regression coefficients indicating the relative effect of particular explanatory variable on the outcome 
the model is usually put into more compact form as follows the regression coefficients are grouped into single vector of size for each data point an additional explanatory pseudo variable is added with fixed value of corresponding to the intercept coefficient 
the resulting explanatory variables xm are then grouped into single vector xi of size this makes it possible to write the linear predictor function as follows using the notation for dot product between two vectors 
many explanatory variables two categories the above example of binary logistic regression on one explanatory variable can be generalized to binary logistic regression on any number of explanatory variables and any number of categorical values 
to begin with we may consider logistic model with explanatory variables xm and as in the example above two categorical values and 
for the simple binary logistic regression model we assumed linear relationship between the predictor variable and the log odds also called logit of the event that this linear relationship may be extended to the case of explanatory variables log where is the log odds and are parameters of the model 
an additional generalization has been introduced in which the base of the model is not restricted to the euler number in most applications the base of the logarithm is usually taken to be however in some cases it can be easier to communicate results by working in base or base for more compact notation we will specify the explanatory variables and the coefficients as dimensional vectors with an added explanatory variable 
the logit may now be written as solving for the probability that yields where is the sigmoid function with base the above formula shows that once the are fixed we can easily compute either the log odds that for given observation or the probability that for given observation 
the main use case of logistic model is to be given an observation and estimate the probability that the optimum beta coefficients may again be found by maximizing the log likelihood 
for measurements defining xk as the explanatory vector of the th measurement and yk as the categorical outcome of that measurement the log likelihood may be written in form very similar to the simple case above log log as in the simple example above finding the optimum parameters will require numerical methods 
one useful technique is to equate the derivatives of the log likelihood with respect to each of the parameters to zero yielding set of equations which will hold at the maximum of the log likelihood where xmk is the value of the xm explanatory variable from the th measurement 
consider an example with explanatory variables and coefficients and which have been determined by the above method 
to be concrete the model is log where is the probability of the event that this can be interpreted as follows is the intercept 
it is the log odds of the event that when the predictors by exponentiating we can see that when the odds of the event that are to or similarly the probability of the event that when can be computed as means that increasing by increases the log odds by so if increases by the odds that increase by factor of note that the probability of has also increased but it has not increased by as much as the odds have increased 
means that increasing by increases the log odds by so if increases by the odds that increase by factor of note how the effect of on the log odds is twice as great as the effect of but the effect on the odds is times greater 
but the effect on the probability of is not as much as times greater it only the effect on the odds that is times greater 
multinomial logistic regression many explanatory variables and many categories in the above cases of two categories binomial logistic regression the categories were indexed by and and we had two probability distributions the probability that the outcome was in category was given by and the probability that the outcome was in category was given by 
the sum of both probabilities is equal to unity as they must be 
in general if we have explanatory variables including and categories we will need separate probability distributions one for each category indexed by which describe the probability that the categorical outcome for explanatory vector will be in category 
it will also be required that the sum of these probabilities over all categories be equal to unity 
using the mathematically convenient base these probabilities are for each of the probabilities except will have their own set of regression coefficients it can be seen that as required the sum of the over all categories is unity 
note that the selection of to be defined in terms of the other probabilities is artificial 
any of the probabilities could have been selected to be so defined 
this special value of is termed the pivot index and the log odds tn are expressed in terms of the pivot probability and are again expressed as linear combination of the explanatory variables ln note also that for the simple case of the two category case is recovered with and 
the log likelihood that particular set of measurements or data points will be generated by the above probabilities can now be calculated 
indexing each measurement by let the th set of measured explanatory variables be denoted by and their categorical outcomes be denoted by which can be equal to any integer in 
the log likelihood is then ln where is an indicator function which is equal to unity if yk and zero otherwise 
in the case of two explanatory variables this indicator function was defined as yk when and yk when this was convenient but not necessary 
again the optimum beta coefficients may be found by maximizing the log likelihood function generally using numerical methods 
possible method of solution is to set the derivatives of the log likelihood with respect to each beta coefficient equal to zero and solve for the beta coefficients where is the th coefficient of the vector and is the th explanatory variable of the th measurement 
once the beta coefficients have been estimated from the data we will be able to estimate the probability that any subsequent set of explanatory variables will result in any of the possible outcome categories 
interpretations there are various equivalent specifications and interpretations of logistic regression which fit into different types of more general models and allow different generalizations 
as generalized linear model the particular model used by logistic regression which distinguishes it from standard linear regression and from other types of regression analysis used for binary valued outcomes is the way the probability of particular outcome is linked to the linear predictor function logit logit ln written using the more compact notation described above this is logit logit ln this formulation expresses logistic regression as type of generalized linear model which predicts variables with various types of probability distributions by fitting linear predictor function of the above form to some sort of arbitrary transformation of the expected value of the variable 
the intuition for transforming using the logit function the natural log of the odds was explained above 
it also has the practical effect of converting the probability which is bounded to be between and to variable that ranges over thereby matching the potential range of the linear prediction function on the right side of the equation 
note that both the probabilities pi and the regression coefficients are unobserved and the means of determining them is not part of the model itself 
they are typically determined by some sort of optimization procedure 
maximum likelihood estimation that finds values that best fit the observed data 
that give the most accurate predictions for the data already observed usually subject to regularization conditions that seek to exclude unlikely values 
extremely large values for any of the regression coefficients 
the use of regularization condition is equivalent to doing maximum posteriori map estimation an extension of maximum likelihood 
regularization is most commonly done using squared regularizing function which is equivalent to placing zero mean gaussian prior distribution on the coefficients but other regularizers are also possible 
whether or not regularization is used it is usually not possible to find closed form solution instead an iterative numerical method must be used such as iteratively reweighted least squares irls or more commonly these days quasi newton method such as the bfgs method the interpretation of the parameter estimates is as the additive effect on the log of the odds for unit change in the the explanatory variable 
in the case of dichotomous explanatory variable for instance gender is the estimate of the odds of having the outcome for say males compared with females 
an equivalent formula uses the inverse of the logit function which is the logistic function 
logit the formula can also be written as probability distribution specifically using probability mass function pr as latent variable model the logistic model has an equivalent formulation as latent variable model 
this formulation is common in the theory of discrete choice models and makes it easier to extend to certain more complicated models with multiple correlated choices as well as to compare logistic regression to the closely related probit model 
imagine that for each trial there is continuous latent variable yi 
an unobserved random variable that is distributed as follows where logistic 
the latent variable can be written directly in terms of the linear predictor function and an additive random error variable that is distributed according to standard logistic distribution 
then yi can be viewed as an indicator for whether this latent variable is positive if 
the choice of modeling the error variable specifically with standard logistic distribution rather than general logistic distribution with the location and scale set to arbitrary values seems restrictive but in fact it is not 
it must be kept in mind that we can choose the regression coefficients ourselves and very often can use them to offset changes in the parameters of the error variable distribution 
for example logistic error variable distribution with non zero location parameter which sets the mean is equivalent to distribution with zero location parameter where has been added to the intercept coefficient 
both situations produce the same value for yi regardless of settings of explanatory variables 
similarly an arbitrary scale parameter is equivalent to setting the scale parameter to and then dividing all regression coefficients by in the latter case the resulting value of yi will be smaller by factor of than in the former case for all sets of explanatory variables but critically it will always remain on the same side of and hence lead to the same yi choice 
note that this predicts that the irrelevancy of the scale parameter may not carry over into more complex models where more than two choices are available 
it turns out that this formulation is exactly equivalent to the preceding one phrased in terms of the generalized linear model and without any latent variables 
this can be shown as follows using the fact that the cumulative distribution function cdf of the standard logistic distribution is the logistic function which is the inverse of the logit function 
pr logit then pr pr pr pr pr because the logistic distribution is symmetric logit see above this formulation which is standard in discrete choice models makes clear the relationship between logistic regression the logit model and the probit model which uses an error variable distributed according to standard normal distribution instead of standard logistic distribution 
both the logistic and normal distributions are symmetric with basic unimodal bell curve shape 
the only difference is that the logistic distribution has somewhat heavier tails which means that it is less sensitive to outlying data and hence somewhat more robust to model mis specifications or erroneous data 
two way latent variable model yet another formulation uses two separate latent variables where ev ev where ev is standard type extreme value distribution 
pr pr then if otherwise 
this model has separate latent variable and separate set of regression coefficients for each possible outcome of the dependent variable 
the reason for this separation is that it makes it easy to extend logistic regression to multi outcome categorical variables as in the multinomial logit model 
in such model it is natural to model each possible outcome using different set of regression coefficients 
it is also possible to motivate each of the separate latent variables as the theoretical utility associated with making the associated choice and thus motivate logistic regression in terms of utility theory 
in terms of utility theory rational actor always chooses the choice with the greatest associated utility 
this is the approach taken by economists when formulating discrete choice models because it both provides theoretically strong foundation and facilitates intuitions about the model which in turn makes it easy to consider various sorts of extensions 
see the example below 
the choice of the type extreme value distribution seems fairly arbitrary but it makes the mathematics work out and it may be possible to justify its use through rational choice theory 
it turns out that this model is equivalent to the previous model although this seems non obvious since there are now two sets of regression coefficients and error variables and the error variables have different distribution 
in fact this model reduces directly to the previous one with the following substitutions an intuition for this comes from the fact that since we choose based on the maximum of two values only their difference matters not the exact values and this effectively removes one degree of freedom 
another critical fact is that the difference of two type extreme value distributed variables is logistic distribution 
we can demonstrate the equivalent as follows pr pr pr pr pr pr pr substitute as above pr substitute as above pr now same as above model pr logit example as an example consider province level election where the choice is between right of center party left of center party and secessionist party 
the parti qu cois which wants quebec to secede from canada 
we would then use three latent variables one for each choice 
then in accordance with utility theory we can then interpret the latent variables as expressing the utility that results from making each of the choices 
we can also interpret the regression coefficients as indicating the strength that the associated factor 
explanatory variable has in contributing to the utility or more correctly the amount by which unit change in an explanatory variable changes the utility of given choice 
voter might expect that the right of center party would lower taxes especially on rich people 
this would give low income people no benefit 
no change in utility since they usually don pay taxes would cause moderate benefit 
somewhat more money or moderate utility increase for middle incoming people would cause significant benefits for high income people 
on the other hand the left of center party might be expected to raise taxes and offset it with increased welfare and other assistance for the lower and middle classes 
this would cause significant positive benefit to low income people perhaps weak benefit to middle income people and significant negative benefit to high income people 
finally the secessionist party would take no direct actions on the economy but simply secede 
low income or middle income voter might expect basically no clear utility gain or loss from this but high income voter might expect negative utility since he she is likely to own companies which will have harder time doing business in such an environment and probably lose money 
these intuitions can be expressed as follows this clearly shows that separate sets of regression coefficients need to exist for each choice 
when phrased in terms of utility this can be seen very easily 
different choices have different effects on net utility furthermore the effects vary in complex ways that depend on the characteristics of each individual so there need to be separate sets of coefficients for each characteristic not simply single extra per choice characteristic 
even though income is continuous variable its effect on utility is too complex for it to be treated as single variable 
either it needs to be directly split up into ranges or higher powers of income need to be added so that polynomial regression on income is effectively done 
as log linear model yet another formulation combines the two way latent variable formulation above with the original formulation higher up without latent variables and in the process provides link to one of the standard formulations of the multinomial logit 
here instead of writing the logit of the probabilities pi as linear predictor we separate the linear predictor into two one for each of the two outcomes ln pr ln ln pr ln two separate sets of regression coefficients have been introduced just as in the two way latent variable model and the two equations appear form that writes the logarithm of the associated probability as linear predictor with an extra term ln at the end 
this term as it turns out serves as the normalizing factor ensuring that the result is distribution 
this can be seen by exponentiating both sides pr pr in this form it is clear that the purpose of is to ensure that the resulting distribution over yi is in fact probability distribution 
it sums to this means that is simply the sum of all un normalized probabilities and by dividing each probability by the probabilities become normalized 
that is and the resulting equations are pr pr or generally pr this shows clearly how to generalize this formulation to more than two outcomes as in multinomial logit 
note that this general formulation is exactly the softmax function as in pr softmax 
in order to prove that this is equivalent to the previous model note that the above model is overspecified in that pr and pr cannot be independently specified rather pr pr so knowing one automatically determines the other 
as result the model is nonidentifiable in that multiple combinations of and will produce the same probabilities for all possible explanatory variables 
in fact it can be seen that adding any constant vector to both of them will produce the same probabilities pr as result we can simplify matters and restore identifiability by picking an arbitrary value for one of the two vectors 
we choose to set then and so pr which shows that this formulation is indeed equivalent to the previous formulation 
as in the two way latent variable formulation any settings where will produce equivalent results 
note that most treatments of the multinomial logit model start out either by extending the log linear formulation presented here or the two way latent variable formulation presented above since both clearly show the way that the model could be extended to multi way outcomes 
in general the presentation with latent variables is more common in econometrics and political science where discrete choice models and utility theory reign while the log linear formulation here is more common in computer science 
machine learning and natural language processing 
as single layer perceptron the model has an equivalent formulation 
this functional form is commonly called single layer perceptron or single layer artificial neural network 
single layer neural network computes continuous output instead of step function 
the derivative of pi with respect to xk is computed from the general form where is an analytic function in with this choice the single layer neural network is identical to the logistic regression model 
this function has continuous derivative which allows it to be used in backpropagation 
this function is also preferred because its derivative is easily calculated 
in terms of binomial data closely related model assumes that each is associated not with single bernoulli trial but with ni independent identically distributed trials where the observation yi is the number of successes observed the sum of the individual bernoulli distributed random variables and hence follows binomial distribution bin for an example of this distribution is the fraction of seeds pi that germinate after ni are planted 
in terms of expected values this model is expressed as follows so that logit logit ln or equivalently pr this model can be fit using the same sorts of methods as the above more basic model 
model fitting maximum likelihood estimation mle the regression coefficients are usually estimated using maximum likelihood estimation 
unlike linear regression with normally distributed residuals it is not possible to find closed form expression for the coefficient values that maximize the likelihood function so that an iterative process must be used instead for example newton method 
this process begins with tentative solution revises it slightly to see if it can be improved and repeats this revision until no more improvement is made at which point the process is said to have converged in some instances the model may not reach convergence 
non convergence of model indicates that the coefficients are not meaningful because the iterative process was unable to find appropriate solutions 
failure to converge may occur for number of reasons having large ratio of predictors to cases multicollinearity sparseness or complete separation 
having large ratio of variables to cases results in an overly conservative wald statistic discussed below and can lead to non convergence 
regularized logistic regression is specifically intended to be used in this situation 
multicollinearity refers to unacceptably high correlations between predictors 
as multicollinearity increases coefficients remain unbiased but standard errors increase and the likelihood of model convergence decreases 
to detect multicollinearity amongst the predictors one can conduct linear regression analysis with the predictors of interest for the sole purpose of examining the tolerance statistic used to assess whether multicollinearity is unacceptably high 
sparseness in the data refers to having large proportion of empty cells cells with zero counts 
zero cell counts are particularly problematic with categorical predictors 
with continuous predictors the model can infer values for the zero cell counts but this is not the case with categorical predictors 
the model will not converge with zero cell counts for categorical predictors because the natural logarithm of zero is an undefined value so that the final solution to the model cannot be reached 
to remedy this problem researchers may collapse categories in theoretically meaningful way or add constant to all cells 
another numerical problem that may lead to lack of convergence is complete separation which refers to the instance in which the predictors perfectly predict the criterion all cases are accurately classified and the likelihood maximized with infinite coefficients 
in such instances one should re examine the data as there may be some kind of error 
one can also take semi parametric or non parametric approaches via local likelihood or nonparametric quasi likelihood methods which avoid assumptions of parametric form for the index function and is robust to the choice of the link function probit or logit 
iteratively reweighted least squares irls binary logistic regression or can for example be calculated using iteratively reweighted least squares irls which is equivalent to maximizing the log likelihood of bernoulli distributed process using newton method 
if the problem is written in vector matrix form with parameters explanatory variables and expected value of the bernoulli distribution the parameters can be found using the following iterative algorithm where diag is diagonal weighting matrix the vector of expected values the regressor matrix and the vector of response variables 
more details can be found in the literature 
bayesian in bayesian statistics context prior distributions are normally placed on the regression coefficients for example in the form of gaussian distributions 
there is no conjugate prior of the likelihood function in logistic regression 
when bayesian inference was performed analytically this made the posterior distribution difficult to calculate except in very low dimensions 
now though automatic software such as openbugs jags pymc stan or turing jl allows these posteriors to be computed using simulation so lack of conjugacy is not concern 
however when the sample size or the number of parameters is large full bayesian simulation can be slow and people often use approximate methods such as variational bayesian methods and expectation propagation 
rule of ten widely used rule of thumb the one in ten rule states that logistic regression models give stable values for the explanatory variables if based on minimum of about events per explanatory variable epv where event denotes the cases belonging to the less frequent category in the dependent variable 
thus study designed to use explanatory variables for an event 
myocardial infarction expected to occur in proportion of participants in the study will require total of participants 
however there is considerable debate about the reliability of this rule which is based on simulation studies and lacks secure theoretical underpinning 
according to some authors the rule is overly conservative in some circumstances with the authors stating if we somewhat subjectively regard confidence interval coverage less than percent type error greater than percent or relative bias greater than percent as problematic our results indicate that problems are fairly frequent with epv uncommon with epv and still observed with epv 
the worst instances of each problem were not severe with epv and usually comparable to those with epv others have found results that are not consistent with the above using different criteria 
useful criterion is whether the fitted model will be expected to achieve the same predictive discrimination in new sample as it appeared to achieve in the model development sample 
for that criterion events per candidate variable may be required 
also one can argue that observations are needed only to estimate the model intercept precisely enough that the margin of error in predicted probabilities is with confidence level 
error and significance of fit deviance and likelihood ratio test simple case in any fitting procedure the addition of another fitting parameter to model 
the beta parameters in logistic regression model will almost always improve the ability of the model to predict the measured outcomes 
this will be true even if the additional term has no predictive value since the model will simply be overfitting to the noise in the data 
the question arises as to whether the improvement gained by the addition of another fitting parameter is significant enough to recommend the inclusion of the additional term or whether the improvement is simply that which may be expected from overfitting 
in short for logistic regression statistic known as the deviance is defined which is measure of the error between the logistic model fit and the outcome data 
in the limit of large number of data points the deviance is chi squared distributed which allows chi squared test to be implemented in order to determine the significance of the explanatory variables 
linear regression and logistic regression have many similarities 
for example in simple linear regression set of data points xk yk are fitted to proposed model function of the form the fit is obtained by choosing the parameters which minimize the sum of the squares of the residuals the squared error term for each data point the minimum value which constitutes the fit will be denoted by the idea of null model may be introduced in which it is assumed that the variable is of no use in predicting the yk outcomes the data points are fitted to null model function of the form with squared error term the fitting process consists of choosing value of which minimizes of the fit to the null model denoted by where the subscript denotes the null model 
it is seen that the null model is optimized by where is the mean of the yk values and the optimized is which is proportional to the square of the uncorrected sample standard deviation of the yk data points 
we can imagine case where the yk data points are randomly assigned to the various xk and then fitted using the proposed model 
specifically we can consider the fits of the proposed model to every permutation of the yk outcomes 
it can be shown that the optimized error of any of these fits will never be less than the optimum error of the null model and that the difference between these minimum error will follow chi squared distribution distribution with degrees of freedom equal those of the proposed model minus those of the null model which in this case will be 
using the chi squared test we may then estimate how many of these permuted sets of yk will yield an minimum error less than or equal to the minimum error using the original yk and so we can estimate how significant an improvement is given by the inclusion of the variable in the proposed model 
for logistic regression the measure of goodness of fit is the likelihood function or its logarithm the log likelihood the likelihood function is analogous to the in the linear regression case except that the likelihood is maximized rather than minimized 
denote the maximized log likelihood of the proposed model by 
in the case of simple binary logistic regression the set of data points are fitted in probabilistic sense to function of the form where is the probability that the log odds are given by and the log likelihood is ln ln for the null model the probability that is given by the log odds for the null model are given by and the log likelihood is ln ln since we have at the maximum of the maximum log likelihood for the null model is ln ln the optimum is ln where is again the mean of the yk values 
again we can conceptually consider the fit of the proposed model to every permutation of the yk and it can be shown that the maximum log likelihood of these permutation fits will never be smaller than that of the null model also as an analog to the error of the linear regression case we may define the deviance of logistic regression fit as ln which will always be positive or zero 
the reason for this choice is that not only is the deviance good measure of the goodness of fit it is also approximately chi squared distributed with the approximation improving as the number of data points increases becoming exactly chi square distributed in the limit of an infinite number of data points 
as in the case of linear regression we may use this fact to estimate the probability that random set of data points will give better fit than the fit obtained by the proposed model and so have an estimate how significantly the model is improved by including the xk data points in the proposed model 
for the simple model of student test scores described above the maximum value of the log likelihood of the null model is the maximum value of the log likelihood for the simple model is so that the deviance is using the chi squared test of significance the integral of the chi squared distribution with one degree of freedom from to infinity is equal to this effectively means that about out of fits to random yk can be expected to have better fit smaller deviance than the given yk and so we can conclude that the inclusion of the variable and data in the proposed model is very significant improvement over the null model 
in other words we reject the null hypothesis with confidence 
goodness of fit summary goodness of fit in linear regression models is generally measured using 
since this has no direct analog in logistic regression various methods ch including the following can be used instead 
deviance and likelihood ratio tests in linear regression analysis one is concerned with partitioning variance via the sum of squares calculations variance in the criterion is essentially divided into variance accounted for by the predictors and residual variance 
in logistic regression analysis deviance is used in lieu of sum of squares calculations 
deviance is analogous to the sum of squares calculations in linear regression and is measure of the lack of fit to the data in logistic regression model 
when saturated model is available model with theoretically perfect fit deviance is calculated by comparing given model with the saturated model 
this computation gives the likelihood ratio test ln likelihood of the fitted model likelihood of the saturated model 
in the above equation represents the deviance and ln represents the natural logarithm 
the log of this likelihood ratio the ratio of the fitted model to the saturated model will produce negative value hence the need for negative sign 
can be shown to follow an approximate chi squared distribution 
smaller values indicate better fit as the fitted model deviates less from the saturated model 
when assessed upon chi square distribution nonsignificant chi square values indicate very little unexplained variance and thus good model fit 
conversely significant chi square value indicates that significant amount of the variance is unexplained 
when the saturated model is not available common case deviance is calculated simply as log likelihood of the fitted model and the reference to the saturated model log likelihood can be removed from all that follows without harm 
two measures of deviance are particularly important in logistic regression null deviance and model deviance 
the null deviance represents the difference between model with only the intercept which means no predictors and the saturated model 
the model deviance represents the difference between model with at least one predictor and the saturated model 
in this respect the null model provides baseline upon which to compare predictor models 
given that deviance is measure of the difference between given model and the saturated model smaller values indicate better fit 
thus to assess the contribution of predictor or set of predictors one can subtract the model deviance from the null deviance and assess the difference on chi square distribution with degrees of freedom equal to the difference in the number of parameters estimated 
let null ln likelihood of null model likelihood of the saturated model fitted ln likelihood of fitted model likelihood of the saturated model 
then the difference of both is null fitted ln likelihood of null model likelihood of the saturated model ln likelihood of fitted model likelihood of the saturated model ln likelihood of null model likelihood of the saturated model likelihood of fitted model likelihood of the saturated model ln likelihood of the null model likelihood of fitted model 
if the model deviance is significantly smaller than the null deviance then one can conclude that the predictor or set of predictors significantly improve the model fit 
this is analogous to the test used in linear regression analysis to assess the significance of prediction 
pseudo squared in linear regression the squared multiple correlation is used to assess goodness of fit as it represents the proportion of variance in the criterion that is explained by the predictors 
in logistic regression analysis there is no agreed upon analogous measure but there are several competing measures each with limitations four of the most commonly used indices and one less commonly used one are examined on this page likelihood ratio cox and snell cs nagelkerke mcfadden mcf tjur hosmer lemeshow test the hosmer lemeshow test uses test statistic that asymptotically follows distribution to assess whether or not the observed event rates match expected event rates in subgroups of the model population 
this test is considered to be obsolete by some statisticians because of its dependence on arbitrary binning of predicted probabilities and relative low power 
coefficient significance after fitting the model it is likely that researchers will want to examine the contribution of individual predictors 
to do so they will want to examine the regression coefficients 
in linear regression the regression coefficients represent the change in the criterion for each unit change in the predictor 
in logistic regression however the regression coefficients represent the change in the logit for each unit change in the predictor 
given that the logit is not intuitive researchers are likely to focus on predictor effect on the exponential function of the regression coefficient the odds ratio see definition 
in linear regression the significance of regression coefficient is assessed by computing test 
in logistic regression there are several different tests designed to assess the significance of an individual predictor most notably the likelihood ratio test and the wald statistic 
likelihood ratio test the likelihood ratio test discussed above to assess model fit is also the recommended procedure to assess the contribution of individual predictors to given model 
in the case of single predictor model one simply compares the deviance of the predictor model with that of the null model on chi square distribution with single degree of freedom 
if the predictor model has significantly smaller deviance 
chi square using the difference in degrees of freedom of the two models then one can conclude that there is significant association between the predictor and the outcome 
although some common statistical packages 
spss do provide likelihood ratio test statistics without this computationally intensive test it would be more difficult to assess the contribution of individual predictors in the multiple logistic regression case 
to assess the contribution of individual predictors one can enter the predictors hierarchically comparing each new model with the previous to determine the contribution of each predictor 
there is some debate among statisticians about the appropriateness of so called stepwise procedures 
the fear is that they may not preserve nominal statistical properties and may become misleading 
wald statistic alternatively when assessing the contribution of individual predictors in given model one may examine the significance of the wald statistic 
the wald statistic analogous to the test in linear regression is used to assess the significance of coefficients 
the wald statistic is the ratio of the square of the regression coefficient to the square of the standard error of the coefficient and is asymptotically distributed as chi square distribution 
although several statistical packages spss sas report the wald statistic to assess the contribution of individual predictors the wald statistic has limitations 
when the regression coefficient is large the standard error of the regression coefficient also tends to be larger increasing the probability of type ii error 
the wald statistic also tends to be biased when data are sparse 
case control sampling suppose cases are rare 
then we might wish to sample them more frequently than their prevalence in the population 
for example suppose there is disease that affects person in and to collect our data we need to do complete physical 
it may be too expensive to do thousands of physicals of healthy people in order to obtain data for only few diseased individuals 
thus we may evaluate more diseased individuals perhaps all of the rare outcomes 
this is also retrospective sampling or equivalently it is called unbalanced data 
as rule of thumb sampling controls at rate of five times the number of cases will produce sufficient control data logistic regression is unique in that it may be estimated on unbalanced data rather than randomly sampled data and still yield correct coefficient estimates of the effects of each independent variable on the outcome 
that is to say if we form logistic model from such data if the model is correct in the general population the parameters are all correct except for we can correct if we know the true prevalence as follows log log where is the true prevalence and is the prevalence in the sample 
discussion like other forms of regression analysis logistic regression makes use of one or more predictor variables that may be either continuous or categorical 
unlike ordinary linear regression however logistic regression is used for predicting dependent variables that take membership in one of limited number of categories treating the dependent variable in the binomial case as the outcome of bernoulli trial rather than continuous outcome 
given this difference the assumptions of linear regression are violated 
in particular the residuals cannot be normally distributed 
in addition linear regression may make nonsensical predictions for binary dependent variable 
what is needed is way to convert binary variable into continuous one that can take on any real value negative or positive 
to do that binomial logistic regression first calculates the odds of the event happening for different levels of each independent variable and then takes its logarithm to create continuous criterion as transformed version of the dependent variable 
the logarithm of the odds is the logit of the probability the logit is defined as follows although the dependent variable in logistic regression is bernoulli the logit is on an unrestricted scale 
the logit function is the link function in this kind of generalized linear model 
is the bernoulli distributed response variable and is the predictor variable the values are the linear parameters 
the logit of the probability of success is then fitted to the predictors 
the predicted value of the logit is converted back into predicted odds via the inverse of the natural logarithm the exponential function 
thus although the observed dependent variable in binary logistic regression is or variable the logistic regression estimates the odds as continuous variable that the dependent variable is success 
in some applications the odds are all that is needed 
in others specific yes or no prediction is needed for whether the dependent variable is or is not success this categorical prediction can be based on the computed odds of success with predicted odds above some chosen cutoff value being translated into prediction of success 
maximum entropy of all the functional forms used for estimating the probabilities of particular categorical outcome which optimize the fit by maximizing the likelihood function 
probit regression poisson regression etc 
the logistic regression solution is unique in that it is maximum entropy solution 
this is case of general property an exponential family of distributions maximizes entropy given an expected value 
in the case of the logistic model the logistic function is the natural parameter of the bernoulli distribution it is in canonical form and the logistic function is the canonical link function while other sigmoid functions are non canonical link functions this underlies its mathematical elegance and ease of optimization 
see exponential family maximum entropy derivation for details 
proof in order to show this we use the method of lagrange multipliers 
the lagrangian is equal to the entropy plus the sum of the products of lagrange multipliers times various constraint expressions 
the general multinomial case will be considered since the proof is not made that much simpler by considering simpler cases 
equating the derivative of the lagrangian with respect to the various probabilities to zero yields functional form for those probabilities which corresponds to those used in logistic regression as in the above section on multinomial logistic regression we will consider explanatory variables denoted and which include there will be total of data points indexed by and the data points are given by and the xmk will also be represented as an dimensional vector 
there will be possible values of the categorical variable ranging from to let pn be the probability given explanatory variable vector that the outcome will be define which is the probability that for the th measurement the categorical outcome is the lagrangian will be expressed as function of the probabilities pnk and will minimized by equating the derivatives of the lagrangian with respect to these probabilities to zero 
an important point is that the probabilities are treated equally and the fact that they sum to unity is part of the lagrangian formulation rather than being assumed from the beginning 
the first contribution to the lagrangian is the entropy ln the log likelihood is ln assuming the multinomial logistic function the derivative of the log likelihood with respect the beta coefficients was found to be very important point here is that this expression is remarkably not an explicit function of the beta coefficients 
it is only function of the probabilities pnk and the data 
rather than being specific to the assumed multinomial logistic case it is taken to be general statement of the condition at which the log likelihood is maximized and makes no reference to the functional form of pnk 
there are then fitting constraints and the fitting constraint term in the lagrangian is then where the nm are the appropriate lagrange multipliers 
there are normalization constraints which may be written so that the normalization term in the lagrangian is where the are the appropriate lagrange multipliers 
the lagrangian is then the sum of the above three terms setting the derivative of the lagrangian with respect to one of the probabilities to zero yields ln using the more condensed vector notation and dropping the primes on the and indices and then solving for yields where imposing the normalization constraint we can solve for the zk and write the probabilities as the are not all independent 
we can add any constant dimensional vector to each of the without changing the value of the probabilities so that there are only rather than independent in the multinomial logistic regression section above the was subtracted from each which set the exponential term involving to unity and the beta coefficients were given by 
other approaches in machine learning applications where logistic regression is used for binary classification the mle minimises the cross entropy loss function 
logistic regression is an important machine learning algorithm 
the goal is to model the probability of random variable being or given experimental data consider generalized linear model function parameterized by pr therefore pr and since we see that pr is given by pr 
we now calculate the likelihood function assuming that all the observations in the sample are independently bernoulli distributed pr pr typically the log likelihood is maximized log log pr which is maximized using optimization techniques such as gradient descent 
assuming the pairs are drawn uniformly from the underlying distribution then in the limit of large lim log pr pr log pr pr log pr pr log pr kl where is the conditional entropy and kl is the kullback leibler divergence 
this leads to the intuition that by maximizing the log likelihood of model you are minimizing the kl divergence of your model from the maximal entropy distribution 
intuitively searching for the model that makes the fewest assumptions in its parameters 
comparison with linear regression logistic regression can be seen as special case of the generalized linear model and thus analogous to linear regression 
the model of logistic regression however is based on quite different assumptions about the relationship between the dependent and independent variables from those of linear regression 
in particular the key differences between these two models can be seen in the following two features of logistic regression 
first the conditional distribution is bernoulli distribution rather than gaussian distribution because the dependent variable is binary 
second the predicted values are probabilities and are therefore restricted to through the logistic distribution function because logistic regression predicts the probability of particular outcomes rather than the outcomes themselves 
alternatives common alternative to the logistic model logit model is the probit model as the related names suggest 
from the perspective of generalized linear models these differ in the choice of link function the logistic model uses the logit function inverse logistic function while the probit model uses the probit function inverse error function 
equivalently in the latent variable interpretations of these two methods the first assumes standard logistic distribution of errors and the second standard normal distribution of errors 
other sigmoid functions or error distributions can be used instead 
logistic regression is an alternative to fisher method linear discriminant analysis 
if the assumptions of linear discriminant analysis hold the conditioning can be reversed to produce logistic regression 
the converse is not true however because logistic regression does not require the multivariate normal assumption of discriminant analysis the assumption of linear predictor effects can easily be relaxed using techniques such as spline functions 
history detailed history of the logistic regression is given in cramer 
the logistic function was developed as model of population growth and named logistic by pierre fran ois verhulst in the and under the guidance of adolphe quetelet see logistic function history for details 
in his earliest paper verhulst did not specify how he fit the curves to the data 
in his more detailed paper verhulst determined the three parameters of the model by making the curve pass through three observed points which yielded poor predictions the logistic function was independently developed in chemistry as model of autocatalysis wilhelm ostwald 
an autocatalytic reaction is one in which one of the products is itself catalyst for the same reaction while the supply of one of the reactants is fixed 
this naturally gives rise to the logistic equation for the same reason as population growth the reaction is self reinforcing but constrained 
the logistic function was independently rediscovered as model of population growth in by raymond pearl and lowell reed published as pearl reed which led to its use in modern statistics 
they were initially unaware of verhulst work and presumably learned about it from gustave du pasquier but they gave him little credit and did not adopt his terminology 
verhulst priority was acknowledged and the term logistic revived by udny yule in and has been followed since 
pearl and reed first applied the model to the population of the united states and also initially fitted the curve by making it pass through three points as with verhulst this again yielded poor results in the the probit model was developed and systematized by chester ittner bliss who coined the term probit in bliss and by john gaddum in gaddum and the model fit by maximum likelihood estimation by ronald fisher in fisher as an addendum to bliss work 
the probit model was principally used in bioassay and had been preceded by earlier work dating to see probit model history 
the probit model influenced the subsequent development of the logit model and these models competed with each other the logistic model was likely first used as an alternative to the probit model in bioassay by edwin bidwell wilson and his student jane worcester in wilson worcester 
however the development of the logistic model as general alternative to the probit model was principally due to the work of joseph berkson over many decades beginning in berkson where he coined logit by analogy with probit and continuing through berkson and following years 
the logit model was initially dismissed as inferior to the probit model but gradually achieved an equal footing with the logit particularly between and by the logit model achieved parity with the probit model in use in statistics journals and thereafter surpassed it 
this relative popularity was due to the adoption of the logit outside of bioassay rather than displacing the probit within bioassay and its informal use in practice the logit popularity is credited to the logit model computational simplicity mathematical properties and generality allowing its use in varied fields various refinements occurred during that time notably by david cox as in cox the multinomial logit model was introduced independently in cox and thiel which greatly increased the scope of application and the popularity of the logit model 
in daniel mcfadden linked the multinomial logit to the theory of discrete choice specifically luce choice axiom showing that the multinomial logit followed from the assumption of independence of irrelevant alternatives and interpreting odds of alternatives as relative preferences this gave theoretical foundation for the logistic regression 
extensions there are large numbers of extensions multinomial logistic regression or multinomial logit handles the case of multi way categorical dependent variable with unordered values also called classification 
note that the general case of having dependent variables with more than two values is termed polytomous regression 
ordered logistic regression or ordered logit handles ordinal dependent variables ordered values 
mixed logit is an extension of multinomial logit that allows for correlations among the choices of the dependent variable 
an extension of the logistic model to sets of interdependent variables is the conditional random field 
conditional logistic regression handles matched or stratified data when the strata are small 
it is mostly used in the analysis of observational studies 
software most statistical software can do binary logistic regression 
spss for basic logistic regression 
stata sas proc logistic for basic logistic regression 
proc catmod when all the variables are categorical 
proc glimmix for multilevel model logistic regression 
glm in the stats package using family binomial lrm in the rms package glmnet package for an efficient implementation regularized logistic regression lmer for mixed effects logistic regression rfast package command gm logistic for fast and heavy calculations involving large scale data 
arm package for bayesian logistic regression python logit in the statsmodels module 
logisticregression in the scikit learn module 
logisticregressor in the tensorflow module 
full example of logistic regression in the theano tutorial bayesian logistic regression with ard prior code tutorial variational bayes logistic regression with ard prior code tutorial bayesian logistic regression code tutorial ncss logistic regression in ncss matlab mnrfit in the statistics and machine learning toolbox with incorrect coded as instead of fminunc fmincon fitglm mnrfit fitclinear mle can all do logistic regression 
java jvm liblinear apache flink apache spark sparkml supports logistic regression fpga logistic regresesion ip core in hls for fpga notably microsoft excel statistics extension package does not include it 
see also logistic function discrete choice jarrow turnbull model limited dependent variable multinomial logit model ordered logit hosmer lemeshow test brier score mlpack contains implementation of logistic regression local case control sampling logistic model tree references further reading external links media related to logistic regression at wikimedia commons econometrics lecture topic logit model on youtube by mark thoma logistic regression tutorial mlelr software in for teaching purposes
the chromatic circle is clock diagram for displaying relationships among the equal tempered pitch classes making up the familiar chromatic scale on circle 
explanation if one starts on any equal tempered pitch and repeatedly ascends by the musical interval of semitone one will eventually land on pitch with the same pitch class as the initial one having passed through all the other equal tempered chromatic pitch classes in between 
since the space is circular it is also possible to descend by semitone 
the chromatic circle is useful because it represents melodic distance which is often correlated with physical distance on musical instruments 
for instance to move from any on piano keyboard to the nearest one must move up four semitones corresponding to four clockwise steps on the chromatic circle 
one can also move down by eight semitones corresponding to eight counterclockwise steps on the pitch class circle 
larger motions on the piano or in pitch space can be represented in pitch class space by paths that wrap around the chromatic circle one or more times 
one can represent the twelve equal tempered pitch classes by the cyclic group of order twelve or equivalently the residue classes modulo twelve 
the group has four generators which can be identified with the ascending and descending semitones and the ascending and descending perfect fifths 
the semitonal generator gives rise to the chromatic circle while the perfect fifth gives rise to the circle of fifths 
comparison with circle of fifths key difference between the chromatic circle and the circle of fifths is that the former is truly continuous space every point on the circle corresponds to conceivable pitch class and every conceivable pitch class corresponds to point on the circle 
by contrast the circle of fifths is fundamentally discrete structure and there is no obvious way to assign pitch classes to each of its points 
pitch constellation pitch constellation is graphical representation of pitches used to describe musical scales modes chords or other groupings of pitches within an octave range 
it consists of circle with markings along the circumference or lines from the center which indicate pitches 
most pitch constellations use of subset of pitches chosen from the twelve pitch chromatic scale 
in this case the points on the circle are spaced like the twelve hour markings on an analog clock where each tick mark represents semitone 
scales and modes the pitch constellation provides an easy way to identify certain patterns and similarities between harmonic structures 
major scale consists of circle with markings at or and clock 
minor scale consists of circle with markings at or and clock 
the diagrams above show the two scales marked with scale degrees 
it can be observed that the tonic second fourth and fifth are shared while the minor scale flattens the third sixth and seventh notes relative to the major scale 
another observation is that the minor scale constellation is the same as the major scale but rotated degrees 
in the following drawing all of the major minor scales are drawn 
note that the constellation for all the major scales or all the minor scales are identical 
the different scales are generated by rotating the note overlay 
the notes that need to be sharpened flattened can be easily identified 
moreover if we draw all seven diatonic modes we can see them all as rotations of the ionian mode 
note also the significance of the clock point 
this corresponds to tritone 
the modes including pitches tritone from the tonic locrian and lydian are least used 
the clock and clock pitches are also important points corresponding to perfect fourth and perfect fifth respectively 
the most used scales modes major ionian mode minor aeolian mode and mixolydian include these pitches 
symmetric scales have simple representations in this scheme 
more exotic scales such as the pentatonic blues and octatonic can also be drawn and related to the common scales 
more complete list of musical scales and modes other overlays in previous sections we saw how various overlays scale degrees semi tone numbering notes can be used to notate the circumference of the constellation 
various other overlays can be laid around the constellation 
pitch ratios ratios of pitch frequencies 
note that once pitch constellation has been determined any number of overlays notes solf ge intervals etc 
may be placed on top for analysis comparison 
often generating one harmonic relationship from another is simply matter of rotating the overlay or constellation or shifting one or two pitch locations 
chords similarities between chords can also be observed as well as the significance of augmented diminished notes for triads we have the following and for seventh chords circle of fifths beginning with pitch constellation of chromatic scale the notes of circle of fifths can be easily generated 
starting at and moving across the circle and then one tick clockwise line is drawn with an arrow indicating the direction moved 
continuing from that point across the circle and one tick clockwise all points are connected 
moving through this pattern the notes of the circle of fifths can be determined 
one can also depict non tempered intervals on chromatic circle which allows one to depict commas small intervals particularly comma pumps 
for example using sequence of twelve just fifths ratio does not quite return to the starting point the size of the gap is the pythagorean comma resulting in broken circle of fifths 
technical note the ratio of the frequencies between two pitches in the constellation can be determined as follows 
take the length of the arc measured clockwise between the two points and divide by the circumference of the circle 
the frequency ratio is two raised to this power 
for example for fifth which is located at clock relative to the tonic the frequency ratio is references further reading brower candace cognitive theory of musical meaning journal of music theory duke university press doi jstor ku inskas darius symmetry in creative work of mikalojus konstantinas iurlionis pdf menotyra olson harry music physics and engineering dover publications isbn external links notenscheibe web application pitch constellations of scales triads intervals and the circle of fifths with basic audio on line app illustrating pitch constellations scaletapper iphone app which utilizes pitch constellations 
pdf of musical scales
in cryptography optimal asymmetric encryption padding oaep is padding scheme often used together with rsa encryption
oaep was introduced by bellare and rogaway and subsequently standardized in pkcs and rfc the oaep algorithm is form of feistel network which uses pair of random oracles and to process the plaintext prior to asymmetric encryption
when combined with any secure trapdoor one way permutation this processing is proved in the random oracle model to result in combined scheme which is semantically secure under chosen plaintext attack ind cpa
when implemented with certain trapdoor permutations rsa oaep is also proven to be secure against chosen ciphertext attack
oaep can be used to build an all or nothing transform
oaep satisfies the following two goals add an element of randomness which can be used to convert deterministic encryption scheme traditional rsa into probabilistic scheme
prevent partial decryption of ciphertexts or other information leakage by ensuring that an adversary cannot recover any portion of the plaintext without being able to invert the trapdoor one way permutation the original version of oaep bellare rogaway showed form of plaintext awareness which they claimed implies security against chosen ciphertext attack in the random oracle model when oaep is used with any trapdoor permutation
subsequent results contradicted this claim showing that oaep was only ind cca secure
however the original scheme was proved in the random oracle model to be ind cca secure when oaep is used with the rsa permutation using standard encryption exponents as in the case of rsa oaep
an improved scheme called oaep that works with any trapdoor one way permutation was offered by victor shoup to solve this problem
more recent work has shown that in the standard model that is when hash functions are not modeled as random oracles it is impossible to prove the ind cca security of rsa oaep under the assumed hardness of the rsa problem
algorithm in the diagram mgf is the mask generating function usually mgf hash is the chosen hash function hlen is the length of the output of the hash function in bytes is the length of the rsa modulus in bytes is the message to be padded at most bytes is an optional label to be associated with the message the label is the empty string by default and can be used to authenticate data without requiring encryption ps is byte string of null bytes
is an xor operation
encoding rfc for pkcs specifies the oaep scheme as follows for encoding hash the label using the chosen hash function generate padding string ps consisting of bytes with the value
concatenate lhash ps the single byte and the message to form data block db this data block has length bytes
generate random seed of length hlen
use the mask generating function to generate mask of the appropriate length for the data block mask the data block with the generated mask use the mask generating function to generate mask of length hlen for the seed mask the seed with the generated mask the encoded padded message is the byte concatenated with the maskedseed and maskeddb decoding decoding works by reversing the steps taken in the encoding algorithm hash the label using the chosen hash function to reverse step split the encoded message em into the byte the maskedseed with length hlen and the maskeddb generate the seedmask which was used to mask the seed to reverse step recover the seed with the seedmask generate the dbmask which was used to mask the data block to reverse step recover the data block db to reverse step split the data block into its parts verify that lhash is equal to the computed lhash ps only consists of bytes ps and are separated by the byte and the first byte of em is the byte
if any of these conditions aren met then the padding is invalid usage in rsa the encoded message can then be encrypted with rsa
the deterministic property of rsa is now avoided by using the oaep encoding because the seed is randomly generated and influences the entire encoded message
security the all or nothing security is from the fact that to recover one must recover the entire maskeddb and the entire maskedseed maskeddb is required to recover the seed from the maskedseed and the seed is required to recover the data block db from maskeddb
since any changed bit of cryptographic hash completely changes the result the entire maskeddb and the entire maskedseed must both be completely recovered
implementation in the pkcs standard the random oracles are identical
the pkcs standard further requires that the random oracles be mgf with an appropriate hash function
see also key encapsulation references
the digital signature algorithm dsa is public key cryptosystem and federal information processing standard for digital signatures based on the mathematical concept of modular exponentiation and the discrete logarithm problem
dsa is variant of the schnorr and elgamal signature schemes
the national institute of standards and technology nist proposed dsa for use in their digital signature standard dss in and adopted it as fips in four revisions to the initial specification have been released
the newest specification is fips from july dsa is patented but nist has made this patent available worldwide royalty free
draft version of the specification fips indicates dsa will no longer be approved for digital signature generation but may be used to verify signatures generated prior to the implementation date of that standard
overview the dsa works in the framework of public key cryptosystems and is based on the algebraic properties of modular exponentiation together with the discrete logarithm problem which is considered to be computationally intractable
the algorithm uses key pair consisting of public key and private key
the private key is used to generate digital signature for message and such signature can be verified by using the signer corresponding public key
the digital signature provides message authentication the receiver can verify the origin of the message integrity the receiver can verify that the message has not been modified since it was signed and non repudiation the sender cannot falsely claim that they have not signed the message
history in the government solicited proposals for public key signature standard
in august the national institute of standards and technology nist proposed dsa for use in their digital signature standard dss
initially there was significant criticism especially from software companies that had already invested effort in developing digital signature software based on the rsa cryptosystem
nevertheless nist adopted dsa as federal standard fips in four revisions to the initial specification have been released fips in fips in fips in and fips in draft version of standard fips forbids signing with dsa while allowing verification of signatures generated prior to the implementation date of the standard as document
it is to be replaced by newer signature schemes such as eddsa dsa is covered by patent filed july and now expired and attributed to david kravitz former nsa employee
this patent was given to the united states of america as represented by the secretary of commerce washington and nist has made this patent available worldwide royalty free
claus schnorr claims that his patent also now expired covered dsa this claim is disputed
operation the dsa algorithm involves four operations key generation which creates the key pair key distribution signing and signature verification
key generation key generation has two phases
the first phase is choice of algorithm parameters which may be shared between different users of the system while the second phase computes single key pair for one user
parameter generation choose an approved cryptographic hash function with output length bits
in the original dss was always sha but the stronger sha hash functions are approved for use in the current dss
if is greater than the modulus length only the leftmost bits of the hash output are used
choose key length the original dss constrained to be multiple of between and inclusive
nist recommends lengths of or for keys with security lifetimes extending beyond or
choose the modulus length such that and
fips specifies and to have one of the values or
choose an bit prime choose an bit prime such that is multiple of choose an integer randomly from
compute mod in the rare case that try again with different commonly is used
this modular exponentiation can be computed efficiently even if the values are large the algorithm parameters are
these may be shared between different users of the system
per user keys given set of parameters the second phase computes the key pair for single user choose an integer randomly from
compute mod is the private key and is the public key
key distribution the signer should publish the public key that is they should send the key to the receiver via reliable but not necessarily secret mechanism
the signer should keep the private key secret
signing message is signed as follows choose an integer randomly from compute mod mod in the unlikely case that start again with different random compute mod in the unlikely case that start again with different random the signature is the calculation of and amounts to creating new per message key
the modular exponentiation in computing is the most computationally expensive part of the signing operation but it may be computed before the message is known
calculating the modular inverse mod is the second most expensive part and it may also be computed before the message is known
it may be computed using the extended euclidean algorithm or using fermat little theorem as mod
signature verification one can verify that signature is valid signature for message as follows verify that and compute mod compute mod compute mod compute mod mod the signature is valid if and only if correctness of the algorithm the signature scheme is correct in the sense that the verifier will always accept genuine signatures
this can be shown as follows first since mod it follows that mod by fermat little theorem
since and is prime must have order the signer computes mod thus mod since has order we have mod finally the correctness of dsa follows from mod mod mod mod sensitivity with dsa the entropy secrecy and uniqueness of the random signature value are critical
it is so critical that violating any one of those three requirements can reveal the entire private key to an attacker
using the same value twice even while keeping secret using predictable value or leaking even few bits of in each of several signatures is enough to reveal the private key this issue affects both dsa and elliptic curve digital signature algorithm ecdsa in december group calling itself fail verflow announced recovery of the ecdsa private key used by sony to sign software for the playstation game console
the attack was made possible because sony failed to generate new random for each signature this issue can be prevented by deriving deterministically from the private key and the message hash as described by rfc this ensures that is different for each and unpredictable for attackers who do not know the private key in addition malicious implementations of dsa and ecdsa can be created where is chosen in order to subliminally leak information via signatures
for example an offline private key could be leaked from perfect offline device that only released innocent looking signatures
implementations below is list of cryptographic libraries that provide support for dsa botan bouncy castle cryptlib crypto libgcrypt nettle openssl wolfcrypt gnutls see also modular arithmetic rsa cryptosystem ecdsa references external links fips pub digital signature standard dss the fourth and current revision of the official dsa specification
recommendation for key management part general nist special publication
in cryptography related key attack is any form of cryptanalysis where the attacker can observe the operation of cipher under several different keys whose values are initially unknown but where some mathematical relationship connecting the keys is known to the attacker
for example the attacker might know that the last bits of the keys are always the same even though they don know at first what the bits are
this appears at first glance to be an unrealistic model it would certainly be unlikely that an attacker could persuade human cryptographer to encrypt plaintexts under numerous secret keys related in some way
kasumi kasumi is an eight round bit block cipher with bit key
it is based upon misty and was designed to form the basis of the confidentiality and integrity algorithms
mark blunden and adrian escott described differential related key attacks on five and six rounds of kasumi
differential attacks were introduced by biham and shamir
related key attacks were first introduced by biham
differential related key attacks are discussed in kelsey et al
wep an important example of cryptographic protocol that failed because of related key attack is wired equivalent privacy wep used in wi fi wireless networks
each client wi fi network adapter and wireless access point in wep protected network shares the same wep key
encryption uses the rc algorithm stream cipher
it is essential that the same key never be used twice with stream cipher
to prevent this from happening wep includes bit initialization vector iv in each message packet
the rc key for that packet is the iv concatenated with the wep key
wep keys have to be changed manually and this typically happens infrequently
an attacker therefore can assume that all the keys used to encrypt packets share single wep key
this fact opened up wep to series of attacks which proved devastating
the simplest to understand uses the fact that the bit iv only allows little under million possibilities
because of the birthday paradox it is likely that for every packets two will share the same iv and hence the same rc key allowing the packets to be attacked
more devastating attacks take advantage of certain weak keys in rc and eventually allow the wep key itself to be recovered
in agents from the federal bureau of investigation publicly demonstrated the ability to do this with widely available software tools in about three minutes
preventing related key attacks one approach to preventing related key attacks is to design protocols and applications so that encryption keys will never have simple relationship with each other
for example each encryption key can be generated from the underlying key material using key derivation function
for example replacement for wep wi fi protected access wpa uses three levels of keys master key working key and rc key
the master wpa key is shared with each client and access point and is used in protocol called temporal key integrity protocol tkip to create new working keys frequently enough to thwart known attack methods
the working keys are then combined with longer bit iv to form the rc key for each packet
this design mimics the wep approach enough to allow wpa to be used with first generation wi fi network cards some of which implemented portions of wep in hardware
however not all first generation access points can run wpa
another more conservative approach is to employ cipher designed to prevent related key attacks altogether usually by incorporating strong key schedule
newer version of wi fi protected access wpa uses the aes block cipher instead of rc in part for this reason
there are related key attacks against aes but unlike those against rc they re far from practical to implement and wpa key generation functions may provide some security against them
many older network cards cannot run wpa
in western music the adjectives major and minor may describe chord scale or key 
as such composition movement section or phrase may be referred to by its key including whether that key is major or minor 
intervals some intervals may be referred to as major and minor 
major interval is one semitone larger than minor interval 
the words perfect diminished and augmented are also used to describe the quality of an interval 
only the intervals of second third sixth and seventh and the compound intervals based on them may be major or minor or rarely diminished or augmented 
unisons fourths fifths and octaves and their compound interval must be perfect or rarely diminished or augmented 
in western music minor chord sounds darker than major chord 
scales and chords the other uses of major and minor generally refer to scales and chords that contain major third or minor third respectively 
major scale is scale in which the third scale degree the mediant is major third above the tonic note 
in minor scale the third degree is minor third above the tonic 
similarly in major triad or major seventh chord the third is major third above the chord root 
in minor triad or minor seventh chord the third is minor third above the root 
keys the hallmark that distinguishes major keys from minor is whether the third scale degree is major or minor 
as musicologist roger kamien explains the crucial difference is that in the minor scale there is only half step between nd and rd note and between th and th note as compared to the major scales where the difference between rd and th note and between th and th note is half step 
this alteration in the third degree greatly changes the mood of the music and music based on minor scales tends to be considered to sound serious or melancholic minor keys are sometimes said to have more interesting possibly darker sound than plain major scales 
harry partch considers minor as the immutable faculty of ratios which in turn represent an immutable faculty of the human ear 
the minor key and scale are also considered less justifiable than the major with paul hindemith calling it clouding of major and moritz hauptmann calling it falsehood of the major changes of mode which involve the alteration of the third and mode mixture are often analyzed as minor changes unless structurally supported because the root and overall key and tonality remain unchanged 
this is in contrast with for instance transposition 
transposition is done by moving all intervals up or down certain constant interval and does change the key but not the mode which requires the alteration of intervals 
the use of triads only available in the minor mode such as the use of major in major is relatively decorative chromaticism considered to add color and weaken the sense of key without entirely destroying or losing it 
intonation and tuning musical tuning of intervals is expressed by the ratio between the pitches frequencies 
simple fractions can sound more harmonious than complex fractions for instance an octave is simple ratio and fifth is the relatively simple ratio 
the table below gives approximations of scale to ratios that are rounded to be as simple as possible 
in just intonation minor chord is often but not exclusively tuned in the frequency ratio play 
in tone equal temperament tet which is now the most common tuning system in the west minor chord has semitones between the root and third between the third and fifth and between the root and fifth 
in tet the perfect fifth cents is only about two cents narrower than the just tuned perfect fifth cents but the minor third cents is noticeably about cents narrower than the just minor third cents 
moreover the minor third cents more closely approximates the limit limit music minor third play cents the nineteenth harmonic with about two cents error alexander ellis proposes that the conflict between mathematicians and physicists on one hand and practicing musicians on the other regarding the supposed inferiority of the minor chord and scale to the major may be explained due to physicists comparison of just minor and major triads in which case minor comes out the loser versus the musicians comparison of the equal tempered triads in which case minor comes out the winner since the et major third is about cents sharp from the just major third cents but just about four cents narrower than the limit major third cents while the et minor third closely approximates the minor third which many find pleasing 
advanced theory in the neo riemannian theory the minor mode is considered the inverse of the major mode an upside down major scale based on theoretical undertones rather than actual overtones harmonics see also utonality 
the root of the minor triad is thus considered the top of the fifth which in the united states is called the fifth 
so in minor the tonic is actually and the leading tone is half step rather than in major the root being and the leading tone half step 
also since all chords are analyzed as having tonic subdominant or dominant function with for instance in minor being considered the tonic parallel us relative tp the use of minor mode root chord progressions in major such as major major major is analyzed as sp dp the minor subdominant parallel see parallel chord the minor dominant parallel and the major tonic 
see also gypsy scale list of major minor compositions music written in all major and or minor keys otonality and utonality references
searchable symmetric encryption sse is form of encryption that allows one to efficiently search over collection of encrypted documents or files without the ability to decrypt them
sse can be used to outsource files to an untrusted cloud storage server without ever revealing the files in the clear but while preserving the server ability to search over them
description searchable symmetric encryption scheme is symmetric key encryption scheme that encrypts collection of documents where each document is viewed as set of keywords from keyword space given the encryption key and keyword one can generate search token with which the encrypted data collection can be searched for the result of the search is the subset of encrypted documents that contain the keyword static sse static sse scheme consists of three algorithms that work as follows takes as input security parameter and document collection and outputs symmetric key and an encrypted document collection takes as input the secret key and keyword and outputs search token takes as input the encrypted document collection and search token and outputs set of encrypted documents static sse scheme is used by client and an untrusted server as follows
the client encrypts its data collection using the algorithm which returns secret key and an encrypted document collection the client keeps secret and sends to the untrusted server
to search for keyword the client runs the algorithm on and to generate search token which it sends to the server
the server runs search with and and returns the resulting encrypted documents back to the server
dynamic sse dynamic sse scheme supports in addition to search the insertion and deletion of documents
dynamic sse scheme consists of seven algorithms where and are as in the static case and the remaining algorithms work as follows takes as input the secret key and new document and outputs an insert token takes as input the encrypted document collection edc and an insert token and outputs an updated encrypted document collection takes as input the secret key and document identifier and outputs delete token takes as input the encrypted data collection and delete token and outputs an updated encrypted data collection to add new document the client runs on and to generate an insert token which it sends to the server
the server runs with and and stores the updated encrypted document collection
to delete document with identifier the client runs the algorithm with and to generate delete token which it sends to the server
the server runs with and and stores the updated encrypted document collection
an sse scheme that does not support and is called semi dynamic
history of searchable symmetric encryption the problem of searching on encrypted data was considered by song wagner and perrig though previous work on oblivious ram by goldreich and ostrovsky could be used in theory to address the problem
this work proposed an sse scheme with search algorithm that runs in time where
goh and chang and mitzenmacher gave new sse constructions with search algorithms that run in time where is the number of documents
curtmola garay kamara and ostrovsky later proposed two static constructions with search time where is the number of documents that contain which is optimal
this work also proposed semi dynamic construction with log search time where is the number of updates
an optimal dynamic sse construction was later proposed by kamara papamanthou and roeder goh and chang and mitzenmacher proposed security definitions for sse
these were strengthened and extended by curtmola garay kamara and ostrovsky who proposed the notion of adaptive security for sse
this work also was the first to observe leakage in sse and to formally capture it as part of the security definition
leakage was further formalized and generalized by chase and kamara
islam kuzu and kantarcioglu described the first leakage attack all the previously mentioned constructions support single keyword search
cash jarecki jutla krawczyk rosu and steiner proposed an sse scheme that supports conjunctive search in sub linear time in the construction can also be extended to support disjunctive and boolean searches that can be expressed in searchable normal form snf in sub linear time
at the same time pappas krell vo kolesnikov malkin choi george keromytis and bellovin described construction that supports conjunctive and all disjunctive and boolean searches in sub linear time
security sse schemes are designed to guarantee that the untrusted server cannot learn any partial information about the documents or the search queries beyond some well defined and reasonable leakage
the leakage of scheme is formally described using leakage profile which itself can consists of several leakage patterns
sse constructions attempt to minimize leakage while achieving the best possible search efficiency
sse security can be analyzed in several adversarial models but the most common are the persistent model where an adversary is given the encrypted data collection and transcript of all the operations executed on the collection the snapshot model where an adversary is only given the encrypted data collection but possibly after each operation
security in the persistent model in the persistent model there are sse schemes that achieve wide variety of leakage profiles
the most common leakage profile for static schemes that achieve single keyword search in optimal time is which reveals the number of documents in the collection the size of each document in the collection if and when query was repeated and which encrypted documents match the search query
it is known however how to construct schemes that leak considerably less at an additional cost in search time and storage when considering dynamic sse schemes the state of the art constructions with optimal time search have leakage profiles that guarantee forward privacy which means that inserts cannot be correlated with past search queries
security in the snapshot model in the snapshot model efficient dynamic sse schemes with no leakage beyond the number of documents and the size of the collection can be constructed
when using an sse construction that is secure in the snapshot model one has to carefully consider how the scheme will be deployed because some systems might cache previous search queries
cryptanalysis leakage profile only describes the leakage of an sse scheme but it says nothing about whether that leakage can be exploited or not
cryptanalysis is therefore used to better understand the real world security of leakage profile
there is wide variety of attacks working in different adversarial models based on variety of assumptions and attacking different leakage profiles
see also homomorphic encryption oblivious ram structured encryption deterministic encryption references
aids is caused by human immunodeficiency virus hiv which originated in non human primates in central and west africa 
while various sub groups of the virus acquired human infectivity at different times the present pandemic had its origins in the emergence of one specific strain hiv subgroup in opoldville in the belgian congo now kinshasa in the democratic republic of the congo in the there are two types of hiv hiv and hiv 
hiv is more virulent easily transmitted and is the cause of the vast majority of hiv infections globally 
the pandemic strain of hiv is closely related to virus found in chimpanzees of the subspecies pan troglodytes troglodytes which live in the forests of the central african nations of cameroon equatorial guinea gabon the republic of the congo and the central african republic 
hiv is less transmittable and is largely confined to west africa along with its closest relative virus of the sooty mangabey cercocebus atys atys an old world monkey inhabiting southern senegal guinea bissau guinea sierra leone liberia and western ivory coast 
transmission from non humans to humans research in this area is conducted using molecular phylogenetics comparing viral genomic sequences to determine relatedness 
hiv from chimpanzees and gorillas to humans scientists generally accept that the known strains or groups of hiv are most closely related to the simian immunodeficiency viruses sivs endemic in wild ape populations of west central african forests 
in particular each of the known hiv strains is either closely related to the siv that infects the chimpanzee subspecies pan troglodytes troglodytes sivcpz or closely related to the siv that infects western lowland gorillas gorilla gorilla gorilla called sivgor 
the pandemic hiv strain group or main and rare strain found only in few cameroonian people group are clearly derived from sivcpz strains endemic in pan troglodytes troglodytes chimpanzee populations living in cameroon 
another very rare hiv strain group is clearly derived from sivgor strains of cameroon 
finally the primate ancestor of hiv group strain infecting people mostly from cameroon but also from neighbouring countries was confirmed in to be sivgor 
the pandemic hiv group is most closely related to the sivcpz collected from the southeastern rain forests of cameroon modern east province near the sangha river 
thus this region is presumably where the virus was first transmitted from chimpanzees to humans 
however reviews of the epidemiological evidence of early hiv infection in stored blood samples and of old cases of aids in central africa have led many scientists to believe that hiv group early human centre was probably not in cameroon but rather further south in the democratic republic of the congo then the belgian congo more probably in its capital city kinshasa formerly opoldville using hiv sequences preserved in human biological samples along with estimates of viral mutation rates scientists calculate that the jump from chimpanzee to human probably happened during the late th or early th century time of rapid urbanisation and colonisation in equatorial africa 
exactly when the zoonosis occurred is not known 
some molecular dating studies suggest that hiv group had its most recent common ancestor mrca that is started to spread in the human population in the early th century probably between and study published in analyzing viral sequences recovered from biopsy made in kinshasa in along with previously known sequences suggested common ancestor between and with central estimates varying between and 
genetic recombination had earlier been thought to seriously confound such phylogenetic analysis but later work has suggested that recombination is not likely to systematically bias results although recombination is expected to increase variance 
the results of phylogenetics study support the later work and indicate that hiv evolves fairly reliably 
further research was hindered due to the primates being critically endangered 
sample analyses resulted in little data due to the rarity of experimental material 
the researchers however were able to hypothesize phylogeny from the gathered data 
they were also able to use the molecular clock of specific strain of hiv to determine the initial date of transmission which is estimated to be around 
hiv from sooty mangabeys to humans similar research has been undertaken with siv strains collected from several wild sooty mangabey cercocebus atys atys sivsmm populations of the west african nations of sierra leone liberia and ivory coast 
the resulting phylogenetic analyses show that the viruses most closely related to the two strains of hiv that spread considerably in humans hiv groups and are the sivsmm found in the sooty mangabeys of the tai forest in western ivory coast there are six additional known hiv groups each having been found in just one person 
they all seem to derive from independent transmissions from sooty mangabeys to humans 
groups and have been found in two people from liberia groups and have been discovered in two people from sierra leone and groups and have been detected in two people from the ivory coast 
these hiv strains are probably dead end infections and each of them is most closely related to sivsmm strains from sooty mangabeys living in the same country where the human infection was found molecular dating studies suggest that both the epidemic groups and started to spread among humans between and with the central estimates varying between and 
bushmeat practice according to the natural transfer theory also called hunter theory or bushmeat theory in the simplest and most plausible explanation for the cross species transmission of siv or hiv post mutation the virus was transmitted from an ape or monkey to human when hunter or bushmeat vendor handler was bitten or cut while hunting or butchering the animal 
the resulting exposure to blood or other bodily fluids of the animal can result in siv infection 
prior to wwii some sub saharan africans were forced out of the rural areas because of the european demand for resources 
since rural africans were not keen to pursue agricultural practices in the jungle they turned to non domesticated animals as their primary source of meat 
this over exposure to bushmeat and malpractice of butchery increased blood to blood contact which then increased the probability of transmission 
recent serological survey showed that human infections by siv are not rare in central africa the percentage of people showing seroreactivity to antigens evidence of current or past siv infection was among the general population of cameroon in villages where bushmeat is hunted or used and in the most exposed people of these villages 
how the siv virus would have transformed into hiv after infection of the hunter or bushmeat handler from the ape monkey is still matter of debate although natural selection would favour any viruses capable of adjusting so that they could infect and reproduce in the cells of human host 
emergence unresolved questions about hiv origins and emergence the discovery of the main hiv siv phylogenetic relationships permits explaining broad hiv biogeography the early centres of the hiv groups were in central africa where the primate reservoirs of the related sivcpz and sivgor viruses chimpanzees and gorillas exist similarly the hiv groups had their centres in west africa where sooty mangabeys which harbour the related sivsmm virus exist 
however these relationships do not explain more detailed patterns of biogeography such as why epidemic hiv groups and only evolved in the ivory coast which is one of only six countries harbouring the sooty mangabey 
it is also unclear why the sivcpz endemic in the chimpanzee subspecies pan troglodytes schweinfurthii inhabiting the democratic republic of congo central african republic rwanda burundi uganda and tanzania did not spawn an epidemic hiv strain to humans while the democratic republic of congo was the main centre of hiv group virus descended from sivcpz strains of subspecies pan troglodytes troglodytes that does not exist in this country 
it is clear that the several hiv and hiv strains descend from sivcpz sivgor and sivsmm viruses and that bushmeat practice provides the most plausible cause of cross species transfer to humans 
however some loose ends remain 
it is not yet explained why only four hiv groups hiv groups and and hiv groups and spread considerably in human populations despite bushmeat practices being widespread in central and west africa and the resulting human siv infections being common it also remains unexplained why all epidemic hiv groups emerged in humans nearly simultaneously and only in the th century despite very old human exposure to siv phylogenetic study demonstrated that siv is at least tens of thousands of years old 
origin and epidemic emergence several of the theories of hiv origin accept the established knowledge of the hiv siv phylogenetic relationships and also accept that bushmeat practice was the most likely cause of the initial transfer to humans 
all of them propose that the simultaneous epidemic emergences of four hiv groups in the late th early th century and the lack of previous known emergences are explained by new factor that appeared in the relevant african regions in that timeframe 
these new factor would have acted either to increase human exposures to siv to help it to adapt to the human organism by mutation thus enhancing its between humans transmissibility or to cause an initial burst of transmissions crossing an epidemiological threshold and therefore increasing the probability of continued spread 
genetic studies of the virus suggested in that the most recent common ancestor of the hiv group dates back to the belgian congo city of opoldville modern kinshasa circa proponents of this dating link the hiv epidemic with the emergence of colonialism and growth of large colonial african cities leading to social changes including higher degree of non monogamous sexual activity the spread of prostitution and the concomitant high frequency of genital ulcer diseases such as syphilis in nascent colonial cities in study conducted by scientists from the university of oxford and the university of leuven in belgium revealed that because approximately one million people every year would flow through the prominent city of kinshasa which served as the origin of the first known hiv cases in the passengers riding on the region belgian railway trains were able to spread the virus to larger areas 
the study also identified roaring sex trade rapid population growth and unsterilised needles used in health clinics as other factors which contributed to the emergence of the africa hiv epidemic 
social changes and urbanization beatrice hahn paul sharp and their colleagues proposed that the epidemic emergence of hiv most likely reflects changes in population structure and behaviour in africa during the th century and perhaps medical interventions that provided the opportunity for rapid human to human spread of the virus 
after the scramble for africa started in the european colonial powers established cities towns and other colonial stations 
largely masculine labor force was hastily recruited to work in fluvial and sea ports railways other infrastructures and in plantations 
this disrupted traditional tribal values and favored casual sexual activity with an increased number of partners 
in the nascent cities women felt relatively liberated from rural tribal rules and many remained unmarried or divorced during long periods this being rare in african traditional societies 
this was accompanied by unprecedented increase in people movements 
michael worobey and colleagues observed that the growth of cities probably played role in the epidemic emergence of hiv since the phylogenetic dating of the two older strains of hiv groups and suggest that these viruses started to spread soon after the main central african colonial cities were founded 
colonialism in africa amit chitnis diana rawls and jim moore proposed that hiv may have emerged epidemically as result of harsh conditions forced labor displacement and unsafe injection and vaccination practices associated with colonialism particularly in french equatorial africa 
the workers in plantations construction projects and other colonial enterprises were supplied with bushmeat which would have contributed to an increase in hunting and it follows higher incidence of human exposure to siv 
several historical sources support the view that bushmeat hunting indeed increased both because of the necessity to supply workers and because firearms became more widely available the colonial authorities also gave many vaccinations against smallpox and injections of which many would be made without sterilising the equipment between uses 
proposed that both these parenteral risks and the prostitution associated with forced labor camps could have caused serial transmission or serial passage of siv between humans see discussion of this in the next section 
in addition they proposed that the conditions of extreme stress associated with forced labor could depress the immune system of workers therefore prolonging the primary acute infection period of someone newly infected by siv thus increasing the odds of both adaptation of the virus to humans and of further transmissions the authors proposed that hiv originated in the area of french equatorial africa in the early th century when the colonial abuses and forced labor were at their peak 
later research established that these theories were mostly correct hiv groups and started to spread in humans in late th early th century 
in addition all groups of hiv descend from either sivcpz or sivgor from apes living to the west of the ubangi river either in countries that belonged to the french equatorial africa federation of colonies in equatorial guinea then spanish colony or in cameroon which was german colony between and and then fell to allied forces in world war and had most of its area administered by france in close association with french equatorial africa 
this theory was later dubbed heart of darkness by jim moore alluding to the book of the same title written by joseph conrad the main focus of which is colonial abuses in equatorial africa 
unsterile injections in several articles published since preston marx philip alcabes and ernest drucker proposed that hiv emerged because of rapid serial human to human transmission of siv after bushmeat hunter or handler became siv infected through unsafe or unsterile injections 
although both chitnis et al 
and sharp et al 
also suggested that this may have been one of the major risk factors at play in hiv emergence see above marx et al 
enunciated the underlying mechanisms in greater detail and wrote the first review of the injection campaigns made in colonial africa central to the marx et al 
argument is the concept of adaptation by serial passage or serial transmission an adventitious virus or other pathogen can increase its biological adaptation to new host species if it is rapidly transmitted between hosts while each host is still in the acute infection period 
this process favors the accumulation of adaptive mutations more rapidly therefore increasing the odds that better adapted viral variant will appear in the host before the immune system suppresses the virus 
such better adapted variants could then survive in the human host for longer than the short acute infection period in high numbers high viral load which would grant it more possibilities of epidemic spread 
reported experiments of cross species transfer of siv in captive monkeys some of which made by themselves in which the use of serial passage helped to adapt siv to the new monkey species after passage by three or four animals in agreement with this model is also the fact that while both hiv and hiv attain substantial viral loads in the human organism adventitious siv infecting humans seldom does so people with siv antibodies often have very low or even undetectable siv viral load 
this suggests that both hiv and hiv are adapted to humans and serial passage could have been the process responsible for it 
proposed that unsterile injections that is injections where the needle or syringe is reused without sterilization or cleaning between uses which were likely very prevalent in africa during both the colonial period and afterwards provided the mechanism of serial passage that permitted hiv to adapt to humans therefore explaining why it emerged epidemically only in the th century 
massive injections of the antibiotic era marx et al 
emphasize the massive number of injections administered in africa after antibiotics were introduced around as being the most likely implicated in the origin of hiv because by these times roughly in the period to injection intensity in africa was maximal 
they argued that serial passage chain of or transmissions between humans is an unlikely event the probability of transmission after needle reuse is something between and and only few people have an acute siv infection at any time and so hiv emergence may have required the very high frequency of injections of the antibiotic era the molecular dating studies place the initial spread of the epidemic hiv groups before that time see above 
according to marx et al these studies could have overestimated the age of the hiv groups because they depend on molecular clock assumption may not have accounted for the effects of natural selection in the viruses and the serial passage process alone would be associated with strong natural selection 
injection campaigns against sleeping sickness david gisselquist proposed that the mass injection campaigns to treat trypanosomiasis sleeping sickness in central africa were responsible for the emergence of hiv 
unlike marx et al gisselquist argued that the millions of unsafe injections administered during these campaigns were sufficient to spread rare hiv infections into an epidemic and that evolution of hiv through serial passage was not essential to the emergence of the hiv epidemic in the th century this theory focuses on injection campaigns that peaked in the period that is around the time the hiv groups started to spread 
it also focuses on the fact that many of the injections in these campaigns were intravenous which are more likely to transmit siv hiv than subcutaneous or intramuscular injections and many of the patients received many often more than injections per year therefore increasing the odds of siv serial passage 
other early injection campaigns jacques pin and annie claude labb reviewed the colonial health reports of cameroon and french equatorial africa for the period calculating the incidences of the diseases requiring intravenous injections 
they concluded that trypanosomiasis leprosy yaws and syphilis were responsible for most intravenous injections 
schistosomiasis tuberculosis and vaccinations against smallpox represented lower parenteral risks schistosomiasis cases were relatively few tuberculosis patients only became numerous after mid century and there were few smallpox vaccinations in the lifetime of each person the authors suggested that the very high prevalence of the hepatitis virus in southern cameroon and forested areas of french equatorial africa around can be better explained by the unsterile injections used to treat yaws because this disease was much more prevalent than syphilis trypanosomiasis and leprosy in these areas 
they suggested that all these parenteral risks caused not only the massive spread of hepatitis but also the spread of other pathogens and the emergence of hiv the same procedures could have exponentially amplified hiv from single hunter cook occupationally infected with sivcpz to several thousand patients treated with arsenicals or other drugs threshold beyond which sexual transmission could prosper 
they do not suggest specifically serial passage as the mechanism of adaptation 
according to pin book the origins of aids the virus can be traced to central african bush hunter in with colonial medical campaigns using improperly sterilized syringe and needles playing key role in enabling future epidemic 
pin concludes that aids spread silently in africa for decades fueled by urbanization and prostitution since the initial cross species infection 
pin also claims that the virus was brought to the americas by haitian teacher returning home from zaire in the 
sex tourism and contaminated blood transfusion centers ultimately propelled aids to public consciousness in the and worldwide pandemic 
genital ulcer diseases and evolution of sexual activity jo dinis de sousa viktor ller philippe lemey and anne mieke vandamme proposed that hiv became epidemic through sexual serial transmission in nascent colonial cities helped by high frequency of genital ulcers caused by genital ulcer diseases gud 
gud are simply sexually transmitted diseases that cause genital ulcers examples are syphilis chancroid lymphogranuloma venereum and genital herpes 
these diseases increase the probability of hiv transmission dramatically from around to per heterosexual act because the genital ulcers provide portal of viral entry and contain many activated cells expressing the ccr co receptor the main cell targets of hiv 
probable time interval of cross species transfer sousa et al 
use molecular dating techniques to estimate the time when each hiv group split from its closest siv lineage 
each hiv group necessarily crossed to humans between this time and the time when it started to spread the time of the mrca because after the mrca certainly all lineages were already in humans and before the split with the closest simian strain the lineage was in simian 
hiv groups and split from their closest sivs around and respectively 
this information together with the datations of the hiv groups mrcas mean that all hiv groups likely crossed to humans in the early th century 
strong genital ulcer disease incidence in nascent colonial cities the authors reviewed colonial medical articles and archived medical reports of the countries at or near the ranges of chimpanzees gorillas and sooty mangabeys and found that genital ulcer diseases guds peaked in the colonial cities during their early growth period up to 
the colonial authorities recruited men to work in railways fluvial and sea ports and other infrastructure projects and most of these men did not bring their wives with them 
then the highly male biased sex ratio favoured prostitution which in its turn caused an explosion of gud especially syphilis and chancroid 
after the mid people movements were more tightly controlled and mass surveys and treatments of arsenicals and other drugs were organized and so the gud incidences started to decline 
they declined even further after world war ii because of the heavy use of antibiotics so that by the late opoldville which is the probable center of hiv group had very low gud incidence 
similar processes happened in the cities of cameroon and ivory coast where hiv group and hiv respectively evolved therefore the peak gud incidences in cities have good temporal coincidence with the period when all main hiv groups crossed to humans and started to spread 
in addition the authors gathered evidence that syphilis and the other guds were like injections absent from the densely forested areas of central and west africa before organized colonialism socially disrupted these areas starting in the 
thus this theory also potentially explains why hiv emerged only after the late th century 
female genital mutilation uli linke has argued that the practice of female genital mutilation either or both of clitoridectomy and infibulation is responsible for the high incidence of aids in africa since intercourse with female who has undergone clitoridectomy is conducive to exchange of blood 
male circumcision distribution and hiv origins male circumcision may reduce the probability of hiv acquisition by men 
leaving aside blood transfusions the highest hiv transmissibility ever measured was from female prostitutes with prevalence of hiv to uncircumcised men with gud cumulative seroconverted to hiv after single sexual exposure 
there was no seroconversion in the absence of male gud 
reasoned that the adaptation and epidemic emergence of each hiv group may have required such extreme conditions and thus reviewed the existing ethnographic literature for patterns of male circumcision and hunting of apes and monkeys for bushmeat focusing on the period and on most of the ethnic groups living in central and west africa 
they also collected censuses and other literature showing the ethnic composition of colonial cities in this period 
then they estimated the circumcision frequencies of the central african cities over time 
charts reveal that male circumcision frequencies were much lower in several cities of western and central africa in the early th century than they are currently 
the reason is that many ethnic groups not performing circumcision by that time gradually adopted it to imitate other ethnic groups and enhance the social acceptance of their boys colonialism produced massive intermixing between african ethnic groups 
about of men in opoldville and douala in the early th century should be uncircumcised and these cities were the probable centers of hiv groups and respectively the authors studied early circumcision frequencies in cities of central and west africa to test if this variable correlated with hiv emergence 
this correlation was strong for hiv among west african cities that could have received immigrants infected with sivsmm the two cities from the ivory coast studied abidjan and bouak had much higher frequency of uncircumcised men than the others and epidemic hiv groups emerged initially in this country only 
this correlation was less clear for hiv in central africa 
computer simulations of hiv emergence sousa et al 
then built computer simulations to test if an ill adapted siv meaning simian immunodeficiency virus already infecting human but incapable of transmission beyond the short acute infection period could spread in colonial cities 
the simulations used parameters of sexual transmission obtained from the current hiv literature 
they modelled people sexual links with different levels of sexual partner change among different categories of people prostitutes single women with several partners year married women and men according to data obtained from modern studies of sexual activity in african cities 
the simulations let the parameters city size proportion of people married gud frequency male circumcision frequency and transmission parameters vary and explored several scenarios 
each scenario was run times to test the probability of siv generating long chains of sexual transmission 
the authors postulated that such long chains of sexual transmission were necessary for the siv strain to adapt better to humans becoming an hiv capable of further epidemic emergence 
the main result was that genital ulcer frequency was by far the most decisive factor 
for the gud levels prevailing in opoldville in the early th century long chains of siv transmission had high probability 
for the lower gud levels existing in the same city in the late see above they were much less likely 
and without gud situation typical of villages in forested equatorial africa before colonialism siv could not spread at all 
city size was not an important factor 
the authors propose that these findings explain the temporal patterns of hiv emergence no hiv emerging in tens of thousands of years of human slaughtering of apes and monkeys several hiv groups emerging in the nascent gud riddled colonial cities and no epidemically successful hiv group emerging in mid th century when gud was more controlled and cities were much bigger 
male circumcision had little to moderate effect in their simulations but given the geographical correlation found the authors propose that it could have had an indirect role either by increasing genital ulcer disease itself it is known that syphilis chancroid and several other guds have higher incidences in uncircumcised men or by permitting further spread of the hiv strain after the first chains of sexual transmission permitted adaptation to the human organism 
one of the main advantages of this theory is stressed by the authors it the theory also offers conceptual simplicity because it proposes as causal factors for siv adaptation to humans and initial spread the very same factors that most promote the continued spread of hiv nowadays promiscuous sic sex particularly involving sex workers gud and possibly lack of circumcision 
iatrogenic and other theories iatrogenic theories propose that medical interventions were responsible for hiv origins 
by proposing factors that only appeared in central and west africa after the late th century they seek to explain why all hiv groups also started after that 
the theories centred on the role of parenteral risks such as unsterile injections transfusions or smallpox vaccinations are accepted as plausible by most scientists of the field 
discredited hiv aids origins theories include several iatrogenic theories such as the polio vaccine hypothesis which argues that the early oral polio vaccines were contaminated with chimpanzee virus leading to the central african outbreak 
pathogenicity of siv in non human primates in most non human primate species natural siv infection does not cause fatal disease but see below 
comparison of the gene sequence of siv with hiv should therefore provide information about the factors necessary to cause disease in humans 
the factors that determine the virulence of hiv as compared to most sivs are only now being elucidated 
non human sivs contain nef gene that down regulates cd cd and mhc class expression most non human sivs therefore do not induce immunodeficiency the hiv nef gene however has lost its ability to down regulate cd which results in the immune activation and apoptosis that is characteristic of chronic hiv infection in addition long term survey of chimpanzees naturally infected with sivcpz in gombe national park tanzania found that contrary to the previous paradigm chimpanzees with sivcpz infection do experience an increased mortality and also suffer from human aids like illness 
siv pathogenicity in wild animals could exist in other chimpanzee subspecies and other primate species as well and stay unrecognized by lack of relevant long term studies 
history of spread david carr david carr was an apprentice printer usually mistakenly referred to as sailor carr had served in the navy between and from manchester england who died on august and was for some time mistakenly reported to have died from aids defining opportunistic infections adois 
following the failure of his immune system he succumbed to pneumonia 
doctors baffled by what he had died from preserved of his tissue samples for inspection 
in the tissues were found to be hiv positive 
however in second test by aids researcher david ho found that the strain of hiv present in the tissues was similar to those found in rather than an earlier strain which would have mutated considerably over the course of years 
he concluded that the dna samples provided actually came from patient with aids in the 
upon retesting david carr tissues he found no sign of the virus 
congolese man one of the earliest documented hiv infections was discovered in preserved blood sample taken in from man from opoldville in the belgian congo 
however it is unknown whether this anonymous person ever developed aids and died of its complications 
congolese woman second early documented hiv infection was discovered in preserved lymph node biopsy sample taken in from woman from opoldville belgian congo 
congolese man strain with large amount of the genetic material present was dated to from sample from year old man 
robert rayford in may year old african american robert rayford died at the st louis city hospital from kaposi sarcoma 
in researchers at tulane university school of medicine detected virus closely related or identical to hiv in his preserved blood and tissues 
the doctors who worked on his case at the time suspected he was prostitute or the victim of sexual abuse though the patient did not discuss his sexual history with them in detail 
ugandan children from to researchers drew blood from children in uganda to serve as controls for study of burkitt lymphoma 
in retroactive testing of the frozen blood serum indicated that antibodies to virus related to hiv were present in of the children 
arvid noe in and norwegian sailor with the alias name arvid noe his wife and his seven year old daughter died of aids 
the sailor had first presented symptoms in eight years after he first spent time in ports along the west african coastline 
gonorrhea infection during his first african voyage shows he was sexually active at this time 
tissue samples from the sailor and his wife were tested in and found to contain hiv group 
grethe rask grethe rask was danish surgeon who traveled to za re in then again in to aid the sick 
she was likely directly exposed to blood from many congolese patients one of whom infected her 
she became unwell from then returned to denmark in with her colleagues baffled by her symptoms 
she died of pneumocystis pneumonia in december her tissues were examined and tested by her colleagues and found positive in 
spread to the western hemisphere hiv strains were once thought to have arrived in new york city from haiti around it spread from new york city to san francisco around hiv is believed to have arrived in haiti from central africa possibly from the democratic republic of the congo around the current consensus is that hiv was introduced to haiti by an unknown individual or individuals who contracted it while working in the democratic republic of the congo circa mini epidemic followed and circa yet another unknown individual took hiv from haiti to the united states 
the vast majority of cases of aids outside sub saharan africa can be traced back to that single patient 
later numerous unrelated incidents of aids among haitian immigrants to the were recorded in the early 
also as evidenced by the case of robert rayford isolated occurrences of this infection may have been emerging as early as the virus eventually entered gay male communities in large united states cities where combination of casual multi partner sexual activity with individuals reportedly averaging over unprotected sexual partners per year and relatively high transmission rates associated with anal intercourse allowed it to spread explosively enough to finally be noticed because of the long incubation period of hiv up to decade or longer before symptoms of aids appear and because of the initially low incidence hiv was not noticed at first 
by the time the first reported cases of aids were found in large united states cities the prevalence of hiv infection in some communities had passed 
worldwide hiv infection has spread from urban to rural areas and has appeared in regions such as china and india 
canadian flight attendant theory canadian airline steward named ga tan dugas was referred to as case and later patient with the alphabet letter standing for outside southern california in an early aids study by dr william darrow of the centers for disease control 
because of this many people had considered dugas to be responsible for taking hiv to north america 
however hiv reached new york city around while dugas did not start work at air canada until in randy shilts book and the band played on and the movie based on it dugas is referred to as aids patient zero instead of patient but neither the book nor the movie states that he had been the first to bring the virus to north america 
he was incorrectly called patient zero because at least of the people known to be infected by hiv in had had sex with him or with person who had sexual intercourse with dugas 
homeless people and intravenous drug users in new york volunteer social worker called betty williams quaker who worked with the homeless in new york from the seventies and early eighties onwards has talked about people at that time whose death would be labelled as junkie flu or the dwindles 
in an interview for the act up oral history project in she said of course the horror stories came mainly concerning women who were injection drug users who had pcp pneumonia pneumocystis pneumonia and were told that they just had bronchitis 
she continues actually believe that aids kind of existed among this group of people first because if you look back there was something called junkie pneumonia there was something called the dwindles that addicts got and think this was another early aids population way too helpless to ever do anything for themselves on their own behalf 
julia epstein writes in her book altered conditions disease medicine and storytelling that as we uncover more of the early history of hiv infection it becomes clear that by at least the the virus was already making major inroads into the immune systems of number of diverse populations in the united states the retrospectively diagnosed epidemic of junkie pneumonia in new york city in the late for example and had for some time been causing devastation in several countries in africa 
anecdotal evidence suggests that so called junkie pneumonia first began to afflict heroin addicts in new york in in her book engendering aids deconstructing sex text and epidemic tamsin wilton writes people had been sickening and dying of mysterious conditions since the early conditions that we can retrospectively diagnose as aids related 
there was for example phenomenon known as junkie pneumonia which spread among some populations of injecting street drug users in the and which is now believed to have been caused by hiv infection 
melinda cooper writes in her book family values between neoliberalism and the new social conservatism it is plausible that these cases of aids did not come to light in the for the same reason that junkie pneumonia was not recognized as the sign of an emerging infectious disease the people in question had such precarious access to health care that news of their death was never communicated to public health authorities 
an article by pattrice maurer in the newspaper agenda from april explores some of the issues surrounding junkie pneumonia 
it starts in the late while the epidemic known as disco fever swept through the an epidemic known as junkie pneumonia raged among injection drug users in new york city 
it continues few people were aware that large numbers of injections drug users were inexplicably dying of pneumonia 
those few who did notice these deaths did not feel compelled to investigate the public health puzzle they posed 
the author opinion is that if anyone had bothered to investigate these deaths they would have found an immune system disorder that is now called aids steven thrasher writes in the guardian indeed those of us who study aids have long known that long before common symptoms such as kaposi sarcoma and pneumonia were showing up among hemophiliacs and gay men they were likely affecting homeless people who lived off society radar people who used iv intravenous drugs and those who avoided medical treatment out of fear 
chapter in the proceedings of the world conference of therapeutic communities th san francisco california september gives details about serum samples that were tested for signs of hiv then called htlv iii lav antibodies 
quoting we have also conducted historical studies of the epidemic in new york city using serum samples that were originally collected for other purposes 
we have sera from iv drug users that go back to the middle 
the first indication of htlv iii lav antibody presence is in one of eleven samples from of samples in of samples from and of samples from the htlv iii lav virus appears to have been introduced among iv drug users in the late in new york city 
anna thompson writes on the website thebody com in an article dated autumn many women were dying in the late of pneumonia cervical cancer and other illnesses complicated by mysteriously suppressed immune systems 
yet it was not until that case of aids in woman was first reported by the centers for disease control cdc 
she continues the cdc refusal to address women issues led to the overall perception that women do not get aids 
in an article published in aids cultural analysis cultural activism author douglas crimp draws attention to anecdotal evidence about junkie pneumonia 
quoting even these statistics are based on cdc epidemiology that continues to see the beginning of the epidemic as in spite of widespread anecdotal reporting of high rate of deaths throughout the from what was known as junkie pneumonia and was likely pneumocystis pneumonia 
the statistics crimp writes about were taken from new york times article from october about nyc department of health study that showed that of aids sufferers were people who injected drugs more than percent higher than previously reported 
quoting city health officials estimated that half of the city intravenous drug users were infected with the virus that causes aids the study hiv infection among intravenous drug users in manhattan new york city from through published in february seeks to understand long term trends in the spread of hiv among intravenous drug users idus 
aids surveillance data and studies which detail the number of persons who tested hiv positive in manhattan are used to compile information deemed critical to realising the extent of the aids epidemic 
it starts by stating that up to september idu was the risk behaviour in or of the first cases of aids in the us 
cases among idus in new york city in the same period numbered approximately third of national idu cases 
the study continues to outline the methodology used in the compilation of data 
it says that while truly representative samples of idus within community are probably impossible to obtain samples of idus entering treatment provide good source for monitoring trends 
in the results section it states quoting the first evidence for hiv infection among iv drug users in new york is from three cases of aids in children born in these cases were later reported to the new york city department of health aids surveillance unit 
these children did not receive any known transfusions prior to developing aids and were born to mothers known to be iv drug users 
it continues to outline that the earliest known case of aids in an adult idu occurred in mixed risk and that known cases among idus increased rapidly from the cases in mixed risk to cases in to cases in and to cases in statistics on the incidence of positive tests for hiv mainly using archived samples are out of in out of in out of in out of between out of and out of in out of in and out of in in the comments section it states the three cases in of apparent perinatal transmission mother to child from iv drug using women strongly suggest that the introduction of hiv into the iv drug use group occurred around or or perhaps even earlier 
it says that without extensive samples from this period it is not possible to be certain about the spread of hiv among idus but the samples from idus with chronic liver disease suggest that the rates of infection were below for the first or years after its introduction hiv is thought to have entered the population of people using intravenous drugs in new york city in approximately in spring the government of new york city underwent fiscal crisis which led to the closing of many social services with people who used intravenous drugs living in hostile sociopolitical and legal environment 
this fiscal crisis led to many agencies with health responsibilities being particularly hard hit which in turn might have led to an increase in hiv aids and tuberculosis tb 
quoting from american journal of public health study between and the department of health doh budget in ny was cut by and by the department had lost staff members of its workforce 
to achieve these reductions the department closed of district health centers cut million from its methadone program terminated the employment of of health educators and closed of child health stations and of chest clinics the units responsible for tb screening and diagnosis 
study published in the journal of the american medical association in linked tb and hiv aids severe and unusual presentation of overwhelming tuberculosis in appropriate clinical circumstances may be considered an infection predictive of the presence of aids 
further study from stated there was link between the rise in tb aids and drug users within the united states aids thus compounds the risk of acquiring tuberculosis and in the united states most patients with aids and tuberculosis have been drug users 
newsletter from spring by the national coalition of gay std services featured an article titled tuberculosis and aids connecticut that suggested an association between tb and aids within that state 
from grid to aids the aids epidemic officially began on june when the centers for disease control and prevention in its morbidity and mortality weekly report newsletter reported unusual clusters of pneumocystis pneumonia pcp caused by form of pneumocystis carinii now recognized as distinct species pneumocystis jirovecii in five homosexual men in los angeles 
over the next months more pcp clusters were discovered among otherwise healthy men in cities throughout the country along with other opportunistic diseases such as kaposi sarcoma and persistent generalized lymphadenopathy common in immunosuppressed patients 
in june report of group of cases amongst gay men in southern california suggested that sexually transmitted infectious agent might be the etiological agent 
the syndrome was initially termed grid or gay related immune deficiency other less common gay specific terms included gay compromise syndrome gay lymph node syndrome gay cancer gay plague homosexual syndrome community acquired immunodeficiency caid and acquired community immunodeficiency syndrome acids 
health authorities soon realized however that nearly half of the people identified with the syndrome were not homosexual men 
the same opportunistic infections were also reported among hemophiliacs users of intravenous drugs such as heroin and haitian immigrants leading some researchers to call it the disease 
by august the disease was being referred to by its new cdc coined name acquired immune deficiency syndrome aids 
activism by aids patients and families in new york city nathan fain larry kramer larry mass paul popham paul rapoport and edmund white officially established the gay men health crisis gmhc in also in michael callen and richard berkowitz published how to have sex in an epidemic one approach 
in this short work they described ways gay men could be sexual and affectionate while dramatically reducing the risk of contracting or spreading hiv 
both authors were themselves gay men living with aids 
this booklet was one of the first times men were advised to use condoms when having sexual relations with other men at the beginning of the aids epidemic in the there was very little information about the disease 
because aids affected stigmatized groups such as lgbtq people people of low socioeconomic status sex workers and addicts there was also initially little mass media coverage when the epidemic started 
however with the rise of activist groups composed of people suffering from aids either directly or through loved one more public attention was brought to the epidemic 
identification of the virus may lav in may team of doctors at the pasteur institute in france including fran oise barr sinoussi and luc montagnier reported that they had isolated new retrovirus from lymphoid ganglions that they believed was the cause of aids 
the virus was later named lymphadenopathy associated virus lav and sample was sent to the centers for disease control which was later passed to the national cancer institute nci 
may htlv iii in may team led by robert gallo of the united states confirmed the discovery of the virus but they renamed it human lymphotropic virus type iii htlv iii 
august arv dr jay levy group at the university of california san francisco also played role in the discovery of hiv 
he independently isolated the aids virus in and named it the aids associated retrovirus arv publishing his findings in the journal science in 
january both found to be the same in january number of more detailed reports were published concerning lav and htlv iii and by march it was clear that the viruses were the same indeed it was later determined that the virus isolated by the gallo lab was from the lymph nodes of the patient studied in the original report by montagnier and was the etiological agent of aids 
may the name hiv in may the international committee on taxonomy of viruses ruled that both names should be dropped and new name hiv human immunodeficiency virus be used 
nobel whether barr sinoussi and montagnier deserve more credit than gallo for the discovery of the virus that causes aids has been matter of considerable controversy 
barr sinoussi and montagnier were awarded the nobel prize in physiology or medicine for their discovery of human immunodeficiency virus and harald zur hausen also shared the prize for his discovery that human papilloma virus leads to cervical cancer but gallo was left out 
gallo said that it was disappointment that he was not named co recipient 
montagnier said he was surprised gallo was not recognized by the nobel committee it was important to prove that hiv was the cause of aids and gallo had very important role in that 
very sorry for robert gallo 
dr levy contribution to the discovery of hiv was also cited in the nobel prize ceremony 
case definition for epidemiological surveillance since june many definitions have been developed for epidemiological surveillance such as the bangui definition and the expanded world health organization aids case definition 
genetic studies according to study published in the proceedings of the national academy of sciences in team led by robert shafer at stanford university school of medicine discovered that the gray mouse lemur has an endogenous lentivirus the genus to which hiv belongs in its genetic makeup 
this suggests that lentiviruses have existed for at least million years much longer than the currently known existence of hiv 
in addition the time frame falls in the period when madagascar was still connected to what is now the african continent the said lemurs later developed immunity to the virus strain and survived an era when the lentivirus was widespread among other mammals 
the study was hailed as crucial as it fills the blanks in the origin of the virus as well as in its evolution and could be important in the development of new antiviral drugs in researchers reported that siv had infected monkeys in bioko for at least years 
previous to this time it was thought that siv infection in monkeys had happened over the past few hundred years 
scientists estimated that it would take similar amount of time before humans adapted naturally to hiv infection in the way monkeys in africa have adapted to siv and not suffer any harm from the infection czech study of the genome of malayan flying lemurs an order of mammals parallel to primates and sharing an immediate common ancestor with them found endogenous lentiviruses that emerged an estimated million years ago based on rates of viral mutation versus modern lentiviruses 
debunked hiv aids conspiracy theories aids denialism aids denialists argue that aids does not exist or that aids is not caused by hiv some of its proponents believe that aids is caused by lifestyle including sexuality or drug use and not by hiv 
both forms of aids denialism contradict scientific consensus 
the evidence that hiv causes aids is generally considered conclusive among pathologists 
most arguments for denialism are based on misrepresentations of outdated data 
the belief that hiv was created by the us government as bioweapon an idea invented by soviet propaganda operation is held by disproportionately high number of africans and african americans 
influence on bolsonaro conspiracy theorists influence reached peak in with brazilian president jair bolsonaro claiming that covid vaccines can lead to aids 
the supreme federal court of brazil ordered an investigation into bolsonaro for falsely claiming that covid vaccines could increase the risk of contracting aids 
see also timeline of hiv aids references further reading shilts randy 
and the band played on politics people and the aids epidemic 
new york st martin press 
isbn oclc brier jennier 
infectious ideas political responses to the aids crisis 
chapel hill university of north carolina press
in mathematics the linear span also called the linear hull or just span of set of vectors from vector space denoted span is defined as the set of all linear combinations of the vectors in it can be characterized either as the intersection of all linear subspaces that contain or as the smallest subspace containing the linear span of set of vectors is therefore vector space itself
spans can be generalized to matroids and modules
to express that vector space is linear span of subset one commonly uses the following phrases either spans is spanning set of is spanned generated by or is generator or generator set of definition given vector space over field the span of set of vectors not necessarily infinite is defined to be the intersection of all subspaces of that contain is referred to as the subspace spanned by or by the vectors in conversely is called spanning set of and we say that spans alternatively the span of may be defined as the set of all finite linear combinations of elements vectors of which follows from the above definition
in the case of infinite infinite linear combinations
where combination may involve an infinite sum assuming that such sums are defined somehow as in say banach space are excluded by the definition generalization that allows these is not equivalent
examples the real vector space has as spanning set
this particular spanning set is also basis
if were replaced by it would also form the canonical basis of another spanning set for the same space is given by but this set is not basis because it is linearly dependent
the set is not spanning set of since its span is the space of all vectors in whose last component is zero
that space is also spanned by the set as is linear combination of and
it does however span
when interpreted as subset of
the empty set is spanning set of since the empty set is subset of all possible vector spaces in and is the intersection of all of these vector spaces
the set of functions xn where is non negative integer spans the space of polynomials
theorems equivalence of definitions the set of all linear combinations of subset of vector space over is the smallest linear subspace of containing proof
we first prove that span is subspace of since is subset of we only need to prove the existence of zero vector in span that span is closed under addition and that span is closed under scalar multiplication
letting it is trivial that the zero vector of exists in span since adding together two linear combinations of also produces linear combination of where all and multiplying linear combination of by scalar will produce another linear combination of thus is subspace of suppose that is linear subspace of containing it follows that span since every vi is linear combination of trivially
since is closed under addition and scalar multiplication then every linear combination must be contained in thus span is contained in every subspace of containing and the intersection of all such subspaces or the smallest such subspace is equal to the set of all linear combinations of size of spanning set is at least size of linearly independent set every spanning set of vector space must contain at least as many elements as any linearly independent set of vectors from proof
let be spanning set and be linearly independent set of vectors from we want to show that since spans then must also span and must be linear combination of thus is linearly dependent and we can remove one vector from that is linear combination of the other elements
this vector cannot be any of the wi since is linearly indepedent
the resulting set is which is spanning set of we repeat this step times where the resulting set after the pth step is the union of and vectors of it is ensured until the nth step that there will always be some to remove out of for every adjoint of and thus there are at least as many vi as there are wi
to verify this we assume by way of contradiction that then at the mth step we have the set and we can adjoin another vector but since is spanning set of is linear combination of
this is contradiction since is linearly independent
spanning set can be reduced to basis let be finite dimensional vector space
any set of vectors that spans can be reduced to basis for by discarding vectors if necessary
if there are linearly dependent vectors in the set
if the axiom of choice holds this is true without the assumption that has finite dimension
this also indicates that basis is minimal spanning set when is finite dimensional
generalizations generalizing the definition of the span of points in space subset of the ground set of matroid is called spanning set if the rank of equals the rank of the entire ground set
the vector space definition can also be generalized to modules
given an module and collection of elements an of the submodule of spanned by an is the sum of cyclic modules consisting of all linear combinations of the elements ai
as with the case of vector spaces the submodule of spanned by any subset of is the intersection of all submodules containing that subset
closed linear span functional analysis in functional analysis closed linear span of set of vectors is the minimal closed set which contains the linear span of that set
suppose that is normed vector space and let be any non empty subset of the closed linear span of denoted by sp or span is the intersection of all the closed linear subspaces of which contain one mathematical formulation of this is sp sp
the closed linear span of the set of functions xn on the interval where is non negative integer depends on the norm used
if the norm is used then the closed linear span is the hilbert space of square integrable functions on the interval
but if the maximum norm is used the closed linear span will be the space of continuous functions on the interval
in either case the closed linear span contains functions that are not polynomials and so are not in the linear span itself
however the cardinality of the set of functions in the closed linear span is the cardinality of the continuum which is the same cardinality as for the set of polynomials
notes the linear span of set is dense in the closed linear span
moreover as stated in the lemma below the closed linear span is indeed the closure of the linear span
closed linear spans are important when dealing with closed linear subspaces which are themselves highly important see riesz lemma
useful lemma let be normed space and let be any non empty subset of then so the usual way to find the closed linear span is to find the linear span first and then the closure of that linear span
see also affine hull conical combination convex hull citations sources textbooks axler sheldon jay
linear algebra done right rd ed
linear algebra th ed
isbn lane saunders mac birkhoff garrett
advanced linear algebra nd ed
isbn rynne brian youngson martin
isbn lay david linear algebra and its applications th edition
web lankham isaiah nachtergaele bruno schilling anne february
linear algebra as an introduction to abstract mathematics pdf
university of california davis
retrieved september weisstein eric wolfgang
retrieved feb cs maint url status link linear hull
april retrieved feb cs maint url status link external links linear combinations and span understanding linear combinations and spans of vectors khanacademy org
linear combinations span and basis vectors
essence of linear algebra
archived from the original on via youtube
in mathematics normed vector space or normed space is vector space over the real or complex numbers on which norm is defined 
norm is the formalization and the generalization to real vector spaces of the intuitive notion of length in the real physical world 
norm is real valued function defined on the vector space that is commonly denoted and has the following properties it is nonnegative meaning that for every vector it is positive on nonzero vectors that is for every vector and every scalar the triangle inequality holds that is for every vectors and norm induces distance called its norm induced metric by the formula which makes any normed vector space into metric space and topological vector space 
if this metric is complete then the normed space is banach space 
every normed vector space can be uniquely extended to banach space which makes normed spaces intimately related to banach spaces 
every banach space is normed space but converse is not true 
for example the set of the finite sequences of real numbers can be normed with the euclidean norm but it is not complete for this norm 
an inner product space is normed vector space whose norm is the square root of the inner product of vector and itself 
the euclidean norm of euclidean vector space is special case that allows defining euclidean distance by the formula the study of normed spaces and banach spaces is fundamental part of functional analysis which is major subfield of mathematics 
definition normed vector space is vector space equipped with norm 
seminormed vector space is vector space equipped with seminorm 
useful variation of the triangle inequality is for any vectors and this also shows that vector norm is uniformly continuous function 
property depends on choice of norm on the field of scalars 
when the scalar field is or more generally subset of this is usually taken to be the ordinary absolute value but other choices are possible 
for example for vector space over one could take to be the adic absolute value 
topological structure if is normed vector space the norm induces metric notion of distance and therefore topology on this metric is defined in the natural way the distance between two vectors and is given by 
this topology is precisely the weakest topology which makes continuous and which is compatible with the linear structure of in the following sense the vector addition is jointly continuous with respect to this topology 
this follows directly from the triangle inequality 
the scalar multiplication where is the underlying scalar field of is jointly continuous 
this follows from the triangle inequality and homogeneity of the norm similarly for any seminormed vector space we can define the distance between two vectors and as 
this turns the seminormed space into pseudometric space notice this is weaker than metric and allows the definition of notions such as continuity and convergence 
to put it more abstractly every seminormed vector space is topological vector space and thus carries topological structure which is induced by the semi norm 
of special interest are complete normed spaces which are known as banach spaces 
every normed vector space sits as dense subspace inside some banach space this banach space is essentially uniquely defined by and is called the completion of two norms on the same vector space are called equivalent if they define the same topology 
on finite dimensional vector space all norms are equivalent but this is not true for infinite dimensional vector spaces 
all norms on finite dimensional vector space are equivalent from topological viewpoint as they induce the same topology although the resulting metric spaces need not be the same 
and since any euclidean space is complete we can thus conclude that all finite dimensional normed vector spaces are banach spaces 
normed vector space is locally compact if and only if the unit ball is compact which is the case if and only if is finite dimensional this is consequence of riesz lemma 
in fact more general result is true topological vector space is locally compact if and only if it is finite dimensional 
the point here is that we don assume the topology comes from norm 
the topology of seminormed vector space has many nice properties 
given neighbourhood system around we can construct all other neighbourhood systems as with moreover there exists neighbourhood basis for the origin consisting of absorbing and convex sets 
as this property is very useful in functional analysis generalizations of normed vector spaces with this property are studied under the name locally convex spaces 
norm or seminorm on topological vector space is continuous if and only if the topology that induces on is coarser than meaning which happens if and only if there exists some open ball in such as maybe for example that is open in said different such that 
normable spaces topological vector space is called normable if there exists norm on such that the canonical metric induces the topology on the following theorem is due to kolmogorov kolmogorov normability criterion hausdorff topological vector space is normable if and only if there exists convex von neumann bounded neighborhood of product of family of normable spaces is normable if and only if only finitely many of the spaces are non trivial that is 
furthermore the quotient of normable space by closed vector subspace is normable and if in addition topology is given by norm then the map given by inf is well defined norm on that induces the quotient topology on if is hausdorff locally convex topological vector space then the following are equivalent is normable 
has bounded neighborhood of the origin 
the strong dual space of is normable 
the strong dual space of is metrizable furthermore is finite dimensional if and only if is normable here denotes endowed with the weak topology 
the topology of the fr chet space as defined in the article on spaces of test functions and distributions is defined by countable family of norms but it is not normable space because there does not exist any norm on such that the topology that this norm induces is equal to even if metrizable topological vector space has topology that is defined by family of norms then it may nevertheless still fail to be normable space meaning that its topology can not be defined by any single norm 
an example of such space is the fr chet space whose definition can be found in the article on spaces of test functions and distributions because its topology is defined by countable family of norms but it is not normable space because there does not exist any norm on such that the topology this norm induces is equal to in fact the topology of locally convex space can be defined by family of norms on if and only if there exists at least one continuous norm on 
linear maps and dual spaces the most important maps between two normed vector spaces are the continuous linear maps 
together with these maps normed vector spaces form category 
the norm is continuous function on its vector space 
all linear maps between finite dimensional vector spaces are also continuous 
an isometry between two normed vector spaces is linear map which preserves the norm meaning for all vectors 
isometries are always continuous and injective 
surjective isometry between the normed vector spaces and is called an isometric isomorphism and and are called isometrically isomorphic 
isometrically isomorphic normed vector spaces are identical for all practical purposes 
when speaking of normed vector spaces we augment the notion of dual space to take the norm into account 
the dual of normed vector space is the space of all continuous linear maps from to the base field the complexes or the reals such linear maps are called functionals 
the norm of functional is defined as the supremum of where ranges over all unit vectors that is vectors of norm in this turns into normed vector space 
an important theorem about continuous linear functionals on normed vector spaces is the hahn banach theorem 
normed spaces as quotient spaces of seminormed spaces the definition of many normed spaces in particular banach spaces involves seminorm defined on vector space and then the normed space is defined as the quotient space by the subspace of elements of seminorm zero 
for instance with the spaces the function defined by is seminorm on the vector space of all functions on which the lebesgue integral on the right hand side is defined and finite 
however the seminorm is equal to zero for any function supported on set of lebesgue measure zero 
these functions form subspace which we quotient out making them equivalent to the zero function 
finite product spaces given seminormed spaces with seminorms denote the product space by where vector addition defined as and scalar multiplication defined as define new function by which is seminorm on the function is norm if and only if all are norms 
more generally for each real the map defined by is semi norm 
for each this defines the same topological space 
straightforward argument involving elementary linear algebra shows that the only finite dimensional seminormed spaces are those arising as the product space of normed space and space with trivial seminorm 
consequently many of the more interesting examples and applications of seminormed spaces occur for infinite dimensional vector spaces 
see also banach space normed vector spaces which are complete with respect to the metric induced by the norm banach mazur compactum set of dimensional subspaces of normed space made into compact metric space 
finsler manifold where the length of each tangent vector is determined by norm inner product space normed vector spaces where the norm is given by an inner product kolmogorov normability criterion characterization of normable spaces locally convex topological vector space vector space with topology defined by convex open sets space mathematics mathematical set with some added structure topological vector space vector space with notion of nearness references bibliography rudin walter 
international series in pure and applied mathematics 
new york ny mcgraw hill science engineering math 
isbn oclc banach stefan 
th orie des op rations lin aires theory of linear operations pdf 
monografie matematyczne in french 
warszawa subwencji funduszu kultury narodowej 
zbl archived from the original pdf on retrieved rolewicz stefan functional analysis and control theory linear systems mathematics and its applications east european series vol 
translated from the polish by ewa bednarczuk ed 
dordrecht warsaw reidel publishing co pwn polish scientific publishers pp 
xvi doi isbn mr oclc schaefer 
new york ny springer new york imprint springer 
isbn oclc tr ves fran ois 
topological vector spaces distributions and kernels 
external links media related to normed spaces at wikimedia commons
digital signature is mathematical scheme for verifying the authenticity of digital messages or documents
valid digital signature where the prerequisites are satisfied gives recipient very high confidence that the message was created by known sender authenticity and that the message was not altered in transit integrity digital signatures are standard element of most cryptographic protocol suites and are commonly used for software distribution financial transactions contract management software and in other cases where it is important to detect forgery or tampering
digital signatures are often used to implement electronic signatures which includes any electronic data that carries the intent of signature but not all electronic signatures use digital signatures
electronic signatures have legal significance in some countries including canada south africa the united states algeria turkey india brazil indonesia mexico saudi arabia uruguay switzerland chile and the countries of the european union digital signatures employ asymmetric cryptography
in many instances they provide layer of validation and security to messages sent through non secure channel properly implemented digital signature gives the receiver reason to believe the message was sent by the claimed sender
digital signatures are equivalent to traditional handwritten signatures in many respects but properly implemented digital signatures are more difficult to forge than the handwritten type
digital signature schemes in the sense used here are cryptographically based and must be implemented properly to be effective
they can also provide non repudiation meaning that the signer cannot successfully claim they did not sign message while also claiming their private key remains secret
further some non repudiation schemes offer timestamp for the digital signature so that even if the private key is exposed the signature is valid
digitally signed messages may be anything representable as bitstring examples include electronic mail contracts or message sent via some other cryptographic protocol
definition digital signature scheme typically consists of three algorithms key generation algorithm that selects private key uniformly at random from set of possible private keys
the algorithm outputs the private key and corresponding public key
signing algorithm that given message and private key produces signature
signature verifying algorithm that given the message public key and signature either accepts or rejects the message claim to authenticity two main properties are required
first the authenticity of signature generated from fixed message and fixed private key can be verified by using the corresponding public key
secondly it should be computationally infeasible to generate valid signature for party without knowing that party private key
digital signature is an authentication mechanism that enables the creator of the message to attach code that acts as signature
the digital signature algorithm dsa developed by the national institute of standards and technology is one of many examples of signing algorithm
in the following discussion refers to unary number
formally digital signature scheme is triple of probabilistic polynomial time algorithms satisfying key generator generates public key pk and corresponding private key sk on input where is the security parameter
signing returns tag on the inputs the private key sk and string
verifying outputs accepted or rejected on the inputs the public key pk string and tag for correctness and must satisfy pr pk sk pk sk accepted digital signature scheme is secure if for every non uniform probabilistic polynomial time adversary pr pk sk as sk pk pk accepted negl where as sk denotes that has access to the oracle sk denotes the set of the queries on made by which knows the public key pk and the security parameter and denotes that the adversary may not directly query the string on history in whitfield diffie and martin hellman first described the notion of digital signature scheme although they only conjectured that such schemes existed based on functions that are trapdoor one way permutations
soon afterwards ronald rivest adi shamir and len adleman invented the rsa algorithm which could be used to produce primitive digital signatures although only as proof of concept plain rsa signatures are not secure
the first widely marketed software package to offer digital signature was lotus notes released in which used the rsa algorithm other digital signature schemes were soon developed after rsa the earliest being lamport signatures merkle signatures also known as merkle trees or simply hash trees and rabin signatures in shafi goldwasser silvio micali and ronald rivest became the first to rigorously define the security requirements of digital signature schemes
they described hierarchy of attack models for signature schemes and also presented the gmr signature scheme the first that could be proved to prevent even an existential forgery against chosen message attack which is the currently accepted security definition for signature schemes
the first such scheme which is not built on trapdoor functions but rather on family of function with much weaker required property of one way permutation was presented by moni naor and moti yung
method one digital signature scheme of many is based on rsa
to create signature keys generate an rsa key pair containing modulus that is the product of two random secret distinct large primes along with integers and such that mod where is euler totient function
the signer public key consists of and and the signer secret key contains to sign message the signer computes signature such that md mod where md is modular exponentiation operation
to verify the receiver checks that mod
several early signature schemes were of similar type they involve the use of trapdoor permutation such as the rsa function or in the case of the rabin signature scheme computing square modulo composite trapdoor permutation family is family of permutations specified by parameter that is easy to compute in the forward direction but is difficult to compute in the reverse direction without already knowing the private key trapdoor
trapdoor permutations can be used for digital signature schemes where computing the reverse direction with the secret key is required for signing and computing the forward direction is used to verify signatures
used directly this type of signature scheme is vulnerable to key only existential forgery attack
to create forgery the attacker picks random signature and uses the verification procedure to determine the message corresponding to that signature
in practice however this type of signature is not used directly but rather the message to be signed is first hashed to produce short digest that is then padded to larger width comparable to then signed with the reverse trapdoor function
this forgery attack then only produces the padded hash function output that corresponds to but not message that leads to that value which does not lead to an attack
in the random oracle model hash then sign an idealized version of that practice where hash and padding combined have close to possible outputs this form of signature is existentially unforgeable even against chosen plaintext attack there are several reasons to sign such hash or message digest instead of the whole document
for efficiency the signature will be much shorter and thus save time since hashing is generally much faster than signing in practice
for compatibility messages are typically bit strings but some signature schemes operate on other domains such as in the case of rsa numbers modulo composite number
hash function can be used to convert an arbitrary input into the proper format
for integrity without the hash function the text to be signed may have to be split separated in blocks small enough for the signature scheme to act on them directly
however the receiver of the signed blocks is not able to recognize if all the blocks are present and in the appropriate order
applications as organizations move away from paper documents with ink signatures or authenticity stamps digital signatures can provide added assurances of the evidence to provenance identity and status of an electronic document as well as acknowledging informed consent and approval by signatory
the united states government printing office gpo publishes electronic versions of the budget public and private laws and congressional bills with digital signatures
universities including penn state university of chicago and stanford are publishing electronic student transcripts with digital signatures
below are some common reasons for applying digital signature to communications authentication although messages may often include information about the entity sending message that information may not be accurate
digital signatures can be used to authenticate the identity of the source messages
when ownership of digital signature secret key is bound to specific user valid signature shows that the message was sent by that user
the importance of high confidence in sender authenticity is especially obvious in financial context
for example suppose bank branch office sends instructions to the central office requesting change in the balance of an account
if the central office is not convinced that such message is truly sent from an authorized source acting on such request could be grave mistake
integrity in many scenarios the sender and receiver of message may have need for confidence that the message has not been altered during transmission
although encryption hides the contents of message it may be possible to change an encrypted message without understanding it
some encryption algorithms called nonmalleable prevent this but others do not
however if message is digitally signed any change in the message after signature invalidates the signature
furthermore there is no efficient way to modify message and its signature to produce new message with valid signature because this is still considered to be computationally infeasible by most cryptographic hash functions see collision resistance
non repudiation non repudiation or more specifically non repudiation of origin is an important aspect of digital signatures
by this property an entity that has signed some information cannot at later time deny having signed it
similarly access to the public key only does not enable fraudulent party to fake valid signature
note that these authentication non repudiation etc
properties rely on the secret key not having been revoked prior to its usage
public revocation of key pair is required ability else leaked secret keys would continue to implicate the claimed owner of the key pair
checking revocation status requires an online check checking certificate revocation list or via the online certificate status protocol
very roughly this is analogous to vendor who receives credit cards first checking online with the credit card issuer to find if given card has been reported lost or stolen
of course with stolen key pairs the theft is often discovered only after the secret key use to sign bogus certificate for espionage purpose
notions of security in their foundational paper goldwasser micali and rivest lay out hierarchy of attack models against digital signatures in key only attack the attacker is only given the public verification key
in known message attack the attacker is given valid signatures for variety of messages known by the attacker but not chosen by the attacker
in an adaptive chosen message attack the attacker first learns signatures on arbitrary messages of the attacker choice they also describe hierarchy of attack results total break results in the recovery of the signing key
universal forgery attack results in the ability to forge signatures for any message
selective forgery attack results in signature on message of the adversary choice
an existential forgery merely results in some valid message signature pair not already known to the adversary the strongest notion of security therefore is security against existential forgery under an adaptive chosen message attack
additional security precautions putting the private key on smart card all public key private key cryptosystems depend entirely on keeping the private key secret
private key can be stored on user computer and protected by local password but this has two disadvantages the user can only sign documents on that particular computer the security of the private key depends entirely on the security of the computera more secure alternative is to store the private key on smart card
many smart cards are designed to be tamper resistant although some designs have been broken notably by ross anderson and his students
in typical digital signature implementation the hash calculated from the document is sent to the smart card whose cpu signs the hash using the stored private key of the user and then returns the signed hash
typically user must activate their smart card by entering personal identification number or pin code thus providing two factor authentication
it can be arranged that the private key never leaves the smart card although this is not always implemented
if the smart card is stolen the thief will still need the pin code to generate digital signature
this reduces the security of the scheme to that of the pin system although it still requires an attacker to possess the card
mitigating factor is that private keys if generated and stored on smart cards are usually regarded as difficult to copy and are assumed to exist in exactly one copy
thus the loss of the smart card may be detected by the owner and the corresponding certificate can be immediately revoked
private keys that are protected by software only may be easier to copy and such compromises are far more difficult to detect
using smart card readers with separate keyboard entering pin code to activate the smart card commonly requires numeric keypad
some card readers have their own numeric keypad
this is safer than using card reader integrated into pc and then entering the pin using that computer keyboard
readers with numeric keypad are meant to circumvent the eavesdropping threat where the computer might be running keystroke logger potentially compromising the pin code
specialized card readers are also less vulnerable to tampering with their software or hardware and are often eal certified
other smart card designs smart card design is an active field and there are smart card schemes which are intended to avoid these particular problems despite having few security proofs so far
using digital signatures only with trusted applications one of the main differences between digital signature and written signature is that the user does not see what they sign
the user application presents hash code to be signed by the digital signing algorithm using the private key
an attacker who gains control of the user pc can possibly replace the user application with foreign substitute in effect replacing the user own communications with those of the attacker
this could allow malicious application to trick user into signing any document by displaying the user original on screen but presenting the attacker own documents to the signing application
to protect against this scenario an authentication system can be set up between the user application word processor email client etc
and the signing application
the general idea is to provide some means for both the user application and signing application to verify each other integrity
for example the signing application may require all requests to come from digitally signed binaries
using network attached hardware security module one of the main differences between cloud based digital signature service and locally provided one is risk
many risk averse companies including governments financial and medical institutions and payment processors require more secure standards like fips level and fips certification to ensure the signature is validated and secure
wysiwys technically speaking digital signature applies to string of bits whereas humans and applications believe that they sign the semantic interpretation of those bits
in order to be semantically interpreted the bit string must be transformed into form that is meaningful for humans and applications and this is done through combination of hardware and software based processes on computer system
the problem is that the semantic interpretation of bits can change as function of the processes used to transform the bits into semantic content
it is relatively easy to change the interpretation of digital document by implementing changes on the computer system where the document is being processed
from semantic perspective this creates uncertainty about what exactly has been signed
wysiwys what you see is what you sign means that the semantic interpretation of signed message cannot be changed
in particular this also means that message cannot contain hidden information that the signer is unaware of and that can be revealed after the signature has been applied
wysiwys is requirement for the validity of digital signatures but this requirement is difficult to guarantee because of the increasing complexity of modern computer systems
the term wysiwys was coined by peter landrock and torben pedersen to describe some of the principles in delivering secure and legally binding digital signatures for pan european projects
digital signatures versus ink on paper signatures an ink signature could be replicated from one document to another by copying the image manually or digitally but to have credible signature copies that can resist some scrutiny is significant manual or technical skill and to produce ink signature copies that resist professional scrutiny is very difficult
digital signatures cryptographically bind an electronic identity to an electronic document and the digital signature cannot be copied to another document
paper contracts sometimes have the ink signature block on the last page and the previous pages may be replaced after signature is applied
digital signatures can be applied to an entire document such that the digital signature on the last page will indicate tampering if any data on any of the pages have been altered but this can also be achieved by signing with ink and numbering all pages of the contract
some digital signature algorithms rsa dsa ecdsa eddsa rsa with sha ecdsa with sha elgamal signature scheme as the predecessor to dsa and variants schnorr signature and pointcheval stern signature algorithm rabin signature algorithm pairing based schemes such as bls ntrusign is an example of digital signature scheme based on hard lattice problems undeniable signatures aggregate signatureru signature scheme that supports aggregation given signatures on messages from users it is possible to aggregate all these signatures into single signature whose size is constant in the number of users
this single signature will convince the verifier that the users did indeed sign the original messages
scheme by mihir bellare and gregory neven may be used with bitcoin
signatures with efficient protocols are signature schemes that facilitate efficient cryptographic protocols such as zero knowledge proofs or secure computation
the current state of use legal and practical most digital signature schemes share the following goals regardless of cryptographic theory or legal provision quality algorithms some public key algorithms are known to be insecure as practical attacks against them having been discovered
quality implementations an implementation of good algorithm or protocol with mistake will not work
users and their software must carry out the signature protocol properly
the private key must remain private if the private key becomes known to any other party that party can produce perfect digital signatures of anything
the public key owner must be verifiable public key associated with bob actually came from bob
this is commonly done using public key infrastructure pki and the public key user association is attested by the operator of the pki called certificate authority
for open pkis in which anyone can request such an attestation universally embodied in cryptographically protected public key certificate the possibility of mistaken attestation is non trivial
commercial pki operators have suffered several publicly known problems
such mistakes could lead to falsely signed and thus wrongly attributed documents
closed pki systems are more expensive but less easily subverted in this way only if all of these conditions are met will digital signature actually be any evidence of who sent the message and therefore of their assent to its contents
legal enactment cannot change this reality of the existing engineering possibilities though some such have not reflected this actuality
legislatures being importuned by businesses expecting to profit from operating pki or by the technological avant garde advocating new solutions to old problems have enacted statutes and or regulations in many jurisdictions authorizing endorsing encouraging or permitting digital signatures and providing for or limiting their legal effect
the first appears to have been in utah in the united states followed closely by the states massachusetts and california
other countries have also passed statutes or issued regulations in this area as well and the un has had an active model law project for some time
these enactments or proposed enactments vary from place to place have typically embodied expectations at variance optimistically or pessimistically with the state of the underlying cryptographic engineering and have had the net effect of confusing potential users and specifiers nearly all of whom are not cryptographically knowledgeable
adoption of technical standards for digital signatures have lagged behind much of the legislation delaying more or less unified engineering position on interoperability algorithm choice key lengths and so on what the engineering is attempting to provide
industry standards some industries have established common interoperability standards for the use of digital signatures between members of the industry and with regulators
these include the automotive network exchange for the automobile industry and the safe biopharma association for the healthcare industry
using separate key pairs for signing and encryption in several countries digital signature has status somewhat like that of traditional pen and paper signature as in the eu digital signature directive and eu follow on legislation
generally these provisions mean that anything digitally signed legally binds the signer of the document to the terms therein
for that reason it is often thought best to use separate key pairs for encrypting and signing
using the encryption key pair person can engage in an encrypted conversation regarding real estate transaction but the encryption does not legally sign every message he or she sends
only when both parties come to an agreement do they sign contract with their signing keys and only then are they legally bound by the terms of specific document
after signing the document can be sent over the encrypted link
if signing key is lost or compromised it can be revoked to mitigate any future transactions
if an encryption key is lost backup or key escrow should be utilized to continue viewing encrypted content
signing keys should never be backed up or escrowed unless the backup destination is securely encrypted
see also cfr advanced electronic signature blind signature detached signature digital certificate digital signature in estonia electronic lab notebook electronic signature electronic signatures and law esign india gnu privacy guard public key infrastructure public key fingerprint server based signatures probabilistic signature scheme notes references goldreich oded foundations of cryptography basic tools cambridge cambridge university press isbn goldreich oded foundations of cryptography ii basic applications publ
press isbn pass rafael course in cryptography pdf retrieved december further reading katz and lindell introduction to modern cryptography chapman hall crc press lorna brazell electronic signatures and identities law and regulation nd edn london sweet maxwell dennis campbell editor commerce and the law of digital signatures oceana publications
schellenkens electronic signatures authentication technology from legal perspective tmc asser press
jeremiah buckley john kromer margo tank and david whitaker the law of electronic signatures rd edition west publishing
digital evidence and electronic signature law review free open source
in probability theory and statistics variance is the expectation of the squared deviation of random variable from its population mean or sample mean 
variance is measure of dispersion meaning it is measure of how far set of numbers is spread out from their average value 
variance has central role in statistics where some ideas that use it include descriptive statistics statistical inference hypothesis testing goodness of fit and monte carlo sampling 
variance is an important tool in the sciences where statistical analysis of data is common 
the variance is the square of the standard deviation the second central moment of distribution and the covariance of the random variable with itself and it is often represented by var or an advantage of variance as measure of dispersion is that it is more amenable to algebraic manipulation than other measures of dispersion such as the expected absolute deviation for example the variance of sum of uncorrelated random variables is equal to the sum of their variances 
disadvantage of the variance for practical applications is that unlike the standard deviation its units differ from the random variable which is why the standard deviation is more commonly reported as measure of dispersion once the calculation is finished 
there are two distinct concepts that are both called variance 
one as discussed above is part of theoretical probability distribution and is defined by an equation 
the other variance is characteristic of set of observations 
when variance is calculated from observations those observations are typically measured from real world system 
if all possible observations of the system are present then the calculated variance is called the population variance 
normally however only subset is available and the variance calculated from this is called the sample variance 
the variance calculated from sample is considered an estimate of the full population variance 
there are multiple ways to calculate an estimate of the population variance as discussed in the section below 
the two kinds of variance are closely related 
to see how consider that theoretical probability distribution can be used as generator of hypothetical observations 
if an infinite number of observations are generated using distribution then the sample variance calculated from that infinite set will match the value calculated using the distribution equation for variance 
etymology the term variance was first introduced by ronald fisher in his paper the correlation between relatives on the supposition of mendelian inheritance the great body of available statistics show us that the deviations of human measurement from its mean follow very closely the normal law of errors and therefore that the variability may be uniformly measured by the standard deviation corresponding to the square root of the mean square error 
when there are two independent causes of variability capable of producing in an otherwise uniform population distributions with standard deviations and it is found that the distribution when both causes act together has standard deviation it is therefore desirable in analysing the causes of variability to deal with the square of the standard deviation as the measure of variability 
we shall term this quantity the variance definition the variance of random variable is the expected value of the squared deviation from the mean of var 
this definition encompasses random variables that are generated by processes that are discrete continuous neither or mixed 
the variance can also be thought of as the covariance of random variable with itself var cov 
the variance is also equivalent to the second cumulant of probability distribution that generates the variance is typically designated as var or sometimes as or or symbolically as or simply pronounced sigma squared 
the expression for the variance can be expanded as follows var in other words the variance of is equal to the mean of the square of minus the square of the mean of this equation should not be used for computations using floating point arithmetic because it suffers from catastrophic cancellation if the two components of the equation are similar in magnitude 
for other numerically stable alternatives see algorithms for calculating variance 
discrete random variable if the generator of random variable is discrete with probability mass function then var where is the expected value 
when such discrete weighted variance is specified by weights whose sum is not then one divides by the sum of the weights 
the variance of collection of equally likely values can be written as var where is the average value 
that is the variance of set of equally likely values can be equivalently expressed without directly referring to the mean in terms of squared deviations of all pairwise squared distances of points from each other var 
absolutely continuous random variable if the random variable has probability density function and is the corresponding cumulative distribution function then var or equivalently var where is the expected value of given by 
in these formulas the integrals with respect to and are lebesgue and lebesgue stieltjes integrals respectively 
if the function is riemann integrable on every finite interval then var where the integral is an improper riemann integral 
examples exponential distribution the exponential distribution with parameter is continuous distribution whose probability density function is given by on the interval 
its mean can be shown to be using integration by parts and making use of the expected value already calculated we have thus the variance of is given by var 
fair die fair six sided die can be modeled as discrete random variable with outcomes through each with equal probability 
the expected value of is therefore the variance of is var the general formula for the variance of the outcome of an sided die is var 
commonly used probability distributions the following table lists the variance for some commonly used probability distributions 
properties basic properties variance is non negative because the squares are positive or zero var the variance of constant is zero 
var conversely if the variance of random variable is then it is almost surely constant 
that is it always has the same value var 
issues of finiteness if distribution does not have finite expected value as is the case for the cauchy distribution then the variance cannot be finite either 
however some distributions may not have finite variance despite their expected value being finite 
an example is pareto distribution whose index satisfies 
decomposition the general formula for variance decomposition or the law of total variance is if and are two random variables and the variance of exists then var var var 
the conditional expectation of given and the conditional variance var may be understood as follows 
given any particular value of the random variable there is conditional expectation given the event this quantity depends on the particular value it is function 
that same function evaluated at the random variable is the conditional expectation 
in particular if is discrete random variable assuming possible values with corresponding probabilities then in the formula for total variance the first term on the right hand side becomes var where var 
similarly the second term on the right hand side becomes var where and thus the total variance is given by var 
similar formula is applied in analysis of variance where the corresponding formula is total between within here refers to the mean of the squares 
in linear regression analysis the corresponding formula is total regression residual 
this can also be derived from the additivity of variances since the total observed score is the sum of the predicted score and the error score where the latter two are uncorrelated 
similar decompositions are possible for the sum of squared deviations sum of squares total between within total regression residual 
calculation from the cdf the population variance for non negative random variable can be expressed in terms of the cumulative distribution function using this expression can be used to calculate the variance in situations where the cdf but not the density can be conveniently expressed 
characteristic property the second moment of random variable attains the minimum value when taken around the first moment mean of the random variable 
conversely if continuous function satisfies for all random variables then it is necessarily of the form where this also holds in the multidimensional case 
units of measurement unlike the expected absolute deviation the variance of variable has units that are the square of the units of the variable itself 
for example variable measured in meters will have variance measured in meters squared 
for this reason describing data sets via their standard deviation or root mean square deviation is often preferred over using the variance 
in the dice example the standard deviation is slightly larger than the expected absolute deviation of the standard deviation and the expected absolute deviation can both be used as an indicator of the spread of distribution 
the standard deviation is more amenable to algebraic manipulation than the expected absolute deviation and together with variance and its generalization covariance is used frequently in theoretical statistics however the expected absolute deviation tends to be more robust as it is less sensitive to outliers arising from measurement anomalies or an unduly heavy tailed distribution 
propagation addition and multiplication by constant variance is invariant with respect to changes in location parameter 
that is if constant is added to all values of the variable the variance is unchanged var var 
if all values are scaled by constant the variance is scaled by the square of that constant var var 
the variance of sum of two random variables is given by var var var cov var var var cov where cov is the covariance 
linear combinations in general for the sum of random variables the variance becomes var cov var cov see also general bienaym identity 
these results lead to the variance of linear combination as var cov var cov var cov 
if the random variables are such that cov then they are said to be uncorrelated 
it follows immediately from the expression given earlier that if the random variables are uncorrelated then the variance of their sum is equal to the sum of their variances or expressed symbolically var var 
since independent random variables are always uncorrelated see covariance uncorrelatedness and independence the equation above holds in particular when the random variables are independent 
thus independence is sufficient but not necessary for the variance of the sum to equal the sum of the variances 
matrix notation for the variance of linear combination define as column vector of random variables and as column vector of scalars therefore is linear combination of these random variables where denotes the transpose of also let be the covariance matrix of the variance of is then given by var this implies that the variance of the mean can be written as with column vector of ones var var 
sum of variables sum of uncorrelated variables one reason for the use of the variance in preference to other measures of dispersion is that the variance of the sum or the difference of uncorrelated random variables is the sum of their variances var var 
this statement is called the bienaym formula and was discovered in it is often made with the stronger condition that the variables are independent but being uncorrelated suffices 
so if all the variables have the same variance then since division by is linear transformation this formula immediately implies that the variance of their mean is var var var that is the variance of the mean decreases when increases 
this formula for the variance of the mean is used in the definition of the standard error of the sample mean which is used in the central limit theorem 
to prove the initial statement it suffices to show that var var var 
the general result then follows by induction 
starting with the definition var using the linearity of the expectation operator and the assumption of independence or uncorrelatedness of and this further simplifies as follows var var var 
sum of correlated variables sum of correlated variables with fixed sample size in general the variance of the sum of variables is the sum of their covariances var cov var cov 
note the second equality comes from the fact that cov xi xi var xi 
here cov is the covariance which is zero for independent random variables if it exists 
the formula states that the variance of sum is equal to the sum of all elements in the covariance matrix of the components 
the next expression states equivalently that the variance of the sum is the sum of the diagonal of covariance matrix plus two times the sum of its upper triangular elements or its lower triangular elements this emphasizes that the covariance matrix is symmetric 
this formula is used in the theory of cronbach alpha in classical test theory 
so if the variables have equal variance and the average correlation of distinct variables is then the variance of their mean is var this implies that the variance of the mean increases with the average of the correlations 
in other words additional correlated observations are not as effective as additional independent observations at reducing the uncertainty of the mean 
moreover if the variables have unit variance for example if they are standardized then this simplifies to var this formula is used in the spearman brown prediction formula of classical test theory 
this converges to if goes to infinity provided that the average correlation remains constant or converges too 
so for the variance of the mean of standardized variables with equal correlations or converging average correlation we have lim var therefore the variance of the mean of large number of standardized variables is approximately equal to their average correlation 
this makes clear that the sample mean of correlated variables does not generally converge to the population mean even though the law of large numbers states that the sample mean will converge for independent variables 
sum of uncorrelated variables with random sample size there are cases when sample is taken without knowing in advance how many observations will be acceptable according to some criterion 
in such cases the sample size is random variable whose variation adds to the variation of such that var var var which follows from the law of total variance 
if has poisson distribution then var with estimator so the estimator of var becomes giving se weighted sum of variables the scaling property and the bienaym formula along with the property of the covariance cov ax by ab cov jointly imply that var var var cov 
this implies that in weighted sum of variables the variable with the largest weight will have disproportionally large weight in the variance of the total 
for example if and are uncorrelated and the weight of is two times the weight of then the weight of the variance of will be four times the weight of the variance of the expression above can be extended to weighted sum of multiple variables var var cov product of variables product of independent variables if two variables and are independent the variance of their product is given by var var var var var 
equivalently using the basic properties of expectation it is given by var 
product of statistically dependent variables in general if two variables are statistically dependent then the variance of their product is given by var cov cov var var cov arbitrary functions the delta method uses second order taylor expansions to approximate the variance of function of one or more random variables see taylor expansions for the moments of functions of random variables 
for example the approximate variance of function of one variable is given by var var provided that is twice differentiable and that the mean and variance of are finite 
population variance and sample variance real world observations such as the measurements of yesterday rain throughout the day typically cannot be complete sets of all possible observations that could be made 
as such the variance calculated from the finite set will in general not match the variance that would have been calculated from the full population of possible observations 
this means that one estimates the mean and variance from limited set of observations by using an estimator equation 
the estimator is function of the sample of observations drawn without observational bias from the whole population of potential observations 
in this example that sample would be the set of actual measurements of yesterday rainfall from available rain gauges within the geography of interest 
the simplest estimators for population mean and population variance are simply the mean and variance of the sample the sample mean and uncorrected sample variance these are consistent estimators they converge to the correct value as the number of samples increases but can be improved 
estimating the population variance by taking the sample variance is close to optimal in general but can be improved in two ways 
most simply the sample variance is computed as an average of squared deviations about the sample mean by dividing by however using values other than improves the estimator in various ways 
four common values for the denominator are and is the simplest population variance of the sample eliminates bias minimizes mean squared error for the normal distribution and mostly eliminates bias in unbiased estimation of standard deviation for the normal distribution 
firstly if the true population mean is unknown then the sample variance which uses the sample mean in place of the true mean is biased estimator it underestimates the variance by factor of correcting by this factor dividing by instead of is called bessel correction 
the resulting estimator is unbiased and is called the corrected sample variance or unbiased sample variance 
for example when the variance of single observation about the sample mean itself is obviously zero regardless of the population variance 
if the mean is determined in some other way than from the same samples used to estimate the variance then this bias does not arise and the variance can safely be estimated as that of the samples about the independently known mean 
secondly the sample variance does not generally minimize mean squared error between sample variance and population variance 
correcting for bias often makes this worse one can always choose scale factor that performs better than the corrected sample variance though the optimal scale factor depends on the excess kurtosis of the population see mean squared error variance and introduces bias 
this always consists of scaling down the unbiased estimator dividing by number larger than and is simple example of shrinkage estimator one shrinks the unbiased estimator towards zero 
for the normal distribution dividing by instead of or minimizes mean squared error 
the resulting estimator is biased however and is known as the biased sample variation 
population variance in general the population variance of finite population of size with values xi is given by where the population mean is the population variance can also be computed using this is true because the population variance matches the variance of the generating probability distribution 
in this sense the concept of population can be extended to continuous random variables with infinite populations 
sample variance biased sample variance in many practical situations the true variance of population is not known priori and must be computed somehow 
when dealing with extremely large populations it is not possible to count every object in the population so the computation must be performed on sample of the population 
sample variance can also be applied to the estimation of the variance of continuous distribution from sample of that distribution 
we take sample with replacement of values yn from the population where and estimate the variance on the basis of this sample 
directly taking the variance of the sample data gives the average of the squared deviations here denotes the sample mean since the yi are selected randomly both and are random variables 
their expected values can be evaluated by averaging over the ensemble of all possible samples of size from the population 
for this gives hence gives an estimate of the population variance that is biased by factor of for this reason is referred to as the biased sample variance 
unbiased sample variance correcting for this bias yields the unbiased sample variance denoted either estimator may be simply referred to as the sample variance when the version can be determined by context 
the same proof is also applicable for samples taken from continuous probability distribution 
the use of the term is called bessel correction and it is also used in sample covariance and the sample standard deviation the square root of variance 
the square root is concave function and thus introduces negative bias by jensen inequality which depends on the distribution and thus the corrected sample standard deviation using bessel correction is biased 
the unbiased estimation of standard deviation is technically involved problem though for the normal distribution using the term yields an almost unbiased estimator 
the unbiased sample variance is statistic for the function meaning that it is obtained by averaging sample statistic over element subsets of the population 
distribution of the sample variance being function of random variables the sample variance is itself random variable and it is natural to study its distribution 
in the case that yi are independent observations from normal distribution cochran theorem shows that follows scaled chi squared distribution see also asymptotic properties as direct consequence it follows that and var var var if the yi are independent and identically distributed but not necessarily normally distributed then var where is the kurtosis of the distribution and is the fourth central moment 
if the conditions of the law of large numbers hold for the squared observations is consistent estimator of 
one can see indeed that the variance of the estimator tends asymptotically to zero 
an asymptotically equivalent formula was given in kenney and keeping rose and smith and weisstein 
samuelson inequality samuelson inequality is result that states bounds on the values that individual observations in sample can take given that the sample mean and biased variance have been calculated 
values must lie within the limits 
relations with the harmonic and arithmetic means it has been shown that for sample of positive real numbers max where ymax is the maximum of the sample is the arithmetic mean is the harmonic mean of the sample and is the biased variance of the sample 
this bound has been improved and it is known that variance is bounded by max max max min min min where ymin is the minimum of the sample 
tests of equality of variances the test of equality of variances and the chi square tests are adequate when the sample is normally distributed 
non normality makes testing for the equality of two or more variances more difficult 
several non parametric tests have been proposed these include the barton david ansari freund siegel tukey test the capon test mood test the klotz test and the sukhatme test 
the sukhatme test applies to two variances and requires that both medians be known and equal to zero 
the mood klotz capon and barton david ansari freund siegel tukey tests also apply to two variances 
they allow the median to be unknown but do require that the two medians are equal 
the lehmann test is parametric test of two variances 
of this test there are several variants known 
other tests of the equality of variances include the box test the box anderson test and the moses test 
resampling methods which include the bootstrap and the jackknife may be used to test the equality of variances 
moment of inertia the variance of probability distribution is analogous to the moment of inertia in classical mechanics of corresponding mass distribution along line with respect to rotation about its center of mass 
it is because of this analogy that such things as the variance are called moments of probability distributions 
the covariance matrix is related to the moment of inertia tensor for multivariate distributions 
the moment of inertia of cloud of points with covariance matrix of is given by tr 
this difference between moment of inertia in physics and in statistics is clear for points that are gathered along line 
suppose many points are close to the axis and distributed along it 
the covariance matrix might look like 
that is there is the most variance in the direction 
physicists would consider this to have low moment about the axis so the moment of inertia tensor is 
semivariance the semivariance is calculated in the same manner as the variance but only those observations that fall below the mean are included in the calculation it is also described as specific measure in different fields of application 
for skewed distributions the semivariance can provide additional information that variance does not for inequalities associated with the semivariance see chebyshev inequality semivariances 
generalizations for complex variables if is scalar complex valued random variable with values in then its variance is where is the complex conjugate of this variance is real scalar 
for vector valued random variables as matrix if is vector valued random variable with values in and thought of as column vector then natural generalization of variance is where and is the transpose of and so is row vector 
the result is positive semi definite square matrix commonly referred to as the variance covariance matrix or simply as the covariance matrix 
if is vector and complex valued random variable with values in then the covariance matrix is where is the conjugate transpose of this matrix is also positive semi definite and square 
as scalar another generalization of variance for vector valued random variables which results in scalar value rather than in matrix is the generalized variance det the determinant of the covariance matrix 
the generalized variance can be shown to be related to the multidimensional scatter of points around their mean different generalization is obtained by considering the euclidean distance between the random variable and its mean 
this results in tr which is the trace of the covariance matrix 
see also bhatia davis inequality coefficient of variation homoscedasticity least squares spectral analysis for computing frequency spectrum with spectral magnitudes in of variance or in db popoviciu inequality on variances measures for statistical dispersion variance stabilizing transformation types of variance correlation distance variance explained variance pooled variance pseudo variance references
symmetric key algorithms are algorithms for cryptography that use the same cryptographic keys for both the encryption of plaintext and the decryption of ciphertext
the keys may be identical or there may be simple transformation to go between the two keys
the keys in practice represent shared secret between two or more parties that can be used to maintain private information link
the requirement that both parties have access to the secret key is one of the main drawbacks of symmetric key encryption in comparison to public key encryption also known as asymmetric key encryption
however symmetric key encryption algorithms are usually better for bulk encryption
they have smaller key size which means less storage space and faster transmission
due to this asymmetric key encryption is often used to exchange the secret key for symmetric key encryption
types symmetric key encryption can use either stream ciphers or block ciphers
stream ciphers encrypt the digits typically bytes or letters in substitution ciphers of message one at time
an example is chacha
substitution ciphers are well known ciphers but can be easily decrypted using frequency table
block ciphers take number of bits and encrypt them in single unit padding the plaintext to achieve multiple of the block size
the advanced encryption standard aes algorithm approved by nist in december uses bit blocks
implementations examples of popular symmetric key algorithms include twofish serpent aes rijndael camellia salsa chacha blowfish cast kuznyechik rc des des skipjack safer and idea
use as cryptographic primitive symmetric ciphers are commonly used to achieve other cryptographic primitives than just encryption encrypting message does not guarantee that it will remain unchanged while encrypted
hence often message authentication code is added to ciphertext to ensure that changes to the ciphertext will be noted by the receiver
message authentication codes can be constructed from an aead cipher
however symmetric ciphers cannot be used for non repudiation purposes except by involving additional parties
see the iso iec standard
another application is to build hash functions from block ciphers
see one way compression function for descriptions of several such methods
construction of symmetric ciphers many modern block ciphers are based on construction proposed by horst feistel
feistel construction makes it possible to build invertible functions from other functions that are themselves not invertible
security of symmetric ciphers symmetric ciphers have historically been susceptible to known plaintext attacks chosen plaintext attacks differential cryptanalysis and linear cryptanalysis
careful construction of the functions for each round can greatly reduce the chances of successful attack
it is also possible to increase the key length or the rounds in the encryption process to better protect against attack
this however tends to increase the processing power and decrease the speed at which the process runs due to the amount of operations the system needs to do most modern symmetric key algorithms appear to be resistant to the threat of post quantum cryptography
quantum computers would exponentially increase the speed at which these ciphers can be decoded notably grover algorithm would take the square root of the time traditionally required for brute force attack although these vulnerabilities can be compensated for by doubling key length
for example bit aes cipher would not be secure against such an attack as it would reduce the time required to test all possible iterations from over quintillion years to about six months
by contrast it would still take quantum computer the same amount of time to decode bit aes cipher as it would conventional computer to decode bit aes cipher
for this reason aes is believed to be quantum resistant
key management key establishment symmetric key algorithms require both the sender and the recipient of message to have the same secret key
all early cryptographic systems required either the sender or the recipient to somehow receive copy of that secret key over physically secure channel
nearly all modern cryptographic systems still use symmetric key algorithms internally to encrypt the bulk of the messages but they eliminate the need for physically secure channel by using diffie hellman key exchange or some other public key protocol to securely come to agreement on fresh new secret key for each session conversation forward secrecy
key generation when used with asymmetric ciphers for key transfer pseudorandom key generators are nearly always used to generate the symmetric cipher session keys
however lack of randomness in those generators or in their initialization vectors is disastrous and has led to cryptanalytic breaks in the past
therefore it is essential that an implementation use source of high entropy for its initialization
reciprocal cipher reciprocal cipher is cipher where just as one enters the plaintext into the cryptography system to get the ciphertext one could enter the ciphertext into the same place in the system to get the plaintext
reciprocal cipher is also sometimes referred as self reciprocal cipher practically all mechanical cipher machines implement reciprocal cipher mathematical involution on each typed in letter
instead of designing two kinds of machines one for encrypting and one for decrypting all the machines can be identical and can be set up keyed the same way examples of reciprocal ciphers include atbash beaufort cipher enigma machine marie antoinette and axel von fersen communicated with self reciprocal cipher
the porta polyalphabetic cipher is self reciprocal
purple cipher rc rot xor cipher vatsyayana cipherthe majority of all modern ciphers can be classified as either stream cipher most of which use reciprocal xor cipher combiner or block cipher most of which use feistel cipher or lai massey scheme with reciprocal transformation in each round
in statistics mediation model seeks to identify and explain the mechanism or process that underlies an observed relationship between an independent variable and dependent variable via the inclusion of third hypothetical variable known as mediator variable also mediating variable intermediary variable or intervening variable 
rather than direct causal relationship between the independent variable and the dependent variable mediation model proposes that the independent variable influences the mediator variable which in turn influences the dependent variable 
thus the mediator variable serves to clarify the nature of the relationship between the independent and dependent variables mediation analyses are employed to understand known relationship by exploring the underlying mechanism or process by which one variable influences another variable through mediator variable 
in particular mediation analysis can contribute to better understanding the relationship between an independent variable and dependent variable when these variables do not have an obvious direct connection 
baron and kenny steps for mediation analysis baron and kenny laid out several requirements that must be met to form true mediation relationship 
they are outlined below using real world example 
see the diagram above for visual representation of the overall mediating relationship to be explained 
note hayes critiqued baron and kenny mediation steps approach and as of david kenny on his website stated that mediation can exist in the absence of significant total effect and therefore step below may not be needed 
this situation is sometimes referred to as inconsistent mediation 
later publications by hayes also questioned the concepts of full or partial mediation and advocated for these terms along with the classical mediation steps approach outlined below to be abandoned 
step regress the dependent variable on the independent variable to confirm that the independent variable is significant predictor of the dependent variable independent variable dependent variable is significant step regress the mediator on the independent variable to confirm that the independent variable is significant predictor of the mediator 
if the mediator is not associated with the independent variable then it couldn possibly mediate anything independent variable mediator is significant step regress the dependent variable on both the mediator and independent variable to confirm that the mediator is significant predictor of the dependent variable and the strength of the coefficient of the previously significant independent variable in step is now greatly reduced if not rendered nonsignificant 
is significant should be smaller in absolute value than the original effect for the independent variable above example the following example drawn from howell explains each step of baron and kenny requirements to understand further how mediation effect is characterized 
step and step use simple regression analysis whereas step uses multiple regression analysis 
how you were parented independent variable predicts how confident you feel about parenting your own children dependent variable 
how you were parented independent variable predicts your feelings of competence and self esteem mediator 
your feelings of competence and self esteem mediator predict how confident you feel about parenting your own children dependent variable while controlling for how you were parented independent variable such findings would lead to the conclusion implying that your feelings of competence and self esteem mediate the relationship between how you were parented and how confident you feel about parenting your own children 
if step does not yield significant result one may still have grounds to move to step sometimes there is actually significant relationship between independent and dependent variables but because of small sample sizes or other extraneous factors there could not be enough power to predict the effect that actually exists 
direct versus indirect effects in the diagram shown above the indirect effect is the product of path coefficients and 
the direct effect is the coefficient 
the direct effect measures the extent to which the dependent variable changes when the independent variable increases by one unit and the mediator variable remains unaltered 
in contrast the indirect effect measures the extent to which the dependent variable changes when the independent variable is held constant and the mediator variable changes by the amount it would have changed had the independent variable increased by one unit 
in linear systems the total effect is equal to the sum of the direct and indirect ab in the model above 
in nonlinear models the total effect is not generally equal to the sum of the direct and indirect effects but to modified combination of the two 
full versus partial mediation mediator variable can either account for all or some of the observed relationship between two variables 
full mediation maximum evidence for mediation also called full mediation would occur if inclusion of the mediation variable drops the relationship between the independent variable and dependent variable see pathway in diagram above to zero 
partial mediation partial mediation maintains that the mediating variable accounts for some but not all of the relationship between the independent variable and dependent variable 
partial mediation implies that there is not only significant relationship between the mediator and the dependent variable but also some direct relationship between the independent and dependent variable 
in order for either full or partial mediation to be established the reduction in variance explained by the independent variable must be significant as determined by one of several tests such as the sobel test 
the effect of an independent variable on the dependent variable can become nonsignificant when the mediator is introduced simply because trivial amount of variance is explained not true mediation 
thus it is imperative to show significant reduction in variance explained by the independent variable before asserting either full or partial mediation 
it is possible to have statistically significant indirect effects in the absence of total effect 
this can be explained by the presence of several mediating paths that cancel each other out and become noticeable when one of the cancelling mediators is controlled for 
this implies that the terms partial and full mediation should always be interpreted relative to the set of variables that are present in the model 
in all cases the operation of fixing variable must be distinguished from that of controlling for variable which has been inappropriately used in the literature 
the former stands for physically fixing while the latter stands for conditioning on adjusting for or adding to the regression model 
the two notions coincide only when all error terms not shown in the diagram are statistically uncorrelated 
when errors are correlated adjustments must be made to neutralize those correlations before embarking on mediation analysis see bayesian networks 
sobel test sobel test is performed to determine if the relationship between the independent variable and dependent variable has been significantly reduced after inclusion of the mediator variable 
in other words this test assesses whether mediation effect is significant 
it examines the relationship between the independent variable and the dependent variable compared to the relationship between the independent variable and dependent variable including the mediation factor 
the sobel test is more accurate than the baron and kenny steps explained above however it does have low statistical power 
as such large sample sizes are required in order to have sufficient power to detect significant effects 
this is because the key assumption of sobel test is the assumption of normality 
because sobel test evaluates given sample on the normal distribution small sample sizes and skewness of the sampling distribution can be problematic see normal distribution for more details 
thus the rule of thumb as suggested by mackinnon et al is that sample size of is required to detect small effect sample size of is sufficient in detecting medium effect and sample size of is required to detect large effect 
the equation for sobel is preacher hayes bootstrap method the bootstrapping method provides some advantages to the sobel test primarily an increase in power 
the preacher and hayes bootstrapping method is non parametric test 
as such the bootstrap method does not violate assumptions of normality and is therefore recommended for small sample sizes 
bootstrapping involves repeatedly randomly sampling observations with replacement from the data set to compute the desired statistic in each resample 
computing over hundreds or thousands of bootstrap resamples provide an approximation of the sampling distribution of the statistic of interest 
the preacher hayes method provides point estimates and confidence intervals by which one can assess the significance or nonsignificance of mediation effect 
point estimates reveal the mean over the number of bootstrapped samples and if zero does not fall between the resulting confidence intervals of the bootstrapping method one can confidently conclude that there is significant mediation effect to report 
significance of mediation as outlined above there are few different options one can choose from to evaluate mediation model 
bootstrapping is becoming the most popular method of testing mediation because it does not require the normality assumption to be met and because it can be effectively utilized with smaller sample sizes 
however mediation continues to be most frequently determined using the logic of baron and kenny or the sobel test 
it is becoming increasingly more difficult to publish tests of mediation based purely on the baron and kenny method or tests that make distributional assumptions such as the sobel test 
thus it is important to consider your options when choosing which test to conduct 
approaches to mediation while the concept of mediation as defined within psychology is theoretically appealing the methods used to study mediation empirically have been challenged by statisticians and epidemiologists and interpreted formally 
experimental causal chain design an experimental causal chain design is used when the proposed mediator is experimentally manipulated 
such design implies that one manipulates some controlled third variable that they have reason to believe could be the underlying mechanism of given relationship 
measurement of mediation design measurement of mediation design can be conceptualized as statistical approach 
such design implies that one measures the proposed intervening variable and then uses statistical analyses to establish mediation 
this approach does not involve manipulation of the hypothesized mediating variable but only involves measurement 
criticisms of mediation measurement experimental approaches to mediation must be carried out with caution 
first it is important to have strong theoretical support for the exploratory investigation of potential mediating variable 
criticism of mediation approach rests on the ability to manipulate and measure mediating variable 
thus one must be able to manipulate the proposed mediator in an acceptable and ethical fashion 
as such one must be able to measure the intervening process without interfering with the outcome 
the mediator must also be able to establish construct validity of manipulation 
one of the most common criticisms of the measurement of mediation approach is that it is ultimately correlational design 
consequently it is possible that some other third variable independent from the proposed mediator could be responsible for the proposed effect 
however researchers have worked hard to provide counter evidence to this disparagement 
specifically the following counter arguments have been put forward temporal precedence for example if the independent variable precedes the dependent variable in time this would provide evidence suggesting directional and potentially causal link from the independent variable to the dependent variable 
nonspuriousness and or no confounds for example should one identify other third variables and prove that they do not alter the relationship between the independent variable and the dependent variable he she would have stronger argument for their mediation effect 
see other rd variables below mediation can be an extremely useful and powerful statistical test however it must be used properly 
it is important that the measures used to assess the mediator and the dependent variable are theoretically distinct and that the independent variable and mediator cannot interact 
should there be an interaction between the independent variable and the mediator one would have grounds to investigate moderation 
other third variables confounding another model that is often tested is one in which competing variables in the model are alternative potential mediators or an unmeasured cause of the dependent variable 
an additional variable in causal model may obscure or confound the relationship between the independent and dependent variables 
potential confounders are variables that may have causal impact on both the independent variable and dependent variable 
they include common sources of measurement error as discussed above as well as other influences shared by both the independent and dependent variables 
in experimental studies there is special concern about aspects of the experimental manipulation or setting that may account for study effects rather than the motivating theoretical factor 
any of these problems may produce spurious relationships between the independent and dependent variables as measured 
ignoring confounding variable may bias empirical estimates of the causal effect of the independent variable 
suppression suppressor variable increases the predictive validity of another variable when included in regression equation 
suppression can occur when single causal variable is related to an outcome variable through two separate mediator variables and when one of those mediated effects is positive and one is negative 
in such case each mediator variable suppresses or conceals the effect that is carried through the other mediator variable 
for example higher intelligence scores causal variable may cause an increase in error detection mediator variable which in turn may cause decrease in errors made at work on an assembly line an outcome variable at the same time intelligence could also cause an increase in boredom which in turn may cause an increase in errors 
thus in one causal path intelligence decreases errors and in the other it increases them 
when neither mediator is included in the analysis intelligence appears to have no effect or weak effect on errors 
however when boredom is controlled intelligence will appear to decrease errors and when error detection is controlled intelligence will appear to increase errors 
if intelligence could be increased while only boredom was held constant errors would decrease if intelligence could be increased while holding only error detection constant errors would increase 
in general the omission of suppressors or confounders will lead to either an underestimation or an overestimation of the effect of on thereby either reducing or artificially inflating the magnitude of relationship between two variables 
moderators other important third variables are moderators 
moderators are variables that can make the relationship between two variables either stronger or weaker 
such variables further characterize interactions in regression by affecting the direction and or strength of the relationship between and moderating relationship can be thought of as an interaction 
it occurs when the relationship between variables and depends on the level of see moderation for further discussion 
moderated mediation mediation and moderation can co occur in statistical models 
it is possible to mediate moderation and moderate mediation 
moderated mediation is when the effect of the treatment on the mediator and or the partial effect on the dependent variable depend in turn on levels of another variable moderator 
essentially in moderated mediation mediation is first established and then one investigates if the mediation effect that describes the relationship between the independent variable and dependent variable is moderated by different levels of another variable moderator 
this definition has been outlined by muller judd and yzerbyt and preacher rucker and hayes 
models of moderated mediation there are five possible models of moderated mediation as illustrated in the diagrams below 
in the first model the independent variable also moderates the relationship between the mediator and the dependent variable 
the second possible model of moderated mediation involves new variable which moderates the relationship between the independent variable and the mediator the path 
the third model of moderated mediation involves new moderator variable which moderates the relationship between the mediator and the dependent variable the path 
moderated mediation can also occur when one moderating variable affects both the relationship between the independent variable and the mediator the path and the relationship between the mediator and the dependent variable the path 
the fifth and final possible model of moderated mediation involves two new moderator variables one moderating the path and the other moderating the path 
in addition to the models mentioned above new variable can also exist which moderates the relationship between the independent variable and mediator the path while at the same time have the new variable moderate the relationship between the independent variable and dependent variable the path 
mediated moderation mediated moderation is variant of both moderation and mediation 
this is where there is initially overall moderation and the direct effect of the moderator variable on the outcome is mediated 
the main difference between mediated moderation and moderated mediation is that for the former there is initial overall moderation and this effect is mediated and for the latter there is no moderation but the effect of either the treatment on the mediator path is moderated or the effect of the mediator on the outcome path is moderated in order to establish mediated moderation one must first establish moderation meaning that the direction and or the strength of the relationship between the independent and dependent variables path differs depending on the level of third variable the moderator variable 
researchers next look for the presence of mediated moderation when they have theoretical reason to believe that there is fourth variable that acts as the mechanism or process that causes the relationship between the independent variable and the moderator path or between the moderator and the dependent variable path 
example the following is published example of mediated moderation in psychological research 
participants were presented with an initial stimulus prime that made them think of morality or made them think of might 
they then participated in the prisoner dilemma game pdg in which participants pretend that they and their partner in crime have been arrested and they must decide whether to remain loyal to their partner or to compete with their partner and cooperate with the authorities 
the researchers found that prosocial individuals were affected by the morality and might primes whereas proself individuals were not 
thus social value orientation proself vs prosocial moderated the relationship between the prime independent variable morality vs might and the behaviour chosen in the pdg dependent variable competitive vs cooperative 
the researchers next looked for the presence of mediated moderation effect 
regression analyses revealed that the type of prime morality vs might mediated the moderating relationship of participants social value orientation on pdg behaviour 
prosocial participants who experienced the morality prime expected their partner to cooperate with them so they chose to cooperate themselves 
prosocial participants who experienced the might prime expected their partner to compete with them which made them more likely to compete with their partner and cooperate with the authorities 
in contrast participants with pro self social value orientation always acted competitively 
regression equations for moderated mediation and mediated moderation muller judd and yzerbyt outline three fundamental models that underlie both moderated mediation and mediated moderation 
mo represents the moderator variable me represents the mediator variable and represents the measurement error of each regression equation 
step moderation of the relationship between the independent variable and the dependent variable also called the overall treatment effect path in the diagram 
to establish overall moderation the regression weight must be significant first step for establishing mediated moderation 
establishing moderated mediation requires that there be no moderation effect so the regression weight must not be significant 
step moderation of the relationship between the independent variable and the mediator path 
if the regression weight is significant the moderator affects the relationship between the independent variable and the mediator 
step moderation of both the relationship between the independent and dependent variables path and the relationship between the mediator and the dependent variable path 
if both in step and in step are significant the moderator affects the relationship between the independent variable and the mediator path 
if both in step and in step are significant the moderator affects the relationship between the mediator and the dependent variable path 
either or both of the conditions above may be true 
causal mediation analysis fixing versus conditioning mediation analysis quantifies the extent to which variable participates in the transmittance of change from cause to its effect 
it is inherently causal notion hence it cannot be defined in statistical terms 
traditionally however the bulk of mediation analysis has been conducted within the confines of linear regression with statistical terminology masking the causal character of the relationships involved 
this led to difficulties biases and limitations that have been alleviated by modern methods of causal analysis based on causal diagrams and counterfactual logic 
the source of these difficulties lies in defining mediation in terms of changes induced by adding third variables into regression equation 
such statistical changes are epiphenomena which sometimes accompany mediation but in general fail to capture the causal relationships that mediation analysis aims to quantify 
the basic premise of the causal approach is that it is not always appropriate to control for the mediator when we seek to estimate the direct effect of on see the figure above 
the classical rationale for controlling for is that if we succeed in preventing from changing then whatever changes we measure in are attributable solely to variations in and we are justified then in proclaiming the effect observed as direct effect of on 
unfortunately controlling for does not physically prevent from changing it merely narrows the analyst attention to cases of equal values 
moreover the language of probability theory does not possess the notation to express the idea of preventing from changing or physically holding constant 
the only operator probability provides is conditioning which is what we do when we control for or add as regressor in the equation for the result is that instead of physically holding constant say at and comparing for units under to those under we allow to vary but ignore all units except those in which achieves the value these two operations are fundamentally different and yield different results except in the case of no omitted variables 
to illustrate assume that the error terms of and are correlated 
under such conditions the structural coefficient and between and and between and can no longer be estimated by regressing on and in fact the regression slopes may both be nonzero even when is zero 
this has two consequences 
first new strategies must be devised for estimating the structural coefficients and second the basic definitions of direct and indirect effects must go beyond regression analysis and should invoke an operation that mimics fixing rather than conditioning on definitions such an operator denoted do was defined in pearl and it operates by removing the equation of and replacing it by constant for example if the basic mediation model consists of the equations then after applying the operator do the model becomes and after applying the operator do the model becomes where the functions and as well as the distributions of the error terms and remain unaltered 
if we further rename the variables and resulting from do as and respectively we obtain what came to be known as potential outcomes or structural counterfactuals 
these new variables provide convenient notation for defining direct and indirect effects 
in particular four types of effects have been defined for the transition from to total effect controlled direct effect natural direct effect natural indirect effect where stands for expectation taken over the error terms 
these effects have the following interpretations te measures the expected increase in the outcome as changes from to while the mediator is allowed to track the change in as dictated by the function 
cde measures the expected increase in the outcome as changes from to while the mediator is fixed at pre specified level uniformly over the entire population nde measures the expected increase in as changes from to while setting the mediator variable to whatever value it would have obtained under before the change 
nie measures the expected increase in when the is held constant at and changes to whatever value it would have attained for each individual under the difference te nde measures the extent to which mediation is necessary for explaining the effect while the nie measures the extent to which mediation is sufficient for sustaining it controlled version of the indirect effect does not exist because there is no way of disabling the direct effect by fixing variable to constant 
according to these definitions the total effect can be decomposed as sum where nier stands for the reverse transition from to it becomes additive in linear systems where reversal of transitions entails sign reversal 
the power of these definitions lies in their generality they are applicable to models with arbitrary nonlinear interactions arbitrary dependencies among the disturbances and both continuous and categorical variables 
the mediation formula in linear analysis all effects are determined by sums of products of structural coefficients giving independent of therefore all effects are estimable whenever the model is identified 
in non linear systems more stringent conditions are needed for estimating the direct and indirect effects 
for example if no confounding exists and are mutually independent the following formulas can be derived 
the last two equations are called mediation formulas and have become the target of estimation in many studies of mediation 
they give distribution free expressions for direct and indirect effects and demonstrate that despite the arbitrary nature of the error distributions and the functions and mediated effects can nevertheless be estimated from data using regression 
the analyses of moderated mediation and mediating moderators fall as special cases of the causal mediation analysis and the mediation formulas identify how various interactions coefficients contribute to the necessary and sufficient components of mediation 
example assume the model takes the form where the parameter quantifies the degree to which modifies the effect of on even when all parameters are estimated from data it is still not obvious what combinations of parameters measure the direct and indirect effect of on or more practically how to assess the fraction of the total effect that is explained by mediation and the fraction of that is owed to mediation 
in linear analysis the former fraction is captured by the product the latter by the difference and the two quantities coincide 
in the presence of interaction however each fraction demands separate analysis as dictated by the mediation formula which yields thus the fraction of output response for which mediation would be sufficient is while the fraction for which mediation would be necessary is 
these fractions involve non obvious combinations of the model parameters and can be constructed mechanically with the help of the mediation formula 
significantly due to interaction direct effect can be sustained even when the parameter vanishes and moreover total effect can be sustained even when both the direct and indirect effects vanish 
this illustrates that estimating parameters in isolation tells us little about the effect of mediation and more generally mediation and moderation are intertwined and cannot be assessed separately 
references as of june this article is derived in whole or in part from causal analysis in theory and practice 
the copyright holder has licensed the content in manner that permits reuse under cc by sa and gfdl 
all relevant terms must be followed 
notes bibliographypreacher kristopher hayes andrew 
spss and sas procedures for estimating indirect effects in simple mediation models 
behavior research methods instruments and computers 
pmid preacher kristopher hayes andrew 
asymptotic and resampling strategies for assessing and comparing indirect effects in multiple mediator models 
pmid preacher zyphur zhang 
general multilevel sem framework for assessing multilevel mediation 
pmid baron and kenny 
the moderator mediator variable distinction in social psychological research conceptual strategic and statistical considerations journal of personality and social psychology vol 
statistical power analysis for the behavioral sciences nd ed 
new york ny academic press 
beyond baron and kenny statistical mediation analysis in the new millennium 
statistical methods for psychology th ed 
belmot ca cengage learning 
advances in statistical methods for substance abuse prevention research 
doi pmc pmid preacher kelley 
effect sizes measures for mediation models quantitative strategies for communicating indirect effects 
pmid rucker preacher tormala 
mediation analysis in social psychology current practices and new recommendations 
social and personality psychology compass 
asymptotic confidence intervals for indirect effects in structural equation models 
jstor spencer zanna fong 
establishing causal chain why experiments are often more effective than mediational analyses in examining psychological processes 
journal of personality and social psychology 
the mediation formula guide to the assessment of causal pathways in nonlinear models 
in berzuini dawid bernardinelli 
causality statistical perspectives and applications 
chichester uk john wiley and sons ltd pp 
research methods in psychology th ed pp 
new york mcgraw hill 
the determiners of behavior at choice point 
degrees of hunger reward and nonreward and maze learning in rats 
university of california publications in psychology 
explanation in causal inference 
external links summary of mediation methods at psychwiki example of causal mediation using propensity scores the methodology center penn state university book on moderation and mediation analysis including an introduction to the process macro for spss and sas andrew hayes ohio state university online text of the determiner of behavior at choice point kenneth maccorquodale and paul meehl on distinction between hypothetical constructs and intervening variables classics in the history of psychology retr
the sample mean or empirical mean and the sample covariance are statistics computed from sample of data on one or more random variables 
the sample mean is the average value or mean value of sample of numbers taken from larger population of numbers where population indicates not number of people but the entirety of relevant data whether collected or not 
sample of companies sales from the fortune might be used for convenience instead of looking at the population all companies sales 
the sample mean is used as an estimator for the population mean the average value in the entire population where the estimate is more likely to be close to the population mean if the sample is large and representative 
the reliability of the sample mean is estimated using the standard error which in turn is calculated using the variance of the sample 
if the sample is random the standard error falls with the size of the sample and the sample mean distribution approaches the normal distribution as the sample size increases 
the term sample mean can also be used to refer to vector of average values when the statistician is looking at the values of several variables in the sample 
the sales profits and employees of sample of fortune companies 
in this case there is not just sample variance for each variable but sample variance covariance matrix or simply covariance matrix showing also the relationship between each pair of variables 
this would be matrix when variables are being considered 
the sample covariance is useful in judging the reliability of the sample means as estimators and is also useful as an estimate of the population covariance matrix 
due to their ease of calculation and other desirable characteristics the sample mean and sample covariance are widely used in statistics to represent the location and dispersion of the distribution of values in the sample and to estimate the values for the population 
definition of the sample mean the sample mean is the average of the values of variable in sample which is the sum of those values divided by the number of values 
using mathematical notation if sample of observations on variable is taken from the population the sample mean is under this definition if the sample is taken from the population then the sample mean is as compared to the population mean of even if sample is random it is rarely perfectly representative and other samples would have other sample means even if the samples were all from the same population 
the sample for example would have sample mean of if the statistician is interested in variables rather than one each observation having value for each of those variables the overall sample mean consists of sample means for individual variables 
let be the ith independently drawn observation on the jth random variable 
these observations can be arranged into column vectors each with entries with the column vector giving the th observations of all variables being denoted 
the sample mean vector is column vector whose th element is the average value of the observations of the jth variable thus the sample mean vector contains the average of the observations for each variable and is written definition of sample covariance the sample covariance matrix is by matrix with entries where is an estimate of the covariance between the jth variable and the kth variable of the population underlying the data 
in terms of the observation vectors the sample covariance is 
alternatively arranging the observation vectors as the columns of matrix so that which is matrix of rows and columns 
here the sample covariance matrix can be computed as where is an by vector of ones 
if the observations are arranged as rows instead of columns so is now row vector and is an matrix whose column is the vector of observations on variable then applying transposes in the appropriate places yields 
like covariance matrices for random vector sample covariance matrices are positive semi definite 
to prove it note that for any matrix the matrix is positive semi definite 
furthermore covariance matrix is positive definite if and only if the rank of the 
vectors is unbiasedness the sample mean and the sample covariance matrix are unbiased estimates of the mean and the covariance matrix of the random vector row vector whose jth element is one of the random variables 
the sample covariance matrix has in the denominator rather than due to variant of bessel correction in short the sample covariance relies on the difference between each observation and the sample mean but the sample mean is slightly correlated with each observation since it is defined in terms of all observations 
if the population mean is known the analogous unbiased estimate using the population mean has in the denominator 
this is an example of why in probability and statistics it is essential to distinguish between random variables upper case letters and realizations of the random variables lower case letters 
the maximum likelihood estimate of the covariance for the gaussian distribution case has in the denominator as well 
the ratio of to approaches for large so the maximum likelihood estimate approximately equals the unbiased estimate when the sample is large 
distribution of the sample mean for each random variable the sample mean is good estimator of the population mean where good estimator is defined as being efficient and unbiased 
of course the estimator will likely not be the true value of the population mean since different samples drawn from the same distribution will give different sample means and hence different estimates of the true mean 
thus the sample mean is random variable not constant and consequently has its own distribution 
for random sample of observations on the jth random variable the sample mean distribution itself has mean equal to the population mean and variance equal to where is the population variance 
the arithmetic mean of population or population mean is often denoted the sample mean the arithmetic mean of sample of values drawn from the population makes good estimator of the population mean as its expected value is equal to the population mean that is it is an unbiased estimator 
the sample mean is random variable not constant since its calculated value will randomly differ depending on which members of the population are sampled and consequently it will have its own distribution 
for random sample of independent observations the expected value of the sample mean is and the variance of the sample mean is var if the samples are not independent but correlated then special care has to be taken in order to avoid the problem of pseudoreplication 
if the population is normally distributed then the sample mean is normally distributed as follows 
if the population is not normally distributed the sample mean is nonetheless approximately normally distributed if is large and 
this is consequence of the central limit theorem 
weighted samples in weighted sample each vector each set of single observations on each of the random variables is assigned weight without loss of generality assume that the weights are normalized 
if they are not divide the weights by their sum 
then the weighted mean vector is given by and the elements of the weighted covariance matrix are 
if all weights are the same the weighted mean and covariance reduce to the biased sample mean and covariance mentioned above 
criticism the sample mean and sample covariance are not robust statistics meaning that they are sensitive to outliers 
as robustness is often desired trait particularly in real world applications robust alternatives may prove desirable notably quantile based statistics such as the sample median for location and interquartile range iqr for dispersion 
other alternatives include trimming and winsorising as in the trimmed mean and the winsorized mean 
see also estimation of covariance matrices scatter matrix unbiased estimation of standard deviation references
in the field of multivariate statistics kernel principal component analysis kernel pca is an extension of principal component analysis pca using techniques of kernel methods 
using kernel the originally linear operations of pca are performed in reproducing kernel hilbert space 
background linear pca recall that conventional pca operates on zero centered data that is where is one of the multivariate observations 
it operates by diagonalizing the covariance matrix in other words it gives an eigendecomposition of the covariance matrix which can be rewritten as for see also covariance matrix as linear operator introduction of the kernel to pca to understand the utility of kernel pca particularly for clustering observe that while points cannot in general be linearly separated in dimensions they can almost always be linearly separated in dimensions 
that is given points if we map them to an dimensional space with where it is easy to construct hyperplane that divides the points into arbitrary clusters 
of course this creates linearly independent vectors so there is no covariance on which to perform eigendecomposition explicitly as we would in linear pca 
instead in kernel pca non trivial arbitrary function is chosen that is never calculated explicitly allowing the possibility to use very high dimensional if we never have to actually evaluate the data in that space 
since we generally try to avoid working in the space which we will call the feature space we can create the by kernel which represents the inner product space see gramian matrix of the otherwise intractable feature space 
the dual form that arises in the creation of kernel allows us to mathematically formulate version of pca in which we never actually solve the eigenvectors and eigenvalues of the covariance matrix in the space see kernel trick 
the elements in each column of represent the dot product of one point of the transformed data with respect to all the transformed points points 
some well known kernels are shown in the example below 
because we are never working directly in the feature space the kernel formulation of pca is restricted in that it computes not the principal components themselves but the projections of our data onto those components 
to evaluate the projection from point in the feature space onto the kth principal component where superscript means the component not powers of we note that denotes dot product which is simply the elements of the kernel it seems all that left is to calculate and normalize the which can be done by solving the eigenvector equation where is the number of data points in the set and and are the eigenvalues and eigenvectors of then to normalize the eigenvectors we require that care must be taken regarding the fact that whether or not has zero mean in its original space it is not guaranteed to be centered in the feature space which we never compute explicitly 
since centered data is required to perform an effective principal component analysis we centralize to become where denotes by matrix for which each element takes value we use to perform the kernel pca algorithm described above 
one caveat of kernel pca should be illustrated here 
in linear pca we can use the eigenvalues to rank the eigenvectors based on how much of the variation of the data is captured by each principal component 
this is useful for data dimensionality reduction and it could also be applied to kpca 
however in practice there are cases that all variations of the data are same 
this is typically caused by wrong choice of kernel scale 
large datasets in practice large data set leads to large and storing may become problem 
one way to deal with this is to perform clustering on the dataset and populate the kernel with the means of those clusters 
since even this method may yield relatively large it is common to compute only the top eigenvalues and eigenvectors of the eigenvalues are calculated in this way 
example consider three concentric clouds of points shown we wish to use kernel pca to identify these groups 
the color of the points does not represent information involved in the algorithm but only shows how the transformation relocates the data points 
first consider the kernel applying this to kernel pca yields the next image 
now consider gaussian kernel that is this kernel is measure of closeness equal to when the points coincide and equal to at infinity 
note in particular that the first principal component is enough to distinguish the three different groups which is impossible using only linear pca because linear pca operates only in the given in this case two dimensional space in which these concentric point clouds are not linearly separable 
applications kernel pca has been demonstrated to be useful for novelty detection and image de noising 
see also cluster analysis nonlinear dimensionality reduction spectral clustering references
in music theory the circle of fifths is way of organizing the chromatic pitches as sequence of perfect fifths 
this is strictly true in the standard tone equal temperament system using different system requires one interval of diminished sixth to be treated as fifth 
if is chosen as starting point the sequence is continuing the pattern from returns the sequence to its starting point of this order places the most closely related key signatures adjacent to one another 
it is usually illustrated in the form of circle 
definition the circle of fifths organizes pitches in sequence of perfect fifths generally shown as circle with the pitches and their corresponding keys in clockwise progression 
musicians and composers often use the circle of fifths to describe the musical relationships between pitches 
its design is helpful in composing and harmonizing melodies building chords and modulating to different keys within composition using the system of just intonation perfect fifth consists of two pitches with frequency ratio of but generating twelve successive perfect fifths in this way does not result in return to the pitch class of the starting note 
to adjust for this instruments are generally tuned with the equal temperament system 
twelve equal temperament fifths lead to note exactly seven octaves above the initial tone this results in perfect fifth that is equivalent to seven equal temperament semitones 
the top of the circle shows the key of major with no sharps or flats 
proceeding clockwise the pitches ascend by fifths 
the key signatures associated with those pitches also change the key of has one sharp the key of has sharps and so on 
similarly proceeding counterclockwise from the top of the circle the notes change by descending fifths and the key signatures change accordingly the key of has one flat the key of has flats and so on 
some keys at the bottom of the circle can be notated either in sharps or in flats 
starting at any pitch and ascending by fifth generates all twelve tones before returning to the beginning pitch class pitch class consists of all of the notes indicated by given letter regardless of octave all for example belong to the same pitch class 
moving counterclockwise the pitches descend by fifth but ascending by perfect fourth will lead to the same note an octave higher therefore in the same pitch class 
moving counter clockwise from could be thought of as descending by fifth to or ascending by fourth to structure and use diatonic key signatures each of the twelve pitches can serve as the tonic of major or minor key and each of these keys will have diatonic scale associated with it 
the circle diagram shows the number of sharps or flats in each key signature with the major key indicated by capital letter and the minor key indicated by lower case letter 
major and minor keys that have the same key signature are referred to as relative major and relative minor of one another 
modulation and chord progression tonal music often modulates to new tonal center whose key signature differs from the original by only one flat or sharp 
these closely related keys are fifth apart from each other and are therefore adjacent in the circle of fifths 
chord progressions also often move between chords whose roots are related by perfect fifth making the circle of fifths useful in illustrating the harmonic distance between chords 
the circle of fifths is used to organize and describe the harmonic function of chords 
chords can progress in pattern of ascending perfect fourths alternately viewed as descending perfect fifths in functional succession 
this can be shown by the circle of fifths in which therefore scale degree ii is closer to the dominant than scale degree iv 
in this view the tonic is considered the end point of chord progression derived from the circle of fifths 
according to richard franko goldman harmony in western music the iv chord is in the simplest mechanisms of diatonic relationships at the greatest distance from in terms of the descending circle of fifths it leads away from rather than toward it 
he states that the progression ii an authentic cadence would feel more final or resolved than iv plagal cadence 
goldman concurs with nattiez who argues that the chord on the fourth degree appears long before the chord on ii and the subsequent final in the progression iv viio iii vi ii and is farther from the tonic there as well 
in this and related articles upper case roman numerals indicate major triads while lower case roman numerals indicate minor triads 
circle closure in non equal tuning systems using the exact ratio of frequencies to define perfect fifth just intonation does not quite result in return to the pitch class of the starting note after going around the circle of fifths 
equal temperament tuning produces fifths that return to tone exactly seven octaves above the initial tone and makes the frequency ratio of each half step the same 
an equal tempered fifth has frequency ratio of or about approximately two cents narrower than justly tuned fifth at ratio of ascending by justly tuned fifths fails to close the circle by an excess of approximately cents roughly quarter of semitone an interval known as the pythagorean comma 
in pythagorean tuning this problem is solved by markedly shortening the width of one of the twelve fifths which makes it severely dissonant 
this anomalous fifth is called the wolf fifth humorous reference to wolf howling an off pitch note 
the quarter comma meantone tuning system uses eleven fifths slightly narrower than the equally tempered fifth and requires much wider and even more dissonant wolf fifth to close the circle 
more complex tuning systems based on just intonation such as limit tuning use at most eight justly tuned fifths and at least three non just fifths some slightly narrower and some slightly wider than the just fifth to close the circle 
other tuning systems use up to tones the original tones and more between them in order to close the circle of fifths 
history the circle of fifths developed in the late and early to theorize the modulation of the baroque era see baroque era 
the first circle of fifths diagram appears in the grammatika of the composer and theorist nikolay diletsky who intended to present music theory as tool for composition 
it was the first of its kind aimed at teaching russian audience how to write western style polyphonic compositions 
circle of fifths diagram was independently created by german composer and theorist johann david heinichen in his neu erfundene und gr ndliche anweisung which he called the musical circle german musicalischer circul 
this was also published in his der general bass in der composition 
heinichen placed the relative minor key next to the major key which did not reflect the actual proximity of keys 
johann mattheson and others attempted to improve this david kellner proposed having the major keys on one circle and the relative minor keys on second inner circle 
this was later developed into chordal space incorporating the parallel minor as well some sources imply that the circle of fifths was known in antiquity by pythagoras 
this is misunderstanding and an anachronism 
tuning by fifths so called pythagorean tuning dates to ancient mesopotamia see music of mesopotamia music theory though they did not extend this to twelve note scale stopping at seven 
the pythagorean comma was calculated by euclid and by chinese mathematicians in the huainanzi see pythagorean comma history 
thus it was known in antiquity that cycle of twelve fifths was almost exactly seven octaves more practically alternating ascending fifths and descending fourths was almost exactly an octave 
however this was theoretical knowledge and was not used to construct repeating twelve tone scale nor to modulate 
this was done later in meantone temperament and twelve tone equal temperament which allowed modulation while still being in tune but did not develop in europe until about 
use in musical pieces from the baroque music era and the classical era of music and in western popular music traditional music and folk music when pieces or songs modulate to new key these modulations are often associated with the circle of fifths 
in practice compositions rarely make use of the entire circle of fifths 
more commonly composers make use of the compositional idea of the cycle of ths when music moves consistently through smaller or larger segment of the tonal structural resources which the circle abstractly represents 
the usual practice is to derive the circle of fifths progression from the seven tones of the diatonic scale rather from the full range of twelve tones present in the chromatic scale 
in this diatonic version of the circle one of the fifths is not true fifth it is tritone or diminished fifth 
between and in the natural diatonic scale 
without sharps or flats 
here is how the circle of fifths derives through permutation from the diatonic major scale and from the natural minor scale the following is the basic sequence of chords that can be built over the major bass line and over the minor adding sevenths to the chords creates greater sense of forward momentum to the harmony baroque era according to richard taruskin arcangelo corelli was the most influential composer to establish the pattern as standard harmonic trope it was precisely in corelli time the late seventeenth century that the circle of fifths was being theorized as the main propellor of harmonic motion and it was corelli more than any one composer who put that new idea into telling practice 
the circle of fifths progression occurs frequently in the music of bach 
in the following from jauchzet gott in allen landen bwv even when the solo bass line implies rather than states the chords involved handel uses circle of fifths progression as the basis for the passacaglia movement from his harpsichord suite no 
baroque composers learnt to enhance the propulsive force of the harmony engendered by the circle of fifths by adding sevenths to most of the constituent chords 
these sevenths being dissonances create the need for resolution thus turning each progression of the circle into simultaneous reliever and re stimulator of harmonic tension hence harnessed for expressive purposes 
striking passages that illustrate the use of sevenths occur in the aria pena tiranna in handel opera amadigi di gaula and in bach keyboard arrangement of alessandro marcello concerto for oboe and strings 
nineteenth century during the nineteenth century composers made use of the circle of fifths to enhance the expressive character of their music 
franz schubert poignant impromptu in flat major contains such passage as does the intermezzo movement from mendelssohn string quartet no robert schumann evocative child falling asleep from his kinderszenen springs surprise at the end of the progression the piece ends on an minor chord instead of the expected tonic minor 
in wagner opera tterd mmerung cycle of fifths progression occurs in the music which transitions from the end of the prologue into the first scene of act set in the imposing hall of the wealthy gibichungs 
status and reputation are written all over the motifs assigned to gunther chief of the gibichung clan jazz and popular music the enduring popularity of the circle of fifths as both form building device and as an expressive musical trope is evident in the number of standard popular songs composed during the twentieth century 
it is also favored as vehicle for improvisation by jazz musicians 
bart howard fly me to the moon the song opens with pattern of descending phrases in essence the hook of the song presented with soothing predictability almost as if the future direction of the melody is dictated by the opening five notes 
the harmonic progression for its part rarely departs from the circle of fifths 
jerome kern all the things you are ray noble cherokee 
many jazz musicians have found this particularly challenging as the middle eight progresses so rapidly through the circle creating series of ii progressions that temporarily pass through several tonalities 
kosmo prevert and mercer autumn leaves the beatles you never give me your money mike oldfield incantations carlos santana europa earth cry heaven smile gloria gaynor will survive pet shop boys it sin donna summer love to love you baby related concepts diatonic circle of fifths the diatonic circle of fifths is the circle of fifths encompassing only members of the diatonic scale 
therefore it contains diminished fifth in major between and see structure implies multiplicity 
the circle progression is commonly circle of fifths through the diatonic chords including one diminished chord 
circle progression in major with chords iv viio iii vi ii is shown below 
chromatic circle the circle of fifths is closely related to the chromatic circle which also arranges the twelve equal tempered pitch classes in circular ordering 
key difference between the two circles is that the chromatic circle can be understood as continuous space where every point on the circle corresponds to conceivable pitch class and every conceivable pitch class corresponds to point on the circle 
by contrast the circle of fifths is fundamentally discrete structure and there is no obvious way to assign pitch classes to each of its points 
in this sense the two circles are mathematically quite different 
however the twelve equal tempered pitch classes can be represented by the cyclic group of order twelve or equivalently the residue classes modulo twelve the group has four generators which can be identified with the ascending and descending semitones and the ascending and descending perfect fifths 
the semitonal generator gives rise to the chromatic circle while the perfect fifth gives rise to the circle of fifths 
relation with chromatic scale the circle of fifths or fourths may be mapped from the chromatic scale by multiplication and vice versa 
to map between the circle of fifths and the chromatic scale in integer notation multiply by and for the circle of fourths multiply by 
here is demonstration of this procedure 
start off with an ordered tuple tone row of integers representing the notes of the chromatic scale 
now multiply the entire tuple by and then apply modulo reduction to each of the numbers subtract from each number as many times as necessary until the number becomes smaller than which is equivalent to which is the circle of fifths 
note that this is enharmonically equivalent to 
enharmonic equivalents theoretical keys and the spiral of fifths equal temperament tuning does not use the exact ratio of frequencies that defines perfect fifth wheras the system of just intonation uses this exact ratio 
ascending by fifths in equal temperament leads to return to the starting pitch class starting with and ascending by fifths leads to another after twelve iterations 
this does not occur if an exact ratio is used just intonation 
the adjustment made in equal temperament tuning is called the pythagorean comma 
because of this difference pitches that are enharmonically equivalent in equal temperament tuning and are not equivalent when using just intonation 
in just intonation the sequence of fifths can therefore be visualized as spiral not circle sequence of twelve fifths results in comma pump by the pythagorean comma visualized as going up level in the spiral 
see also circle closure in non equal tuning systems 
without enharmonic equivalence continuing sequence of fifths results in notes with double accidentals double sharps or double flats 
when using equal temperament these can be replaced by an enharmonically equivalent note 
keys with double sharps or flats in the key signatures are called theoretical keys their use is extremely rare 
notation in these cases is not standardized 
the default behaviour of lilypond pictured above writes single sharps or flats in the circle of fifths order before proceeding to double sharps or flats 
this is the format used in john foulds world requiem op 
which ends with the key signature of major as displayed above 
the sharps in the key signature of major here proceed single sharps or flats in the key signature are sometimes repeated as courtesy 
max reger supplement to the theory of modulation which contains minor key signatures on pp 
these have at the start and also at the end with double flat symbol going the convention of lilypond and foulds would suppress the initial 
sometimes the double signs are written at the beginning of the key signature followed by the single signs 
for example the key signature is notated as 
this convention is used by victor ewald by the program finale software and by some theoretical works 
see also approach chord sonata form well temperament circle of fifths text table pitch constellation multiplicative group of integers modulo notes references barnett gregory 
tonal organization in seventeenth century music theory 
in thomas christensen ed 
the cambridge history of western music theory 
cambridge cambridge university press 
the jazz standards guide to the repertoire 
isbn goldman richard franko 
harmony in western music 
theoretical work of late seventeenth century muscovy nikolai diletskii grammatika and the earliest circle of fifths 
journal of the american musicological society 
between modes and keys german theory 
prelude to musical geometry 
the college mathematics journal 
jstor archived from the original on retrieved nattiez jean jacques 
music and discourse toward semiology of music translated by carolyn abbate 
princeton new jersey princeton university press 
originally published in french as musicologie rale et miologie 
the oxford history of western music music in the seventeenth and eighteenth centuries 
further reading indy vincent 
cours de composition musicale 
paris durand et fils 
between modes and keys german theory 
the complete idiot guide to music theory nd ed 
indianapolis in alpha isbn purwins hendrik 
profiles of pitch classes circularity of relative pitch and key experiments models computational music analysis and perspectives 
berlin technische universit berlin 
purwins hendrik benjamin blankertz and klaus obermayer 
toroidal models in tonal theory and pitch class analysis 
in computing in musicology tonal theory for the digital age 
external links decoding the circle of vths interactive circle of fifths interactive circle of fifths for guitarists
in statistical modeling regression analysis is set of statistical processes for estimating the relationships between dependent variable often called the outcome or response variable or label in machine learning parlance and one or more independent variables often called predictors covariates explanatory variables or features 
the most common form of regression analysis is linear regression in which one finds the line or more complex linear combination that most closely fits the data according to specific mathematical criterion 
for example the method of ordinary least squares computes the unique line or hyperplane that minimizes the sum of squared differences between the true data and that line or hyperplane 
for specific mathematical reasons see linear regression this allows the researcher to estimate the conditional expectation or population average value of the dependent variable when the independent variables take on given set of values 
less common forms of regression use slightly different procedures to estimate alternative location parameters quantile regression or necessary condition analysis or estimate the conditional expectation across broader collection of non linear models nonparametric regression 
regression analysis is primarily used for two conceptually distinct purposes 
first regression analysis is widely used for prediction and forecasting where its use has substantial overlap with the field of machine learning 
second in some situations regression analysis can be used to infer causal relationships between the independent and dependent variables 
importantly regressions by themselves only reveal relationships between dependent variable and collection of independent variables in fixed dataset 
to use regressions for prediction or to infer causal relationships respectively researcher must carefully justify why existing relationships have predictive power for new context or why relationship between two variables has causal interpretation 
the latter is especially important when researchers hope to estimate causal relationships using observational data 
history the earliest form of regression was the method of least squares which was published by legendre in and by gauss in legendre and gauss both applied the method to the problem of determining from astronomical observations the orbits of bodies about the sun mostly comets but also later the then newly discovered minor planets 
gauss published further development of the theory of least squares in including version of the gauss markov theorem 
the term regression was coined by francis galton in the th century to describe biological phenomenon 
the phenomenon was that the heights of descendants of tall ancestors tend to regress down towards normal average phenomenon also known as regression toward the mean 
for galton regression had only this biological meaning but his work was later extended by udny yule and karl pearson to more general statistical context 
in the work of yule and pearson the joint distribution of the response and explanatory variables is assumed to be gaussian 
this assumption was weakened by fisher in his works of and fisher assumed that the conditional distribution of the response variable is gaussian but the joint distribution need not be 
in this respect fisher assumption is closer to gauss formulation of in the and economists used electromechanical desk calculators to calculate regressions 
before it sometimes took up to hours to receive the result from one regression regression methods continue to be an area of active research 
in recent decades new methods have been developed for robust regression regression involving correlated responses such as time series and growth curves regression in which the predictor independent variable or response variables are curves images graphs or other complex data objects regression methods accommodating various types of missing data nonparametric regression bayesian methods for regression regression in which the predictor variables are measured with error regression with more predictor variables than observations and causal inference with regression 
regression model in practice researchers first select model they would like to estimate and then use their chosen method ordinary least squares to estimate the parameters of that model 
regression models involve the following components the unknown parameters often denoted as scalar or vector the independent variables which are observed in data and are often denoted as vector where denotes row of data 
the dependent variable which are observed in data and often denoted using the scalar the error terms which are not directly observed in data and are often denoted using the scalar in various fields of application different terminologies are used in place of dependent and independent variables 
most regression models propose that is function of and with representing an additive error term that may stand in for un modeled determinants of or random statistical noise the researchers goal is to estimate the function that most closely fits the data 
to carry out regression analysis the form of the function must be specified 
sometimes the form of this function is based on knowledge about the relationship between and that does not rely on the data 
if no such knowledge is available flexible or convenient form for is chosen 
for example simple univariate regression may propose suggesting that the researcher believes to be reasonable approximation for the statistical process generating the data 
once researchers determine their preferred statistical model different forms of regression analysis provide tools to estimate the parameters for example least squares including its most common variant ordinary least squares finds the value of that minimizes the sum of squared errors given regression method will ultimately provide an estimate of usually denoted to distinguish the estimate from the true unknown parameter value that generated the data 
using this estimate the researcher can then use the fitted value for prediction or to assess the accuracy of the model in explaining the data 
whether the researcher is intrinsically interested in the estimate or the predicted value will depend on context and their goals 
as described in ordinary least squares least squares is widely used because the estimated function approximates the conditional expectation 
however alternative variants least absolute deviations or quantile regression are useful when researchers want to model other functions 
it is important to note that there must be sufficient data to estimate regression model 
for example suppose that researcher has access to rows of data with one dependent and two independent variables 
suppose further that the researcher wants to estimate bivariate linear model via least squares if the researcher only has access to data points then they could find infinitely many combinations that explain the data equally well any combination can be chosen that satisfies all of which lead to and are therefore valid solutions that minimize the sum of squared residuals 
to understand why there are infinitely many options note that the system of equations is to be solved for unknowns which makes the system underdetermined 
alternatively one can visualize infinitely many dimensional planes that go through fixed points 
more generally to estimate least squares model with distinct parameters one must have distinct data points 
if then there does not generally exist set of parameters that will perfectly fit the data 
the quantity appears often in regression analysis and is referred to as the degrees of freedom in the model 
moreover to estimate least squares model the independent variables 
must be linearly independent one must not be able to reconstruct any of the independent variables by adding and multiplying the remaining independent variables 
as discussed in ordinary least squares this condition ensures that is an invertible matrix and therefore that unique solution exists 
underlying assumptions by itself regression is simply calculation using the data 
in order to interpret the output of regression as meaningful statistical quantity that measures real world relationships researchers often rely on number of classical assumptions 
these assumptions often include the sample is representative of the population at large 
the independent variables are measured with no error 
deviations from the model have an expected value of zero conditional on covariates the variance of the residuals is constant across observations homoscedasticity 
the residuals are uncorrelated with one another 
mathematically the variance covariance matrix of the errors is diagonal handful of conditions are sufficient for the least squares estimator to possess desirable properties in particular the gauss markov assumptions imply that the parameter estimates will be unbiased consistent and efficient in the class of linear unbiased estimators 
practitioners have developed variety of methods to maintain some or all of these desirable properties in real world settings because these classical assumptions are unlikely to hold exactly 
for example modeling errors in variables can lead to reasonable estimates independent variables are measured with errors 
heteroscedasticity consistent standard errors allow the variance of to change across values of correlated errors that exist within subsets of the data or follow specific patterns can be handled using clustered standard errors geographic weighted regression or newey west standard errors among other techniques 
when rows of data correspond to locations in space the choice of how to model within geographic units can have important consequences 
the subfield of econometrics is largely focused on developing techniques that allow researchers to make reasonable real world conclusions in real world settings where classical assumptions do not hold exactly 
linear regression in linear regression the model specification is that the dependent variable is linear combination of the parameters but need not be linear in the independent variables 
for example in simple linear regression for modeling data points there is one independent variable and two parameters and straight line in multiple linear regression there are several independent variables or functions of independent variables 
adding term in to the preceding regression gives parabola this is still linear regression although the expression on the right hand side is quadratic in the independent variable it is linear in the parameters and in both cases is an error term and the subscript indexes particular observation 
returning our attention to the straight line case given random sample from the population we estimate the population parameters and obtain the sample linear regression model the residual is the difference between the value of the dependent variable predicted by the model and the true value of the dependent variable one method of estimation is ordinary least squares 
this method obtains parameter estimates that minimize the sum of squared residuals ssr minimization of this function results in set of normal equations set of simultaneous linear equations in the parameters which are solved to yield the parameter estimators in the case of simple regression the formulas for the least squares estimates are where is the mean average of the values and is the mean of the values 
under the assumption that the population error term has constant variance the estimate of that variance is given by this is called the mean square error mse of the regression 
the denominator is the sample size reduced by the number of model parameters estimated from the same data for regressors or if an intercept is used 
in this case so the denominator is the standard errors of the parameter estimates are given by under the further assumption that the population error term is normally distributed the researcher can use these estimated standard errors to create confidence intervals and conduct hypothesis tests about the population parameters 
general linear model in the more general multiple regression model there are independent variables where is the th observation on the th independent variable 
if the first independent variable takes the value for all then is called the regression intercept 
the least squares parameter estimates are obtained from normal equations 
the residual can be written as the normal equations are in matrix notation the normal equations are written as where the element of is the element of the column vector is and the element of is thus is is and is the solution is 
diagnostics once regression model has been constructed it may be important to confirm the goodness of fit of the model and the statistical significance of the estimated parameters 
commonly used checks of goodness of fit include the squared analyses of the pattern of residuals and hypothesis testing 
statistical significance can be checked by an test of the overall fit followed by tests of individual parameters 
interpretations of these diagnostic tests rest heavily on the model assumptions 
although examination of the residuals can be used to invalidate model the results of test or test are sometimes more difficult to interpret if the model assumptions are violated 
for example if the error term does not have normal distribution in small samples the estimated parameters will not follow normal distributions and complicate inference 
with relatively large samples however central limit theorem can be invoked such that hypothesis testing may proceed using asymptotic approximations 
limited dependent variables limited dependent variables which are response variables that are categorical variables or are variables constrained to fall only in certain range often arise in econometrics 
the response variable may be non continuous limited to lie on some subset of the real line 
for binary zero or one variables if analysis proceeds with least squares linear regression the model is called the linear probability model 
nonlinear models for binary dependent variables include the probit and logit model 
the multivariate probit model is standard method of estimating joint relationship between several binary dependent variables and some independent variables 
for categorical variables with more than two values there is the multinomial logit 
for ordinal variables with more than two values there are the ordered logit and ordered probit models 
censored regression models may be used when the dependent variable is only sometimes observed and heckman correction type models may be used when the sample is not randomly selected from the population of interest 
an alternative to such procedures is linear regression based on polychoric correlation or polyserial correlations between the categorical variables 
such procedures differ in the assumptions made about the distribution of the variables in the population 
if the variable is positive with low values and represents the repetition of the occurrence of an event then count models like the poisson regression or the negative binomial model may be used 
nonlinear regression when the model function is not linear in the parameters the sum of squares must be minimized by an iterative procedure 
this introduces many complications which are summarized in differences between linear and non linear least squares 
interpolation and extrapolation regression models predict value of the variable given known values of the variables 
prediction within the range of values in the dataset used for model fitting is known informally as interpolation 
prediction outside this range of the data is known as extrapolation 
performing extrapolation relies strongly on the regression assumptions 
the further the extrapolation goes outside the data the more room there is for the model to fail due to differences between the assumptions and the sample data or the true values 
it is generally advised that when performing extrapolation one should accompany the estimated value of the dependent variable with prediction interval that represents the uncertainty 
such intervals tend to expand rapidly as the values of the independent variable moved outside the range covered by the observed data 
for such reasons and others some tend to say that it might be unwise to undertake extrapolation however this does not cover the full set of modeling errors that may be made in particular the assumption of particular form for the relation between and properly conducted regression analysis will include an assessment of how well the assumed form is matched by the observed data but it can only do so within the range of values of the independent variables actually available 
this means that any extrapolation is particularly reliant on the assumptions being made about the structural form of the regression relationship 
best practice advice here is that linear in variables and linear in parameters relationship should not be chosen simply for computational convenience but that all available knowledge should be deployed in constructing regression model 
if this knowledge includes the fact that the dependent variable cannot go outside certain range of values this can be made use of in selecting the model even if the observed dataset has no values particularly near such bounds 
the implications of this step of choosing an appropriate functional form for the regression can be great when extrapolation is considered 
at minimum it can ensure that any extrapolation arising from fitted model is realistic or in accord with what is known 
power and sample size calculations there are no generally agreed methods for relating the number of observations versus the number of independent variables in the model 
one method conjectured by good and hardin is where is the sample size is the number of independent variables and is the number of observations needed to reach the desired precision if the model had only one independent variable 
for example researcher is building linear regression model using dataset that contains patients 
if the researcher decides that five observations are needed to precisely define straight line then the maximum number of independent variables the model can support is because log log 
other methods although the parameters of regression model are usually estimated using the method of least squares other methods which have been used include bayesian methods 
bayesian linear regression percentage regression for situations where reducing percentage errors is deemed more appropriate 
least absolute deviations which is more robust in the presence of outliers leading to quantile regression nonparametric regression requires large number of observations and is computationally intensive scenario optimization leading to interval predictor models distance metric learning which is learned by the search of meaningful distance metric in given input space 
software all major statistical software packages perform least squares regression analysis and inference 
simple linear regression and multiple regression using least squares can be done in some spreadsheet applications and on some calculators 
while many statistical software packages can perform various types of nonparametric and robust regression these methods are less standardized 
different software packages implement different methods and method with given name may be implemented differently in different packages 
specialized regression software has been developed for use in fields such as survey analysis and neuroimaging 
see also references further reading william kruskal and judith tanur ed 
linear hypotheses international encyclopedia of statistics 
free press evan williams regression pp 
analysis of variance pp 
regression and correlation analysis new palgrave dictionary of economics pp 
birkes david and dodge alternative methods of regression 
isbn chatfield calculating interval forecasts journal of business and economic statistics pp 
applied regression analysis rd ed 
applied regression analysis linear models and related methods 
sage hardle applied nonparametric regression isbn meade nigel islam towhidul 
prediction intervals for growth curve forecasts 
sen srivastava regression analysis theory methods and applications springer verlag berlin th printing 
strutz data fitting and uncertainty practical introduction to weighted least squares and beyond 
vieweg teubner isbn stulp freek and olivier sigaud 
many regression algorithms one unified model review 
https doi org neunet 
operations and production systems with multiple objectives 
external links regression analysis encyclopedia of mathematics ems press earliest uses regression basic history and references what is multiple regression used for 
multiple regression regression of weakly correlated data how linear regression mistakes can appear when range is much smaller than range
in cryptography the elliptic curve digital signature algorithm ecdsa offers variant of the digital signature algorithm dsa which uses elliptic curve cryptography
key and signature size as with elliptic curve cryptography in general the bit size of the private key believed to be needed for ecdsa is about twice the size of the security level in bits
for example at security level of bits meaning an attacker requires maximum of about operations to find the private key the size of an ecdsa private key would be bits
on the other hand the signature size is the same for both dsa and ecdsa approximately bits where is the security level measured in bits that is about bits for security level of bits
signature generation algorithm suppose alice wants to send signed message to bob
initially they must agree on the curve parameters curve
in addition to the field and equation of the curve we need base point of prime order on the curve is the multiplicative order of the point the order of the base point must be prime
indeed we assume that every nonzero element of the ring is invertible so that must be field
it implies that must be prime cf
alice creates key pair consisting of private key integer randomly selected in the interval and public key curve point we use to denote elliptic curve point multiplication by scalar
for alice to sign message she follows these steps calculate hash
here hash is cryptographic hash function such as sha with the output converted to an integer
let be the leftmost bits of where is the bit length of the group order note that can be greater than but not longer
select cryptographically secure random integer from
calculate the curve point calculate mod if go back to step calculate mod if go back to step the signature is the pair
and mod is also valid signature
as the standard notes it is not only required for to be secret but it is also crucial to select different for different signatures otherwise the equation in step can be solved for the private key given two signatures and employing the same unknown for different known messages and an attacker can calculate and and since all operations in this paragraph are done modulo the attacker can find
since the attacker can now calculate the private key this implementation failure was used for example to extract the signing key used for the playstation gaming console another way ecdsa signature may leak private keys is when is generated by faulty random number generator
such failure in random number generation caused users of android bitcoin wallet to lose their funds in august to ensure that is unique for each message one may bypass random number generation completely and generate deterministic signatures by deriving from both the message and the private key
signature verification algorithm for bob to authenticate alice signature he must have copy of her public key curve point bob can verify is valid curve point as follows check that is not equal to the identity element and its coordinates are otherwise valid check that lies on the curve check that after that bob follows these steps verify that and are integers in
if not the signature is invalid
calculate hash where hash is the same function used in the signature generation
let be the leftmost bits of calculate mod and mod calculate the curve point if then the signature is invalid
the signature is valid if mod invalid otherwise note that an efficient implementation would compute inverse mod only once
also using shamir trick sum of two scalar multiplications can be calculated faster than two scalar multiplications done independently
correctness of the algorithm it is not immediately obvious why verification even functions correctly
to see why denote as the curve point computed in step of verification from the definition of the public key as because elliptic curve scalar multiplication distributes over addition expanding the definition of and from verification step collecting the common term expanding the definition of from signature step since the inverse of an inverse is the original element and the product of an element inverse and the element is the identity we are left with from the definition of this is verification step this shows only that correctly signed message will verify correctly many other properties are required for secure signature algorithm
public key recovery given message and alice signature on that message bob can potentially recover alice public key verify that and are integers in
if not the signature is invalid
calculate curve point where is one of etc
provided is not too large for field element and is value such that the curve equation is satisfied
note that there may be several curve points satisfying these conditions and each different value results in distinct recovered key
calculate hash where hash is the same function used in the signature generation
let be the leftmost bits of calculate mod and mod calculate the curve point the signature is valid if matches alice public key
the signature is invalid if all the possible points have been tried and none match alice public key note that an invalid signature or signature from different message will result in the recovery of an incorrect public key
the recovery algorithm can only be used to check validity of signature if the signer public key or its hash is known beforehand
correctness of the recovery algorithm start with the definition of from recovery step from the definition from signing step because elliptic curve scalar multiplication distributes over addition expanding the definition of and from recovery step expanding the definition of from signature step since the product of an element inverse and the element is the identity we are left with the first and second terms cancel each other out from the definition of this is alice public key
this shows that correctly signed message will recover the correct public key provided additional information was shared to uniquely calculate curve point from signature value security in december group calling itself fail verflow announced recovery of the ecdsa private key used by sony to sign software for the playstation game console
however this attack only worked because sony did not properly implement the algorithm because was static instead of random
as pointed out in the signature generation algorithm section above this makes solvable rendering the entire algorithm useless on march two researchers published an iacr paper demonstrating that it is possible to retrieve tls private key of server using openssl that authenticates with elliptic curves dsa over binary field via timing attack
the vulnerability was fixed in openssl in august it was revealed that bugs in some implementations of the java class securerandom sometimes generated collisions in the value
this allowed hackers to recover private keys giving them the same control over bitcoin transactions as legitimate keys owners had using the same exploit that was used to reveal the ps signing key on some android app implementations which use java and rely on ecdsa to authenticate transactions this issue can be prevented by an unpredictable generation of deterministic procedure as described by rfc
concerns some concerns expressed about ecdsa political concerns the trustworthiness of nist produced curves being questioned after revelations that the nsa willingly inserts backdoors into software hardware components and published standards were made well known cryptographers have expressed doubts about how the nist curves were designed and voluntary tainting has already been proved in the past
see also the libssh curve introduction
nevertheless proof that the named nist curves exploit rare weakness is missing yet
technical concerns the difficulty of properly implementing the standard its slowness and design flaws which reduce security in insufficiently defensive implementations
implementations below is list of cryptographic libraries that provide support for ecdsa botan bouncy castle cryptlib crypto crypto api linux gnutls libgcrypt libressl mbed tls microsoft cryptoapi openssl wolfcrypt see also eddsa rsa cryptosystem references further reading accredited standards committee asc issues new standard for public key cryptography ecdsa oct source accredited standards committee american national standard public key cryptography for the financial services industry the elliptic curve digital signature algorithm ecdsa november certicom research standards for efficient cryptography sec elliptic curve cryptography version may pez and dahab an overview of elliptic curve cryptography technical report ic state university of campinas daniel bernstein pippenger exponentiation algorithm daniel brown generic groups collision resistance and ecdsa designs codes and cryptography eprint version ian blake gadiel seroussi and nigel smart editors advances in elliptic curve cryptography london mathematical society lecture note series cambridge university press hankerson vanstone menezes
guide to elliptic curve cryptography
external links digital signature standard includes info on ecdsa the elliptic curve digital signature algorithm ecdsa provides an in depth guide on ecdsa
linear combination of atomic orbitals or lcao is quantum superposition of atomic orbitals and technique for calculating molecular orbitals in quantum chemistry
in quantum mechanics electron configurations of atoms are described as wavefunctions
in mathematical sense these wave functions are the basis set of functions the basis functions which describe the electrons of given atom
in chemical reactions orbital wavefunctions are modified
the electron cloud shape is changed according to the type of atoms participating in the chemical bond
it was introduced in by sir john lennard jones with the description of bonding in the diatomic molecules of the first main row of the periodic table but had been used earlier by linus pauling for
mathematical description an initial assumption is that the number of molecular orbitals is equal to the number of atomic orbitals included in the linear expansion
in sense atomic orbitals combine to form molecular orbitals which can be numbered to and which may not all be the same
the expression linear expansion for the th molecular orbital would be or where is molecular orbital represented as the sum of atomic orbitals each multiplied by corresponding coefficient and numbered to represents which atomic orbital is combined in the term
the coefficients are the weights of the contributions of the atomic orbitals to the molecular orbital
the hartree fock method is used to obtain the coefficients of the expansion
the orbitals are thus expressed as linear combinations of basis functions and the basis functions are single electron functions which may or may not be centered on the nuclei of the component atoms of the molecule
in either case the basis functions are usually also referred to as atomic orbitals even though only in the former case this name seems to be adequate
the atomic orbitals used are typically those of hydrogen like atoms since these are known analytically
slater type orbitals but other choices are possible such as the gaussian functions from standard basis sets or the pseudo atomic orbitals from plane wave pseudopotentials
by minimizing the total energy of the system an appropriate set of coefficients of the linear combinations is determined
this quantitative approach is now known as the hartree fock method
however since the development of computational chemistry the lcao method often refers not to an actual optimization of the wave function but to qualitative discussion which is very useful for predicting and rationalizing results obtained via more modern methods
in this case the shape of the molecular orbitals and their respective energies are deduced approximately from comparing the energies of the atomic orbitals of the individual atoms or molecular fragments and applying some recipes known as level repulsion and the like
the graphs that are plotted to make this discussion clearer are called correlation diagrams
the required atomic orbital energies can come from calculations or directly from experiment via koopmans theorem
this is done by using the symmetry of the molecules and orbitals involved in bonding and thus is sometimes called symmetry adapted linear combination salc
the first step in this process is assigning point group to the molecule
each operation in the point group is performed upon the molecule
the number of bonds that are unmoved is the character of that operation
this reducible representation is decomposed into the sum of irreducible representations
these irreducible representations correspond to the symmetry of the orbitals involved
molecular orbital diagrams provide simple qualitative lcao treatment
the ckel method the extended ckel method and the pariser parr pople method provide some quantitative theories
see also quantum chemistry computer programs hartree fock method basis set chemistry tight binding holstein herring method external links lcao chemistry umeche maine edu link references
weighted least squares wls also known as weighted linear regression is generalization of ordinary least squares and linear regression in which knowledge of the variance of observations is incorporated into the regression 
wls is also specialization of generalized least squares 
introduction special case of generalized least squares called weighted least squares can be used when all the off diagonal entries of the covariance matrix of the residuals are null the variances of the observations along the covariance matrix diagonal may still be unequal heteroscedasticity 
the fit of model to data point is measured by its residual defined as the difference between measured value of the dependent variable and the value predicted by the model 
if the errors are uncorrelated and have equal variance then the function is minimised at such that the gauss markov theorem shows that when this is so is best linear unbiased estimator blue 
if however the measurements are uncorrelated but have different uncertainties modified approach might be adopted 
aitken showed that when weighted sum of squared residuals is minimized is the blue if each weight is equal to the reciprocal of the variance of the measurement the gradient equations for this sum of squares are which in linear least squares system give the modified normal equations when the observational errors are uncorrelated and the weight matrix is diagonal these may be written as if the errors are correlated the resulting estimator is the blue if the weight matrix is equal to the inverse of the variance covariance matrix of the observations 
when the errors are uncorrelated it is convenient to simplify the calculations to factor the weight matrix as the normal equations can then be written in the same form as ordinary least squares where we define the following scaled matrix and vector diag diag this is type of whitening transformation the last expression involves an entrywise division 
for non linear least squares systems similar argument shows that the normal equations should be modified as follows 
note that for empirical tests the appropriate is not known for sure and must be estimated 
for this feasible generalized least squares fgls techniques may be used in this case it is specialized for diagonal covariance matrix thus yielding feasible weighted least squares solution 
if the uncertainty of the observations is not known from external sources then the weights could be estimated from the given observations 
this can be useful for example to identify outliers 
after the outliers have been removed from the data set the weights should be reset to one 
motivation in some cases the observations may be weighted for example they may not be equally reliable 
in this case one can minimize the weighted sum of squares where wi is the weight of the ith observation and is the diagonal matrix of such weights 
the weights should ideally be equal to the reciprocal of the variance of the measurement 
this implies that the observations are uncorrelated 
if the observations are correlated the expression applies 
in this case the weight matrix should ideally be equal to the inverse of the variance covariance matrix of the observations 
the normal equations are then this method is used in iteratively reweighted least squares 
parameter errors and correlation the estimated parameter values are linear combinations of the observed values therefore an expression for the estimated variance covariance matrix of the parameter estimates can be obtained by error propagation from the errors in the observations 
let the variance covariance matrix for the observations be denoted by and that of the estimated parameters by 
then when this simplifies to when unit weights are used the identity matrix it is implied that the experimental errors are uncorrelated and all equal where is the priori variance of an observation 
in any case is approximated by the reduced chi squared where is the minimum value of the weighted objective function the denominator is the number of degrees of freedom see effective degrees of freedom for generalizations for the case of correlated observations 
in all cases the variance of the parameter estimate is given by and the covariance between the parameter estimates and is given by the standard deviation is the square root of variance and the correlation coefficient is given by 
these error estimates reflect only random errors in the measurements 
the true uncertainty in the parameters is larger due to the presence of systematic errors which by definition cannot be quantified 
note that even though the observations may be uncorrelated the parameters are typically correlated 
parameter confidence limits it is often assumed for want of any concrete evidence but often appealing to the central limit theorem see normal distribution occurrence and applications that the error on each observation belongs to normal distribution with mean of zero and standard deviation under that assumption the following probabilities can be derived for single scalar parameter estimate in terms of its estimated standard error given here that the interval encompasses the true coefficient value that the interval encompasses the true coefficient value that the interval encompasses the true coefficient valuethe assumption is not unreasonable when if the experimental errors are normally distributed the parameters will belong to student distribution with degrees of freedom 
when student distribution approximates normal distribution 
note however that these confidence limits cannot take systematic error into account 
also parameter errors should be quoted to one significant figure only as they are subject to sampling error when the number of observations is relatively small chebychev inequality can be used for an upper bound on probabilities regardless of any assumptions about the distribution of experimental errors the maximum probabilities that parameter will be more than or standard deviations away from its expectation value are and respectively 
residual values and correlation the residuals are related to the observations by where is the idempotent matrix known as the hat matrix and is the identity matrix 
the variance covariance matrix of the residuals is given by thus the residuals are correlated even if the observations are not 
when the sum of weighted residual values is equal to zero whenever the model function contains constant term 
left multiply the expression for the residuals by xt wt say for example that the first term of the model is constant so that for all in that case it follows that thus in the motivational example above the fact that the sum of residual values is equal to zero is not accidental but is consequence of the presence of the constant term in the model 
if experimental error follows normal distribution then because of the linear relationship between residuals and observations so should residuals but since the observations are only sample of the population of all possible observations the residuals should belong to student distribution 
studentized residuals are useful in making statistical test for an outlier when particular residual appears to be excessively large 
see also iteratively reweighted least squares heteroscedasticity consistent standard errors weighted mean references
dependent and independent variables are variables in mathematical modeling statistical modeling and experimental sciences 
dependent variables receive this name because in an experiment their values are studied under the supposition or demand that they depend by some law or rule by mathematical function on the values of other variables 
independent variables in turn are not seen as depending on any other variable in the scope of the experiment in question 
in this sense some common independent variables are time space density mass fluid flow rate and previous values of some observed value of interest 
human population size to predict future values the dependent variable of the two it is always the dependent variable whose variation is being studied by altering inputs also known as regressors in statistical context 
in an experiment any variable that can be attributed value without attributing value to any other variable is called an independent variable 
models and experiments test the effects that the independent variables have on the dependent variables 
sometimes even if their influence is not of direct interest independent variables may be included for other reasons such as to account for their potential confounding effect 
mathematics in mathematics function is rule for taking an input in the simplest case number or set of numbers and providing an output which may also be number 
symbol that stands for an arbitrary input is called an independent variable while symbol that stands for an arbitrary output is called dependent variable 
the most common symbol for the input is and the most common symbol for the output is the function itself is commonly written it is possible to have multiple independent variables or multiple dependent variables 
for instance in multivariable calculus one often encounters functions of the form where is dependent variable and and are independent variables 
functions with multiple outputs are often referred to as vector valued functions 
modeling in mathematical modeling the dependent variable is studied to see if and how much it varies as the independent variables vary 
in the simple stochastic linear model yi bxi ei the term yi is the ith value of the dependent variable and xi is the ith value of the independent variable 
the term ei is known as the error and contains the variability of the dependent variable not explained by the independent variable 
with multiple independent variables the model is yi bxi bxi bxi ei where is the number of independent variables the linear regression model is now discussed 
to use linear regression scatter plot of data is generated with as the independent variable and as the dependent variable 
this is also called bivariate dataset xi yi 
the simple linear regression model takes the form of yi bxi ui for in this case ui un are independent random variables 
this occurs when the measurements do not influence each other 
through propagation of independence the independence of ui implies independence of yi even though each yi has different expectation value 
each ui has an expectation value of and variance of expectation of yi proof the line of best fit for the bivariate dataset takes the form and is called the regression line 
and correspond to the intercept and slope respectively 
simulation in simulation the dependent variable is changed in response to changes in the independent variables 
statistics in an experiment the variable manipulated by an experimenter is something that is proven to work called an independent variable 
the dependent variable is the event expected to change when the independent variable is manipulated in data mining tools for multivariate statistics and machine learning the dependent variable is assigned role as target variable or in some tools as label attribute while an independent variable may be assigned role as regular variable 
known values for the target variable are provided for the training data set and test data set but should be predicted for other data 
the target variable is used in supervised learning algorithms but not in unsupervised learning 
statistics synonyms depending on the context an independent variable is sometimes called predictor variable regressor covariate manipulated variable explanatory variable exposure variable see reliability theory risk factor see medical statistics feature in machine learning and pattern recognition or input variable 
in econometrics the term control variable is usually used instead of covariate 
explanatory variable is preferred by some authors over independent variable when the quantities treated as independent variables may not be statistically independent or independently manipulable by the researcher 
if the independent variable is referred to as an explanatory variable then the term response variable is preferred by some authors for the dependent variable from the economics community the independent variables are also called exogenous 
depending on the context dependent variable is sometimes called response variable regressand criterion predicted variable measured variable explained variable experimental variable responding variable outcome variable output variable target or label 
in economics endogenous variables are usually referencing the target 
explained variable is preferred by some authors over dependent variable when the quantities treated as dependent variables may not be statistically dependent 
if the dependent variable is referred to as an explained variable then the term predictor variable is preferred by some authors for the independent variable variables may also be referred to by their form continuous or categorical which in turn may be binary dichotomous nominal categorical and ordinal categorical among others 
an example is provided by the analysis of trend in sea level by woodworth 
here the dependent variable and variable of most interest was the annual mean sea level at given location for which series of yearly values were available 
the primary independent variable was time 
use was made of covariate consisting of yearly values of annual mean atmospheric pressure at sea level 
the results showed that inclusion of the covariate allowed improved estimates of the trend against time to be obtained compared to analyses which omitted the covariate 
other variables variable may be thought to alter the dependent or independent variables but may not actually be the focus of the experiment 
so that the variable will be kept constant or monitored to try to minimize its effect on the experiment 
such variables may be designated as either controlled variable control variable or fixed variable 
extraneous variables if included in regression analysis as independent variables may aid researcher with accurate response parameter estimation prediction and goodness of fit but are not of substantive interest to the hypothesis under examination 
for example in study examining the effect of post secondary education on lifetime earnings some extraneous variables might be gender ethnicity social class genetics intelligence age and so forth 
variable is extraneous only when it can be assumed or shown to influence the dependent variable 
if included in regression it can improve the fit of the model 
if it is excluded from the regression and if it has non zero covariance with one or more of the independent variables of interest its omission will bias the regression result for the effect of that independent variable of interest 
this effect is called confounding or omitted variable bias in these situations design changes and or controlling for variable statistical control is necessary 
extraneous variables are often classified into three types subject variables which are the characteristics of the individuals being studied that might affect their actions 
these variables include age gender health status mood background etc 
blocking variables or experimental variables are characteristics of the persons conducting the experiment which might influence how person behaves 
gender the presence of racial discrimination language or other factors may qualify as such variables 
situational variables are features of the environment in which the study or research was conducted which have bearing on the outcome of the experiment in negative way 
included are the air temperature level of activity lighting and time of day in modelling variability that is not covered by the independent variable is designated by and is known as the residual side effect error unexplained share residual variable disturbance or tolerance 
examples effect of fertilizer on plant growths in study measuring the influence of different quantities of fertilizer on plant growth the independent variable would be the amount of fertilizer used 
the dependent variable would be the growth in height or mass of the plant 
the controlled variables would be the type of plant the type of fertilizer the amount of sunlight the plant gets the size of the pots etc effect of drug dosage on symptom severity in study of how different doses of drug affect the severity of symptoms researcher could compare the frequency and intensity of symptoms when different doses are administered 
here the independent variable is the dose and the dependent variable is the frequency intensity of symptoms effect of temperature on pigmentation in measuring the amount of color removed from beetroot samples at different temperatures temperature is the independent variable and amount of pigment removed is the dependent variable effect of sugar added in coffee the taste varies with the amount of sugar added in the coffee 
here the sugar is the independent variable while the taste is the dependent variable 
see also abscissa and ordinate blocking statistics latent variable versus observable variable notes references
database encryption can generally be defined as process that uses an algorithm to transform data stored in database into cipher text that is incomprehensible without first being decrypted
it can therefore be said that the purpose of database encryption is to protect the data stored in database from being accessed by individuals with potentially malicious intentions
the act of encrypting database also reduces the incentive for individuals to hack the aforementioned database as meaningless encrypted data is of little to no use for hackers
there are multiple techniques and technologies available for database encryption the most important of which will be detailed in this article
transparent external database encryption transparent data encryption often abbreviated as tde is used to encrypt an entire database which therefore involves encrypting data at rest
data at rest can generally be defined as inactive data that is not currently being edited or pushed across network
as an example text file stored on computer is at rest until it is opened and edited
data at rest are stored on physical storage media solutions such as tapes or hard disk drives
the act of storing large amounts of sensitive data on physical storage media naturally raises concerns of security and theft
tde ensures that the data on physical storage media cannot be read by malicious individuals that may have the intention to steal them
data that cannot be read is worthless thus reducing the incentive for theft
perhaps the most important strength that is attributed to tde is its transparency
given that tde encrypts all data it can be said that no applications need to be altered in order for tde to run correctly
it is important to note that tde encrypts the entirety of the database as well as backups of the database
the transparent element of tde has to do with the fact that tde encrypts on the page level which essentially means that data is encrypted when stored and decrypted when it is called into the system memory
the contents of the database are encrypted using symmetric key that is often referred to as database encryption key
column level encryption in order to explain column level encryption it is important to outline basic database structure
typical relational database is divided into tables that are divided into columns that each have rows of data
whilst tde usually encrypts an entire database column level encryption allows for individual columns within database to be encrypted
it is important to establish that the granularity of column level encryption causes specific strengths and weaknesses to arise when compared to encrypting an entire database
firstly the ability to encrypt individual columns allows for column level encryption to be significantly more flexible when compared to encryption systems that encrypt an entire database such as tde
secondly it is possible to use an entirely unique and separate encryption key for each column within database
this effectively increases the difficulty of generating rainbow tables which thus implies that the data stored within each column is less likely to be lost or leaked
the main disadvantage associated with column level database encryption is speed or loss thereof
encrypting separate columns with different unique keys in the same database can cause database performance to decrease and additionally also decreases the speed at which the contents of the database can be indexed or searched
field level encryption experimental work is being done on providing database operations like searching or arithmetical operations on encrypted fields without the need to decrypt them
strong encryption is required to be randomized different result must be generated each time
this is known as probabilistic encryption
field level encryption is weaker than randomized encryption but it allows users to test for equality without decrypting the data
filesystem level encryption encrypting file system efs it is important to note that traditional database encryption techniques normally encrypt and decrypt the contents of database
databases are managed by database management systems dbms that run on top of an existing operating system os
this raises potential security concern as an encrypted database may be running on an accessible and potentially vulnerable operating system
efs can encrypt data that is not part of database system which implies that the scope of encryption for efs is much wider when compared to system such as tde that is only capable of encrypting database files
whilst efs does widen the scope of encryption it also decreases database performance and can cause administration issues as system administrators require operating system access to use efs
due to the issues concerning performance efs is not typically used in databasing applications that require frequent database input and output
in order to offset the performance issues it is often recommended that efs systems be used in environments with few users
full disk encryption bitlocker does not have the same performance concerns associated with efs
symmetric and asymmetric database encryption symmetric database encryption symmetric encryption in the context of database encryption involves private key being applied to data that is stored and called from database
this private key alters the data in way that causes it to be unreadable without first being decrypted
data is encrypted when saved and decrypted when opened given that the user knows the private key
thus if the data is to be shared through database the receiving individual must have copy of the secret key used by the sender in order to decrypt and view the data
clear disadvantage related to symmetric encryption is that sensitive data can be leaked if the private key is spread to individuals that should not have access to the data
however given that only one key is involved in the encryption process it can generally be said that speed is an advantage of symmetric encryption
asymmetric database encryption asymmetric encryption expands on symmetric encryption by incorporating two different types of keys into the encryption method private and public keys
public key can be accessed by anyone and is unique to one user whereas private key is secret key that is unique to and only known by one user
in most scenarios the public key is the encryption key whereas the private key is the decryption key
as an example if individual would like to send message to individual using asymmetric encryption he would encrypt the message using individual public key and then send the encrypted version
individual would then be able to decrypt the message using his private key
individual would not be able to decrypt individual message as individual private key is not the same as individual private key
asymmetric encryption is often described as being more secure in comparison to symmetric database encryption given that private keys do not need to be shared as two separate keys handle encryption and decryption processes
for performance reasons asymmetric encryption is used in key management rather than to encrypt the data which is usually done with symmetric encryption
key management the symmetric asymmetric database encryption section introduced the concept of public and private keys with basic examples in which users exchange keys
the act of exchanging keys becomes impractical from logistical point of view when many different individuals need to communicate with each other
in database encryption the system handles the storage and exchange of keys
this process is called key management
if encryption keys are not managed and stored properly highly sensitive data may be leaked
additionally if key management system deletes or loses key the information that was encrypted via said key is essentially rendered lost as well
the complexity of key management logistics is also topic that needs to be taken into consideration
as the number of application that firm uses increases the number of keys that need to be stored and managed increases as well
thus it is necessary to establish way in which keys from all applications can be managed through single channel which is also known as enterprise key management
enterprise key management solutions are sold by great number of suppliers in the technology industry
these systems essentially provide centralised key management solution that allows administrators to manage all keys in system through one hub
thus it can be said that the introduction of enterprise key management solutions has the potential to lessen the risks associated with key management in the context of database encryption as well as to reduce the logistical troubles that arise when many individuals attempt to manually share keys
hashing hashing is used in database systems as method to protect sensitive data such as passwords however it is also used to improve the efficiency of database referencing
inputted data is manipulated by hashing algorithm
the hashing algorithm converts the inputted data into string of fixed length that can then be stored in database
hashing systems have two crucially important characteristics that will now be outlined
firstly hashes are unique and repeatable
as an example running the word cat through the same hashing algorithm multiple times will always yield the same hash however it is extremely difficult to find word that will return the same hash that cat does
secondly hashing algorithms are not reversible
to relate this back to the example provided above it would be nearly impossible to convert the output of the hashing algorithm back to the original input which was cat
in the context of database encryption hashing is often used in password systems
when user first creates their password it is run through hashing algorithm and saved as hash
when the user logs back into the website the password that they enter is run through the hashing algorithm and is then compared to the stored hash
given the fact that hashes are unique if both hashes match then it is said that the user inputted the correct password
one example of popular hash function is sha secure hash algorithm
salting one issue that arises when using hashing for password management in the context of database encryption is the fact that malicious user could potentially use an input to hash table rainbow table for the specific hashing algorithm that the system uses
this would effectively allow the individual to decrypt the hash and thus have access to stored passwords
solution for this issue is to salt the hash
salting is the process of encrypting more than just the password in database
the more information that is added to string that is to be hashed the more difficult it becomes to collate rainbow tables
as an example system may combine user email and password into single hash
this increase in the complexity of hash means that it is far more difficult and thus less likely for rainbow tables to be generated
this naturally implies that the threat of sensitive data loss is minimised through salting hashes
pepper some systems incorporate pepper in addition to salts in their hashing systems
pepper systems are controversial however it is still necessary to explain their use
pepper is value that is added to hashed password that has been salted
this pepper is often unique to one website or service and it is important to note that the same pepper is usually added to all passwords saved in database
in theory the inclusion of peppers in password hashing systems has the potential to decrease the risk of rainbow input hash tables given the system level specificity of peppers however the real world benefits of pepper implementation are highly disputed
application level encryption in application level encryption the process of encrypting data is completed by the application that has been used to generate or modify the data that is to be encrypted
essentially this means that data is encrypted before it is written to the database
this unique approach to encryption allows for the encryption process to be tailored to each user based on the information such as entitlements or roles that the application knows about its users according to eugene pilyankevich application level encryption is becoming good practice for systems with increased security requirements with general drift toward perimeter less and more exposed cloud systems
advantages of application level encryption one of the most important advantages of application level encryption is the fact that application level encryption has the potential to simplify the encryption process used by company
if an application encrypts the data that it writes modifies from database then secondary encryption tool will not need to be integrated into the system
the second main advantage relates to the overarching theme of theft
given that data is encrypted before it is written to the server hacker would need to have access to the database contents as well as the applications that were used to encrypt and decrypt the contents of the database in order to decrypt sensitive data
disadvantages of application level encryption the first important disadvantage of application level encryption is that applications used by firm will need to be modified to encrypt data themselves
this has the potential to consume significant amount of time and other resources
given the nature of opportunity cost firms may not believe that application level encryption is worth the investment
in addition application level encryption may have limiting effect on database performance
if all data on database is encrypted by multitude of different applications then it becomes impossible to index or search data on the database
to ground this in reality in the form of basic example it would be impossible to construct glossary in single language for book that was written in languages
lastly the complexity of key management increases as multiple different applications need to have the authority and access to encrypt data and write it to the database
risks of database encryption when discussing the topic of database encryption it is imperative to be aware of the risks that are involved in the process
the first set of risks are related to key management
if private keys are not managed in an isolated system system administrators with malicious intentions may have the ability to decrypt sensitive data using keys that they have access to
the fundamental principle of keys also gives rise to potentially devastating risk if keys are lost then the encrypted data is essentially lost as well as decryption without keys is almost impossible
in statistics principal component regression pcr is regression analysis technique that is based on principal component analysis pca 
more specifically pcr is used for estimating the unknown regression coefficients in standard linear regression model 
in pcr instead of regressing the dependent variable on the explanatory variables directly the principal components of the explanatory variables are used as regressors 
one typically uses only subset of all the principal components for regression making pcr kind of regularized procedure and also type of shrinkage estimator 
often the principal components with higher variances the ones based on eigenvectors corresponding to the higher eigenvalues of the sample variance covariance matrix of the explanatory variables are selected as regressors 
however for the purpose of predicting the outcome the principal components with low variances may also be important in some cases even more important one major use of pcr lies in overcoming the multicollinearity problem which arises when two or more of the explanatory variables are close to being collinear 
pcr can aptly deal with such situations by excluding some of the low variance principal components in the regression step 
in addition by usually regressing on only subset of all the principal components pcr can result in dimension reduction through substantially lowering the effective number of parameters characterizing the underlying model 
this can be particularly useful in settings with high dimensional covariates 
also through appropriate selection of the principal components to be used for regression pcr can lead to efficient prediction of the outcome based on the assumed model 
the principle the pcr method may be broadly divided into three major steps perform pca on the observed data matrix for the explanatory variables to obtain the principal components and then usually select subset based on some appropriate criteria of the principal components so obtained for further use 
now regress the observed vector of outcomes on the selected principal components as covariates using ordinary least squares regression linear regression to get vector of estimated regression coefficients with dimension equal to the number of selected principal components now transform this vector back to the scale of the actual covariates using the selected pca loadings the eigenvectors corresponding to the selected principal components to get the final pcr estimator with dimension equal to the total number of covariates for estimating the regression coefficients characterizing the original model 
details of the method data representation let denote the vector of observed outcomes and denote the corresponding data matrix of observed covariates where and denote the size of the observed sample and the number of covariates respectively with each of the rows of denotes one set of observations for the dimensional covariate and the respective entry of denotes the corresponding observed outcome 
data pre processing assume that and each of the columns of have already been centered so that all of them have zero empirical means 
this centering step is crucial at least for the columns of since pcr involves the use of pca on and pca is sensitive to centering of the data 
underlying model following centering the standard gauss markov linear regression model for on can be represented as where denotes the unknown parameter vector of regression coefficients and denotes the vector of random errors with and var for some unknown variance parameter objective the primary goal is to obtain an efficient estimator for the parameter based on the data 
one frequently used approach for this is ordinary least squares regression which assuming is full column rank gives the unbiased estimator of pcr is another technique that may be used for the same purpose of estimating pca step pcr starts by performing pca on the centered data matrix for this let denote the singular value decomposition of where diag with denoting the non negative singular values of while the columns of and are both orthonormal sets of vectors denoting the left and right singular vectors of respectively 
the principal components gives spectral decomposition of where diag diag with denoting the non negative eigenvalues also known as the principal values of while the columns of denote the corresponding orthonormal set of eigenvectors 
then and respectively denote the principal component and the principal component direction or pca loading corresponding to the th largest principal value for each 
derived covariates for any let denote the matrix with orthonormal columns consisting of the first columns of let denote the matrix having the first principal components as its columns 
may be viewed as the data matrix obtained by using the transformed covariates instead of using the original covariates the pcr estimator let denote the vector of estimated regression coefficients obtained by ordinary least squares regression of the response vector on the data matrix then for any the final pcr estimator of based on using the first principal components is given by fundamental characteristics and applications of the pcr estimator two basic properties the fitting process for obtaining the pcr estimator involves regressing the response vector on the derived data matrix which has orthogonal columns for any since the principal components are mutually orthogonal to each other 
thus in the regression step performing multiple linear regression jointly on the selected principal components as covariates is equivalent to carrying out independent simple linear regressions or univariate regressions separately on each of the selected principal components as covariate 
when all the principal components are selected for regression so that then the pcr estimator is equivalent to the ordinary least squares estimator 
thus this is easily seen from the fact that and also observing that is an orthogonal matrix 
variance reduction for any the variance of is given by var diag in particular var var hence for all we have var var thus for all we have var var where indicates that square symmetric matrix is non negative definite 
consequently any given linear form of the pcr estimator has lower variance compared to that of the same linear form of the ordinary least squares estimator 
addressing multicollinearity under multicollinearity two or more of the covariates are highly correlated so that one can be linearly predicted from the others with non trivial degree of accuracy 
consequently the columns of the data matrix that correspond to the observations for these covariates tend to become linearly dependent and therefore tends to become rank deficient losing its full column rank structure 
more quantitatively one or more of the smaller eigenvalues of get very close or become exactly equal to under such situations 
the variance expressions above indicate that these small eigenvalues have the maximum inflation effect on the variance of the least squares estimator thereby destabilizing the estimator significantly when they are close to this issue can be effectively addressed through using pcr estimator obtained by excluding the principal components corresponding to these small eigenvalues 
dimension reduction pcr may also be used for performing dimension reduction 
to see this let denote any matrix having orthonormal columns for any 
suppose now that we want to approximate each of the covariate observations through the rank linear transformation for some 
then it can be shown that is minimized at the matrix with the first principal component directions as columns and the corresponding dimensional derived covariates 
thus the dimensional principal components provide the best linear approximation of rank to the observed data matrix the corresponding reconstruction error is given by thus any potential dimension reduction may be achieved by choosing the number of principal components to be used through appropriate thresholding on the cumulative sum of the eigenvalues of since the smaller eigenvalues do not contribute significantly to the cumulative sum the corresponding principal components may be continued to be dropped as long as the desired threshold limit is not exceeded 
the same criteria may also be used for addressing the multicollinearity issue whereby the principal components corresponding to the smaller eigenvalues may be ignored as long as the threshold limit is maintained 
regularization effect since the pcr estimator typically uses only subset of all the principal components for regression it can be viewed as some sort of regularized procedure 
more specifically for any the pcr estimator denotes the regularized solution to the following constrained minimization problem min subject to 
the constraint may be equivalently written as where 
thus when only proper subset of all the principal components are selected for regression the pcr estimator so obtained is based on hard form of regularization that constrains the resulting solution to the column space of the selected principal component directions and consequently restricts it to be orthogonal to the excluded directions 
optimality of pcr among class of regularized estimators given the constrained minimization problem as defined above consider the following generalized version of it min subject to where denotes any full column rank matrix of order with let denote the corresponding solution 
thus arg min subject to then the optimal choice of the restriction matrix for which the corresponding estimator achieves the minimum prediction error is given by where diag 
quite clearly the resulting optimal estimator is then simply given by the pcr estimator based on the first principal components 
efficiency since the ordinary least squares estimator is unbiased for we have var mse where mse denotes the mean squared error 
now if for some we additionally have then the corresponding is also unbiased for and therefore var mse 
we have already seen that var var which then implies mse mse for that particular thus in that case the corresponding would be more efficient estimator of compared to based on using the mean squared error as the performance criteria 
in addition any given linear form of the corresponding would also have lower mean squared error compared to that of the same linear form of now suppose that for given then the corresponding is biased for however since var var it is still possible that mse mse especially if is such that the excluded principal components correspond to the smaller eigenvalues thereby resulting in lower bias 
in order to ensure efficient estimation and prediction performance of pcr as an estimator of park proposes the following guideline for selecting the principal components to be used for regression drop the principal component if and only if practical implementation of this guideline of course requires estimates for the unknown model parameters and in general they may be estimated using the unrestricted least squares estimates obtained from the original full model 
park however provides slightly modified set of estimates that may be better suited for this purpose unlike the criteria based on the cumulative sum of the eigenvalues of which is probably more suited for addressing the multicollinearity problem and for performing dimension reduction the above criteria actually attempts to improve the prediction and estimation efficiency of the pcr estimator by involving both the outcome as well as the covariates in the process of selecting the principal components to be used in the regression step 
alternative approaches with similar goals include selection of the principal components based on cross validation or the mallow cp criteria 
often the principal components are also selected based on their degree of association with the outcome 
shrinkage effect of pcr in general pcr is essentially shrinkage estimator that usually retains the high variance principal components corresponding to the higher eigenvalues of as covariates in the model and discards the remaining low variance components corresponding to the lower eigenvalues of 
thus it exerts discrete shrinkage effect on the low variance components nullifying their contribution completely in the original model 
in contrast the ridge regression estimator exerts smooth shrinkage effect through the regularization parameter or the tuning parameter inherently involved in its construction 
while it does not completely discard any of the components it exerts shrinkage effect over all of them in continuous manner so that the extent of shrinkage is higher for the low variance components and lower for the high variance components 
frank and friedman conclude that for the purpose of prediction itself the ridge estimator owing to its smooth shrinkage effect is perhaps better choice compared to the pcr estimator having discrete shrinkage effect 
in addition the principal components are obtained from the eigen decomposition of that involves the observations for the explanatory variables only 
therefore the resulting pcr estimator obtained from using these principal components as covariates need not necessarily have satisfactory predictive performance for the outcome 
somewhat similar estimator that tries to address this issue through its very construction is the partial least squares pls estimator 
similar to pcr pls also uses derived covariates of lower dimensions 
however unlike pcr the derived covariates for pls are obtained based on using both the outcome as well as the covariates 
while pcr seeks the high variance directions in the space of the covariates pls seeks the directions in the covariate space that are most useful for the prediction of the outcome 
variant of the classical pcr known as the supervised pcr was proposed 
in spirit similar to that of pls it attempts at obtaining derived covariates of lower dimensions based on criterion that involves both the outcome as well as the covariates 
the method starts by performing set of simple linear regressions or univariate regressions wherein the outcome vector is regressed separately on each of the covariates taken one at time 
then for some the first covariates that turn out to be the most correlated with the outcome based on the degree of significance of the corresponding estimated regression coefficients are selected for further use 
conventional pcr as described earlier is then performed but now it is based on only the data matrix corresponding to the observations for the selected covariates 
the number of covariates used and the subsequent number of principal components used are usually selected by cross validation 
generalization to kernel settings the classical pcr method as described above is based on classical pca and considers linear regression model for predicting the outcome based on the covariates 
however it can be easily generalized to kernel machine setting whereby the regression function need not necessarily be linear in the covariates but instead it can belong to the reproducing kernel hilbert space associated with any arbitrary possibly non linear symmetric positive definite kernel 
the linear regression model turns out to be special case of this setting when the kernel function is chosen to be the linear kernel 
in general under the kernel machine setting the vector of covariates is first mapped into high dimensional potentially infinite dimensional feature space characterized by the kernel function chosen 
the mapping so obtained is known as the feature map and each of its coordinates also known as the feature elements corresponds to one feature may be linear or non linear of the covariates 
the regression function is then assumed to be linear combination of these feature elements 
thus the underlying regression model in the kernel machine setting is essentially linear regression model with the understanding that instead of the original set of covariates the predictors are now given by the vector potentially infinite dimensional of feature elements obtained by transforming the actual covariates using the feature map 
however the kernel trick actually enables us to operate in the feature space without ever explicitly computing the feature map 
it turns out that it is only sufficient to compute the pairwise inner products among the feature maps for the observed covariate vectors and these inner products are simply given by the values of the kernel function evaluated at the corresponding pairs of covariate vectors 
the pairwise inner products so obtained may therefore be represented in the form of symmetric non negative definite matrix also known as the kernel matrix 
pcr in the kernel machine setting can now be implemented by first appropriately centering this kernel matrix say with respect to the feature space and then performing kernel pca on the centered kernel matrix say whereby an eigendecomposition of is obtained 
kernel pcr then proceeds by usually selecting subset of all the eigenvectors so obtained and then performing standard linear regression of the outcome vector on these selected eigenvectors 
the eigenvectors to be used for regression are usually selected using cross validation 
the estimated regression coefficients having the same dimension as the number of selected eigenvectors along with the corresponding selected eigenvectors are then used for predicting the outcome for future observation 
in machine learning this technique is also known as spectral regression 
clearly kernel pcr has discrete shrinkage effect on the eigenvectors of quite similar to the discrete shrinkage effect of classical pcr on the principal components as discussed earlier 
however the feature map associated with the chosen kernel could potentially be infinite dimensional and hence the corresponding principal components and principal component directions could be infinite dimensional as well 
therefore these quantities are often practically intractable under the kernel machine setting 
kernel pcr essentially works around this problem by considering an equivalent dual formulation based on using the spectral decomposition of the associated kernel matrix 
under the linear regression model which corresponds to choosing the kernel function as the linear kernel this amounts to considering spectral decomposition of the corresponding kernel matrix and then regressing the outcome vector on selected subset of the eigenvectors of so obtained 
it can be easily shown that this is the same as regressing the outcome vector on the corresponding principal components which are finite dimensional in this case as defined in the context of the classical pcr 
thus for the linear kernel the kernel pcr based on dual formulation is exactly equivalent to the classical pcr based on primal formulation 
however for arbitrary and possibly non linear kernels this primal formulation may become intractable owing to the infinite dimensionality of the associated feature map 
thus classical pcr becomes practically infeasible in that case but kernel pcr based on the dual formulation still remains valid and computationally scalable 
see also principal component analysis partial least squares regression ridge regression canonical correlation deming regression total sum of squares references further reading amemiya takeshi
in theoretical computer science and cryptography trapdoor function is function that is easy to compute in one direction yet difficult to compute in the opposite direction finding its inverse without special information called the trapdoor
trapdoor functions are special case of one way functions and are widely used in public key cryptography in mathematical terms if is trapdoor function then there exists some secret information such that given and it is easy to compute consider padlock and its key
it is trivial to change the padlock from open to closed without using the key by pushing the shackle into the lock mechanism
opening the padlock easily however requires the key to be used
here the key is the trapdoor and the padlock is the trapdoor function
an example of simple mathematical trapdoor is is the product of two prime numbers
what are those numbers
typical brute force solution would be to try dividing by several prime numbers until finding the answer
however if one is told that is one of the numbers one can find the answer by entering into any calculator
this example is not sturdy trapdoor function modern computers can guess all of the possible answers within second but this sample problem could be improved by using the product of two much larger primes
trapdoor functions came to prominence in cryptography in the mid with the publication of asymmetric or public key encryption techniques by diffie hellman and merkle
indeed diffie hellman coined the term
several function classes had been proposed and it soon became obvious that trapdoor functions are harder to find than was initially thought
for example an early suggestion was to use schemes based on the subset sum problem
this turned out rather quickly to be unsuitable
as of the best known trapdoor function family candidates are the rsa and rabin families of functions
both are written as exponentiation modulo composite number and both are related to the problem of prime factorization
functions related to the hardness of the discrete logarithm problem either modulo prime or in group defined over an elliptic curve are not known to be trapdoor functions because there is no known trapdoor information about the group that enables the efficient computation of discrete logarithms
trapdoor in cryptography has the very specific aforementioned meaning and is not to be confused with backdoor these are frequently used interchangeably which is incorrect
backdoor is deliberate mechanism that is added to cryptographic algorithm key pair generation algorithm digital signing algorithm etc
or operating system for example that permits one or more unauthorized parties to bypass or subvert the security of the system in some fashion
definition trapdoor function is collection of one way functions satisfying the following conditions there exists probabilistic polynomial time ppt sampling algorithm gen
gen tk with satisfies tk in which is some polynomial
each tk is called the trapdoor corresponding to each trapdoor can be efficiently sampled
given input there also exists ppt algorithm that outputs dk
that is each dk can be efficiently sampled
for any there exists ppt algorithm that correctly computes fk
for any there exists ppt algorithm
for any dk let fk tk and then we have fk fk
that is given trapdoor it is easy to invert
for any without trapdoor tk for any ppt algorithm the probability to correctly invert fk given fk find pre image such that fk fk is negligible if each function in the collection above is one way permutation then the collection is also called trapdoor permutation
examples in the following two examples we always assume it is difficult to factorize large composite number see integer factorization
rsa assumption in this example the inverse of modulo euler totient function of is the trapdoor mod if the factorization of is known then can be computed
with this the inverse of can be computed mod and then given we can find mod mod mod its hardness follows from the rsa assumption
rabin quadratic residue assumption let be large composite number such that where and are large primes such that mod mod and kept confidential to the adversary
the problem is to compute given such that mod
the trapdoor is the factorization of with the trapdoor the solutions of can be given as where mod mod mod mod mod mod
see chinese remainder theorem for more details
note that given primes and we can find mod and mod
here the conditions mod and mod guarantee that the solutions and can be well defined
see also one way function notes references diffie hellman new directions in cryptography pdf ieee transactions on information theory citeseerx doi tit pass rafael course in cryptography pdf retrieved november goldwasser shafi lecture notes on cryptography pdf retrieved november ostrovsky rafail foundations of cryptography pdf retrieved november dodis yevgeniy introduction to cryptography lecture notes fall retrieved december lindell yehuda foundations of cryptography pdf retrieved december
the circle of fifths text table shows the number of flats or sharps in each of the diatonic musical scales and keys 
both major and minor keys have no flats or sharps 
in the table minor keys are written with lowercase letters for brevity 
however in common guitar tabs notation minor key is designated with lowercase 
for example minor is am and sharp minor is 
the small interval between equivalent notes such as sharp and flat is the pythagorean comma 
minor scales start with major scales start with 
see also circle of fifths key signature musical notation notes
the axolotl from classical nahuatl tl lo listen ambystoma mexicanum is paedomorphic salamander closely related to the tiger salamander 
axolotls are unusual among amphibians in that they reach adulthood without undergoing metamorphosis 
instead of taking to the land adults remain aquatic and gilled 
the species was originally found in several lakes underlying what is now mexico city such as lake xochimilco and lake chalco 
these lakes were drained by spanish settlers after the conquest of the aztec empire leading to the destruction of much of the axolotl natural habitat 
axolotls should not be confused with the larval stage of the closely related tiger salamander tigrinum which are widespread in much of north america and occasionally become paedomorphic 
neither should they be confused with mudpuppies necturus spp 
fully aquatic salamanders from different family that are not closely related to the axolotl but bear superficial resemblance as of wild axolotls were near extinction due to urbanization in mexico city and consequent water pollution as well as the introduction of invasive species such as tilapia and perch 
they are listed as critically endangered in the wild with decreasing population of around to adult individuals by the international union for conservation of nature and natural resources iucn and are listed under appendix ii of the convention on international trade in endangered species cites 
axolotls are used extensively in scientific research due to their ability to regenerate limbs gills and parts of their eyes and brains 
axolotls were also sold as food in mexican markets and were staple in the aztec diet 
description sexually mature adult axolotl at age months ranges in length from to cm to in although size close to cm in is most common and greater than cm in is rare 
axolotls possess features typical of salamander larvae including external gills and caudal fin extending from behind the head to the vent 
external gills are usually lost when salamander species mature into adulthood although the axolotl maintains this feature 
this is due to their neoteny evolution where axolotls are much more aquatic than other salamander species their heads are wide and their eyes are lidless 
their limbs are underdeveloped and possess long thin digits 
males are identified by their swollen cloacae lined with papillae while females are noticeable for their wider bodies full of eggs 
three pairs of external gill stalks rami originate behind their heads and are used to move oxygenated water 
the external gill rami are lined with filaments fimbriae to increase surface area for gas exchange 
four gill slits lined with gill rakers are hidden underneath the external gills which prevent food from entering and allow particles to filter through 
axolotls have barely visible vestigial teeth which develop during metamorphosis 
the primary method of feeding is by suction during which their rakers interlock to close the gill slits 
external gills are used for respiration although buccal pumping gulping air from the surface may also be used to provide oxygen to their lungs 
buccal pumping can occur in two stroke manner that pumps air from the mouth to the lungs and with four stroke that reverses this pathway with compression forces 
axolotls have four pigmentation genes when mutated they create different color variants 
the normal wild type animal is brown tan with gold speckles and an olive undertone 
the five more common mutant colors are leucistic pale pink with black eyes golden albino golden with gold eyes xanthic grey with black eyes albino pale pink white with red eyes which is more common in axolotls than some other creatures and melanoid all black dark blue with no gold speckling or olive tone 
in addition there is wide individual variability in the size frequency and intensity of the gold speckling and at least one variant that develops black and white piebald appearance on reaching maturity 
because pet breeders frequently cross the variant colors double homozygous mutants are common in the pet trade especially white pink animals with pink eyes that are double homozygous mutants for both the albino and leucistic trait 
axolotls also have some limited ability to alter their color to provide better camouflage by changing the relative size and thickness of their melanophores 
habitat and ecology the axolotl is native only to the freshwater of lake xochimilco and lake chalco in the valley of mexico 
lake chalco no longer exists having been drained as flood control measure and lake xochimilco remains remnant of its former self existing mainly as canals 
the water temperature in xochimilco rarely rises above although it may fall to in the winter and perhaps lower surveys in and found and axolotls per square kilometer in its lake xochimilco habitat respectively 
four month long search in however turned up no surviving individuals in the wild 
just month later two wild ones were spotted in network of canals leading from xochimilco the wild population has been put under heavy pressure by the growth of mexico city 
the axolotl is currently on the international union for conservation of nature annual red list of threatened species 
non native fish such as african tilapia and asian carp have also recently been introduced to the waters 
these new fish have been eating the axolotls young as well as their primary source of food axolotls are members of the tiger salamander or ambystoma tigrinum species complex along with all other mexican species of ambystoma 
their habitat is like that of most neotenic species high altitude body of water surrounded by risky terrestrial environment 
these conditions are thought to favor neoteny 
however terrestrial population of mexican tiger salamanders occupies and breeds in the axolotl habitat the axolotl is carnivorous consuming small prey such as mollusks worms insects other arthropods and small fish in the wild 
axolotls locate food by smell and will snap at any potential meal sucking the food into their stomachs with vacuum force 
use as model organism today the axolotl is still used in research as model organism and large numbers are bred in captivity 
they are especially easy to breed compared to other salamanders in their family which are rarely captive bred due to the demands of terrestrial life 
one attractive feature for research is the large and easily manipulated embryo which allows viewing of the full development of vertebrate 
axolotls are used in heart defect studies due to the presence of mutant gene that causes heart failure in embryos 
since the embryos survive almost to hatching with no heart function the defect is very observable 
the axolotl is also considered an ideal animal model for the study of neural tube closure due to the similarities between human and axolotl neural plate and tube formation the axolotl neural tube unlike the frog is not hidden under layer of superficial epithelium 
there are also mutations affecting other organ systems some of which are not well characterized and others that are 
the genetics of the color variants of the axolotl have also been widely studied 
regeneration the feature of the axolotl that attracts most attention is its healing ability the axolotl does not heal by scarring and is capable of the regeneration of entire lost appendages in period of months and in certain cases more vital structures such as tail limb central nervous system and tissues of the eye and heart 
they can even restore less vital parts of their brains 
they can also readily accept transplants from other individuals including eyes and parts of the brain restoring these alien organs to full functionality 
in some cases axolotls have been known to repair damaged limb as well as regenerating an additional one ending up with an extra appendage that makes them attractive to pet owners as novelty 
in metamorphosed individuals however the ability to regenerate is greatly diminished 
the axolotl is therefore used as model for the development of limbs in vertebrates 
there are three basic requirements for regeneration of the limb the wound epithelium nerve signaling and the presence of cells from the different limb axes 
wound epidermis is quickly formed by the cells to cover up the site of the wound 
in the following days the cells of the wound epidermis divide and grow quickly forming blastema which means the wound is ready to heal and undergo patterning to form the new limb 
it is believed that during limb generation axolotls have different system to regulate their internal macrophage level and suppress inflammation as scarring prevents proper healing and regeneration 
however this belief has been questioned by other studies 
axolotl regenerative properties leave the species as the perfect model to study the process of stem cells and its own neoteny feature 
current research can record specific examples of these regenerative properties through tracking cell fates and behaviors lineage tracing skin triploid cell grafts pigmentation imaging electroporation tissue clearing and lineage tracing from dye labeling 
the newer technologies of germline modification and transgenesis are better suited for live imaging the regenerative processes that occur for axolotls 
genome the billion base pair long sequence of the axolotl genome was published in and was the largest animal genome completed at the time 
it revealed species specific genetic pathways that may be responsible for limb regeneration 
although the axolotl genome is about times as large as the human genome it encodes similar number of proteins namely the human genome encodes about proteins 
the size difference is mostly explained by large fraction of repetitive sequences but such repeated elements also contribute to increased median intron sizes bp which are and times that observed in human bp mouse bp and tibetan frog bp respectively 
neoteny when most amphibians are young they live in water and they use gills that can breathe in the water 
when they become adults they go through process called metamorphosis in which they lose their gills and start living on land 
however the axolotl is unusual in that it has lack of thyroid stimulating hormone which is needed for the thyroid to produce thyroxine in order for the axolotl to go through metamorphosis therefore it keeps its gills and lives in water all its life even after it becomes an adult and is able to reproduce 
its body has the capacity to go through metamorphosis if given the necessary hormone but axolotls do not produce it and must be exposed to it from an external source after which an axolotl undergoes an artificially induced metamorphosis and begins living on land 
one method of artificial metamorphosis induction is through an injection of iodine which is used in the production of thyroid hormones 
an axolotl undergoing metamorphosis experiences number of physiological changes that help them adapt to life on land 
these include increased muscle tone in limbs the absorption of gills and fins into the body the development of eyelids and reduction in the skin permeability to water allowing the axolotl to stay more easily hydrated when on land 
the lungs of an axolotl though present alongside gills after reaching non metamorphosed adulthood develop further during metamorphosis an axolotl that has gone through metamorphosis resembles an adult plateau tiger salamander though the axolotl differs in its longer toes 
the process of artificially inducing metamorphosis can often result in death during or even following successful attempt and so casual hobbyists are generally discouraged from attempting to induce metamorphosis in pet axolotls neoteny is the term for reaching sexual maturity without undergoing metamorphosis 
many other species within the axolotl genus are also either entirely neotenic or have neotenic populations 
sirens and necturus are other neotenic salamanders although unlike axolotls they cannot be induced to metamorphose by an injection of iodine or thyroxine hormone 
the genes responsible for neoteny in laboratory animals may have been identified however they are not linked in wild populations suggesting artificial selection is the cause of complete neoteny in laboratory and pet axolotls six adult axolotls including leucistic specimen were shipped from mexico city to the jardin des plantes in paris in unaware of their neoteny auguste dum ril was surprised when instead of the axolotl he found in the vivarium new species similar to the salamander 
this discovery was the starting point of research about neoteny 
it is not certain that ambystoma velasci specimens were not included in the original shipment 
vilem laufberger in prague used thyroid hormone injections to induce an axolotl to grow into terrestrial adult salamander 
the experiment was repeated by englishman julian huxley who was unaware the experiment had already been done using ground thyroids 
since then experiments have been done often with injections of iodine or various thyroid hormones used to induce metamorphosis neoteny has been observed in all salamander families in which it seems to be survival mechanism in aquatic environments only of mountain and hill with little food and in particular with little iodine 
in this way salamanders can reproduce and survive in the form of smaller larval stage which is aquatic and requires lower quality and quantity of food compared to the big adult which is terrestrial 
if the salamander larvae ingest sufficient amount of iodine directly or indirectly through cannibalism they quickly begin metamorphosis and transform into bigger terrestrial adults with higher dietary requirements 
in fact in some high mountain lakes there live dwarf forms of salmonids that are caused by deficiencies in food and in particular iodine which causes cretinism and dwarfism due to hypothyroidism as it does in humans 
captive care the axolotl is popular exotic pet like its relative the tiger salamander ambystoma tigrinum 
as for all poikilothermic organisms lower temperatures result in slower metabolism and very unhealthily reduced appetite 
temperatures at approximately to are suggested for captive axolotls to ensure sufficient food intake stress resulting from more than day exposure to lower temperatures may quickly lead to disease and death and temperatures higher than may lead to metabolic rate increase also causing stress and eventually death 
chlorine commonly added to tapwater is harmful to axolotls 
single axolotl typically requires litre us gallon tank 
axolotls spend the majority of the time at the bottom of the tank 
salts such as holtfreter solution are often added to the water to prevent infection in captivity axolotls eat variety of readily available foods including trout and salmon pellets frozen or live bloodworms earthworms and waxworms 
axolotls can also eat feeder fish but care should be taken as fish may contain parasites substrates are another important consideration for captive axolotls as axolotls like other amphibians and reptiles tend to ingest bedding material together with food and are commonly prone to gastrointestinal obstruction and foreign body ingestion 
some common substrates used for animal enclosures can be harmful for amphibians and reptiles 
gravel common in aquarium use should not be used and is recommended that any sand consists of smooth particles with grain size of under mm 
one guide to axolotl care for laboratories notes that bowel obstructions are common cause of death and recommends that no items with diameter below cm or approximately the size of the animal head should be available to the animal there is some evidence that axolotls might seek out appropriately sized gravel for use as gastroliths based on experiments conducted at the university of manitoba axolotl colony but these studies are outdated and not conclusive 
as there is no conclusive evidence pointing to gastrolith use gravel should be avoided due to the high risk of impaction 
cultural significance the species is named after the aztec deity xolotl who transformed himself into an axolotl 
they continue to play an outsized cultural role in mexico and have appeared in cartoons and murals in it was announced that the axolotl will be featured on the new design for mexico peso banknote along with images of maize and chinampas 
see also mudpuppies olm texas salamander texas blind salamander lake patzcuaro salamander barred tiger salamander amphibious fish handfish regenerative biomedicine references external links ambystomatidae at curlie follow the eggs hatchlings and juveniles mating dance and laying eggs follow the eggs and hatchlings nd batch indiana axolotl colony university of ky axolotl colony mystical amphibian venerated by aztecs nears extinction the animal that everywhere and nowhere axolotl 
the tao of axolotl thetolteciching com on folklore
in western music the adjectives major and minor may describe chord scale or key 
as such composition movement section or phrase may be referred to by its key including whether that key is major or minor 
intervals some intervals may be referred to as major and minor 
major interval is one semitone larger than minor interval 
the words perfect diminished and augmented are also used to describe the quality of an interval 
only the intervals of second third sixth and seventh and the compound intervals based on them may be major or minor or rarely diminished or augmented 
unisons fourths fifths and octaves and their compound interval must be perfect or rarely diminished or augmented 
in western music minor chord sounds darker than major chord 
scales and chords the other uses of major and minor generally refer to scales and chords that contain major third or minor third respectively 
major scale is scale in which the third scale degree the mediant is major third above the tonic note 
in minor scale the third degree is minor third above the tonic 
similarly in major triad or major seventh chord the third is major third above the chord root 
in minor triad or minor seventh chord the third is minor third above the root 
keys the hallmark that distinguishes major keys from minor is whether the third scale degree is major or minor 
as musicologist roger kamien explains the crucial difference is that in the minor scale there is only half step between nd and rd note and between th and th note as compared to the major scales where the difference between rd and th note and between th and th note is half step 
this alteration in the third degree greatly changes the mood of the music and music based on minor scales tends to be considered to sound serious or melancholic minor keys are sometimes said to have more interesting possibly darker sound than plain major scales 
harry partch considers minor as the immutable faculty of ratios which in turn represent an immutable faculty of the human ear 
the minor key and scale are also considered less justifiable than the major with paul hindemith calling it clouding of major and moritz hauptmann calling it falsehood of the major changes of mode which involve the alteration of the third and mode mixture are often analyzed as minor changes unless structurally supported because the root and overall key and tonality remain unchanged 
this is in contrast with for instance transposition 
transposition is done by moving all intervals up or down certain constant interval and does change the key but not the mode which requires the alteration of intervals 
the use of triads only available in the minor mode such as the use of major in major is relatively decorative chromaticism considered to add color and weaken the sense of key without entirely destroying or losing it 
intonation and tuning musical tuning of intervals is expressed by the ratio between the pitches frequencies 
simple fractions can sound more harmonious than complex fractions for instance an octave is simple ratio and fifth is the relatively simple ratio 
the table below gives approximations of scale to ratios that are rounded to be as simple as possible 
in just intonation minor chord is often but not exclusively tuned in the frequency ratio play 
in tone equal temperament tet which is now the most common tuning system in the west minor chord has semitones between the root and third between the third and fifth and between the root and fifth 
in tet the perfect fifth cents is only about two cents narrower than the just tuned perfect fifth cents but the minor third cents is noticeably about cents narrower than the just minor third cents 
moreover the minor third cents more closely approximates the limit limit music minor third play cents the nineteenth harmonic with about two cents error alexander ellis proposes that the conflict between mathematicians and physicists on one hand and practicing musicians on the other regarding the supposed inferiority of the minor chord and scale to the major may be explained due to physicists comparison of just minor and major triads in which case minor comes out the loser versus the musicians comparison of the equal tempered triads in which case minor comes out the winner since the et major third is about cents sharp from the just major third cents but just about four cents narrower than the limit major third cents while the et minor third closely approximates the minor third which many find pleasing 
advanced theory in the neo riemannian theory the minor mode is considered the inverse of the major mode an upside down major scale based on theoretical undertones rather than actual overtones harmonics see also utonality 
the root of the minor triad is thus considered the top of the fifth which in the united states is called the fifth 
so in minor the tonic is actually and the leading tone is half step rather than in major the root being and the leading tone half step 
also since all chords are analyzed as having tonic subdominant or dominant function with for instance in minor being considered the tonic parallel us relative tp the use of minor mode root chord progressions in major such as major major major is analyzed as sp dp the minor subdominant parallel see parallel chord the minor dominant parallel and the major tonic 
see also gypsy scale list of major minor compositions music written in all major and or minor keys otonality and utonality references
in probability theory and statistics collection of random variables is independent and identically distributed if each random variable has the same probability distribution as the others and all are mutually independent 
this property is usually abbreviated as iid or iid 
iid was first defined in statistics and finds application in different fields such as data mining and signal processing 
introduction in statistics we commonly deal with random samples 
random sample can be thought of as set of objects that are chosen randomly 
or more formally it sequence of independent identically distributed iid random variables 
in other words the terms random sample and iid are basically one and the same 
in statistics we usually say random sample but in probability it more common to say iid identically distributed means that there are no overall trends the distribution doesn fluctuate and all items in the sample are taken from the same probability distribution 
independent means that the sample items are all independent events 
in other words they aren connected to each other in any way 
in other words knowledge of the value of one variable gives no information about the value of the other and vice versa 
application independent and identically distributed random variables are often used as an assumption which tends to simplify the underlying mathematics 
in practical applications of statistical modeling however the assumption may or may not be realistic the 
assumption is also used in central limit theorem which states that the probability distribution of the sum or average of 
variables with finite variance approaches normal distribution often the 
assumption arises in the context of sequences of random variables 
then independent and identically distributed implies that an element in the sequence is independent of the random variables that came before it 
in this way an 
sequence is different from markov sequence where the probability distribution for the nth random variable is function of the previous random variable in the sequence for first order markov sequence 
sequence does not imply the probabilities for all elements of the sample space or event space must be the same 
for example repeated throws of loaded dice will produce sequence that is despite the outcomes being biased 
definition definition for two random variables suppose that the random variables and are defined to assume values in let and be the cumulative distribution functions of and respectively and denote their joint cumulative distribution function by 
two random variables and are identically distributed if and only if two random variables and are independent if and only if 
see further independence probability theory two random variables 
two random variables and are 
if they are independent and identically distributed 
if and only if definition for more than two random variables the definition extends naturally to more than two random variables 
we say that random variables are 
if they are independent see further independence probability theory more than two random variables and identically distributed 
if and only if where denotes the joint cumulative distribution function of definition for independence in probability theory two events are called independent if and only if and 
in the following ab is short for and 
suppose there are the two events of the experiment if there is possibility 
generally the occurrence of has an effect on the probability of which is called conditional probability and only when the occurrence of has no effect on the occurrence of there is 
note if then are mutually independent which cannot be established with mutually incompatible at the same time that is independence must be compatible and mutual exclusion must be related 
suppose are three events 
if ab bc ac abc are satisfied then the events are independent of each other 
more general definition is there are events an 
if the probabilities of the product events for any events are equal to the product of the probabilities of each event then the events an are independent of each other 
examples example sequence of outcomes of spins of fair or unfair roulette wheel is 
one implication of this is that if the roulette ball lands on red for example times in row the next spin is no more or less likely to be black than on any other spin see the gambler fallacy 
sequence of fair or loaded dice rolls is 
sequence of fair or unfair coin flips is 
in signal processing and image processing the notion of transformation to 
implies two specifications the 
the signal level must be balanced on the time axis 
the signal spectrum must be flattened 
transformed by filtering such as deconvolution to white noise signal 
signal where all frequencies are equally present 
example toss coin times and record how many times does the coin lands on head 
independent each outcome of landing will not affect the other outcome which means the results are independent from each other 
identically distributed if the coin is homogeneous material each time the probability for head is which means the probability is identical for each time 
example roll dice times and record how many time the result is independent each outcome of the dice will not affect the next one which means the results are independent from each other 
identically distributed if the dice is homogeneous material each time the probability for the number is which means the probability is identical for each time 
example choose card from standard deck of cards containing cards then place the card back in the deck 
repeat it for times 
record the number of king appears independent each outcome of the card will not affect the next one which means the results are independent from each other 
identically distributed after drawing one card from it each time the probability for king is which means the probability is identical for each time 
generalizations many results that were first proven under the assumption that the random variables are 
have been shown to be true even under weaker distributional assumption 
exchangeable random variables the most general notion which shares the main properties of 
variables are exchangeable random variables introduced by bruno de finetti 
exchangeability means that while variables may not be independent future ones behave like past ones formally any value of finite sequence is as likely as any permutation of those values the joint probability distribution is invariant under the symmetric group 
this provides useful generalization for example sampling without replacement is not independent but is exchangeable 
vy process in stochastic calculus 
variables are thought of as discrete time vy process each variable gives how much one changes from one time to another 
for example sequence of bernoulli trials is interpreted as the bernoulli process 
one may generalize this to include continuous time vy processes and many vy processes can be seen as limits of 
variables for instance the wiener process is the limit of the bernoulli process 
in machine learning why assume the data in machine learning are independent and identically distributed 
machine learning uses currently acquired massive quantities of data to deliver faster more accurate results 
therefore we need to use historical data with overall representativeness 
if the data obtained is not representative of the overall situation then the rules will be summarized badly or wrongly 
hypothesis the number of individual cases in the training sample can be greatly reduced 
this assumption makes maximization very easy to calculate mathematically 
observing the assumption of independent and identical distribution in mathematics simplifies the calculation of the likelihood function in optimization problems 
because of the assumption of independence the likelihood function can be written like this 
in order to maximize the probability of the observed event take the log function and maximize the parameter that is to say to compute log where log log log log 
log the computer is very efficient to calculate multiple additions but it is not efficient to calculate the multiplication 
this simplification is the core reason for the increase in computational efficiency 
and this log transformation is also in the process of maximizing turning many exponential functions into linear functions 
for two reasons this hypothesis is easy to use the central limit theorem in practical applications 
even if the sample comes from more complex non gaussian distribution it can also approximate well 
because it can be simplified from the central limit theorem to gaussian distribution 
for large number of observable samples the sum of many random variables will have an approximately normal distribution 
the second reason is that the accuracy of the model depends on the simplicity and representative power of the model unit as well as the data quality 
because the simplicity of the unit makes it easy to interpret and scale and the representative power scale out of the unit improves the model accuracy 
like in deep neural network each neuron is very simple but has strong representative power layer by layer to represent more complex features to improve model accuracy 
see also de finetti theorem pairwise independent variables central limit theorem references further reading
in mathematics the dimension of vector space is the cardinality the number of vectors of basis of over its base field 
it is sometimes called hamel dimension after georg hamel or algebraic dimension to distinguish it from other types of dimension 
for every vector space there exists basis and all bases of vector space have equal cardinality as result the dimension of vector space is uniquely defined 
we say is finite dimensional if the dimension of is finite and infinite dimensional if its dimension is infinite 
the dimension of the vector space over the field can be written as dim or as read dimension of over 
when can be inferred from context dim is typically written 
examples the vector space has as standard basis and therefore dim more generally dim and even more generally dim for any field the complex numbers are both real and complex vector space we have dim and dim so the dimension depends on the base field 
the only vector space with dimension is the vector space consisting only of its zero element 
properties if is linear subspace of then dim dim 
to show that two finite dimensional vector spaces are equal the following criterion can be used if is finite dimensional vector space and is linear subspace of with dim dim then the space has the standard basis where is the th column of the corresponding identity matrix 
therefore has dimension any two finite dimensional vector spaces over with the same dimension are isomorphic 
any bijective map between their bases can be uniquely extended to bijective linear map between the vector spaces 
if is some set vector space with dimension over can be constructed as follows take the set of all functions such that for all but finitely many in these functions can be added and multiplied with elements of to obtain the desired vector space 
an important result about dimensions is given by the rank nullity theorem for linear maps 
if is field extension then is in particular vector space over furthermore every vector space is also vector space 
the dimensions are related by the formula in particular every complex vector space of dimension is real vector space of dimension some formulae relate the dimension of vector space with the cardinality of the base field and the cardinality of the space itself 
if is vector space over field then and if the dimension of is denoted by dim then if dim is finite then dim if dim is infinite then max dim 
generalizations vector space can be seen as particular case of matroid and in the latter there is well defined notion of dimension 
the length of module and the rank of an abelian group both have several properties similar to the dimension of vector spaces 
the krull dimension of commutative ring named after wolfgang krull is defined to be the maximal number of strict inclusions in an increasing chain of prime ideals in the ring 
trace the dimension of vector space may alternatively be characterized as the trace of the identity operator 
for instance tr id tr this appears to be circular definition but it allows useful generalizations 
firstly it allows for definition of notion of dimension when one has trace but no natural sense of basis 
for example one may have an algebra with maps the inclusion of scalars called the unit and map corresponding to trace called the counit 
the composition is scalar being linear operator on dimensional space corresponds to trace of identity and gives notion of dimension for an abstract algebra 
in practice in bialgebras this map is required to be the identity which can be obtained by normalizing the counit by dividing by dimension tr so in these cases the normalizing constant corresponds to dimension 
alternatively it may be possible to take the trace of operators on an infinite dimensional space in this case finite trace is defined even though no finite dimension exists and gives notion of dimension of the operator 
these fall under the rubric of trace class operators on hilbert space or more generally nuclear operators on banach space 
subtler generalization is to consider the trace of family of operators as kind of twisted dimension 
this occurs significantly in representation theory where the character of representation is the trace of the representation hence scalar valued function on group whose value on the identity is the dimension of the representation as representation sends the identity in the group to the identity matrix tr dim the other values of the character can be viewed as twisted dimensions and find analogs or generalizations of statements about dimensions to statements about characters or representations 
sophisticated example of this occurs in the theory of monstrous moonshine the invariant is the graded dimension of an infinite dimensional graded representation of the monster group and replacing the dimension with the character gives the mckay thompson series for each element of the monster group 
see also fractal dimension ratio providing statistical index of complexity variation with scale krull dimension in mathematics dimension of ring matroid rank maximum size of an independent set of the matroid rank linear algebra dimension of the column space of matrix topological dimension also called lebesgue covering dimension notes references sources axler sheldon 
linear algebra done right 
undergraduate texts in mathematics rd ed 
external links mit linear algebra lecture on independence basis and dimension by gilbert strang at mit opencourseware
linear least squares lls is the least squares approximation of linear functions to data 
it is set of formulations for solving statistical problems involved in linear regression including variants for ordinary unweighted weighted and generalized correlated residuals 
numerical methods for linear least squares include inverting the matrix of the normal equations and orthogonal decomposition methods 
main formulations the three main linear least squares formulations are ordinary least squares ols is the most common estimator 
ols estimates are commonly used to analyze both experimental and observational data 
the ols method minimizes the sum of squared residuals and leads to closed form expression for the estimated value of the unknown parameter vector where is vector whose ith element is the ith observation of the dependent variable and is matrix whose ij element is the ith observation of the jth independent variable 
the estimator is unbiased and consistent if the errors have finite variance and are uncorrelated with the regressors where is the transpose of row of the matrix it is also efficient under the assumption that the errors have finite variance and are homoscedastic meaning that xi does not depend on the condition that the errors are uncorrelated with the regressors will generally be satisfied in an experiment but in the case of observational data it is difficult to exclude the possibility of an omitted covariate that is related to both the observed covariates and the response variable 
the existence of such covariate will generally lead to correlation between the regressors and the response variable and hence to an inconsistent estimator of the condition of homoscedasticity can fail with either experimental or observational data 
if the goal is either inference or predictive modeling the performance of ols estimates can be poor if multicollinearity is present unless the sample size is large 
weighted least squares wls are used when heteroscedasticity is present in the error terms of the model 
generalized least squares gls is an extension of the ols method that allows efficient estimation of when either heteroscedasticity or correlations or both are present among the error terms of the model as long as the form of heteroscedasticity and correlation is known independently of the data 
to handle heteroscedasticity when the error terms are uncorrelated with each other gls minimizes weighted analogue to the sum of squared residuals from ols regression where the weight for the ith case is inversely proportional to var 
this special case of gls is called weighted least squares 
the gls solution to an estimation problem is where is the covariance matrix of the errors 
gls can be viewed as applying linear transformation to the data so that the assumptions of ols are met for the transformed data 
for gls to be applied the covariance structure of the errors must be known up to multiplicative constant 
alternative formulations other formulations include iteratively reweighted least squares irls is used when heteroscedasticity or correlations or both are present among the error terms of the model but where little is known about the covariance structure of the errors independently of the data 
in the first iteration ols or gls with provisional covariance structure is carried out and the residuals are obtained from the fit 
based on the residuals an improved estimate of the covariance structure of the errors can usually be obtained 
subsequent gls iteration is then performed using this estimate of the error structure to define the weights 
the process can be iterated to convergence but in many cases only one iteration is sufficient to achieve an efficient estimate of instrumental variables regression iv can be performed when the regressors are correlated with the errors 
in this case we need the existence of some auxiliary instrumental variables zi such that zi if is the matrix of instruments then the estimator can be given in closed form as optimal instruments regression is an extension of classical iv regression to the situation where zi total least squares tls is an approach to least squares estimation of the linear regression model that treats the covariates and response variable in more geometrically symmetric manner than ols 
it is one approach to handling the errors in variables problem and is also sometimes used even when the covariates are assumed to be error free percentage least squares focuses on reducing percentage errors which is useful in the field of forecasting or time series analysis 
it is also useful in situations where the dependent variable has wide range without constant variance as here the larger residuals at the upper end of the range would dominate if ols were used 
when the percentage or relative error is normally distributed least squares percentage regression provides maximum likelihood estimates 
percentage regression is linked to multiplicative error model whereas ols is linked to models containing an additive error term constrained least squares indicates linear least squares problem with additional constraints on the solution 
objective function in ols assuming unweighted observations the optimal value of the objective function is found by substituting the optimal expression for the coefficient vector where the latter equality holding since is symmetric and idempotent 
it can be shown from this that under an appropriate assignment of weights the expected value of is if instead unit weights are assumed the expected value of is where is the variance of each observation 
if it is assumed that the residuals belong to normal distribution the objective function being sum of weighted squared residuals will belong to chi squared distribution with degrees of freedom 
some illustrative percentile values of are given in the following table 
these values can be used for statistical criterion as to the goodness of fit 
when unit weights are used the numbers should be divided by the variance of an observation 
for wls the ordinary objective function above is replaced for weighted average of residuals 
discussion in statistics and mathematics linear least squares is an approach to fitting mathematical or statistical model to data in cases where the idealized value provided by the model for any data point is expressed linearly in terms of the unknown parameters of the model 
the resulting fitted model can be used to summarize the data to predict unobserved values from the same system and to understand the mechanisms that may underlie the system 
mathematically linear least squares is the problem of approximately solving an overdetermined system of linear equations where is not an element of the column space of the matrix the approximate solution is realized as an exact solution to where is the projection of onto the column space of the best approximation is then that which minimizes the sum of squared differences between the data values and their corresponding modeled values 
the approach is called linear least squares since the assumed function is linear in the parameters to be estimated 
linear least squares problems are convex and have closed form solution that is unique provided that the number of data points used for fitting equals or exceeds the number of unknown parameters except in special degenerate situations 
in contrast non linear least squares problems generally must be solved by an iterative procedure and the problems can be non convex with multiple optima for the objective function 
if prior distributions are available then even an underdetermined system can be solved using the bayesian mmse estimator 
in statistics linear least squares problems correspond to particularly important type of statistical model called linear regression which arises as particular form of regression analysis 
one basic form of such model is an ordinary least squares model 
the present article concentrates on the mathematical aspects of linear least squares problems with discussion of the formulation and interpretation of statistical regression models and statistical inferences related to these being dealt with in the articles just mentioned 
see outline of regression analysis for an outline of the topic 
properties if the experimental errors are uncorrelated have mean of zero and constant variance the gauss markov theorem states that the least squares estimator has the minimum variance of all estimators that are linear combinations of the observations 
in this sense it is the best or optimal estimator of the parameters 
note particularly that this property is independent of the statistical distribution function of the errors 
in other words the distribution function of the errors need not be normal distribution 
however for some probability distributions there is no guarantee that the least squares solution is even possible given the observations still in such cases it is the best estimator that is both linear and unbiased 
for example it is easy to show that the arithmetic mean of set of measurements of quantity is the least squares estimator of the value of that quantity 
if the conditions of the gauss markov theorem apply the arithmetic mean is optimal whatever the distribution of errors of the measurements might be 
however in the case that the experimental errors do belong to normal distribution the least squares estimator is also maximum likelihood estimator these properties underpin the use of the method of least squares for all types of data fitting even when the assumptions are not strictly valid 
limitations an assumption underlying the treatment given above is that the independent variable is free of error 
in practice the errors on the measurements of the independent variable are usually much smaller than the errors on the dependent variable and can therefore be ignored 
when this is not the case total least squares or more generally errors in variables models or rigorous least squares should be used 
this can be done by adjusting the weighting scheme to take into account errors on both the dependent and independent variables and then following the standard procedure in some cases the weighted normal equations matrix xtx is ill conditioned 
when fitting polynomials the normal equations matrix is vandermonde matrix 
vandermonde matrices become increasingly ill conditioned as the order of the matrix increases 
in these cases the least squares estimate amplifies the measurement noise and may be grossly inaccurate 
various regularization techniques can be applied in such cases the most common of which is called ridge regression 
if further information about the parameters is known for example range of possible values of then various techniques can be used to increase the stability of the solution 
for example see constrained least squares 
another drawback of the least squares estimator is the fact that the norm of the residuals is minimized whereas in some cases one is truly interested in obtaining small error in the parameter small value of 
however since the true parameter is necessarily unknown this quantity cannot be directly minimized 
if prior probability on is known then bayes estimator can be used to minimize the mean squared error 
the least squares method is often applied when no prior is known 
surprisingly when several parameters are being estimated jointly better estimators can be constructed an effect known as stein phenomenon 
for example if the measurement error is gaussian several estimators are known which dominate or outperform the least squares technique the best known of these is the james stein estimator 
this is an example of more general shrinkage estimators that have been applied to regression problems 
applications polynomial fitting models are polynomials in an independent variable straight line quadratic cubic quartic and higher polynomials 
for regression with high order polynomials the use of orthogonal polynomials is recommended 
numerical smoothing and differentiation this is an application of polynomial fitting 
multinomials in more than one independent variable including surface fitting curve fitting with splines chemometrics calibration curve standard addition gran plot analysis of mixtures uses in data fitting the primary application of linear least squares is in data fitting 
given set of data points consisting of experimentally measured values taken at values of an independent variable may be scalar or vector quantities and given model function with it is desired to find the parameters such that the model function best fits the data 
in linear least squares linearity is meant to be with respect to parameters so here the functions may be nonlinear with respect to the variable ideally the model function fits the data exactly so for all this is usually not possible in practice as there are more data points than there are parameters to be determined 
the approach chosen then is to find the minimal possible value of the sum of squares of the residuals so to minimize the function after substituting for and then for this minimization problem becomes the quadratic minimization problem above with and the best fit can be found by solving the normal equations 
example as result of an experiment four data points were obtained and shown in red in the diagram on the right 
we hope to find line that best fits these four points 
in other words we would like to find the numbers and that approximately solve the overdetermined linear system of four equations in two unknowns in some best sense 
represents the residual at each point between the curve fit and the data the least squares approach to solving this problem is to try to make the sum of the squares of these residuals as small as possible that is to find the minimum of the function the minimum is determined by calculating the partial derivatives of with respect to and and setting them to zero this results in system of two equations in two unknowns called the normal equations which when solved give and the equation is the line of best fit 
the residuals that is the differences between the values from the observations and the predicated variables by using the line of best fit are then found to be and see the diagram on the right 
the minimum value of the sum of squares of the residuals is more generally one can have regressors and linear model using quadratic model importantly in linear least squares we are not restricted to using line as the model as in the above example 
for instance we could have chosen the restricted quadratic model this model is still linear in the parameter so we can still perform the same analysis constructing system of equations from the data points the partial derivatives with respect to the parameters this time there is only one are again computed and set to and solved leading to the resulting best fit model 
see also line line intersection nearest point to non intersecting lines an application line fitting nonlinear least squares regularized least squares simple linear regression partial least squares regression linear function references further reading bevington philip robinson keith 
data reduction and error analysis for the physical sciences 
external links least squares fitting from mathworld least squares fitting polynomial from mathworld
in statistics econometrics epidemiology and related disciplines the method of instrumental variables iv is used to estimate causal relationships when controlled experiments are not feasible or when treatment is not successfully delivered to every unit in randomized experiment 
intuitively ivs are used when an explanatory variable of interest is correlated with the error term in which case ordinary least squares and anova give biased results 
valid instrument induces changes in the explanatory variable but has no independent effect on the dependent variable allowing researcher to uncover the causal effect of the explanatory variable on the dependent variable 
instrumental variable methods allow for consistent estimation when the explanatory variables covariates are correlated with the error terms in regression model 
such correlation may occur when changes in the dependent variable change the value of at least one of the covariates reverse causation there are omitted variables that affect both the dependent and independent variables or the covariates are subject to non random measurement error explanatory variables that suffer from one or more of these issues in the context of regression are sometimes referred to as endogenous 
in this situation ordinary least squares produces biased and inconsistent estimates 
however if an instrument is available consistent estimates may still be obtained 
an instrument is variable that does not itself belong in the explanatory equation but is correlated with the endogenous explanatory variables conditionally on the value of other covariates 
in linear models there are two main requirements for using ivs the instrument must be correlated with the endogenous explanatory variables conditionally on the other covariates 
if this correlation is strong then the instrument is said to have strong first stage 
weak correlation may provide misleading inferences about parameter estimates and standard errors 
the instrument cannot be correlated with the error term in the explanatory equation conditionally on the other covariates 
in other words the instrument cannot suffer from the same problem as the original predicting variable 
if this condition is met then the instrument is said to satisfy the exclusion restriction 
history first use of an instrument variable occurred in book by philip wright best known for his excellent description of the production transport and sale of vegetable and animal oils in the early in the united states while in olav reiers applied the same approach in the context of errors in variables models in his dissertation giving the method its name wright attempted to determine the supply and demand for butter using panel data on prices and quantities sold in the united states 
the idea was that regression analysis could produce demand or supply curve because they are formed by the path between prices and quantities demanded or supplied 
the problem was that the observational data did not form demand or supply curve as such but rather cloud of point observations that took different shapes under varying market conditions 
it seemed that making deductions from the data remained elusive 
the problem was that price affected both supply and demand so that function describing only one of the two could not be constructed directly from the observational data 
wright correctly concluded that he needed variable that correlated with either demand or supply but not both that is an instrumental variable 
after much deliberation wright decided to use regional rainfall as his instrumental variable he concluded that rainfall affected grass production and hence milk production and ultimately butter supply but not butter demand 
in this way he was able to construct regression equation with only the instrumental variable of price and supply 
theory while the ideas behind iv extend to broad class of models very common context for iv is in linear regression 
traditionally an instrumental variable is defined as variable that is correlated with the independent variable and uncorrelated with the error term in the linear equation is vector 
is matrix usually with column of ones and perhaps with additional columns for other covariates 
consider how an instrument allows to be recovered 
recall that ols solves for such that cov when we minimize the sum of squared errors min the first order condition is exactly 
if the true model is believed to have cov due to any of the reasons listed above for example if there is an omitted variable which affects both and separately then this ols procedure will not yield the causal impact of on ols will simply pick the parameter that makes the resulting errors appear uncorrelated with consider for simplicity the single variable case 
suppose we are considering regression with one variable and constant perhaps no other covariates are necessary or perhaps we have partialed out any other relevant covariates in this case the coefficient on the regressor of interest is given by cov var 
substituting for gives cov var cov var cov var cov var cov var where is what the estimated coefficient vector would be if were not correlated with in this case it can be shown that is an unbiased estimator of if cov in the underlying model that we believe then ols gives coefficient which does not reflect the underlying causal effect of interest 
iv helps to fix this problem by identifying the parameters not based on whether is uncorrelated with but based on whether another variable is uncorrelated with if theory suggests that is related to the first stage but uncorrelated with the exclusion restriction then iv may identify the causal parameter of interest where ols fails 
because there are multiple specific ways of using and deriving iv estimators even in just the linear case iv sls gmm we save further discussion for the estimation section below 
example informally in attempting to estimate the causal effect of some variable on another an instrument is third variable which affects only through its effect on for example suppose researcher wishes to estimate the causal effect of smoking on general health 
correlation between health and smoking does not imply that smoking causes poor health because other variables such as depression may affect both health and smoking or because health may affect smoking 
it is at best difficult and expensive to conduct controlled experiments on smoking status in the general population 
the researcher may attempt to estimate the causal effect of smoking on health from observational data by using the tax rate for tobacco products as an instrument for smoking 
the tax rate for tobacco products is reasonable choice for an instrument because the researcher assumes that it can only be correlated with health through its effect on smoking 
if the researcher then finds tobacco taxes and state of health to be correlated this may be viewed as evidence that smoking causes changes in health 
angrist and krueger present survey of the history and uses of instrumental variable techniques 
graphical definition of course iv techniques have been developed among much broader class of non linear models 
general definitions of instrumental variables using counterfactual and graphical formalism were given by pearl 
the graphical definition requires that satisfy the following conditions where stands for separation and stands for the graph in which all arrows entering are cut off 
the counterfactual definition requires that satisfies where yx stands for the value that would attain had been and stands for independence 
if there are additional covariates then the above definitions are modified so that qualifies as an instrument if the given criteria hold conditional on the essence of pearl definition is the equations of interest are structural not regression 
the error term stands for all exogenous factors that affect when is held constant 
the instrument should be independent of the instrument should not affect when is held constant exclusion restriction 
the instrument should not be independent of these conditions do not rely on specific functional form of the equations and are applicable therefore to nonlinear equations where can be non additive see non parametric analysis 
they are also applicable to system of multiple equations in which and other factors affect through several intermediate variables 
an instrumental variable need not be cause of proxy of such cause may also be used if it satisfies conditions 
the exclusion restriction condition is redundant it follows from conditions and 
selecting suitable instruments since is unobserved the requirement that be independent of cannot be inferred from data and must instead be determined from the model structure the data generating process 
causal graphs are representation of this structure and the graphical definition given above can be used to quickly determine whether variable qualifies as an instrumental variable given set of covariates to see how consider the following example 
suppose that we wish to estimate the effect of university tutoring program on grade point average gpa 
the relationship between attending the tutoring program and gpa may be confounded by number of factors 
students who attend the tutoring program may care more about their grades or may be struggling with their work 
this confounding is depicted in the figures on the right through the bidirected arc between tutoring program and gpa 
if students are assigned to dormitories at random the proximity of the student dorm to the tutoring program is natural candidate for being an instrumental variable 
however what if the tutoring program is located in the college library 
in that case proximity may also cause students to spend more time at the library which in turn improves their gpa see figure 
using the causal graph depicted in the figure we see that proximity does not qualify as an instrumental variable because it is connected to gpa through the path proximity library hours gpa in 
however if we control for library hours by adding it as covariate then proximity becomes an instrumental variable since proximity is separated from gpa given library hours in 
now suppose that we notice that student natural ability affects his or her number of hours in the library as well as his or her gpa as in figure using the causal graph we see that library hours is collider and conditioning on it opens the path proximity library hours gpa 
as result proximity cannot be used as an instrumental variable 
finally suppose that library hours does not actually affect gpa because students who do not study in the library simply study elsewhere as in figure in this case controlling for library hours still opens spurious path from proximity to gpa 
however if we do not control for library hours and remove it as covariate then proximity can again be used an instrumental variable 
estimation we now revisit and expand upon the mechanics of iv in greater detail 
suppose the data are generated by process of the form where indexes observations is the th value of the dependent variable is vector of the th values of the independent variable and constant is the th value of an unobserved error term representing all causes of other than and is an unobserved parameter vector the parameter vector is the causal effect on of one unit change in each element of holding all other causes of constant 
the econometric goal is to estimate for simplicity sake assume the draws of are uncorrelated and that they are drawn from distributions with the same variance that is that the errors are serially uncorrelated and homoskedastic 
suppose also that regression model of nominally the same form is proposed 
given random sample of observations from this process the ordinary least squares estimator is where and denote column vectors of length this equation is similar to the equation involving cov in the introduction this is the matrix version of that equation 
when and are uncorrelated under certain regularity conditions the second term has an expected value conditional on of zero and converges to zero in the limit so the estimator is unbiased and consistent 
when and the other unmeasured causal variables collapsed into the term are correlated however the ols estimator is generally biased and inconsistent for in this case it is valid to use the estimates to predict values of given values of but the estimate does not recover the causal effect of on to recover the underlying parameter we introduce set of variables that is highly correlated with each endogenous component of but in our underlying model is not correlated with for simplicity one might consider to be matrix composed of column of constants and one endogenous variable and to be consisting of column of constants and one instrumental variable 
however this technique generalizes to being matrix of constant and say endogenous variables with being matrix composed of constant and instruments 
in the discussion that follows we will assume that is matrix and leave this value unspecified 
an estimator in which and are both matrices is referred to as just identified 
suppose that the relationship between each endogenous component xi and the instruments is given by the most common iv specification uses the following estimator this specification approaches the true parameter as the sample gets large so long as in the true model as long as in the underlying process which generates the data the appropriate use of the iv estimator will identify this parameter 
this works because iv solves for the unique parameter that satisfies and therefore hones in on the true underlying parameter as the sample size grows 
now an extension suppose that there are more instruments than there are covariates in the equation of interest so that is matrix with this is often called the over identified case 
in this case the generalized method of moments gmm can be used 
the gmm iv estimator is where refers to the projection matrix this expression collapses to the first when the number of instruments is equal to the number of covariates in the equation of interest 
the over identified iv is therefore generalization of the just identified iv 
there is an equivalent under identified estimator for the case where since the parameters are the solutions to set of linear equations an under identified model using the set of equations does not have unique solution 
interpretation as two stage least squares one computational method which can be used to calculate iv estimates is two stage least squares sls or tsls 
in the first stage each explanatory variable that is an endogenous covariate in the equation of interest is regressed on all of the exogenous variables in the model including both exogenous covariates in the equation of interest and the excluded instruments 
the predicted values from these regressions are obtained stage regress each column of on errors and save the predicted values in the second stage the regression of interest is estimated as usual except that in this stage each endogenous covariate is replaced with the predicted values from the first stage stage regress on the predicted values from the first stage which gives sls this method is only valid in linear models 
for categorical endogenous covariates one might be tempted to use different first stage than ordinary least squares such as probit model for the first stage followed by ols for the second 
this is commonly known in the econometric literature as the forbidden regression because second stage iv parameter estimates are consistent only in special cases 
the resulting estimator of is numerically identical to the expression displayed above 
small correction must be made to the sum of squared residuals in the second stage fitted model in order that the covariance matrix of is calculated correctly 
non parametric analysis when the form of the structural equations is unknown an instrumental variable can still be defined through the equations where and are two arbitrary functions and is independent of unlike linear models however measurements of and do not allow for the identification of the average causal effect of on denoted ace ace pr do 
balke and pearl derived tight bounds on ace and showed that these can provide valuable information on the sign and size of ace in linear analysis there is no test to falsify the assumption the is instrumental relative to the pair 
this is not the case when is discrete 
pearl has shown that for all and the following constraint called instrumental inequality must hold whenever satisfies the two equations above max max pr 
interpretation under treatment effect heterogeneity the exposition above assumes that the causal effect of interest does not vary across observations that is that is constant 
generally different subjects will respond in different ways to changes in the treatment when this possibility is recognized the average effect in the population of change in on may differ from the effect in given subpopulation 
for example the average effect of job training program may substantially differ across the group of people who actually receive the training and the group which chooses not to receive training 
for these reasons iv methods invoke implicit assumptions on behavioral response or more generally assumptions over the correlation between the response to treatment and propensity to receive treatment the standard iv estimator can recover local average treatment effects late rather than average treatment effects ate 
imbens and angrist demonstrate that the linear iv estimate can be interpreted under weak conditions as weighted average of local average treatment effects where the weights depend on the elasticity of the endogenous regressor to changes in the instrumental variables 
roughly that means that the effect of variable is only revealed for the subpopulations affected by the observed changes in the instruments and that subpopulations which respond most to changes in the instruments will have the largest effects on the magnitude of the iv estimate 
for example if researcher uses presence of land grant college as an instrument for college education in an earnings regression she identifies the effect of college on earnings in the subpopulation which would obtain college degree if college is present but which would not obtain degree if college is not present 
this empirical approach does not without further assumptions tell the researcher anything about the effect of college among people who would either always or never get college degree regardless of whether local college exists 
weak instruments problem as bound jaeger and baker note problem is caused by the selection of weak instruments instruments that are poor predictors of the endogenous question predictor in the first stage equation 
in this case the prediction of the question predictor by the instrument will be poor and the predicted values will have very little variation 
consequently they are unlikely to have much success in predicting the ultimate outcome when they are used to replace the question predictor in the second stage equation 
in the context of the smoking and health example discussed above tobacco taxes are weak instruments for smoking if smoking status is largely unresponsive to changes in taxes 
if higher taxes do not induce people to quit smoking or not start smoking then variation in tax rates tells us nothing about the effect of smoking on health 
if taxes affect health through channels other than through their effect on smoking then the instruments are invalid and the instrumental variables approach may yield misleading results 
for example places and times with relatively health conscious populations may both implement high tobacco taxes and exhibit better health even holding smoking rates constant so we would observe correlation between health and tobacco taxes even if it were the case that smoking has no effect on health 
in this case we would be mistaken to infer causal effect of smoking on health from the observed correlation between tobacco taxes and health 
testing for weak instruments the strength of the instruments can be directly assessed because both the endogenous covariates and the instruments are observable 
common rule of thumb for models with one endogenous regressor is the statistic against the null that the excluded instruments are irrelevant in the first stage regression should be larger than 
statistical inference and hypothesis testing when the covariates are exogenous the small sample properties of the ols estimator can be derived in straightforward manner by calculating moments of the estimator conditional on when some of the covariates are endogenous so that instrumental variables estimation is implemented simple expressions for the moments of the estimator cannot be so obtained 
generally instrumental variables estimators only have desirable asymptotic not finite sample properties and inference is based on asymptotic approximations to the sampling distribution of the estimator 
even when the instruments are uncorrelated with the error in the equation of interest and when the instruments are not weak the finite sample properties of the instrumental variables estimator may be poor 
for example exactly identified models produce finite sample estimators with no moments so the estimator can be said to be neither biased nor unbiased the nominal size of test statistics may be substantially distorted and the estimates may commonly be far away from the true value of the parameter 
testing the exclusion restriction the assumption that the instruments are not correlated with the error term in the equation of interest is not testable in exactly identified models 
if the model is overidentified there is information available which may be used to test this assumption 
the most common test of these overidentifying restrictions called the sargan hansen test is based on the observation that the residuals should be uncorrelated with the set of exogenous variables if the instruments are truly exogenous 
the sargan hansen test statistic can be calculated as the number of observations multiplied by the coefficient of determination from the ols regression of the residuals onto the set of exogenous variables 
this statistic will be asymptotically chi squared with degrees of freedom under the null that the error term is uncorrelated with the instruments 
see also control function econometrics optimal instruments references further reading greene william 
econometric analysis sixth ed 
upper saddle river pearson prentice hall 
isbn gujarati damodar porter dawn 
basic econometrics fifth ed 
new york mcgraw hill irwin 
lectures on advanced econometric theory 
introductory econometrics modern approach fifth international ed 
mason oh south western 
quasi likelihood methods for count data handbook of applied econometrics volume ed 
pesaran and schmidt oxford blackwell pp 
terza estimating count models with endogenous switching sample selection and endogenous treatment effects 
journal of econometrics pp 
econometric analysis of cross section and panel data mit press cambridge massachusetts 
external links chapter from daniel mcfadden textbook econometrics lecture topic instrumental variable on youtube by mark thoma 
econometrics lecture topic two stages least square on youtube by mark thoma
in music closely related key or close key is one sharing many common tones with an original key as opposed to distantly related key or distant key 
in music harmony there are six of them five share all or all except one pitches with key with which it is being compared and is adjacent to it on the circle of fifths and its relative major or minor and one shares the same tonic 
such keys are the most commonly used destinations or transpositions in modulation because of their strong structural links with the home key 
distant keys may be reached sequentially through closely related keys by chain modulation for example to to for example one principle that every composer of haydn day classical music era kept in mind was over all unity of tonality 
no piece dared wander too far from its tonic key and no piece in four movement form dared to present tonality not closely related to the key of the whole series 
for example the first movement of mozart piano sonata no 
modulates only to closely related keys the dominant supertonic and submediant given major key tonic the related keys are ii supertonic the relative minor of the subdominant iii mediant the relative minor of the dominant iv subdominant one less sharp or one more flat around circle of fifths dominant one more sharp or one fewer flat around circle of fifths vi submediant or relative minor different tonic same key signature parallel minor same tonic different key signature specifically starting from minor key the closely related keys are the mediant or relative major iii the subdominant iv the minor dominant the submediant vi the subtonic vii and the parallel major 
in the key of minor when we translate them to keys we get major minor minor major major majoranother view of closely related keys is that there are six closely related keys based on the tonic and the remaining triads of the diatonic scale excluding the dissonant diminished triads 
four of the five differ by one accidental one has the same key signature and one uses the parallel modal form 
in the key of major these would be minor minor major major minor and minor 
despite being three sharps or flats away from the original key in the circle of fifths parallel keys are also considered as closely related keys as the tonal center is the same and this makes this key have an affinity with the original key 
in modern music the closeness of relation between any two keys or sets of pitches may be determined by the number of tones they share in common which allows one to consider modulations not occurring in standard major minor tonality 
for example in music based on the pentatonic scale containing pitches and modulating fifth higher gives the collection of pitches and having four of five tones in common 
however modulating up tritone would produce which shares no common tones with the original scale 
thus the scale fifth higher is very closely related while the scale tritone higher is not 
other modulations may be placed in order from closest to most distant depending upon the number of common tones 
another view in modern music notably in bart common tonic produces closely related keys the other scales being the six other modes 
this usage can be found in several of the mikrokosmos piano pieces 
when modulation causes the new key to traverse the bottom of the circle of fifths this may give rise to theoretical key containing eight or more sharps or flats in its notated key signature in such case notational conventions require recasting the new section in its enharmonically equivalent key 
andranik tangian suggests and visualizations of key chord proximity for both all major and all minor keys chords by locating them along single subdominant dominant axis which wraps torus that is then unfolded 
see also chromatic mediant common chord music monotonality parallel and counter parallel pitch space references further reading howard hanson harmonic materials of modern music 
appleton century crofts inc
public key cryptography or asymmetric cryptography is the field of cryptographic systems that use pairs of related keys
each key pair consists of public key and corresponding private key
key pairs are generated with cryptographic algorithms based on mathematical problems termed one way functions
security of public key cryptography depends on keeping the private key secret the public key can be openly distributed without compromising security in public key encryption system anyone with public key can encrypt message yielding ciphertext but only those who know the corresponding private key can decrypt the ciphertext to obtain the original message for example journalist can publish the public key of an encryption key pair on web site so that sources can send secret messages to the news organization in ciphertext
only the journalist who knows the corresponding private key can decrypt the ciphertexts to obtain the sources messages an eavesdropper reading email on its way to the journalist can decrypt the ciphertexts
however public key encryption doesn conceal metadata like what computer source used to send message when they sent it or how long it is
public key encryption on its own also doesn tell the recipient anything about who sent message it just conceals the content of message in ciphertext that can only be decrypted with the private key
in digital signature system sender can use private key together with message to create signature
anyone with the corresponding public key can verify whether the signature matches the message but forger who doesn know the private key can find any message signature pair that will pass verification with the public key for example software publisher can create signature key pair and include the public key in software installed on computers
later the publisher can distribute an update to the software signed using the private key and any computer receiving an update can confirm it is genuine by verifying the signature using the public key
as long as the software publisher keeps the private key secret even if forger can distribute malicious updates to computers they can convince the computers that any malicious updates are genuine
public key algorithms are fundamental security primitives in modern cryptosystems including applications and protocols which offer assurance of the confidentiality authenticity and non repudiability of electronic communications and data storage
they underpin numerous internet standards such as transport layer security tls ssh mime and pgp
some public key algorithms provide key distribution and secrecy diffie hellman key exchange some provide digital signatures digital signature algorithm and some provide both rsa
compared to symmetric encryption asymmetric encryption is rather slower than good symmetric encryption too slow for many purposes
today cryptosystems such as tls secure shell use both symmetric encryption and asymmetric encryption often by using asymmetric encryption to securely exchange secret key which is then used for symmetric encryption
description before the mid all cipher systems used symmetric key algorithms in which the same cryptographic key is used with the underlying algorithm by both the sender and the recipient who must both keep it secret
of necessity the key in every such system had to be exchanged between the communicating parties in some secure way prior to any use of the system for instance via secure channel
this requirement is never trivial and very rapidly becomes unmanageable as the number of participants increases or when secure channels aren available or when as is sensible cryptographic practice keys are frequently changed
in particular if messages are meant to be secure from other users separate key is required for each possible pair of users
by contrast in public key system the public keys can be disseminated widely and openly and only the corresponding private keys need be kept secret by its owner
two of the best known uses of public key cryptography are public key encryption in which message is encrypted with the intended recipient public key
for properly chosen and used algorithms messages cannot in practice be decrypted by anyone who does not possess the matching private key who is thus presumed to be the owner of that key and so the person associated with the public key
this can be used to ensure confidentiality of message
digital signatures in which message is signed with the sender private key and can be verified by anyone who has access to the sender public key
this verification proves that the sender had access to the private key and therefore is very likely to be the person associated with the public key
it also proves that the signature was prepared for that exact message since verification will fail for any other message one could devise without using the private key one important issue is confidence proof that particular public key is authentic
that it is correct and belongs to the person or entity claimed and has not been tampered with or replaced by some perhaps malicious third party
there are several possible approaches including public key infrastructure pki in which one or more third parties known as certificate authorities certify ownership of key pairs
tls relies upon this
this implies that the pki system software hardware and management is trust able by all involved
web of trust which decentralizes authentication by using individual endorsements of links between user and the public key belonging to that user
pgp uses this approach in addition to lookup in the domain name system dns
the dkim system for digitally signing emails also uses this approach
applications the most obvious application of public key encryption system is for encrypting communication to provide confidentiality message that sender encrypts using the recipient public key which can be decrypted only by the recipient paired private key
another application in public key cryptography is the digital signature
digital signature schemes can be used for sender authentication
non repudiation systems use digital signatures to ensure that one party cannot successfully dispute its authorship of document or communication
further applications built on this foundation include digital cash password authenticated key agreement time stamping services and non repudiation protocols
hybrid cryptosystems because asymmetric key algorithms are nearly always much more computationally intensive than symmetric ones it is common to use public private asymmetric key exchange algorithm to encrypt and exchange symmetric key which is then used by symmetric key cryptography to transmit data using the now shared symmetric key for symmetric key encryption algorithm
pgp ssh and the ssl tls family of schemes use this procedure they are thus called hybrid cryptosystems
the initial asymmetric cryptography based key exchange to share server generated symmetric key from the server to client has the advantage of not requiring that symmetric key be pre shared manually such as on printed paper or discs transported by courier while providing the higher data throughput of symmetric key cryptography over asymmetric key cryptography for the remainder of the shared connection
weaknesses as with all security related systems it is important to identify potential weaknesses
aside from poor choice of an asymmetric key algorithm there are few which are widely regarded as satisfactory or too short key length the chief security risk is that the private key of pair becomes known
all security of messages authentication etc will then be lost
algorithms all public key schemes are in theory susceptible to brute force key search attack
however such an attack is impractical if the amount of computation needed to succeed termed the work factor by claude shannon is out of reach of all potential attackers
in many cases the work factor can be increased by simply choosing longer key
but other algorithms may inherently have much lower work factors making resistance to brute force attack from longer keys irrelevant
some special and specific algorithms have been developed to aid in attacking some public key encryption algorithms both rsa and elgamal encryption have known attacks that are much faster than the brute force approach
none of these are sufficiently improved to be actually practical however
major weaknesses have been found for several formerly promising asymmetric key algorithms
the knapsack packing algorithm was found to be insecure after the development of new attack
as with all cryptographic functions public key implementations may be vulnerable to side channel attacks that exploit information leakage to simplify the search for secret key
these are often independent of the algorithm being used
research is underway to both discover and to protect against new attacks
alteration of public keys another potential security vulnerability in using asymmetric keys is the possibility of man in the middle attack in which the communication of public keys is intercepted by third party the man in the middle and then modified to provide different public keys instead
encrypted messages and responses must in all instances be intercepted decrypted and re encrypted by the attacker using the correct public keys for the different communication segments so as to avoid suspicion
communication is said to be insecure where data is transmitted in manner that allows for interception also called sniffing
these terms refer to reading the sender private data in its entirety
communication is particularly unsafe when interceptions can be prevented or monitored by the sender man in the middle attack can be difficult to implement due to the complexities of modern security protocols
however the task becomes simpler when sender is using insecure media such as public networks the internet or wireless communication
in these cases an attacker can compromise the communications infrastructure rather than the data itself
hypothetical malicious staff member at an internet service provider isp might find man in the middle attack relatively straightforward
capturing the public key would only require searching for the key as it gets sent through the isp communications hardware in properly implemented asymmetric key schemes this is not significant risk
in some advanced man in the middle attacks one side of the communication will see the original data while the other will receive malicious variant
asymmetric man in the middle attacks can prevent users from realizing their connection is compromised
this remains so even when one user data is known to be compromised because the data appears fine to the other user
this can lead to confusing disagreements between users such as it must be on your end
when neither user is at fault
hence man in the middle attacks are only fully preventable when the communications infrastructure is physically controlled by one or both parties such as via wired route inside the sender own building
in summation public keys are easier to alter when the communications hardware used by sender is controlled by an attacker
public key infrastructure one approach to prevent such attacks involves the use of public key infrastructure pki set of roles policies and procedures needed to create manage distribute use store and revoke digital certificates and manage public key encryption
however this has potential weaknesses
for example the certificate authority issuing the certificate must be trusted by all participating parties to have properly checked the identity of the key holder to have ensured the correctness of the public key when it issues certificate to be secure from computer piracy and to have made arrangements with all participants to check all their certificates before protected communications can begin
web browsers for instance are supplied with long list of self signed identity certificates from pki providers these are used to check the bona fides of the certificate authority and then in second step the certificates of potential communicators
an attacker who could subvert one of those certificate authorities into issuing certificate for bogus public key could then mount man in the middle attack as easily as if the certificate scheme were not used at all
in an alternative scenario rarely discussed an attacker who penetrates an authority servers and obtains its store of certificates and keys public and private would be able to spoof masquerade decrypt and forge transactions without limit
despite its theoretical and potential problems this approach is widely used
examples include tls and its predecessor ssl which are commonly used to provide security for web browser transactions for example to securely send credit card details to an online store
aside from the resistance to attack of particular key pair the security of the certification hierarchy must be considered when deploying public key systems
some certificate authority usually purpose built program running on server computer vouches for the identities assigned to specific private keys by producing digital certificate
public key digital certificates are typically valid for several years at time so the associated private keys must be held securely over that time
when private key used for certificate creation higher in the pki server hierarchy is compromised or accidentally disclosed then man in the middle attack is possible making any subordinate certificate wholly insecure
examples examples of well regarded asymmetric key techniques for varied purposes include diffie hellman key exchange protocol dss digital signature standard which incorporates the digital signature algorithm elgamal elliptic curve cryptography elliptic curve digital signature algorithm ecdsa elliptic curve diffie hellman ecdh ed and ed eddsa and ecdh eddh various password authenticated key agreement techniques paillier cryptosystem rsa encryption algorithm pkcs cramer shoup cryptosystem yak authenticated key agreement protocolexamples of asymmetric key algorithms not yet widely adopted include ntruencrypt cryptosystem kyber mceliece cryptosystemexamples of notable yet insecure asymmetric key algorithms include merkle hellman knapsack cryptosystemexamples of protocols using asymmetric key algorithms include mime gpg an implementation of openpgp and an internet standard emv emv certificate authority ipsec pgp zrtp secure voip protocol transport layer security standardized by ietf and its predecessor secure socket layer silc ssh bitcoin off the record messaging history during the early history of cryptography two parties would rely upon key that they would exchange by means of secure but non cryptographic method such as face to face meeting or trusted courier
this key which both parties must then keep absolutely secret could then be used to exchange encrypted messages
number of significant practical difficulties arise with this approach to distributing keys
anticipation in his book the principles of science william stanley jevons wrote can the reader say what two numbers multiplied together will produce the number
think it unlikely that anyone but myself will ever know
here he described the relationship of one way functions to cryptography and went on to discuss specifically the factorization problem used to create trapdoor function
in july mathematician solomon golomb said jevons anticipated key feature of the rsa algorithm for public key cryptography although he certainly did not invent the concept of public key cryptography
classified discovery in james ellis british cryptographer at the uk government communications headquarters gchq conceived of the possibility of non secret encryption now called public key cryptography but could see no way to implement it
in his colleague clifford cocks implemented what has become known as the rsa encryption algorithm giving practical method of non secret encryption and in another gchq mathematician and cryptographer malcolm williamson developed what is now known as diffie hellman key exchange
the scheme was also passed to the us national security agency
both organisations had military focus and only limited computing power was available in any case the potential of public key cryptography remained unrealised by either organization judged it most important for military use if you can share your key rapidly and electronically you have major advantage over your opponent
only at the end of the evolution from berners lee designing an open internet architecture for cern its adaptation and adoption for the arpanet did public key cryptography realise its full potential
ralph benjamin these discoveries were not publicly acknowledged for years until the research was declassified by the british government in
public discovery in an asymmetric key cryptosystem was published by whitfield diffie and martin hellman who influenced by ralph merkle work on public key distribution disclosed method of public key agreement
this method of key exchange which uses exponentiation in finite field came to be known as diffie hellman key exchange
this was the first published practical method for establishing shared secret key over an authenticated but not confidential communications channel without using prior shared secret
merkle public key agreement technique became known as merkle puzzles and was invented in and only published in this makes asymmetric encryption rather new field in cryptography although cryptography itself dates back more than years in generalization of cocks scheme was independently invented by ron rivest adi shamir and leonard adleman all then at mit
the latter authors published their work in in martin gardner scientific american column and the algorithm came to be known as rsa from their initials
rsa uses exponentiation modulo product of two very large primes to encrypt and decrypt performing both public key encryption and public key digital signatures
its security is connected to the extreme difficulty of factoring large integers problem for which there is no known efficient general technique though prime factorization may be obtained through brute force attacks this grows much more difficult the larger the prime factors are
description of the algorithm was published in the mathematical games column in the august issue of scientific american since the large number and variety of encryption digital signature key agreement and other techniques have been developed including the rabin cryptosystem elgamal encryption dsa and elliptic curve cryptography
see also notes references external links oral history interview with martin hellman charles babbage institute university of minnesota
leading cryptography scholar martin hellman discusses the circumstances and fundamental insights of his invention of public key cryptography with collaborators whitfield diffie and ralph merkle at stanford university in the mid
an account of how gchq kept their invention of pke secret until
in statistics ordinary least squares ols is type of linear least squares method for choosing the unknown parameters in linear regression model with fixed level one effects of linear function of set of explanatory variables by the principle of least squares minimizing the sum of the squares of the differences between the observed dependent variable values of the variable being observed in the input dataset and the output of the linear function of the independent variable 
geometrically this is seen as the sum of the squared distances parallel to the axis of the dependent variable between each data point in the set and the corresponding point on the regression surface the smaller the differences the better the model fits the data 
the resulting estimator can be expressed by simple formula especially in the case of simple linear regression in which there is single regressor on the right side of the regression equation 
the ols estimator is consistent for the level one fixed effects when the regressors are exogenous and forms perfect colinearity rank condition consistent for the variance estimate of the residuals when regressors have finite fourth moments and by the gauss markov theorem optimal in the class of linear unbiased estimators when the errors are homoscedastic and serially uncorrelated 
under these conditions the method of ols provides minimum variance mean unbiased estimation when the errors have finite variances 
under the additional assumption that the errors are normally distributed with zero mean ols is the maximum likelihood estimator that outperforms any non linear unbiased estimator 
linear model suppose the data consists of observations each observation includes scalar response and column vector of parameters regressors in linear regression model the response variable is linear function of the regressors or in vector form where as introduced previously is column vector of the th observation of all the explanatory variables is vector of unknown parameters and the scalar represents unobserved random variables errors of the th observation 
accounts for the influences upon the responses from sources other than the explanators this model can also be written in matrix notation as where and are vectors of the response variables and the errors of the observations and is an matrix of regressors also sometimes called the design matrix whose row is and contains the th observations on all the explanatory variables 
as rule the constant term is always included in the set of regressors say by taking for all the coefficient corresponding to this regressor is called the intercept 
regressors do not have to be independent there can be any desired relationship between the regressors so long as it is not linear relationship 
for instance we might suspect the response depends linearly both on value and its square in which case we would include one regressor whose value is just the square of another regressor 
in that case the model would be quadratic in the second regressor but none the less is still considered linear model because the model is still linear in the parameters 
matrix vector formulation consider an overdetermined system of linear equations in unknown coefficients with note for linear model as above not all elements in contains information on the data points 
the first column is populated with ones only the other columns contain actual data 
so here is equal to the number of regressors plus one 
this can be written in matrix form as where 
such system usually has no exact solution so the goal is instead to find the coefficients which fit the equations best in the sense of solving the quadratic minimization problem where the objective function is given by justification for choosing this criterion is given in properties below 
this minimization problem has unique solution provided that the columns of the matrix are linearly independent given by solving the so called normal equations the matrix is known as the normal matrix or gram matrix and the matrix is known as the moment matrix of regressand by regressors 
finally is the coefficient vector of the least squares hyperplane expressed as or 
estimation suppose is candidate value for the parameter vector the quantity yi xitb called the residual for the th observation measures the vertical distance between the data point xi yi and the hyperplane xtb and thus assesses the degree of fit between the actual data and the model 
the sum of squared residuals ssr also called the error sum of squares ess or residual sum of squares rss is measure of the overall model fit where denotes the matrix transpose and the rows of denoting the values of all the independent variables associated with particular value of the dependent variable are xi xit 
the value of which minimizes this sum is called the ols estimator for the function is quadratic in with positive definite hessian and therefore this function possesses unique global minimum at which can be given by the explicit formula proof argmin the product xt is gram matrix and its inverse is the cofactor matrix of closely related to its covariance matrix 
the matrix xt xt xt is called the moore penrose pseudoinverse matrix of this formulation highlights the point that estimation can be carried out if and only if there is no perfect multicollinearity between the explanatory variables which would cause the gram matrix to have no inverse 
after we have estimated the fitted values or predicted values from the regression will be where xtx xt is the projection matrix onto the space spanned by the columns of this matrix is also sometimes called the hat matrix because it puts hat onto the variable another matrix closely related to is the annihilator matrix in this is projection matrix onto the space orthogonal to both matrices and are symmetric and idempotent meaning that and and relate to the data matrix via identities px and mx matrix creates the residuals from the regression using these residuals we can estimate the value of using the reduced chi squared statistic the denominator is the statistical degrees of freedom 
the first quantity is the ols estimate for whereas the second is the mle estimate for 
the two estimators are quite similar in large samples the first estimator is always unbiased while the second estimator is biased but has smaller mean squared error 
in practice is used more often since it is more convenient for the hypothesis testing 
the square root of is called the regression standard error standard error of the regression or standard error of the equation it is common to assess the goodness of fit of the ols regression by comparing how much the initial variation in the sample can be reduced by regressing onto the coefficient of determination is defined as ratio of explained variance to the total variance of the dependent variable in the cases where the regression sum of squares equals the sum of squares of residuals where tss is the total sum of squares for the dependent variable and is an matrix of ones 
is centering matrix which is equivalent to regression on constant it simply subtracts the mean from variable 
in order for to be meaningful the matrix of data on regressors must contain column vector of ones to represent the constant whose coefficient is the regression intercept 
in that case will always be number between and with values close to indicating good degree of fit 
the variance in the prediction of the independent variable as function of the dependent variable is given in the article polynomial least squares 
simple linear regression model if the data matrix contains only two variables constant and scalar regressor xi then this is called the simple regression model 
this case is often considered in the beginner statistics classes as it provides much simpler formulas even suitable for manual calculation 
the parameters are commonly denoted as the least squares estimates in this case are given by simple formulas alternative derivations in the previous section the least squares estimator was obtained as value that minimizes the sum of squared residuals of the model 
however it is also possible to derive the same estimator from other approaches 
in all cases the formula for ols estimator remains the same xtx xty the only difference is in how we interpret this result 
projection for mathematicians ols is an approximate solution to an overdetermined system of linear equations where is the unknown 
assuming the system cannot be solved exactly the number of equations is much larger than the number of unknowns we are looking for solution that could provide the smallest discrepancy between the right and left hand sides 
in other words we are looking for the solution that satisfies min where is the standard norm in the dimensional euclidean space rn 
the predicted quantity is just certain linear combination of the vectors of regressors 
thus the residual vector will have the smallest length when is projected orthogonally onto the linear subspace spanned by the columns of the ols estimator in this case can be interpreted as the coefficients of vector decomposition of py along the basis of in other words the gradient equations at the minimum can be written as geometrical interpretation of these equations is that the vector of residuals is orthogonal to the column space of since the dot product is equal to zero for any conformal vector this means that is the shortest of all possible vectors that is the variance of the residuals is the minimum possible 
this is illustrated at the right 
introducing and matrix with the assumption that matrix is non singular and kt cf 
orthogonal projections the residual vector should satisfy the following equation 
the equation and solution of linear least squares are thus described as follows another way of looking at it is to consider the regression line to be weighted average of the lines passing through the combination of any two points in the dataset 
although this way of calculation is more computationally expensive it provides better intuition on ols 
maximum likelihood the ols estimator is identical to the maximum likelihood estimator mle under the normality assumption for the error terms 
proof this normality assumption has historical importance as it provided the basis for the early work in linear regression analysis by yule and pearson 
from the properties of mle we can infer that the ols estimator is asymptotically efficient in the sense of attaining the cram rao bound for variance if the normality assumption is satisfied 
generalized method of moments in iid case the ols estimator can also be viewed as gmm estimator arising from the moment conditions these moment conditions state that the regressors should be uncorrelated with the errors 
since xi is vector the number of moment conditions is equal to the dimension of the parameter vector and thus the system is exactly identified 
this is the so called classical gmm case when the estimator does not depend on the choice of the weighting matrix 
note that the original strict exogeneity assumption xi implies far richer set of moment conditions than stated above 
in particular this assumption implies that for any vector function the moment condition xi will hold 
however it can be shown using the gauss markov theorem that the optimal choice of function is to take which results in the moment equation posted above 
properties assumptions there are several different frameworks in which the linear regression model can be cast in order to make the ols technique applicable 
each of these settings produces the same formulas and same results 
the only difference is the interpretation and the assumptions which have to be imposed in order for the method to give meaningful results 
the choice of the applicable framework depends mostly on the nature of data in hand and on the inference task which has to be performed 
one of the lines of difference in interpretation is whether to treat the regressors as random variables or as predefined constants 
in the first case random design the regressors xi are random and sampled together with the yi from some population as in an observational study 
this approach allows for more natural study of the asymptotic properties of the estimators 
in the other interpretation fixed design the regressors are treated as known constants set by design and is sampled conditionally on the values of as in an experiment 
for practical purposes this distinction is often unimportant since estimation and inference is carried out while conditioning on all results stated in this article are within the random design framework 
classical linear regression model the classical model focuses on the finite sample estimation and inference meaning that the number of observations is fixed 
this contrasts with the other approaches which study the asymptotic behavior of ols and in which the number of observations is allowed to grow to infinity 
the linear functional form must coincide with the form of the actual data generating process 
the errors in the regression should have conditional mean zero the immediate consequence of the exogeneity assumption is that the errors have mean zero and that the regressors are uncorrelated with the errors xt the exogeneity assumption is critical for the ols theory 
if it holds then the regressor variables are called exogenous 
if it doesn then those regressors that are correlated with the error term are called endogenous and the ols estimator becomes biased 
in such case the method of instrumental variables may be used to carry out inference no linear dependence 
the regressors in must all be linearly independent 
mathematically this means that the matrix must have full column rank almost surely pr rank usually it is also assumed that the regressors have finite moments up to at least the second moment 
then the matrix qxx xtx is finite and positive semi definite 
when this assumption is violated the regressors are called linearly dependent or perfectly multicollinear 
in such case the value of the regression coefficient cannot be learned although prediction of values is still possible for new values of the regressors that lie in the same linearly dependent subspace spherical errors var where in is the identity matrix in dimension and is parameter which determines the variance of each observation 
this is considered nuisance parameter in the model although usually it is also estimated 
if this assumption is violated then the ols estimates are still valid but no longer efficient 
it is customary to split this assumption into two parts homoscedasticity which means that the error term has the same variance in each observation 
when this requirement is violated this is called heteroscedasticity in such case more efficient estimator would be weighted least squares 
if the errors have infinite variance then the ols estimates will also have infinite variance although by the law of large numbers they will nonetheless tend toward the true values so long as the errors have zero mean 
in this case robust estimation techniques are recommended 
no autocorrelation the errors are uncorrelated between observations for this assumption may be violated in the context of time series data panel data cluster samples hierarchical data repeated measures data longitudinal data and other data with dependencies 
in such cases generalized least squares provides better alternative than the ols 
another expression for autocorrelation is serial correlation normality 
it is sometimes additionally assumed that the errors have normal distribution conditional on the regressors 
this assumption is not needed for the validity of the ols method although certain additional finite sample properties can be established in case when it does especially in the area of hypotheses testing 
also when the errors are normal the ols estimator is equivalent to the maximum likelihood estimator mle and therefore it is asymptotically efficient in the class of all regular estimators 
importantly the normality assumption applies only to the error terms contrary to popular misconception the response dependent variable is not required to be normally distributed 
independent and identically distributed iid in some applications especially with cross sectional data an additional assumption is imposed that all observations are independent and identically distributed 
this means that all observations are taken from random sample which makes all the assumptions listed earlier simpler and easier to interpret 
also this framework allows one to state asymptotic results as the sample size which are understood as theoretical possibility of fetching new independent observations from the data generating process 
the list of assumptions in this case is iid observations xi yi is independent from and has the same distribution as xj yj for all no perfect multicollinearity qxx xi xit is positive definite matrix exogeneity xi homoscedasticity var xi 
time series model the stochastic process is co integrating 
the regressors are predetermined xi for all the matrix qxx xi xit is of full rank and hence positive definite is martingale difference sequence with finite matrix of second moments qxx xi xit 
finite sample properties first of all under the strict exogeneity assumption the ols estimators and are unbiased meaning that their expected values coincide with the true values of the parameters proof if the strict exogeneity does not hold as is the case with many time series models where exogeneity is assumed only with respect to the past shocks but not the future ones then these estimators will be biased in finite samples 
the variance covariance matrix or simply covariance matrix of is equal to var in particular the standard error of each coefficient is equal to square root of the th diagonal element of this matrix 
the estimate of this standard error is obtained by replacing the unknown quantity with its estimate 
thus it can also be easily shown that the estimator is uncorrelated with the residuals from the model cov the gauss markov theorem states that under the spherical errors assumption that is the errors should be uncorrelated and homoscedastic the estimator is efficient in the class of linear unbiased estimators 
this is called the best linear unbiased estimator blue 
efficiency should be understood as if we were to find some other estimator which would be linear in and unbiased then var var in the sense that this is nonnegative definite matrix 
this theorem establishes optimality only in the class of linear unbiased estimators which is quite restrictive 
depending on the distribution of the error terms other non linear estimators may provide better results than ols 
assuming normality the properties listed so far are all valid regardless of the underlying distribution of the error terms 
however if you are willing to assume that the normality assumption holds that is that in then additional properties of the ols estimators can be stated 
the estimator is normally distributed with mean and variance as given before 
this estimator reaches the cram rao bound for the model and thus is optimal in the class of all unbiased estimators 
note that unlike the gauss markov theorem this result establishes optimality among both linear and non linear estimators but only in the case of normally distributed error terms 
the estimator will be proportional to the chi squared distribution the variance of this estimator is equal to which does not attain the cram rao bound of 
however it was shown that there are no unbiased estimators of with variance smaller than that of the estimator 
if we are willing to allow biased estimators and consider the class of estimators that are proportional to the sum of squared residuals ssr of the model then the best in the sense of the mean squared error estimator in this class will be ssr which even beats the cram rao bound in case when there is only one regressor moreover the estimators and are independent the fact which comes in useful when constructing the and tests for the regression 
influential observations as was mentioned before the estimator is linear in meaning that it represents linear combination of the dependent variables yi 
the weights in this linear combination are functions of the regressors and generally are unequal 
the observations with high weights are called influential because they have more pronounced effect on the value of the estimator 
to analyze which observations are influential we remove specific th observation and consider how much the estimated quantities are going to change similarly to the jackknife method 
it can be shown that the change in the ols estimator for will be equal to where hj xjt xtx xj is the th diagonal element of the hat matrix and xj is the vector of regressors corresponding to the th observation 
similarly the change in the predicted value for th observation resulting from omitting that observation from the dataset will be equal to from the properties of the hat matrix hj and they sum up to so that on average hj 
these quantities hj are called the leverages and observations with high hj are called leverage points 
usually the observations with high leverage ought to be scrutinized more carefully in case they are erroneous or outliers or in some other way atypical of the rest of the dataset 
partitioned regression sometimes the variables and corresponding parameters in the regression can be logically split into two groups so that the regression takes form where and have dimensions and are and vectors with the frisch waugh lovell theorem states that in this regression the residuals and the ols estimate will be numerically identical to the residuals and the ols estimate for in the following regression where is the annihilator matrix for regressors 
the theorem can be used to establish number of theoretical results 
for example having regression with constant and another regressor is equivalent to subtracting the means from the dependent variable and the regressor and then running the regression for the de meaned variables but without the constant term 
constrained estimation suppose it is known that the coefficients in the regression satisfy system of linear equations where is matrix of full rank and is vector of known constants where in this case least squares estimation is equivalent to minimizing the sum of squared residuals of the model subject to the constraint the constrained least squares cls estimator can be given by an explicit formula 
this expression for the constrained estimator is valid as long as the matrix xtx is invertible 
it was assumed from the beginning of this article that this matrix is of full rank and it was noted that when the rank condition fails will not be identifiable 
however it may happen that adding the restriction makes identifiable in which case one would like to find the formula for the estimator 
the estimator is equal to where is matrix such that the matrix is non singular and rtq such matrix can always be found although generally it is not unique 
the second formula coincides with the first in case when xtx is invertible 
large sample properties the least squares estimators are point estimates of the linear regression model parameters however generally we also want to know how close those estimates might be to the true values of parameters 
in other words we want to construct the interval estimates 
since we haven made any assumption about the distribution of error term it is impossible to infer the distribution of the estimators and nevertheless we can apply the central limit theorem to derive their asymptotic properties as sample size goes to infinity 
while the sample size is necessarily finite it is customary to assume that is large enough so that the true distribution of the ols estimator is close to its asymptotic limit 
we can show that under the model assumptions the least squares estimator for is consistent that is converges in probability to and asymptotically normal proof where 
intervals using this asymptotic distribution approximate two sided confidence intervals for the th component of the vector can be constructed as at the confidence level where denotes the quantile function of standard normal distribution and jj is the th diagonal element of matrix 
similarly the least squares estimator for is also consistent and asymptotically normal provided that the fourth moment of exists with limiting distribution 
these asymptotic distributions can be used for prediction testing hypotheses constructing other estimators etc as an example consider the problem of prediction 
suppose is some point within the domain of distribution of the regressors and one wants to know what the response variable would have been at that point 
the mean response is the quantity whereas the predicted response is 
clearly the predicted response is random variable its distribution can be derived from that of which allows construct confidence intervals for mean response to be constructed at the confidence level 
hypothesis testing two hypothesis tests are particularly widely used 
first one wants to know if the estimated regression equation is any better than simply predicting that all values of the response variable equal its sample mean if not it is said to have no explanatory power 
the null hypothesis of no explanatory value of the estimated regression is tested using an test 
if the calculated value is found to be large enough to exceed its critical value for the pre chosen level of significance the null hypothesis is rejected and the alternative hypothesis that the regression has explanatory power is accepted 
otherwise the null hypothesis of no explanatory power is accepted 
second for each explanatory variable of interest one wants to know whether its estimated coefficient differs significantly from zero that is whether this particular explanatory variable in fact has explanatory power in predicting the response variable 
here the null hypothesis is that the true coefficient is zero 
this hypothesis is tested by computing the coefficient statistic as the ratio of the coefficient estimate to its standard error 
if the statistic is larger than predetermined value the null hypothesis is rejected and the variable is found to have explanatory power with its coefficient significantly different from zero 
otherwise the null hypothesis of zero value of the true coefficient is accepted 
in addition the chow test is used to test whether two subsamples both have the same underlying true coefficient values 
the sum of squared residuals of regressions on each of the subsets and on the combined data set are compared by computing an statistic if this exceeds critical value the null hypothesis of no difference between the two subsets is rejected otherwise it is accepted 
example with real data the following data set gives average heights and weights for american women aged source the world almanac and book of facts 
when only one dependent variable is being modeled scatterplot will suggest the form and strength of the relationship between the dependent variable and regressors 
it might also reveal outliers heteroscedasticity and other aspects of the data that may complicate the interpretation of fitted regression model 
the scatterplot suggests that the relationship is strong and can be approximated as quadratic function 
ols can handle non linear relationships by introducing the regressor height 
the regression model then becomes multiple linear model the output from most popular statistical packages will look similar to this in this table the value column gives the least squares estimates of parameters the std error column shows standard errors of each coefficient estimate the statistic and value columns are testing whether any of the coefficients might be equal to zero 
the statistic is calculated simply as if the errors follow normal distribution follows student distribution 
under weaker conditions is asymptotically normal 
large values of indicate that the null hypothesis can be rejected and that the corresponding coefficient is not zero 
the second column value expresses the results of the hypothesis test as significance level 
conventionally values smaller than are taken as evidence that the population coefficient is nonzero 
squared is the coefficient of determination indicating goodness of fit of the regression 
this statistic will be equal to one if fit is perfect and to zero when regressors have no explanatory power whatsoever 
this is biased estimate of the population squared and will never decrease if additional regressors are added even if they are irrelevant 
adjusted squared is slightly modified version of designed to penalize for the excess number of regressors which do not add to the explanatory power of the regression 
this statistic is always smaller than can decrease as new regressors are added and even be negative for poorly fitting models log likelihood is calculated under the assumption that errors follow normal distribution 
even though the assumption is not very reasonable this statistic may still find its use in conducting lr tests 
durbin watson statistic tests whether there is any evidence of serial correlation between the residuals 
as rule of thumb the value smaller than will be an evidence of positive correlation 
akaike information criterion and schwarz criterion are both used for model selection 
generally when comparing two alternative models smaller values of one of these criteria will indicate better model 
standard error of regression is an estimate of standard error of the error term 
total sum of squares model sum of squared and residual sum of squares tell us how much of the initial variation in the sample were explained by the regression 
statistic tries to test the hypothesis that all coefficients except the intercept are equal to zero 
this statistic has distribution under the null hypothesis and normality assumption and its value indicates probability that the hypothesis is indeed true 
note that when errors are not normal this statistic becomes invalid and other tests such as wald test or lr test should be used 
ordinary least squares analysis often includes the use of diagnostic plots designed to detect departures of the data from the assumed form of the model 
these are some of the common diagnostic plots residuals against the explanatory variables in the model 
non linear relation between these variables suggests that the linearity of the conditional mean function may not hold 
different levels of variability in the residuals for different levels of the explanatory variables suggests possible heteroscedasticity 
residuals against explanatory variables not in the model 
any relation of the residuals to these variables would suggest considering these variables for inclusion in the model 
residuals against the fitted values 
residuals against the preceding residual 
this plot may identify serial correlations in the residuals an important consideration when carrying out statistical inference using regression models is how the data were sampled 
in this example the data are averages rather than measurements on individual women 
the fit of the model is very good but this does not imply that the weight of an individual woman can be predicted with high accuracy based only on her height 
sensitivity to rounding this example also demonstrates that coefficients determined by these calculations are sensitive to how the data is prepared 
the heights were originally given rounded to the nearest inch and have been converted and rounded to the nearest centimetre 
since the conversion factor is one inch to cm this is not an exact conversion 
the original inches can be recovered by round and then re converted to metric without rounding 
if this is done the results become using either of these equations to predict the weight of woman gives similar values kg with rounding vs kg without rounding 
thus seemingly small variation in the data has real effect on the coefficients but small effect on the results of the equation 
while this may look innocuous in the middle of the data range it could become significant at the extremes or in the case where the fitted model is used to project outside the data range extrapolation 
this highlights common error this example is an abuse of ols which inherently requires that the errors in the independent variable in this case height are zero or at least negligible 
the initial rounding to nearest inch plus any actual measurement errors constitute finite and non negligible error 
as result the fitted parameters are not the best estimates they are presumed to be 
though not totally spurious the error in the estimation will depend upon relative size of the and errors 
another example with less real data problem statement we can use the least square mechanism to figure out the equation of two body orbit in polar base co ordinates 
the equation typically used is cos where is the radius of how far the object is from one of the bodies 
in the equation the parameters and are used to determine the path of the orbit 
we have measured the following data 
we need to find the least squares approximation of and for the given data 
solution first we need to represent and in linear form 
so we are going to rewrite the equation as cos 
now we can use this form to represent our observational data as where is and is and is constructed by the first column being the coefficient of and the second column being the coefficient of and is the values for the respective so and 
on solving we get so and see also bayesian least squares fama macbeth regression nonlinear least squares numerical methods for linear least squares nonlinear system identification references further reading dougherty christopher 
introduction to econometrics nd ed 
new york oxford university press 
isbn gujarati damodar porter dawn 
basic econometics fifth ed 
boston mcgraw hill irwin 
isbn heij christiaan boer paul franses philip kloek teun van dijk herman 
econometric methods with applications in business and economics st ed 
oxford oxford university press 
isbn hill carter griffiths william lim guay 
principles of econometrics rd ed 
hoboken nj john wiley sons 
the simple regression model 
introductory econometrics modern approach th ed 
mason oh cengage learning
in statistics econometrics epidemiology and related disciplines the method of instrumental variables iv is used to estimate causal relationships when controlled experiments are not feasible or when treatment is not successfully delivered to every unit in randomized experiment 
intuitively ivs are used when an explanatory variable of interest is correlated with the error term in which case ordinary least squares and anova give biased results 
valid instrument induces changes in the explanatory variable but has no independent effect on the dependent variable allowing researcher to uncover the causal effect of the explanatory variable on the dependent variable 
instrumental variable methods allow for consistent estimation when the explanatory variables covariates are correlated with the error terms in regression model 
such correlation may occur when changes in the dependent variable change the value of at least one of the covariates reverse causation there are omitted variables that affect both the dependent and independent variables or the covariates are subject to non random measurement error explanatory variables that suffer from one or more of these issues in the context of regression are sometimes referred to as endogenous 
in this situation ordinary least squares produces biased and inconsistent estimates 
however if an instrument is available consistent estimates may still be obtained 
an instrument is variable that does not itself belong in the explanatory equation but is correlated with the endogenous explanatory variables conditionally on the value of other covariates 
in linear models there are two main requirements for using ivs the instrument must be correlated with the endogenous explanatory variables conditionally on the other covariates 
if this correlation is strong then the instrument is said to have strong first stage 
weak correlation may provide misleading inferences about parameter estimates and standard errors 
the instrument cannot be correlated with the error term in the explanatory equation conditionally on the other covariates 
in other words the instrument cannot suffer from the same problem as the original predicting variable 
if this condition is met then the instrument is said to satisfy the exclusion restriction 
history first use of an instrument variable occurred in book by philip wright best known for his excellent description of the production transport and sale of vegetable and animal oils in the early in the united states while in olav reiers applied the same approach in the context of errors in variables models in his dissertation giving the method its name wright attempted to determine the supply and demand for butter using panel data on prices and quantities sold in the united states 
the idea was that regression analysis could produce demand or supply curve because they are formed by the path between prices and quantities demanded or supplied 
the problem was that the observational data did not form demand or supply curve as such but rather cloud of point observations that took different shapes under varying market conditions 
it seemed that making deductions from the data remained elusive 
the problem was that price affected both supply and demand so that function describing only one of the two could not be constructed directly from the observational data 
wright correctly concluded that he needed variable that correlated with either demand or supply but not both that is an instrumental variable 
after much deliberation wright decided to use regional rainfall as his instrumental variable he concluded that rainfall affected grass production and hence milk production and ultimately butter supply but not butter demand 
in this way he was able to construct regression equation with only the instrumental variable of price and supply 
theory while the ideas behind iv extend to broad class of models very common context for iv is in linear regression 
traditionally an instrumental variable is defined as variable that is correlated with the independent variable and uncorrelated with the error term in the linear equation is vector 
is matrix usually with column of ones and perhaps with additional columns for other covariates 
consider how an instrument allows to be recovered 
recall that ols solves for such that cov when we minimize the sum of squared errors min the first order condition is exactly 
if the true model is believed to have cov due to any of the reasons listed above for example if there is an omitted variable which affects both and separately then this ols procedure will not yield the causal impact of on ols will simply pick the parameter that makes the resulting errors appear uncorrelated with consider for simplicity the single variable case 
suppose we are considering regression with one variable and constant perhaps no other covariates are necessary or perhaps we have partialed out any other relevant covariates in this case the coefficient on the regressor of interest is given by cov var 
substituting for gives cov var cov var cov var cov var cov var where is what the estimated coefficient vector would be if were not correlated with in this case it can be shown that is an unbiased estimator of if cov in the underlying model that we believe then ols gives coefficient which does not reflect the underlying causal effect of interest 
iv helps to fix this problem by identifying the parameters not based on whether is uncorrelated with but based on whether another variable is uncorrelated with if theory suggests that is related to the first stage but uncorrelated with the exclusion restriction then iv may identify the causal parameter of interest where ols fails 
because there are multiple specific ways of using and deriving iv estimators even in just the linear case iv sls gmm we save further discussion for the estimation section below 
example informally in attempting to estimate the causal effect of some variable on another an instrument is third variable which affects only through its effect on for example suppose researcher wishes to estimate the causal effect of smoking on general health 
correlation between health and smoking does not imply that smoking causes poor health because other variables such as depression may affect both health and smoking or because health may affect smoking 
it is at best difficult and expensive to conduct controlled experiments on smoking status in the general population 
the researcher may attempt to estimate the causal effect of smoking on health from observational data by using the tax rate for tobacco products as an instrument for smoking 
the tax rate for tobacco products is reasonable choice for an instrument because the researcher assumes that it can only be correlated with health through its effect on smoking 
if the researcher then finds tobacco taxes and state of health to be correlated this may be viewed as evidence that smoking causes changes in health 
angrist and krueger present survey of the history and uses of instrumental variable techniques 
graphical definition of course iv techniques have been developed among much broader class of non linear models 
general definitions of instrumental variables using counterfactual and graphical formalism were given by pearl 
the graphical definition requires that satisfy the following conditions where stands for separation and stands for the graph in which all arrows entering are cut off 
the counterfactual definition requires that satisfies where yx stands for the value that would attain had been and stands for independence 
if there are additional covariates then the above definitions are modified so that qualifies as an instrument if the given criteria hold conditional on the essence of pearl definition is the equations of interest are structural not regression 
the error term stands for all exogenous factors that affect when is held constant 
the instrument should be independent of the instrument should not affect when is held constant exclusion restriction 
the instrument should not be independent of these conditions do not rely on specific functional form of the equations and are applicable therefore to nonlinear equations where can be non additive see non parametric analysis 
they are also applicable to system of multiple equations in which and other factors affect through several intermediate variables 
an instrumental variable need not be cause of proxy of such cause may also be used if it satisfies conditions 
the exclusion restriction condition is redundant it follows from conditions and 
selecting suitable instruments since is unobserved the requirement that be independent of cannot be inferred from data and must instead be determined from the model structure the data generating process 
causal graphs are representation of this structure and the graphical definition given above can be used to quickly determine whether variable qualifies as an instrumental variable given set of covariates to see how consider the following example 
suppose that we wish to estimate the effect of university tutoring program on grade point average gpa 
the relationship between attending the tutoring program and gpa may be confounded by number of factors 
students who attend the tutoring program may care more about their grades or may be struggling with their work 
this confounding is depicted in the figures on the right through the bidirected arc between tutoring program and gpa 
if students are assigned to dormitories at random the proximity of the student dorm to the tutoring program is natural candidate for being an instrumental variable 
however what if the tutoring program is located in the college library 
in that case proximity may also cause students to spend more time at the library which in turn improves their gpa see figure 
using the causal graph depicted in the figure we see that proximity does not qualify as an instrumental variable because it is connected to gpa through the path proximity library hours gpa in 
however if we control for library hours by adding it as covariate then proximity becomes an instrumental variable since proximity is separated from gpa given library hours in 
now suppose that we notice that student natural ability affects his or her number of hours in the library as well as his or her gpa as in figure using the causal graph we see that library hours is collider and conditioning on it opens the path proximity library hours gpa 
as result proximity cannot be used as an instrumental variable 
finally suppose that library hours does not actually affect gpa because students who do not study in the library simply study elsewhere as in figure in this case controlling for library hours still opens spurious path from proximity to gpa 
however if we do not control for library hours and remove it as covariate then proximity can again be used an instrumental variable 
estimation we now revisit and expand upon the mechanics of iv in greater detail 
suppose the data are generated by process of the form where indexes observations is the th value of the dependent variable is vector of the th values of the independent variable and constant is the th value of an unobserved error term representing all causes of other than and is an unobserved parameter vector the parameter vector is the causal effect on of one unit change in each element of holding all other causes of constant 
the econometric goal is to estimate for simplicity sake assume the draws of are uncorrelated and that they are drawn from distributions with the same variance that is that the errors are serially uncorrelated and homoskedastic 
suppose also that regression model of nominally the same form is proposed 
given random sample of observations from this process the ordinary least squares estimator is where and denote column vectors of length this equation is similar to the equation involving cov in the introduction this is the matrix version of that equation 
when and are uncorrelated under certain regularity conditions the second term has an expected value conditional on of zero and converges to zero in the limit so the estimator is unbiased and consistent 
when and the other unmeasured causal variables collapsed into the term are correlated however the ols estimator is generally biased and inconsistent for in this case it is valid to use the estimates to predict values of given values of but the estimate does not recover the causal effect of on to recover the underlying parameter we introduce set of variables that is highly correlated with each endogenous component of but in our underlying model is not correlated with for simplicity one might consider to be matrix composed of column of constants and one endogenous variable and to be consisting of column of constants and one instrumental variable 
however this technique generalizes to being matrix of constant and say endogenous variables with being matrix composed of constant and instruments 
in the discussion that follows we will assume that is matrix and leave this value unspecified 
an estimator in which and are both matrices is referred to as just identified 
suppose that the relationship between each endogenous component xi and the instruments is given by the most common iv specification uses the following estimator this specification approaches the true parameter as the sample gets large so long as in the true model as long as in the underlying process which generates the data the appropriate use of the iv estimator will identify this parameter 
this works because iv solves for the unique parameter that satisfies and therefore hones in on the true underlying parameter as the sample size grows 
now an extension suppose that there are more instruments than there are covariates in the equation of interest so that is matrix with this is often called the over identified case 
in this case the generalized method of moments gmm can be used 
the gmm iv estimator is where refers to the projection matrix this expression collapses to the first when the number of instruments is equal to the number of covariates in the equation of interest 
the over identified iv is therefore generalization of the just identified iv 
there is an equivalent under identified estimator for the case where since the parameters are the solutions to set of linear equations an under identified model using the set of equations does not have unique solution 
interpretation as two stage least squares one computational method which can be used to calculate iv estimates is two stage least squares sls or tsls 
in the first stage each explanatory variable that is an endogenous covariate in the equation of interest is regressed on all of the exogenous variables in the model including both exogenous covariates in the equation of interest and the excluded instruments 
the predicted values from these regressions are obtained stage regress each column of on errors and save the predicted values in the second stage the regression of interest is estimated as usual except that in this stage each endogenous covariate is replaced with the predicted values from the first stage stage regress on the predicted values from the first stage which gives sls this method is only valid in linear models 
for categorical endogenous covariates one might be tempted to use different first stage than ordinary least squares such as probit model for the first stage followed by ols for the second 
this is commonly known in the econometric literature as the forbidden regression because second stage iv parameter estimates are consistent only in special cases 
the resulting estimator of is numerically identical to the expression displayed above 
small correction must be made to the sum of squared residuals in the second stage fitted model in order that the covariance matrix of is calculated correctly 
non parametric analysis when the form of the structural equations is unknown an instrumental variable can still be defined through the equations where and are two arbitrary functions and is independent of unlike linear models however measurements of and do not allow for the identification of the average causal effect of on denoted ace ace pr do 
balke and pearl derived tight bounds on ace and showed that these can provide valuable information on the sign and size of ace in linear analysis there is no test to falsify the assumption the is instrumental relative to the pair 
this is not the case when is discrete 
pearl has shown that for all and the following constraint called instrumental inequality must hold whenever satisfies the two equations above max max pr 
interpretation under treatment effect heterogeneity the exposition above assumes that the causal effect of interest does not vary across observations that is that is constant 
generally different subjects will respond in different ways to changes in the treatment when this possibility is recognized the average effect in the population of change in on may differ from the effect in given subpopulation 
for example the average effect of job training program may substantially differ across the group of people who actually receive the training and the group which chooses not to receive training 
for these reasons iv methods invoke implicit assumptions on behavioral response or more generally assumptions over the correlation between the response to treatment and propensity to receive treatment the standard iv estimator can recover local average treatment effects late rather than average treatment effects ate 
imbens and angrist demonstrate that the linear iv estimate can be interpreted under weak conditions as weighted average of local average treatment effects where the weights depend on the elasticity of the endogenous regressor to changes in the instrumental variables 
roughly that means that the effect of variable is only revealed for the subpopulations affected by the observed changes in the instruments and that subpopulations which respond most to changes in the instruments will have the largest effects on the magnitude of the iv estimate 
for example if researcher uses presence of land grant college as an instrument for college education in an earnings regression she identifies the effect of college on earnings in the subpopulation which would obtain college degree if college is present but which would not obtain degree if college is not present 
this empirical approach does not without further assumptions tell the researcher anything about the effect of college among people who would either always or never get college degree regardless of whether local college exists 
weak instruments problem as bound jaeger and baker note problem is caused by the selection of weak instruments instruments that are poor predictors of the endogenous question predictor in the first stage equation 
in this case the prediction of the question predictor by the instrument will be poor and the predicted values will have very little variation 
consequently they are unlikely to have much success in predicting the ultimate outcome when they are used to replace the question predictor in the second stage equation 
in the context of the smoking and health example discussed above tobacco taxes are weak instruments for smoking if smoking status is largely unresponsive to changes in taxes 
if higher taxes do not induce people to quit smoking or not start smoking then variation in tax rates tells us nothing about the effect of smoking on health 
if taxes affect health through channels other than through their effect on smoking then the instruments are invalid and the instrumental variables approach may yield misleading results 
for example places and times with relatively health conscious populations may both implement high tobacco taxes and exhibit better health even holding smoking rates constant so we would observe correlation between health and tobacco taxes even if it were the case that smoking has no effect on health 
in this case we would be mistaken to infer causal effect of smoking on health from the observed correlation between tobacco taxes and health 
testing for weak instruments the strength of the instruments can be directly assessed because both the endogenous covariates and the instruments are observable 
common rule of thumb for models with one endogenous regressor is the statistic against the null that the excluded instruments are irrelevant in the first stage regression should be larger than 
statistical inference and hypothesis testing when the covariates are exogenous the small sample properties of the ols estimator can be derived in straightforward manner by calculating moments of the estimator conditional on when some of the covariates are endogenous so that instrumental variables estimation is implemented simple expressions for the moments of the estimator cannot be so obtained 
generally instrumental variables estimators only have desirable asymptotic not finite sample properties and inference is based on asymptotic approximations to the sampling distribution of the estimator 
even when the instruments are uncorrelated with the error in the equation of interest and when the instruments are not weak the finite sample properties of the instrumental variables estimator may be poor 
for example exactly identified models produce finite sample estimators with no moments so the estimator can be said to be neither biased nor unbiased the nominal size of test statistics may be substantially distorted and the estimates may commonly be far away from the true value of the parameter 
testing the exclusion restriction the assumption that the instruments are not correlated with the error term in the equation of interest is not testable in exactly identified models 
if the model is overidentified there is information available which may be used to test this assumption 
the most common test of these overidentifying restrictions called the sargan hansen test is based on the observation that the residuals should be uncorrelated with the set of exogenous variables if the instruments are truly exogenous 
the sargan hansen test statistic can be calculated as the number of observations multiplied by the coefficient of determination from the ols regression of the residuals onto the set of exogenous variables 
this statistic will be asymptotically chi squared with degrees of freedom under the null that the error term is uncorrelated with the instruments 
see also control function econometrics optimal instruments references further reading greene william 
econometric analysis sixth ed 
upper saddle river pearson prentice hall 
isbn gujarati damodar porter dawn 
basic econometrics fifth ed 
new york mcgraw hill irwin 
lectures on advanced econometric theory 
introductory econometrics modern approach fifth international ed 
mason oh south western 
quasi likelihood methods for count data handbook of applied econometrics volume ed 
pesaran and schmidt oxford blackwell pp 
terza estimating count models with endogenous switching sample selection and endogenous treatment effects 
journal of econometrics pp 
econometric analysis of cross section and panel data mit press cambridge massachusetts 
external links chapter from daniel mcfadden textbook econometrics lecture topic instrumental variable on youtube by mark thoma 
econometrics lecture topic two stages least square on youtube by mark thoma
symmetric key algorithms are algorithms for cryptography that use the same cryptographic keys for both the encryption of plaintext and the decryption of ciphertext 
the keys may be identical or there may be simple transformation to go between the two keys 
the keys in practice represent shared secret between two or more parties that can be used to maintain private information link 
the requirement that both parties have access to the secret key is one of the main drawbacks of symmetric key encryption in comparison to public key encryption also known as asymmetric key encryption 
however symmetric key encryption algorithms are usually better for bulk encryption 
they have smaller key size which means less storage space and faster transmission 
due to this asymmetric key encryption is often used to exchange the secret key for symmetric key encryption 
types symmetric key encryption can use either stream ciphers or block ciphers 
stream ciphers encrypt the digits typically bytes or letters in substitution ciphers of message one at time 
an example is chacha 
substitution ciphers are well known ciphers but can be easily decrypted using frequency table 
block ciphers take number of bits and encrypt them in single unit padding the plaintext to achieve multiple of the block size 
the advanced encryption standard aes algorithm approved by nist in december uses bit blocks 
implementations examples of popular symmetric key algorithms include twofish serpent aes rijndael camellia salsa chacha blowfish cast kuznyechik rc des des skipjack safer and idea 
use as cryptographic primitive symmetric ciphers are commonly used to achieve other cryptographic primitives than just encryption encrypting message does not guarantee that it will remain unchanged while encrypted 
hence often message authentication code is added to ciphertext to ensure that changes to the ciphertext will be noted by the receiver 
message authentication codes can be constructed from an aead cipher 
however symmetric ciphers cannot be used for non repudiation purposes except by involving additional parties 
see the iso iec standard 
another application is to build hash functions from block ciphers 
see one way compression function for descriptions of several such methods 
construction of symmetric ciphers many modern block ciphers are based on construction proposed by horst feistel 
feistel construction makes it possible to build invertible functions from other functions that are themselves not invertible 
security of symmetric ciphers symmetric ciphers have historically been susceptible to known plaintext attacks chosen plaintext attacks differential cryptanalysis and linear cryptanalysis 
careful construction of the functions for each round can greatly reduce the chances of successful attack 
it is also possible to increase the key length or the rounds in the encryption process to better protect against attack 
this however tends to increase the processing power and decrease the speed at which the process runs due to the amount of operations the system needs to do most modern symmetric key algorithms appear to be resistant to the threat of post quantum cryptography 
quantum computers would exponentially increase the speed at which these ciphers can be decoded notably grover algorithm would take the square root of the time traditionally required for brute force attack although these vulnerabilities can be compensated for by doubling key length 
for example bit aes cipher would not be secure against such an attack as it would reduce the time required to test all possible iterations from over quintillion years to about six months 
by contrast it would still take quantum computer the same amount of time to decode bit aes cipher as it would conventional computer to decode bit aes cipher 
for this reason aes is believed to be quantum resistant 
key management key establishment symmetric key algorithms require both the sender and the recipient of message to have the same secret key 
all early cryptographic systems required either the sender or the recipient to somehow receive copy of that secret key over physically secure channel 
nearly all modern cryptographic systems still use symmetric key algorithms internally to encrypt the bulk of the messages but they eliminate the need for physically secure channel by using diffie hellman key exchange or some other public key protocol to securely come to agreement on fresh new secret key for each session conversation forward secrecy 
key generation when used with asymmetric ciphers for key transfer pseudorandom key generators are nearly always used to generate the symmetric cipher session keys 
however lack of randomness in those generators or in their initialization vectors is disastrous and has led to cryptanalytic breaks in the past 
therefore it is essential that an implementation use source of high entropy for its initialization 
reciprocal cipher reciprocal cipher is cipher where just as one enters the plaintext into the cryptography system to get the ciphertext one could enter the ciphertext into the same place in the system to get the plaintext 
reciprocal cipher is also sometimes referred as self reciprocal cipher practically all mechanical cipher machines implement reciprocal cipher mathematical involution on each typed in letter 
instead of designing two kinds of machines one for encrypting and one for decrypting all the machines can be identical and can be set up keyed the same way examples of reciprocal ciphers include atbash beaufort cipher enigma machine marie antoinette and axel von fersen communicated with self reciprocal cipher 
the porta polyalphabetic cipher is self reciprocal 
purple cipher rc rot xor cipher vatsyayana cipherthe majority of all modern ciphers can be classified as either stream cipher most of which use reciprocal xor cipher combiner or block cipher most of which use feistel cipher or lai massey scheme with reciprocal transformation in each round
linear least squares lls is the least squares approximation of linear functions to data 
it is set of formulations for solving statistical problems involved in linear regression including variants for ordinary unweighted weighted and generalized correlated residuals 
numerical methods for linear least squares include inverting the matrix of the normal equations and orthogonal decomposition methods 
main formulations the three main linear least squares formulations are ordinary least squares ols is the most common estimator 
ols estimates are commonly used to analyze both experimental and observational data 
the ols method minimizes the sum of squared residuals and leads to closed form expression for the estimated value of the unknown parameter vector where is vector whose ith element is the ith observation of the dependent variable and is matrix whose ij element is the ith observation of the jth independent variable 
the estimator is unbiased and consistent if the errors have finite variance and are uncorrelated with the regressors where is the transpose of row of the matrix it is also efficient under the assumption that the errors have finite variance and are homoscedastic meaning that xi does not depend on the condition that the errors are uncorrelated with the regressors will generally be satisfied in an experiment but in the case of observational data it is difficult to exclude the possibility of an omitted covariate that is related to both the observed covariates and the response variable 
the existence of such covariate will generally lead to correlation between the regressors and the response variable and hence to an inconsistent estimator of the condition of homoscedasticity can fail with either experimental or observational data 
if the goal is either inference or predictive modeling the performance of ols estimates can be poor if multicollinearity is present unless the sample size is large 
weighted least squares wls are used when heteroscedasticity is present in the error terms of the model 
generalized least squares gls is an extension of the ols method that allows efficient estimation of when either heteroscedasticity or correlations or both are present among the error terms of the model as long as the form of heteroscedasticity and correlation is known independently of the data 
to handle heteroscedasticity when the error terms are uncorrelated with each other gls minimizes weighted analogue to the sum of squared residuals from ols regression where the weight for the ith case is inversely proportional to var 
this special case of gls is called weighted least squares 
the gls solution to an estimation problem is where is the covariance matrix of the errors 
gls can be viewed as applying linear transformation to the data so that the assumptions of ols are met for the transformed data 
for gls to be applied the covariance structure of the errors must be known up to multiplicative constant 
alternative formulations other formulations include iteratively reweighted least squares irls is used when heteroscedasticity or correlations or both are present among the error terms of the model but where little is known about the covariance structure of the errors independently of the data 
in the first iteration ols or gls with provisional covariance structure is carried out and the residuals are obtained from the fit 
based on the residuals an improved estimate of the covariance structure of the errors can usually be obtained 
subsequent gls iteration is then performed using this estimate of the error structure to define the weights 
the process can be iterated to convergence but in many cases only one iteration is sufficient to achieve an efficient estimate of instrumental variables regression iv can be performed when the regressors are correlated with the errors 
in this case we need the existence of some auxiliary instrumental variables zi such that zi if is the matrix of instruments then the estimator can be given in closed form as optimal instruments regression is an extension of classical iv regression to the situation where zi total least squares tls is an approach to least squares estimation of the linear regression model that treats the covariates and response variable in more geometrically symmetric manner than ols 
it is one approach to handling the errors in variables problem and is also sometimes used even when the covariates are assumed to be error free percentage least squares focuses on reducing percentage errors which is useful in the field of forecasting or time series analysis 
it is also useful in situations where the dependent variable has wide range without constant variance as here the larger residuals at the upper end of the range would dominate if ols were used 
when the percentage or relative error is normally distributed least squares percentage regression provides maximum likelihood estimates 
percentage regression is linked to multiplicative error model whereas ols is linked to models containing an additive error term constrained least squares indicates linear least squares problem with additional constraints on the solution 
objective function in ols assuming unweighted observations the optimal value of the objective function is found by substituting the optimal expression for the coefficient vector where the latter equality holding since is symmetric and idempotent 
it can be shown from this that under an appropriate assignment of weights the expected value of is if instead unit weights are assumed the expected value of is where is the variance of each observation 
if it is assumed that the residuals belong to normal distribution the objective function being sum of weighted squared residuals will belong to chi squared distribution with degrees of freedom 
some illustrative percentile values of are given in the following table 
these values can be used for statistical criterion as to the goodness of fit 
when unit weights are used the numbers should be divided by the variance of an observation 
for wls the ordinary objective function above is replaced for weighted average of residuals 
discussion in statistics and mathematics linear least squares is an approach to fitting mathematical or statistical model to data in cases where the idealized value provided by the model for any data point is expressed linearly in terms of the unknown parameters of the model 
the resulting fitted model can be used to summarize the data to predict unobserved values from the same system and to understand the mechanisms that may underlie the system 
mathematically linear least squares is the problem of approximately solving an overdetermined system of linear equations where is not an element of the column space of the matrix the approximate solution is realized as an exact solution to where is the projection of onto the column space of the best approximation is then that which minimizes the sum of squared differences between the data values and their corresponding modeled values 
the approach is called linear least squares since the assumed function is linear in the parameters to be estimated 
linear least squares problems are convex and have closed form solution that is unique provided that the number of data points used for fitting equals or exceeds the number of unknown parameters except in special degenerate situations 
in contrast non linear least squares problems generally must be solved by an iterative procedure and the problems can be non convex with multiple optima for the objective function 
if prior distributions are available then even an underdetermined system can be solved using the bayesian mmse estimator 
in statistics linear least squares problems correspond to particularly important type of statistical model called linear regression which arises as particular form of regression analysis 
one basic form of such model is an ordinary least squares model 
the present article concentrates on the mathematical aspects of linear least squares problems with discussion of the formulation and interpretation of statistical regression models and statistical inferences related to these being dealt with in the articles just mentioned 
see outline of regression analysis for an outline of the topic 
properties if the experimental errors are uncorrelated have mean of zero and constant variance the gauss markov theorem states that the least squares estimator has the minimum variance of all estimators that are linear combinations of the observations 
in this sense it is the best or optimal estimator of the parameters 
note particularly that this property is independent of the statistical distribution function of the errors 
in other words the distribution function of the errors need not be normal distribution 
however for some probability distributions there is no guarantee that the least squares solution is even possible given the observations still in such cases it is the best estimator that is both linear and unbiased 
for example it is easy to show that the arithmetic mean of set of measurements of quantity is the least squares estimator of the value of that quantity 
if the conditions of the gauss markov theorem apply the arithmetic mean is optimal whatever the distribution of errors of the measurements might be 
however in the case that the experimental errors do belong to normal distribution the least squares estimator is also maximum likelihood estimator these properties underpin the use of the method of least squares for all types of data fitting even when the assumptions are not strictly valid 
limitations an assumption underlying the treatment given above is that the independent variable is free of error 
in practice the errors on the measurements of the independent variable are usually much smaller than the errors on the dependent variable and can therefore be ignored 
when this is not the case total least squares or more generally errors in variables models or rigorous least squares should be used 
this can be done by adjusting the weighting scheme to take into account errors on both the dependent and independent variables and then following the standard procedure in some cases the weighted normal equations matrix xtx is ill conditioned 
when fitting polynomials the normal equations matrix is vandermonde matrix 
vandermonde matrices become increasingly ill conditioned as the order of the matrix increases 
in these cases the least squares estimate amplifies the measurement noise and may be grossly inaccurate 
various regularization techniques can be applied in such cases the most common of which is called ridge regression 
if further information about the parameters is known for example range of possible values of then various techniques can be used to increase the stability of the solution 
for example see constrained least squares 
another drawback of the least squares estimator is the fact that the norm of the residuals is minimized whereas in some cases one is truly interested in obtaining small error in the parameter small value of 
however since the true parameter is necessarily unknown this quantity cannot be directly minimized 
if prior probability on is known then bayes estimator can be used to minimize the mean squared error 
the least squares method is often applied when no prior is known 
surprisingly when several parameters are being estimated jointly better estimators can be constructed an effect known as stein phenomenon 
for example if the measurement error is gaussian several estimators are known which dominate or outperform the least squares technique the best known of these is the james stein estimator 
this is an example of more general shrinkage estimators that have been applied to regression problems 
applications polynomial fitting models are polynomials in an independent variable straight line quadratic cubic quartic and higher polynomials 
for regression with high order polynomials the use of orthogonal polynomials is recommended 
numerical smoothing and differentiation this is an application of polynomial fitting 
multinomials in more than one independent variable including surface fitting curve fitting with splines chemometrics calibration curve standard addition gran plot analysis of mixtures uses in data fitting the primary application of linear least squares is in data fitting 
given set of data points consisting of experimentally measured values taken at values of an independent variable may be scalar or vector quantities and given model function with it is desired to find the parameters such that the model function best fits the data 
in linear least squares linearity is meant to be with respect to parameters so here the functions may be nonlinear with respect to the variable ideally the model function fits the data exactly so for all this is usually not possible in practice as there are more data points than there are parameters to be determined 
the approach chosen then is to find the minimal possible value of the sum of squares of the residuals so to minimize the function after substituting for and then for this minimization problem becomes the quadratic minimization problem above with and the best fit can be found by solving the normal equations 
example as result of an experiment four data points were obtained and shown in red in the diagram on the right 
we hope to find line that best fits these four points 
in other words we would like to find the numbers and that approximately solve the overdetermined linear system of four equations in two unknowns in some best sense 
represents the residual at each point between the curve fit and the data the least squares approach to solving this problem is to try to make the sum of the squares of these residuals as small as possible that is to find the minimum of the function the minimum is determined by calculating the partial derivatives of with respect to and and setting them to zero this results in system of two equations in two unknowns called the normal equations which when solved give and the equation is the line of best fit 
the residuals that is the differences between the values from the observations and the predicated variables by using the line of best fit are then found to be and see the diagram on the right 
the minimum value of the sum of squares of the residuals is more generally one can have regressors and linear model using quadratic model importantly in linear least squares we are not restricted to using line as the model as in the above example 
for instance we could have chosen the restricted quadratic model this model is still linear in the parameter so we can still perform the same analysis constructing system of equations from the data points the partial derivatives with respect to the parameters this time there is only one are again computed and set to and solved leading to the resulting best fit model 
see also line line intersection nearest point to non intersecting lines an application line fitting nonlinear least squares regularized least squares simple linear regression partial least squares regression linear function references further reading bevington philip robinson keith 
data reduction and error analysis for the physical sciences 
external links least squares fitting from mathworld least squares fitting polynomial from mathworld
public key cryptography or asymmetric cryptography is the field of cryptographic systems that use pairs of related keys 
each key pair consists of public key and corresponding private key 
key pairs are generated with cryptographic algorithms based on mathematical problems termed one way functions 
security of public key cryptography depends on keeping the private key secret the public key can be openly distributed without compromising security in public key encryption system anyone with public key can encrypt message yielding ciphertext but only those who know the corresponding private key can decrypt the ciphertext to obtain the original message for example journalist can publish the public key of an encryption key pair on web site so that sources can send secret messages to the news organization in ciphertext 
only the journalist who knows the corresponding private key can decrypt the ciphertexts to obtain the sources messages an eavesdropper reading email on its way to the journalist can decrypt the ciphertexts 
however public key encryption doesn conceal metadata like what computer source used to send message when they sent it or how long it is 
public key encryption on its own also doesn tell the recipient anything about who sent message it just conceals the content of message in ciphertext that can only be decrypted with the private key 
in digital signature system sender can use private key together with message to create signature 
anyone with the corresponding public key can verify whether the signature matches the message but forger who doesn know the private key can find any message signature pair that will pass verification with the public key for example software publisher can create signature key pair and include the public key in software installed on computers 
later the publisher can distribute an update to the software signed using the private key and any computer receiving an update can confirm it is genuine by verifying the signature using the public key 
as long as the software publisher keeps the private key secret even if forger can distribute malicious updates to computers they can convince the computers that any malicious updates are genuine 
public key algorithms are fundamental security primitives in modern cryptosystems including applications and protocols which offer assurance of the confidentiality authenticity and non repudiability of electronic communications and data storage 
they underpin numerous internet standards such as transport layer security tls ssh mime and pgp 
some public key algorithms provide key distribution and secrecy diffie hellman key exchange some provide digital signatures digital signature algorithm and some provide both rsa 
compared to symmetric encryption asymmetric encryption is rather slower than good symmetric encryption too slow for many purposes 
today cryptosystems such as tls secure shell use both symmetric encryption and asymmetric encryption often by using asymmetric encryption to securely exchange secret key which is then used for symmetric encryption 
description before the mid all cipher systems used symmetric key algorithms in which the same cryptographic key is used with the underlying algorithm by both the sender and the recipient who must both keep it secret 
of necessity the key in every such system had to be exchanged between the communicating parties in some secure way prior to any use of the system for instance via secure channel 
this requirement is never trivial and very rapidly becomes unmanageable as the number of participants increases or when secure channels aren available or when as is sensible cryptographic practice keys are frequently changed 
in particular if messages are meant to be secure from other users separate key is required for each possible pair of users 
by contrast in public key system the public keys can be disseminated widely and openly and only the corresponding private keys need be kept secret by its owner 
two of the best known uses of public key cryptography are public key encryption in which message is encrypted with the intended recipient public key 
for properly chosen and used algorithms messages cannot in practice be decrypted by anyone who does not possess the matching private key who is thus presumed to be the owner of that key and so the person associated with the public key 
this can be used to ensure confidentiality of message 
digital signatures in which message is signed with the sender private key and can be verified by anyone who has access to the sender public key 
this verification proves that the sender had access to the private key and therefore is very likely to be the person associated with the public key 
it also proves that the signature was prepared for that exact message since verification will fail for any other message one could devise without using the private key one important issue is confidence proof that particular public key is authentic 
that it is correct and belongs to the person or entity claimed and has not been tampered with or replaced by some perhaps malicious third party 
there are several possible approaches including public key infrastructure pki in which one or more third parties known as certificate authorities certify ownership of key pairs 
tls relies upon this 
this implies that the pki system software hardware and management is trust able by all involved 
web of trust which decentralizes authentication by using individual endorsements of links between user and the public key belonging to that user 
pgp uses this approach in addition to lookup in the domain name system dns 
the dkim system for digitally signing emails also uses this approach 
applications the most obvious application of public key encryption system is for encrypting communication to provide confidentiality message that sender encrypts using the recipient public key which can be decrypted only by the recipient paired private key 
another application in public key cryptography is the digital signature 
digital signature schemes can be used for sender authentication 
non repudiation systems use digital signatures to ensure that one party cannot successfully dispute its authorship of document or communication 
further applications built on this foundation include digital cash password authenticated key agreement time stamping services and non repudiation protocols 
hybrid cryptosystems because asymmetric key algorithms are nearly always much more computationally intensive than symmetric ones it is common to use public private asymmetric key exchange algorithm to encrypt and exchange symmetric key which is then used by symmetric key cryptography to transmit data using the now shared symmetric key for symmetric key encryption algorithm 
pgp ssh and the ssl tls family of schemes use this procedure they are thus called hybrid cryptosystems 
the initial asymmetric cryptography based key exchange to share server generated symmetric key from the server to client has the advantage of not requiring that symmetric key be pre shared manually such as on printed paper or discs transported by courier while providing the higher data throughput of symmetric key cryptography over asymmetric key cryptography for the remainder of the shared connection 
weaknesses as with all security related systems it is important to identify potential weaknesses 
aside from poor choice of an asymmetric key algorithm there are few which are widely regarded as satisfactory or too short key length the chief security risk is that the private key of pair becomes known 
all security of messages authentication etc will then be lost 
algorithms all public key schemes are in theory susceptible to brute force key search attack 
however such an attack is impractical if the amount of computation needed to succeed termed the work factor by claude shannon is out of reach of all potential attackers 
in many cases the work factor can be increased by simply choosing longer key 
but other algorithms may inherently have much lower work factors making resistance to brute force attack from longer keys irrelevant 
some special and specific algorithms have been developed to aid in attacking some public key encryption algorithms both rsa and elgamal encryption have known attacks that are much faster than the brute force approach 
none of these are sufficiently improved to be actually practical however 
major weaknesses have been found for several formerly promising asymmetric key algorithms 
the knapsack packing algorithm was found to be insecure after the development of new attack 
as with all cryptographic functions public key implementations may be vulnerable to side channel attacks that exploit information leakage to simplify the search for secret key 
these are often independent of the algorithm being used 
research is underway to both discover and to protect against new attacks 
alteration of public keys another potential security vulnerability in using asymmetric keys is the possibility of man in the middle attack in which the communication of public keys is intercepted by third party the man in the middle and then modified to provide different public keys instead 
encrypted messages and responses must in all instances be intercepted decrypted and re encrypted by the attacker using the correct public keys for the different communication segments so as to avoid suspicion 
communication is said to be insecure where data is transmitted in manner that allows for interception also called sniffing 
these terms refer to reading the sender private data in its entirety 
communication is particularly unsafe when interceptions can be prevented or monitored by the sender man in the middle attack can be difficult to implement due to the complexities of modern security protocols 
however the task becomes simpler when sender is using insecure media such as public networks the internet or wireless communication 
in these cases an attacker can compromise the communications infrastructure rather than the data itself 
hypothetical malicious staff member at an internet service provider isp might find man in the middle attack relatively straightforward 
capturing the public key would only require searching for the key as it gets sent through the isp communications hardware in properly implemented asymmetric key schemes this is not significant risk 
in some advanced man in the middle attacks one side of the communication will see the original data while the other will receive malicious variant 
asymmetric man in the middle attacks can prevent users from realizing their connection is compromised 
this remains so even when one user data is known to be compromised because the data appears fine to the other user 
this can lead to confusing disagreements between users such as it must be on your end 
when neither user is at fault 
hence man in the middle attacks are only fully preventable when the communications infrastructure is physically controlled by one or both parties such as via wired route inside the sender own building 
in summation public keys are easier to alter when the communications hardware used by sender is controlled by an attacker 
public key infrastructure one approach to prevent such attacks involves the use of public key infrastructure pki set of roles policies and procedures needed to create manage distribute use store and revoke digital certificates and manage public key encryption 
however this has potential weaknesses 
for example the certificate authority issuing the certificate must be trusted by all participating parties to have properly checked the identity of the key holder to have ensured the correctness of the public key when it issues certificate to be secure from computer piracy and to have made arrangements with all participants to check all their certificates before protected communications can begin 
web browsers for instance are supplied with long list of self signed identity certificates from pki providers these are used to check the bona fides of the certificate authority and then in second step the certificates of potential communicators 
an attacker who could subvert one of those certificate authorities into issuing certificate for bogus public key could then mount man in the middle attack as easily as if the certificate scheme were not used at all 
in an alternative scenario rarely discussed an attacker who penetrates an authority servers and obtains its store of certificates and keys public and private would be able to spoof masquerade decrypt and forge transactions without limit 
despite its theoretical and potential problems this approach is widely used 
examples include tls and its predecessor ssl which are commonly used to provide security for web browser transactions for example to securely send credit card details to an online store 
aside from the resistance to attack of particular key pair the security of the certification hierarchy must be considered when deploying public key systems 
some certificate authority usually purpose built program running on server computer vouches for the identities assigned to specific private keys by producing digital certificate 
public key digital certificates are typically valid for several years at time so the associated private keys must be held securely over that time 
when private key used for certificate creation higher in the pki server hierarchy is compromised or accidentally disclosed then man in the middle attack is possible making any subordinate certificate wholly insecure 
examples examples of well regarded asymmetric key techniques for varied purposes include diffie hellman key exchange protocol dss digital signature standard which incorporates the digital signature algorithm elgamal elliptic curve cryptography elliptic curve digital signature algorithm ecdsa elliptic curve diffie hellman ecdh ed and ed eddsa and ecdh eddh various password authenticated key agreement techniques paillier cryptosystem rsa encryption algorithm pkcs cramer shoup cryptosystem yak authenticated key agreement protocolexamples of asymmetric key algorithms not yet widely adopted include ntruencrypt cryptosystem kyber mceliece cryptosystemexamples of notable yet insecure asymmetric key algorithms include merkle hellman knapsack cryptosystemexamples of protocols using asymmetric key algorithms include mime gpg an implementation of openpgp and an internet standard emv emv certificate authority ipsec pgp zrtp secure voip protocol transport layer security standardized by ietf and its predecessor secure socket layer silc ssh bitcoin off the record messaging history during the early history of cryptography two parties would rely upon key that they would exchange by means of secure but non cryptographic method such as face to face meeting or trusted courier 
this key which both parties must then keep absolutely secret could then be used to exchange encrypted messages 
number of significant practical difficulties arise with this approach to distributing keys 
anticipation in his book the principles of science william stanley jevons wrote can the reader say what two numbers multiplied together will produce the number 
think it unlikely that anyone but myself will ever know 
here he described the relationship of one way functions to cryptography and went on to discuss specifically the factorization problem used to create trapdoor function 
in july mathematician solomon golomb said jevons anticipated key feature of the rsa algorithm for public key cryptography although he certainly did not invent the concept of public key cryptography 
classified discovery in james ellis british cryptographer at the uk government communications headquarters gchq conceived of the possibility of non secret encryption now called public key cryptography but could see no way to implement it 
in his colleague clifford cocks implemented what has become known as the rsa encryption algorithm giving practical method of non secret encryption and in another gchq mathematician and cryptographer malcolm williamson developed what is now known as diffie hellman key exchange 
the scheme was also passed to the us national security agency 
both organisations had military focus and only limited computing power was available in any case the potential of public key cryptography remained unrealised by either organization judged it most important for military use if you can share your key rapidly and electronically you have major advantage over your opponent 
only at the end of the evolution from berners lee designing an open internet architecture for cern its adaptation and adoption for the arpanet did public key cryptography realise its full potential 
ralph benjamin these discoveries were not publicly acknowledged for years until the research was declassified by the british government in 
public discovery in an asymmetric key cryptosystem was published by whitfield diffie and martin hellman who influenced by ralph merkle work on public key distribution disclosed method of public key agreement 
this method of key exchange which uses exponentiation in finite field came to be known as diffie hellman key exchange 
this was the first published practical method for establishing shared secret key over an authenticated but not confidential communications channel without using prior shared secret 
merkle public key agreement technique became known as merkle puzzles and was invented in and only published in this makes asymmetric encryption rather new field in cryptography although cryptography itself dates back more than years in generalization of cocks scheme was independently invented by ron rivest adi shamir and leonard adleman all then at mit 
the latter authors published their work in in martin gardner scientific american column and the algorithm came to be known as rsa from their initials 
rsa uses exponentiation modulo product of two very large primes to encrypt and decrypt performing both public key encryption and public key digital signatures 
its security is connected to the extreme difficulty of factoring large integers problem for which there is no known efficient general technique though prime factorization may be obtained through brute force attacks this grows much more difficult the larger the prime factors are 
description of the algorithm was published in the mathematical games column in the august issue of scientific american since the large number and variety of encryption digital signature key agreement and other techniques have been developed including the rabin cryptosystem elgamal encryption dsa and elliptic curve cryptography 
see also notes references external links oral history interview with martin hellman charles babbage institute university of minnesota 
leading cryptography scholar martin hellman discusses the circumstances and fundamental insights of his invention of public key cryptography with collaborators whitfield diffie and ralph merkle at stanford university in the mid 
an account of how gchq kept their invention of pke secret until
aids is caused by human immunodeficiency virus hiv which originated in non human primates in central and west africa 
while various sub groups of the virus acquired human infectivity at different times the present pandemic had its origins in the emergence of one specific strain hiv subgroup in opoldville in the belgian congo now kinshasa in the democratic republic of the congo in the there are two types of hiv hiv and hiv 
hiv is more virulent easily transmitted and is the cause of the vast majority of hiv infections globally 
the pandemic strain of hiv is closely related to virus found in chimpanzees of the subspecies pan troglodytes troglodytes which live in the forests of the central african nations of cameroon equatorial guinea gabon the republic of the congo and the central african republic 
hiv is less transmittable and is largely confined to west africa along with its closest relative virus of the sooty mangabey cercocebus atys atys an old world monkey inhabiting southern senegal guinea bissau guinea sierra leone liberia and western ivory coast 
transmission from non humans to humans research in this area is conducted using molecular phylogenetics comparing viral genomic sequences to determine relatedness 
hiv from chimpanzees and gorillas to humans scientists generally accept that the known strains or groups of hiv are most closely related to the simian immunodeficiency viruses sivs endemic in wild ape populations of west central african forests 
in particular each of the known hiv strains is either closely related to the siv that infects the chimpanzee subspecies pan troglodytes troglodytes sivcpz or closely related to the siv that infects western lowland gorillas gorilla gorilla gorilla called sivgor 
the pandemic hiv strain group or main and rare strain found only in few cameroonian people group are clearly derived from sivcpz strains endemic in pan troglodytes troglodytes chimpanzee populations living in cameroon 
another very rare hiv strain group is clearly derived from sivgor strains of cameroon 
finally the primate ancestor of hiv group strain infecting people mostly from cameroon but also from neighbouring countries was confirmed in to be sivgor 
the pandemic hiv group is most closely related to the sivcpz collected from the southeastern rain forests of cameroon modern east province near the sangha river 
thus this region is presumably where the virus was first transmitted from chimpanzees to humans 
however reviews of the epidemiological evidence of early hiv infection in stored blood samples and of old cases of aids in central africa have led many scientists to believe that hiv group early human centre was probably not in cameroon but rather further south in the democratic republic of the congo then the belgian congo more probably in its capital city kinshasa formerly opoldville using hiv sequences preserved in human biological samples along with estimates of viral mutation rates scientists calculate that the jump from chimpanzee to human probably happened during the late th or early th century time of rapid urbanisation and colonisation in equatorial africa 
exactly when the zoonosis occurred is not known 
some molecular dating studies suggest that hiv group had its most recent common ancestor mrca that is started to spread in the human population in the early th century probably between and study published in analyzing viral sequences recovered from biopsy made in kinshasa in along with previously known sequences suggested common ancestor between and with central estimates varying between and 
genetic recombination had earlier been thought to seriously confound such phylogenetic analysis but later work has suggested that recombination is not likely to systematically bias results although recombination is expected to increase variance 
the results of phylogenetics study support the later work and indicate that hiv evolves fairly reliably 
further research was hindered due to the primates being critically endangered 
sample analyses resulted in little data due to the rarity of experimental material 
the researchers however were able to hypothesize phylogeny from the gathered data 
they were also able to use the molecular clock of specific strain of hiv to determine the initial date of transmission which is estimated to be around 
hiv from sooty mangabeys to humans similar research has been undertaken with siv strains collected from several wild sooty mangabey cercocebus atys atys sivsmm populations of the west african nations of sierra leone liberia and ivory coast 
the resulting phylogenetic analyses show that the viruses most closely related to the two strains of hiv that spread considerably in humans hiv groups and are the sivsmm found in the sooty mangabeys of the tai forest in western ivory coast there are six additional known hiv groups each having been found in just one person 
they all seem to derive from independent transmissions from sooty mangabeys to humans 
groups and have been found in two people from liberia groups and have been discovered in two people from sierra leone and groups and have been detected in two people from the ivory coast 
these hiv strains are probably dead end infections and each of them is most closely related to sivsmm strains from sooty mangabeys living in the same country where the human infection was found molecular dating studies suggest that both the epidemic groups and started to spread among humans between and with the central estimates varying between and 
bushmeat practice according to the natural transfer theory also called hunter theory or bushmeat theory in the simplest and most plausible explanation for the cross species transmission of siv or hiv post mutation the virus was transmitted from an ape or monkey to human when hunter or bushmeat vendor handler was bitten or cut while hunting or butchering the animal 
the resulting exposure to blood or other bodily fluids of the animal can result in siv infection 
prior to wwii some sub saharan africans were forced out of the rural areas because of the european demand for resources 
since rural africans were not keen to pursue agricultural practices in the jungle they turned to non domesticated animals as their primary source of meat 
this over exposure to bushmeat and malpractice of butchery increased blood to blood contact which then increased the probability of transmission 
recent serological survey showed that human infections by siv are not rare in central africa the percentage of people showing seroreactivity to antigens evidence of current or past siv infection was among the general population of cameroon in villages where bushmeat is hunted or used and in the most exposed people of these villages 
how the siv virus would have transformed into hiv after infection of the hunter or bushmeat handler from the ape monkey is still matter of debate although natural selection would favour any viruses capable of adjusting so that they could infect and reproduce in the cells of human host 
emergence unresolved questions about hiv origins and emergence the discovery of the main hiv siv phylogenetic relationships permits explaining broad hiv biogeography the early centres of the hiv groups were in central africa where the primate reservoirs of the related sivcpz and sivgor viruses chimpanzees and gorillas exist similarly the hiv groups had their centres in west africa where sooty mangabeys which harbour the related sivsmm virus exist 
however these relationships do not explain more detailed patterns of biogeography such as why epidemic hiv groups and only evolved in the ivory coast which is one of only six countries harbouring the sooty mangabey 
it is also unclear why the sivcpz endemic in the chimpanzee subspecies pan troglodytes schweinfurthii inhabiting the democratic republic of congo central african republic rwanda burundi uganda and tanzania did not spawn an epidemic hiv strain to humans while the democratic republic of congo was the main centre of hiv group virus descended from sivcpz strains of subspecies pan troglodytes troglodytes that does not exist in this country 
it is clear that the several hiv and hiv strains descend from sivcpz sivgor and sivsmm viruses and that bushmeat practice provides the most plausible cause of cross species transfer to humans 
however some loose ends remain 
it is not yet explained why only four hiv groups hiv groups and and hiv groups and spread considerably in human populations despite bushmeat practices being widespread in central and west africa and the resulting human siv infections being common it also remains unexplained why all epidemic hiv groups emerged in humans nearly simultaneously and only in the th century despite very old human exposure to siv phylogenetic study demonstrated that siv is at least tens of thousands of years old 
origin and epidemic emergence several of the theories of hiv origin accept the established knowledge of the hiv siv phylogenetic relationships and also accept that bushmeat practice was the most likely cause of the initial transfer to humans 
all of them propose that the simultaneous epidemic emergences of four hiv groups in the late th early th century and the lack of previous known emergences are explained by new factor that appeared in the relevant african regions in that timeframe 
these new factor would have acted either to increase human exposures to siv to help it to adapt to the human organism by mutation thus enhancing its between humans transmissibility or to cause an initial burst of transmissions crossing an epidemiological threshold and therefore increasing the probability of continued spread 
genetic studies of the virus suggested in that the most recent common ancestor of the hiv group dates back to the belgian congo city of opoldville modern kinshasa circa proponents of this dating link the hiv epidemic with the emergence of colonialism and growth of large colonial african cities leading to social changes including higher degree of non monogamous sexual activity the spread of prostitution and the concomitant high frequency of genital ulcer diseases such as syphilis in nascent colonial cities in study conducted by scientists from the university of oxford and the university of leuven in belgium revealed that because approximately one million people every year would flow through the prominent city of kinshasa which served as the origin of the first known hiv cases in the passengers riding on the region belgian railway trains were able to spread the virus to larger areas 
the study also identified roaring sex trade rapid population growth and unsterilised needles used in health clinics as other factors which contributed to the emergence of the africa hiv epidemic 
social changes and urbanization beatrice hahn paul sharp and their colleagues proposed that the epidemic emergence of hiv most likely reflects changes in population structure and behaviour in africa during the th century and perhaps medical interventions that provided the opportunity for rapid human to human spread of the virus 
after the scramble for africa started in the european colonial powers established cities towns and other colonial stations 
largely masculine labor force was hastily recruited to work in fluvial and sea ports railways other infrastructures and in plantations 
this disrupted traditional tribal values and favored casual sexual activity with an increased number of partners 
in the nascent cities women felt relatively liberated from rural tribal rules and many remained unmarried or divorced during long periods this being rare in african traditional societies 
this was accompanied by unprecedented increase in people movements 
michael worobey and colleagues observed that the growth of cities probably played role in the epidemic emergence of hiv since the phylogenetic dating of the two older strains of hiv groups and suggest that these viruses started to spread soon after the main central african colonial cities were founded 
colonialism in africa amit chitnis diana rawls and jim moore proposed that hiv may have emerged epidemically as result of harsh conditions forced labor displacement and unsafe injection and vaccination practices associated with colonialism particularly in french equatorial africa 
the workers in plantations construction projects and other colonial enterprises were supplied with bushmeat which would have contributed to an increase in hunting and it follows higher incidence of human exposure to siv 
several historical sources support the view that bushmeat hunting indeed increased both because of the necessity to supply workers and because firearms became more widely available the colonial authorities also gave many vaccinations against smallpox and injections of which many would be made without sterilising the equipment between uses 
proposed that both these parenteral risks and the prostitution associated with forced labor camps could have caused serial transmission or serial passage of siv between humans see discussion of this in the next section 
in addition they proposed that the conditions of extreme stress associated with forced labor could depress the immune system of workers therefore prolonging the primary acute infection period of someone newly infected by siv thus increasing the odds of both adaptation of the virus to humans and of further transmissions the authors proposed that hiv originated in the area of french equatorial africa in the early th century when the colonial abuses and forced labor were at their peak 
later research established that these theories were mostly correct hiv groups and started to spread in humans in late th early th century 
in addition all groups of hiv descend from either sivcpz or sivgor from apes living to the west of the ubangi river either in countries that belonged to the french equatorial africa federation of colonies in equatorial guinea then spanish colony or in cameroon which was german colony between and and then fell to allied forces in world war and had most of its area administered by france in close association with french equatorial africa 
this theory was later dubbed heart of darkness by jim moore alluding to the book of the same title written by joseph conrad the main focus of which is colonial abuses in equatorial africa 
unsterile injections in several articles published since preston marx philip alcabes and ernest drucker proposed that hiv emerged because of rapid serial human to human transmission of siv after bushmeat hunter or handler became siv infected through unsafe or unsterile injections 
although both chitnis et al 
and sharp et al 
also suggested that this may have been one of the major risk factors at play in hiv emergence see above marx et al 
enunciated the underlying mechanisms in greater detail and wrote the first review of the injection campaigns made in colonial africa central to the marx et al 
argument is the concept of adaptation by serial passage or serial transmission an adventitious virus or other pathogen can increase its biological adaptation to new host species if it is rapidly transmitted between hosts while each host is still in the acute infection period 
this process favors the accumulation of adaptive mutations more rapidly therefore increasing the odds that better adapted viral variant will appear in the host before the immune system suppresses the virus 
such better adapted variants could then survive in the human host for longer than the short acute infection period in high numbers high viral load which would grant it more possibilities of epidemic spread 
reported experiments of cross species transfer of siv in captive monkeys some of which made by themselves in which the use of serial passage helped to adapt siv to the new monkey species after passage by three or four animals in agreement with this model is also the fact that while both hiv and hiv attain substantial viral loads in the human organism adventitious siv infecting humans seldom does so people with siv antibodies often have very low or even undetectable siv viral load 
this suggests that both hiv and hiv are adapted to humans and serial passage could have been the process responsible for it 
proposed that unsterile injections that is injections where the needle or syringe is reused without sterilization or cleaning between uses which were likely very prevalent in africa during both the colonial period and afterwards provided the mechanism of serial passage that permitted hiv to adapt to humans therefore explaining why it emerged epidemically only in the th century 
massive injections of the antibiotic era marx et al 
emphasize the massive number of injections administered in africa after antibiotics were introduced around as being the most likely implicated in the origin of hiv because by these times roughly in the period to injection intensity in africa was maximal 
they argued that serial passage chain of or transmissions between humans is an unlikely event the probability of transmission after needle reuse is something between and and only few people have an acute siv infection at any time and so hiv emergence may have required the very high frequency of injections of the antibiotic era the molecular dating studies place the initial spread of the epidemic hiv groups before that time see above 
according to marx et al these studies could have overestimated the age of the hiv groups because they depend on molecular clock assumption may not have accounted for the effects of natural selection in the viruses and the serial passage process alone would be associated with strong natural selection 
injection campaigns against sleeping sickness david gisselquist proposed that the mass injection campaigns to treat trypanosomiasis sleeping sickness in central africa were responsible for the emergence of hiv 
unlike marx et al gisselquist argued that the millions of unsafe injections administered during these campaigns were sufficient to spread rare hiv infections into an epidemic and that evolution of hiv through serial passage was not essential to the emergence of the hiv epidemic in the th century this theory focuses on injection campaigns that peaked in the period that is around the time the hiv groups started to spread 
it also focuses on the fact that many of the injections in these campaigns were intravenous which are more likely to transmit siv hiv than subcutaneous or intramuscular injections and many of the patients received many often more than injections per year therefore increasing the odds of siv serial passage 
other early injection campaigns jacques pin and annie claude labb reviewed the colonial health reports of cameroon and french equatorial africa for the period calculating the incidences of the diseases requiring intravenous injections 
they concluded that trypanosomiasis leprosy yaws and syphilis were responsible for most intravenous injections 
schistosomiasis tuberculosis and vaccinations against smallpox represented lower parenteral risks schistosomiasis cases were relatively few tuberculosis patients only became numerous after mid century and there were few smallpox vaccinations in the lifetime of each person the authors suggested that the very high prevalence of the hepatitis virus in southern cameroon and forested areas of french equatorial africa around can be better explained by the unsterile injections used to treat yaws because this disease was much more prevalent than syphilis trypanosomiasis and leprosy in these areas 
they suggested that all these parenteral risks caused not only the massive spread of hepatitis but also the spread of other pathogens and the emergence of hiv the same procedures could have exponentially amplified hiv from single hunter cook occupationally infected with sivcpz to several thousand patients treated with arsenicals or other drugs threshold beyond which sexual transmission could prosper 
they do not suggest specifically serial passage as the mechanism of adaptation 
according to pin book the origins of aids the virus can be traced to central african bush hunter in with colonial medical campaigns using improperly sterilized syringe and needles playing key role in enabling future epidemic 
pin concludes that aids spread silently in africa for decades fueled by urbanization and prostitution since the initial cross species infection 
pin also claims that the virus was brought to the americas by haitian teacher returning home from zaire in the 
sex tourism and contaminated blood transfusion centers ultimately propelled aids to public consciousness in the and worldwide pandemic 
genital ulcer diseases and evolution of sexual activity jo dinis de sousa viktor ller philippe lemey and anne mieke vandamme proposed that hiv became epidemic through sexual serial transmission in nascent colonial cities helped by high frequency of genital ulcers caused by genital ulcer diseases gud 
gud are simply sexually transmitted diseases that cause genital ulcers examples are syphilis chancroid lymphogranuloma venereum and genital herpes 
these diseases increase the probability of hiv transmission dramatically from around to per heterosexual act because the genital ulcers provide portal of viral entry and contain many activated cells expressing the ccr co receptor the main cell targets of hiv 
probable time interval of cross species transfer sousa et al 
use molecular dating techniques to estimate the time when each hiv group split from its closest siv lineage 
each hiv group necessarily crossed to humans between this time and the time when it started to spread the time of the mrca because after the mrca certainly all lineages were already in humans and before the split with the closest simian strain the lineage was in simian 
hiv groups and split from their closest sivs around and respectively 
this information together with the datations of the hiv groups mrcas mean that all hiv groups likely crossed to humans in the early th century 
strong genital ulcer disease incidence in nascent colonial cities the authors reviewed colonial medical articles and archived medical reports of the countries at or near the ranges of chimpanzees gorillas and sooty mangabeys and found that genital ulcer diseases guds peaked in the colonial cities during their early growth period up to 
the colonial authorities recruited men to work in railways fluvial and sea ports and other infrastructure projects and most of these men did not bring their wives with them 
then the highly male biased sex ratio favoured prostitution which in its turn caused an explosion of gud especially syphilis and chancroid 
after the mid people movements were more tightly controlled and mass surveys and treatments of arsenicals and other drugs were organized and so the gud incidences started to decline 
they declined even further after world war ii because of the heavy use of antibiotics so that by the late opoldville which is the probable center of hiv group had very low gud incidence 
similar processes happened in the cities of cameroon and ivory coast where hiv group and hiv respectively evolved therefore the peak gud incidences in cities have good temporal coincidence with the period when all main hiv groups crossed to humans and started to spread 
in addition the authors gathered evidence that syphilis and the other guds were like injections absent from the densely forested areas of central and west africa before organized colonialism socially disrupted these areas starting in the 
thus this theory also potentially explains why hiv emerged only after the late th century 
female genital mutilation uli linke has argued that the practice of female genital mutilation either or both of clitoridectomy and infibulation is responsible for the high incidence of aids in africa since intercourse with female who has undergone clitoridectomy is conducive to exchange of blood 
male circumcision distribution and hiv origins male circumcision may reduce the probability of hiv acquisition by men 
leaving aside blood transfusions the highest hiv transmissibility ever measured was from female prostitutes with prevalence of hiv to uncircumcised men with gud cumulative seroconverted to hiv after single sexual exposure 
there was no seroconversion in the absence of male gud 
reasoned that the adaptation and epidemic emergence of each hiv group may have required such extreme conditions and thus reviewed the existing ethnographic literature for patterns of male circumcision and hunting of apes and monkeys for bushmeat focusing on the period and on most of the ethnic groups living in central and west africa 
they also collected censuses and other literature showing the ethnic composition of colonial cities in this period 
then they estimated the circumcision frequencies of the central african cities over time 
charts reveal that male circumcision frequencies were much lower in several cities of western and central africa in the early th century than they are currently 
the reason is that many ethnic groups not performing circumcision by that time gradually adopted it to imitate other ethnic groups and enhance the social acceptance of their boys colonialism produced massive intermixing between african ethnic groups 
about of men in opoldville and douala in the early th century should be uncircumcised and these cities were the probable centers of hiv groups and respectively the authors studied early circumcision frequencies in cities of central and west africa to test if this variable correlated with hiv emergence 
this correlation was strong for hiv among west african cities that could have received immigrants infected with sivsmm the two cities from the ivory coast studied abidjan and bouak had much higher frequency of uncircumcised men than the others and epidemic hiv groups emerged initially in this country only 
this correlation was less clear for hiv in central africa 
computer simulations of hiv emergence sousa et al 
then built computer simulations to test if an ill adapted siv meaning simian immunodeficiency virus already infecting human but incapable of transmission beyond the short acute infection period could spread in colonial cities 
the simulations used parameters of sexual transmission obtained from the current hiv literature 
they modelled people sexual links with different levels of sexual partner change among different categories of people prostitutes single women with several partners year married women and men according to data obtained from modern studies of sexual activity in african cities 
the simulations let the parameters city size proportion of people married gud frequency male circumcision frequency and transmission parameters vary and explored several scenarios 
each scenario was run times to test the probability of siv generating long chains of sexual transmission 
the authors postulated that such long chains of sexual transmission were necessary for the siv strain to adapt better to humans becoming an hiv capable of further epidemic emergence 
the main result was that genital ulcer frequency was by far the most decisive factor 
for the gud levels prevailing in opoldville in the early th century long chains of siv transmission had high probability 
for the lower gud levels existing in the same city in the late see above they were much less likely 
and without gud situation typical of villages in forested equatorial africa before colonialism siv could not spread at all 
city size was not an important factor 
the authors propose that these findings explain the temporal patterns of hiv emergence no hiv emerging in tens of thousands of years of human slaughtering of apes and monkeys several hiv groups emerging in the nascent gud riddled colonial cities and no epidemically successful hiv group emerging in mid th century when gud was more controlled and cities were much bigger 
male circumcision had little to moderate effect in their simulations but given the geographical correlation found the authors propose that it could have had an indirect role either by increasing genital ulcer disease itself it is known that syphilis chancroid and several other guds have higher incidences in uncircumcised men or by permitting further spread of the hiv strain after the first chains of sexual transmission permitted adaptation to the human organism 
one of the main advantages of this theory is stressed by the authors it the theory also offers conceptual simplicity because it proposes as causal factors for siv adaptation to humans and initial spread the very same factors that most promote the continued spread of hiv nowadays promiscuous sic sex particularly involving sex workers gud and possibly lack of circumcision 
iatrogenic and other theories iatrogenic theories propose that medical interventions were responsible for hiv origins 
by proposing factors that only appeared in central and west africa after the late th century they seek to explain why all hiv groups also started after that 
the theories centred on the role of parenteral risks such as unsterile injections transfusions or smallpox vaccinations are accepted as plausible by most scientists of the field 
discredited hiv aids origins theories include several iatrogenic theories such as the polio vaccine hypothesis which argues that the early oral polio vaccines were contaminated with chimpanzee virus leading to the central african outbreak 
pathogenicity of siv in non human primates in most non human primate species natural siv infection does not cause fatal disease but see below 
comparison of the gene sequence of siv with hiv should therefore provide information about the factors necessary to cause disease in humans 
the factors that determine the virulence of hiv as compared to most sivs are only now being elucidated 
non human sivs contain nef gene that down regulates cd cd and mhc class expression most non human sivs therefore do not induce immunodeficiency the hiv nef gene however has lost its ability to down regulate cd which results in the immune activation and apoptosis that is characteristic of chronic hiv infection in addition long term survey of chimpanzees naturally infected with sivcpz in gombe national park tanzania found that contrary to the previous paradigm chimpanzees with sivcpz infection do experience an increased mortality and also suffer from human aids like illness 
siv pathogenicity in wild animals could exist in other chimpanzee subspecies and other primate species as well and stay unrecognized by lack of relevant long term studies 
history of spread david carr david carr was an apprentice printer usually mistakenly referred to as sailor carr had served in the navy between and from manchester england who died on august and was for some time mistakenly reported to have died from aids defining opportunistic infections adois 
following the failure of his immune system he succumbed to pneumonia 
doctors baffled by what he had died from preserved of his tissue samples for inspection 
in the tissues were found to be hiv positive 
however in second test by aids researcher david ho found that the strain of hiv present in the tissues was similar to those found in rather than an earlier strain which would have mutated considerably over the course of years 
he concluded that the dna samples provided actually came from patient with aids in the 
upon retesting david carr tissues he found no sign of the virus 
congolese man one of the earliest documented hiv infections was discovered in preserved blood sample taken in from man from opoldville in the belgian congo 
however it is unknown whether this anonymous person ever developed aids and died of its complications 
congolese woman second early documented hiv infection was discovered in preserved lymph node biopsy sample taken in from woman from opoldville belgian congo 
congolese man strain with large amount of the genetic material present was dated to from sample from year old man 
robert rayford in may year old african american robert rayford died at the st louis city hospital from kaposi sarcoma 
in researchers at tulane university school of medicine detected virus closely related or identical to hiv in his preserved blood and tissues 
the doctors who worked on his case at the time suspected he was prostitute or the victim of sexual abuse though the patient did not discuss his sexual history with them in detail 
ugandan children from to researchers drew blood from children in uganda to serve as controls for study of burkitt lymphoma 
in retroactive testing of the frozen blood serum indicated that antibodies to virus related to hiv were present in of the children 
arvid noe in and norwegian sailor with the alias name arvid noe his wife and his seven year old daughter died of aids 
the sailor had first presented symptoms in eight years after he first spent time in ports along the west african coastline 
gonorrhea infection during his first african voyage shows he was sexually active at this time 
tissue samples from the sailor and his wife were tested in and found to contain hiv group 
grethe rask grethe rask was danish surgeon who traveled to za re in then again in to aid the sick 
she was likely directly exposed to blood from many congolese patients one of whom infected her 
she became unwell from then returned to denmark in with her colleagues baffled by her symptoms 
she died of pneumocystis pneumonia in december her tissues were examined and tested by her colleagues and found positive in 
spread to the western hemisphere hiv strains were once thought to have arrived in new york city from haiti around it spread from new york city to san francisco around hiv is believed to have arrived in haiti from central africa possibly from the democratic republic of the congo around the current consensus is that hiv was introduced to haiti by an unknown individual or individuals who contracted it while working in the democratic republic of the congo circa mini epidemic followed and circa yet another unknown individual took hiv from haiti to the united states 
the vast majority of cases of aids outside sub saharan africa can be traced back to that single patient 
later numerous unrelated incidents of aids among haitian immigrants to the were recorded in the early 
also as evidenced by the case of robert rayford isolated occurrences of this infection may have been emerging as early as the virus eventually entered gay male communities in large united states cities where combination of casual multi partner sexual activity with individuals reportedly averaging over unprotected sexual partners per year and relatively high transmission rates associated with anal intercourse allowed it to spread explosively enough to finally be noticed because of the long incubation period of hiv up to decade or longer before symptoms of aids appear and because of the initially low incidence hiv was not noticed at first 
by the time the first reported cases of aids were found in large united states cities the prevalence of hiv infection in some communities had passed 
worldwide hiv infection has spread from urban to rural areas and has appeared in regions such as china and india 
canadian flight attendant theory canadian airline steward named ga tan dugas was referred to as case and later patient with the alphabet letter standing for outside southern california in an early aids study by dr william darrow of the centers for disease control 
because of this many people had considered dugas to be responsible for taking hiv to north america 
however hiv reached new york city around while dugas did not start work at air canada until in randy shilts book and the band played on and the movie based on it dugas is referred to as aids patient zero instead of patient but neither the book nor the movie states that he had been the first to bring the virus to north america 
he was incorrectly called patient zero because at least of the people known to be infected by hiv in had had sex with him or with person who had sexual intercourse with dugas 
homeless people and intravenous drug users in new york volunteer social worker called betty williams quaker who worked with the homeless in new york from the seventies and early eighties onwards has talked about people at that time whose death would be labelled as junkie flu or the dwindles 
in an interview for the act up oral history project in she said of course the horror stories came mainly concerning women who were injection drug users who had pcp pneumonia pneumocystis pneumonia and were told that they just had bronchitis 
she continues actually believe that aids kind of existed among this group of people first because if you look back there was something called junkie pneumonia there was something called the dwindles that addicts got and think this was another early aids population way too helpless to ever do anything for themselves on their own behalf 
julia epstein writes in her book altered conditions disease medicine and storytelling that as we uncover more of the early history of hiv infection it becomes clear that by at least the the virus was already making major inroads into the immune systems of number of diverse populations in the united states the retrospectively diagnosed epidemic of junkie pneumonia in new york city in the late for example and had for some time been causing devastation in several countries in africa 
anecdotal evidence suggests that so called junkie pneumonia first began to afflict heroin addicts in new york in in her book engendering aids deconstructing sex text and epidemic tamsin wilton writes people had been sickening and dying of mysterious conditions since the early conditions that we can retrospectively diagnose as aids related 
there was for example phenomenon known as junkie pneumonia which spread among some populations of injecting street drug users in the and which is now believed to have been caused by hiv infection 
melinda cooper writes in her book family values between neoliberalism and the new social conservatism it is plausible that these cases of aids did not come to light in the for the same reason that junkie pneumonia was not recognized as the sign of an emerging infectious disease the people in question had such precarious access to health care that news of their death was never communicated to public health authorities 
an article by pattrice maurer in the newspaper agenda from april explores some of the issues surrounding junkie pneumonia 
it starts in the late while the epidemic known as disco fever swept through the an epidemic known as junkie pneumonia raged among injection drug users in new york city 
it continues few people were aware that large numbers of injections drug users were inexplicably dying of pneumonia 
those few who did notice these deaths did not feel compelled to investigate the public health puzzle they posed 
the author opinion is that if anyone had bothered to investigate these deaths they would have found an immune system disorder that is now called aids steven thrasher writes in the guardian indeed those of us who study aids have long known that long before common symptoms such as kaposi sarcoma and pneumonia were showing up among hemophiliacs and gay men they were likely affecting homeless people who lived off society radar people who used iv intravenous drugs and those who avoided medical treatment out of fear 
chapter in the proceedings of the world conference of therapeutic communities th san francisco california september gives details about serum samples that were tested for signs of hiv then called htlv iii lav antibodies 
quoting we have also conducted historical studies of the epidemic in new york city using serum samples that were originally collected for other purposes 
we have sera from iv drug users that go back to the middle 
the first indication of htlv iii lav antibody presence is in one of eleven samples from of samples in of samples from and of samples from the htlv iii lav virus appears to have been introduced among iv drug users in the late in new york city 
anna thompson writes on the website thebody com in an article dated autumn many women were dying in the late of pneumonia cervical cancer and other illnesses complicated by mysteriously suppressed immune systems 
yet it was not until that case of aids in woman was first reported by the centers for disease control cdc 
she continues the cdc refusal to address women issues led to the overall perception that women do not get aids 
in an article published in aids cultural analysis cultural activism author douglas crimp draws attention to anecdotal evidence about junkie pneumonia 
quoting even these statistics are based on cdc epidemiology that continues to see the beginning of the epidemic as in spite of widespread anecdotal reporting of high rate of deaths throughout the from what was known as junkie pneumonia and was likely pneumocystis pneumonia 
the statistics crimp writes about were taken from new york times article from october about nyc department of health study that showed that of aids sufferers were people who injected drugs more than percent higher than previously reported 
quoting city health officials estimated that half of the city intravenous drug users were infected with the virus that causes aids the study hiv infection among intravenous drug users in manhattan new york city from through published in february seeks to understand long term trends in the spread of hiv among intravenous drug users idus 
aids surveillance data and studies which detail the number of persons who tested hiv positive in manhattan are used to compile information deemed critical to realising the extent of the aids epidemic 
it starts by stating that up to september idu was the risk behaviour in or of the first cases of aids in the us 
cases among idus in new york city in the same period numbered approximately third of national idu cases 
the study continues to outline the methodology used in the compilation of data 
it says that while truly representative samples of idus within community are probably impossible to obtain samples of idus entering treatment provide good source for monitoring trends 
in the results section it states quoting the first evidence for hiv infection among iv drug users in new york is from three cases of aids in children born in these cases were later reported to the new york city department of health aids surveillance unit 
these children did not receive any known transfusions prior to developing aids and were born to mothers known to be iv drug users 
it continues to outline that the earliest known case of aids in an adult idu occurred in mixed risk and that known cases among idus increased rapidly from the cases in mixed risk to cases in to cases in and to cases in statistics on the incidence of positive tests for hiv mainly using archived samples are out of in out of in out of in out of between out of and out of in out of in and out of in in the comments section it states the three cases in of apparent perinatal transmission mother to child from iv drug using women strongly suggest that the introduction of hiv into the iv drug use group occurred around or or perhaps even earlier 
it says that without extensive samples from this period it is not possible to be certain about the spread of hiv among idus but the samples from idus with chronic liver disease suggest that the rates of infection were below for the first or years after its introduction hiv is thought to have entered the population of people using intravenous drugs in new york city in approximately in spring the government of new york city underwent fiscal crisis which led to the closing of many social services with people who used intravenous drugs living in hostile sociopolitical and legal environment 
this fiscal crisis led to many agencies with health responsibilities being particularly hard hit which in turn might have led to an increase in hiv aids and tuberculosis tb 
quoting from american journal of public health study between and the department of health doh budget in ny was cut by and by the department had lost staff members of its workforce 
to achieve these reductions the department closed of district health centers cut million from its methadone program terminated the employment of of health educators and closed of child health stations and of chest clinics the units responsible for tb screening and diagnosis 
study published in the journal of the american medical association in linked tb and hiv aids severe and unusual presentation of overwhelming tuberculosis in appropriate clinical circumstances may be considered an infection predictive of the presence of aids 
further study from stated there was link between the rise in tb aids and drug users within the united states aids thus compounds the risk of acquiring tuberculosis and in the united states most patients with aids and tuberculosis have been drug users 
newsletter from spring by the national coalition of gay std services featured an article titled tuberculosis and aids connecticut that suggested an association between tb and aids within that state 
from grid to aids the aids epidemic officially began on june when the centers for disease control and prevention in its morbidity and mortality weekly report newsletter reported unusual clusters of pneumocystis pneumonia pcp caused by form of pneumocystis carinii now recognized as distinct species pneumocystis jirovecii in five homosexual men in los angeles 
over the next months more pcp clusters were discovered among otherwise healthy men in cities throughout the country along with other opportunistic diseases such as kaposi sarcoma and persistent generalized lymphadenopathy common in immunosuppressed patients 
in june report of group of cases amongst gay men in southern california suggested that sexually transmitted infectious agent might be the etiological agent 
the syndrome was initially termed grid or gay related immune deficiency other less common gay specific terms included gay compromise syndrome gay lymph node syndrome gay cancer gay plague homosexual syndrome community acquired immunodeficiency caid and acquired community immunodeficiency syndrome acids 
health authorities soon realized however that nearly half of the people identified with the syndrome were not homosexual men 
the same opportunistic infections were also reported among hemophiliacs users of intravenous drugs such as heroin and haitian immigrants leading some researchers to call it the disease 
by august the disease was being referred to by its new cdc coined name acquired immune deficiency syndrome aids 
activism by aids patients and families in new york city nathan fain larry kramer larry mass paul popham paul rapoport and edmund white officially established the gay men health crisis gmhc in also in michael callen and richard berkowitz published how to have sex in an epidemic one approach 
in this short work they described ways gay men could be sexual and affectionate while dramatically reducing the risk of contracting or spreading hiv 
both authors were themselves gay men living with aids 
this booklet was one of the first times men were advised to use condoms when having sexual relations with other men at the beginning of the aids epidemic in the there was very little information about the disease 
because aids affected stigmatized groups such as lgbtq people people of low socioeconomic status sex workers and addicts there was also initially little mass media coverage when the epidemic started 
however with the rise of activist groups composed of people suffering from aids either directly or through loved one more public attention was brought to the epidemic 
identification of the virus may lav in may team of doctors at the pasteur institute in france including fran oise barr sinoussi and luc montagnier reported that they had isolated new retrovirus from lymphoid ganglions that they believed was the cause of aids 
the virus was later named lymphadenopathy associated virus lav and sample was sent to the centers for disease control which was later passed to the national cancer institute nci 
may htlv iii in may team led by robert gallo of the united states confirmed the discovery of the virus but they renamed it human lymphotropic virus type iii htlv iii 
august arv dr jay levy group at the university of california san francisco also played role in the discovery of hiv 
he independently isolated the aids virus in and named it the aids associated retrovirus arv publishing his findings in the journal science in 
january both found to be the same in january number of more detailed reports were published concerning lav and htlv iii and by march it was clear that the viruses were the same indeed it was later determined that the virus isolated by the gallo lab was from the lymph nodes of the patient studied in the original report by montagnier and was the etiological agent of aids 
may the name hiv in may the international committee on taxonomy of viruses ruled that both names should be dropped and new name hiv human immunodeficiency virus be used 
nobel whether barr sinoussi and montagnier deserve more credit than gallo for the discovery of the virus that causes aids has been matter of considerable controversy 
barr sinoussi and montagnier were awarded the nobel prize in physiology or medicine for their discovery of human immunodeficiency virus and harald zur hausen also shared the prize for his discovery that human papilloma virus leads to cervical cancer but gallo was left out 
gallo said that it was disappointment that he was not named co recipient 
montagnier said he was surprised gallo was not recognized by the nobel committee it was important to prove that hiv was the cause of aids and gallo had very important role in that 
very sorry for robert gallo 
dr levy contribution to the discovery of hiv was also cited in the nobel prize ceremony 
case definition for epidemiological surveillance since june many definitions have been developed for epidemiological surveillance such as the bangui definition and the expanded world health organization aids case definition 
genetic studies according to study published in the proceedings of the national academy of sciences in team led by robert shafer at stanford university school of medicine discovered that the gray mouse lemur has an endogenous lentivirus the genus to which hiv belongs in its genetic makeup 
this suggests that lentiviruses have existed for at least million years much longer than the currently known existence of hiv 
in addition the time frame falls in the period when madagascar was still connected to what is now the african continent the said lemurs later developed immunity to the virus strain and survived an era when the lentivirus was widespread among other mammals 
the study was hailed as crucial as it fills the blanks in the origin of the virus as well as in its evolution and could be important in the development of new antiviral drugs in researchers reported that siv had infected monkeys in bioko for at least years 
previous to this time it was thought that siv infection in monkeys had happened over the past few hundred years 
scientists estimated that it would take similar amount of time before humans adapted naturally to hiv infection in the way monkeys in africa have adapted to siv and not suffer any harm from the infection czech study of the genome of malayan flying lemurs an order of mammals parallel to primates and sharing an immediate common ancestor with them found endogenous lentiviruses that emerged an estimated million years ago based on rates of viral mutation versus modern lentiviruses 
debunked hiv aids conspiracy theories aids denialism aids denialists argue that aids does not exist or that aids is not caused by hiv some of its proponents believe that aids is caused by lifestyle including sexuality or drug use and not by hiv 
both forms of aids denialism contradict scientific consensus 
the evidence that hiv causes aids is generally considered conclusive among pathologists 
most arguments for denialism are based on misrepresentations of outdated data 
the belief that hiv was created by the us government as bioweapon an idea invented by soviet propaganda operation is held by disproportionately high number of africans and african americans 
influence on bolsonaro conspiracy theorists influence reached peak in with brazilian president jair bolsonaro claiming that covid vaccines can lead to aids 
the supreme federal court of brazil ordered an investigation into bolsonaro for falsely claiming that covid vaccines could increase the risk of contracting aids 
see also timeline of hiv aids references further reading shilts randy 
and the band played on politics people and the aids epidemic 
new york st martin press 
isbn oclc brier jennier 
infectious ideas political responses to the aids crisis 
chapel hill university of north carolina press
dependent and independent variables are variables in mathematical modeling statistical modeling and experimental sciences 
dependent variables receive this name because in an experiment their values are studied under the supposition or demand that they depend by some law or rule by mathematical function on the values of other variables 
independent variables in turn are not seen as depending on any other variable in the scope of the experiment in question 
in this sense some common independent variables are time space density mass fluid flow rate and previous values of some observed value of interest 
human population size to predict future values the dependent variable of the two it is always the dependent variable whose variation is being studied by altering inputs also known as regressors in statistical context 
in an experiment any variable that can be attributed value without attributing value to any other variable is called an independent variable 
models and experiments test the effects that the independent variables have on the dependent variables 
sometimes even if their influence is not of direct interest independent variables may be included for other reasons such as to account for their potential confounding effect 
mathematics in mathematics function is rule for taking an input in the simplest case number or set of numbers and providing an output which may also be number 
symbol that stands for an arbitrary input is called an independent variable while symbol that stands for an arbitrary output is called dependent variable 
the most common symbol for the input is and the most common symbol for the output is the function itself is commonly written it is possible to have multiple independent variables or multiple dependent variables 
for instance in multivariable calculus one often encounters functions of the form where is dependent variable and and are independent variables 
functions with multiple outputs are often referred to as vector valued functions 
modeling in mathematical modeling the dependent variable is studied to see if and how much it varies as the independent variables vary 
in the simple stochastic linear model yi bxi ei the term yi is the ith value of the dependent variable and xi is the ith value of the independent variable 
the term ei is known as the error and contains the variability of the dependent variable not explained by the independent variable 
with multiple independent variables the model is yi bxi bxi bxi ei where is the number of independent variables the linear regression model is now discussed 
to use linear regression scatter plot of data is generated with as the independent variable and as the dependent variable 
this is also called bivariate dataset xi yi 
the simple linear regression model takes the form of yi bxi ui for in this case ui un are independent random variables 
this occurs when the measurements do not influence each other 
through propagation of independence the independence of ui implies independence of yi even though each yi has different expectation value 
each ui has an expectation value of and variance of expectation of yi proof the line of best fit for the bivariate dataset takes the form and is called the regression line 
and correspond to the intercept and slope respectively 
simulation in simulation the dependent variable is changed in response to changes in the independent variables 
statistics in an experiment the variable manipulated by an experimenter is something that is proven to work called an independent variable 
the dependent variable is the event expected to change when the independent variable is manipulated in data mining tools for multivariate statistics and machine learning the dependent variable is assigned role as target variable or in some tools as label attribute while an independent variable may be assigned role as regular variable 
known values for the target variable are provided for the training data set and test data set but should be predicted for other data 
the target variable is used in supervised learning algorithms but not in unsupervised learning 
statistics synonyms depending on the context an independent variable is sometimes called predictor variable regressor covariate manipulated variable explanatory variable exposure variable see reliability theory risk factor see medical statistics feature in machine learning and pattern recognition or input variable 
in econometrics the term control variable is usually used instead of covariate 
explanatory variable is preferred by some authors over independent variable when the quantities treated as independent variables may not be statistically independent or independently manipulable by the researcher 
if the independent variable is referred to as an explanatory variable then the term response variable is preferred by some authors for the dependent variable from the economics community the independent variables are also called exogenous 
depending on the context dependent variable is sometimes called response variable regressand criterion predicted variable measured variable explained variable experimental variable responding variable outcome variable output variable target or label 
in economics endogenous variables are usually referencing the target 
explained variable is preferred by some authors over dependent variable when the quantities treated as dependent variables may not be statistically dependent 
if the dependent variable is referred to as an explained variable then the term predictor variable is preferred by some authors for the independent variable variables may also be referred to by their form continuous or categorical which in turn may be binary dichotomous nominal categorical and ordinal categorical among others 
an example is provided by the analysis of trend in sea level by woodworth 
here the dependent variable and variable of most interest was the annual mean sea level at given location for which series of yearly values were available 
the primary independent variable was time 
use was made of covariate consisting of yearly values of annual mean atmospheric pressure at sea level 
the results showed that inclusion of the covariate allowed improved estimates of the trend against time to be obtained compared to analyses which omitted the covariate 
other variables variable may be thought to alter the dependent or independent variables but may not actually be the focus of the experiment 
so that the variable will be kept constant or monitored to try to minimize its effect on the experiment 
such variables may be designated as either controlled variable control variable or fixed variable 
extraneous variables if included in regression analysis as independent variables may aid researcher with accurate response parameter estimation prediction and goodness of fit but are not of substantive interest to the hypothesis under examination 
for example in study examining the effect of post secondary education on lifetime earnings some extraneous variables might be gender ethnicity social class genetics intelligence age and so forth 
variable is extraneous only when it can be assumed or shown to influence the dependent variable 
if included in regression it can improve the fit of the model 
if it is excluded from the regression and if it has non zero covariance with one or more of the independent variables of interest its omission will bias the regression result for the effect of that independent variable of interest 
this effect is called confounding or omitted variable bias in these situations design changes and or controlling for variable statistical control is necessary 
extraneous variables are often classified into three types subject variables which are the characteristics of the individuals being studied that might affect their actions 
these variables include age gender health status mood background etc 
blocking variables or experimental variables are characteristics of the persons conducting the experiment which might influence how person behaves 
gender the presence of racial discrimination language or other factors may qualify as such variables 
situational variables are features of the environment in which the study or research was conducted which have bearing on the outcome of the experiment in negative way 
included are the air temperature level of activity lighting and time of day in modelling variability that is not covered by the independent variable is designated by and is known as the residual side effect error unexplained share residual variable disturbance or tolerance 
examples effect of fertilizer on plant growths in study measuring the influence of different quantities of fertilizer on plant growth the independent variable would be the amount of fertilizer used 
the dependent variable would be the growth in height or mass of the plant 
the controlled variables would be the type of plant the type of fertilizer the amount of sunlight the plant gets the size of the pots etc effect of drug dosage on symptom severity in study of how different doses of drug affect the severity of symptoms researcher could compare the frequency and intensity of symptoms when different doses are administered 
here the independent variable is the dose and the dependent variable is the frequency intensity of symptoms effect of temperature on pigmentation in measuring the amount of color removed from beetroot samples at different temperatures temperature is the independent variable and amount of pigment removed is the dependent variable effect of sugar added in coffee the taste varies with the amount of sugar added in the coffee 
here the sugar is the independent variable while the taste is the dependent variable 
see also abscissa and ordinate blocking statistics latent variable versus observable variable notes references
weighted least squares wls also known as weighted linear regression is generalization of ordinary least squares and linear regression in which knowledge of the variance of observations is incorporated into the regression 
wls is also specialization of generalized least squares 
introduction special case of generalized least squares called weighted least squares can be used when all the off diagonal entries of the covariance matrix of the residuals are null the variances of the observations along the covariance matrix diagonal may still be unequal heteroscedasticity 
the fit of model to data point is measured by its residual defined as the difference between measured value of the dependent variable and the value predicted by the model 
if the errors are uncorrelated and have equal variance then the function is minimised at such that the gauss markov theorem shows that when this is so is best linear unbiased estimator blue 
if however the measurements are uncorrelated but have different uncertainties modified approach might be adopted 
aitken showed that when weighted sum of squared residuals is minimized is the blue if each weight is equal to the reciprocal of the variance of the measurement the gradient equations for this sum of squares are which in linear least squares system give the modified normal equations when the observational errors are uncorrelated and the weight matrix is diagonal these may be written as if the errors are correlated the resulting estimator is the blue if the weight matrix is equal to the inverse of the variance covariance matrix of the observations 
when the errors are uncorrelated it is convenient to simplify the calculations to factor the weight matrix as the normal equations can then be written in the same form as ordinary least squares where we define the following scaled matrix and vector diag diag this is type of whitening transformation the last expression involves an entrywise division 
for non linear least squares systems similar argument shows that the normal equations should be modified as follows 
note that for empirical tests the appropriate is not known for sure and must be estimated 
for this feasible generalized least squares fgls techniques may be used in this case it is specialized for diagonal covariance matrix thus yielding feasible weighted least squares solution 
if the uncertainty of the observations is not known from external sources then the weights could be estimated from the given observations 
this can be useful for example to identify outliers 
after the outliers have been removed from the data set the weights should be reset to one 
motivation in some cases the observations may be weighted for example they may not be equally reliable 
in this case one can minimize the weighted sum of squares where wi is the weight of the ith observation and is the diagonal matrix of such weights 
the weights should ideally be equal to the reciprocal of the variance of the measurement 
this implies that the observations are uncorrelated 
if the observations are correlated the expression applies 
in this case the weight matrix should ideally be equal to the inverse of the variance covariance matrix of the observations 
the normal equations are then this method is used in iteratively reweighted least squares 
parameter errors and correlation the estimated parameter values are linear combinations of the observed values therefore an expression for the estimated variance covariance matrix of the parameter estimates can be obtained by error propagation from the errors in the observations 
let the variance covariance matrix for the observations be denoted by and that of the estimated parameters by 
then when this simplifies to when unit weights are used the identity matrix it is implied that the experimental errors are uncorrelated and all equal where is the priori variance of an observation 
in any case is approximated by the reduced chi squared where is the minimum value of the weighted objective function the denominator is the number of degrees of freedom see effective degrees of freedom for generalizations for the case of correlated observations 
in all cases the variance of the parameter estimate is given by and the covariance between the parameter estimates and is given by the standard deviation is the square root of variance and the correlation coefficient is given by 
these error estimates reflect only random errors in the measurements 
the true uncertainty in the parameters is larger due to the presence of systematic errors which by definition cannot be quantified 
note that even though the observations may be uncorrelated the parameters are typically correlated 
parameter confidence limits it is often assumed for want of any concrete evidence but often appealing to the central limit theorem see normal distribution occurrence and applications that the error on each observation belongs to normal distribution with mean of zero and standard deviation under that assumption the following probabilities can be derived for single scalar parameter estimate in terms of its estimated standard error given here that the interval encompasses the true coefficient value that the interval encompasses the true coefficient value that the interval encompasses the true coefficient valuethe assumption is not unreasonable when if the experimental errors are normally distributed the parameters will belong to student distribution with degrees of freedom 
when student distribution approximates normal distribution 
note however that these confidence limits cannot take systematic error into account 
also parameter errors should be quoted to one significant figure only as they are subject to sampling error when the number of observations is relatively small chebychev inequality can be used for an upper bound on probabilities regardless of any assumptions about the distribution of experimental errors the maximum probabilities that parameter will be more than or standard deviations away from its expectation value are and respectively 
residual values and correlation the residuals are related to the observations by where is the idempotent matrix known as the hat matrix and is the identity matrix 
the variance covariance matrix of the residuals is given by thus the residuals are correlated even if the observations are not 
when the sum of weighted residual values is equal to zero whenever the model function contains constant term 
left multiply the expression for the residuals by xt wt say for example that the first term of the model is constant so that for all in that case it follows that thus in the motivational example above the fact that the sum of residual values is equal to zero is not accidental but is consequence of the presence of the constant term in the model 
if experimental error follows normal distribution then because of the linear relationship between residuals and observations so should residuals but since the observations are only sample of the population of all possible observations the residuals should belong to student distribution 
studentized residuals are useful in making statistical test for an outlier when particular residual appears to be excessively large 
see also iteratively reweighted least squares heteroscedasticity consistent standard errors weighted mean references
in machine learning boosting is an ensemble meta algorithm for primarily reducing bias and also variance in supervised learning and family of machine learning algorithms that convert weak learners to strong ones 
boosting is based on the question posed by kearns and valiant can set of weak learners create single strong learner 
weak learner is defined to be classifier that is only slightly correlated with the true classification it can label examples better than random guessing 
in contrast strong learner is classifier that is arbitrarily well correlated with the true classification 
robert schapire affirmative answer in paper to the question of kearns and valiant has had significant ramifications in machine learning and statistics most notably leading to the development of boosting when first introduced the hypothesis boosting problem simply referred to the process of turning weak learner into strong learner 
informally the hypothesis boosting problem asks whether an efficient learning algorithm that outputs hypothesis whose performance is only slightly better than random guessing 
weak learner implies the existence of an efficient algorithm that outputs hypothesis of arbitrary accuracy 
algorithms that achieve hypothesis boosting quickly became simply known as boosting 
freund and schapire arcing adapt at ive resampling and combining as general technique is more or less synonymous with boosting 
boosting algorithms while boosting is not algorithmically constrained most boosting algorithms consist of iteratively learning weak classifiers with respect to distribution and adding them to final strong classifier 
when they are added they are weighted in way that is related to the weak learners accuracy 
after weak learner is added the data weights are readjusted known as re weighting 
misclassified input data gain higher weight and examples that are classified correctly lose weight 
thus future weak learners focus more on the examples that previous weak learners misclassified 
there are many boosting algorithms 
the original ones proposed by robert schapire recursive majority gate formulation and yoav freund boost by majority were not adaptive and could not take full advantage of the weak learners 
schapire and freund then developed adaboost an adaptive boosting algorithm that won the prestigious del prize 
only algorithms that are provable boosting algorithms in the probably approximately correct learning formulation can accurately be called boosting algorithms 
other algorithms that are similar in spirit to boosting algorithms are sometimes called leveraging algorithms although they are also sometimes incorrectly called boosting algorithms the main variation between many boosting algorithms is their method of weighting training data points and hypotheses 
adaboost is very popular and the most significant historically as it was the first algorithm that could adapt to the weak learners 
it is often the basis of introductory coverage of boosting in university machine learning courses 
there are many more recent algorithms such as lpboost totalboost brownboost xgboost madaboost logitboost and others 
many boosting algorithms fit into the anyboost framework which shows that boosting performs gradient descent in function space using convex cost function 
object categorization in computer vision given images containing various known objects in the world classifier can be learned from them to automatically classify the objects in future images 
simple classifiers built based on some image feature of the object tend to be weak in categorization performance 
using boosting methods for object categorization is way to unify the weak classifiers in special way to boost the overall ability of categorization 
problem of object categorization object categorization is typical task of computer vision that involves determining whether or not an image contains some specific category of object 
the idea is closely related with recognition identification and detection 
appearance based object categorization typically contains feature extraction learning classifier and applying the classifier to new examples 
there are many ways to represent category of objects 
from shape analysis bag of words models or local descriptors such as sift etc 
examples of supervised classifiers are naive bayes classifiers support vector machines mixtures of gaussians and neural networks 
however research has shown that object categories and their locations in images can be discovered in an unsupervised manner as well 
status quo for object categorization the recognition of object categories in images is challenging problem in computer vision especially when the number of categories is large 
this is due to high intra class variability and the need for generalization across variations of objects within the same category 
objects within one category may look quite different 
even the same object may appear unalike under different viewpoint scale and illumination 
background clutter and partial occlusion add difficulties to recognition as well 
humans are able to recognize thousands of object types whereas most of the existing object recognition systems are trained to recognize only few 
human faces cars simple objects etc 
research has been very active on dealing with more categories and enabling incremental additions of new categories and although the general problem remains unsolved several multi category objects detectors for up to hundreds or thousands of categories have been developed 
one means is by feature sharing and boosting 
boosting for binary categorization adaboost can be used for face detection as an example of binary categorization 
the two categories are faces versus background 
the general algorithm is as follows form large set of simple features initialize weights for training images for rounds normalize the weights for available features from the set train classifier using single feature and evaluate the training error choose the classifier with the lowest error update the weights of the training images increase if classified wrongly by this classifier decrease if correctly form the final strong classifier as the linear combination of the classifiers coefficient larger if training error is small after boosting classifier constructed from features could yield detection rate under false positive rate another application of boosting for binary categorization is system that detects pedestrians using patterns of motion and appearance 
this work is the first to combine both motion information and appearance information as features to detect walking person 
it takes similar approach to the viola jones object detection framework 
boosting for multi class categorization compared with binary categorization multi class categorization looks for common features that can be shared across the categories at the same time 
they turn to be more generic edge like features 
during learning the detectors for each category can be trained jointly 
compared with training separately it generalizes better needs less training data and requires fewer features to achieve the same performance 
the main flow of the algorithm is similar to the binary case 
what is different is that measure of the joint training error shall be defined in advance 
during each iteration the algorithm chooses classifier of single feature features that can be shared by more categories shall be encouraged 
this can be done via converting multi class classification into binary one set of categories versus the rest or by introducing penalty error from the categories that do not have the feature of the classifier in the paper sharing visual features for multiclass and multiview object detection torralba et al 
used gentleboost for boosting and showed that when training data is limited learning via sharing features does much better job than no sharing given same boosting rounds 
also for given performance level the total number of features required and therefore the run time cost of the classifier for the feature sharing detectors is observed to scale approximately logarithmically with the number of class slower than linear growth in the non sharing case 
similar results are shown in the paper incremental learning of object detectors using visual shape alphabet yet the authors used adaboost for boosting 
convex vs non convex boosting algorithms boosting algorithms can be based on convex or non convex optimization algorithms 
convex algorithms such as adaboost and logitboost can be defeated by random noise such that they can learn basic and learnable combinations of weak hypotheses 
this limitation was pointed out by long servedio in however by multiple authors demonstrated that boosting algorithms based on non convex optimization such as brownboost can learn from noisy datasets and can specifically learn the underlying classifier of the long servedio dataset 
see also implementations scikit learn an open source machine learning library for python orange free data mining software suite module orange ensemble weka is machine learning set of tools that offers variate implementations of boosting algorithms like adaboost and logitboost package gbm generalized boosted regression models implements extensions to freund and schapire adaboost algorithm and friedman gradient boosting machine 
jboost adaboost logitboost robustboost boostexter and alternating decision trees package adabag applies multiclass adaboost adaboost samme and bagging package xgboost an implementation of gradient boosting for linear and tree based models 
notes references further reading yoav freund and robert schapire decision theoretic generalization of on line learning and an application to boosting journal of computer and system sciences robert schapire and yoram singer improved boosting algorithms using confidence rated predictors machine learning external links robert schapire the boosting approach to machine learning an overview msri mathematical sciences research institute workshop on nonlinear estimation and classification zhou zhi hua boosting years ccl keynote 
on the margin explanation of boosting algorithm pdf 
in proceedings of the st annual conference on learning theory colt 
on the doubt about margin explanation of boosting pdf
the chromatic circle is clock diagram for displaying relationships among the equal tempered pitch classes making up the familiar chromatic scale on circle 
explanation if one starts on any equal tempered pitch and repeatedly ascends by the musical interval of semitone one will eventually land on pitch with the same pitch class as the initial one having passed through all the other equal tempered chromatic pitch classes in between 
since the space is circular it is also possible to descend by semitone 
the chromatic circle is useful because it represents melodic distance which is often correlated with physical distance on musical instruments 
for instance to move from any on piano keyboard to the nearest one must move up four semitones corresponding to four clockwise steps on the chromatic circle 
one can also move down by eight semitones corresponding to eight counterclockwise steps on the pitch class circle 
larger motions on the piano or in pitch space can be represented in pitch class space by paths that wrap around the chromatic circle one or more times 
one can represent the twelve equal tempered pitch classes by the cyclic group of order twelve or equivalently the residue classes modulo twelve 
the group has four generators which can be identified with the ascending and descending semitones and the ascending and descending perfect fifths 
the semitonal generator gives rise to the chromatic circle while the perfect fifth gives rise to the circle of fifths 
comparison with circle of fifths key difference between the chromatic circle and the circle of fifths is that the former is truly continuous space every point on the circle corresponds to conceivable pitch class and every conceivable pitch class corresponds to point on the circle 
by contrast the circle of fifths is fundamentally discrete structure and there is no obvious way to assign pitch classes to each of its points 
pitch constellation pitch constellation is graphical representation of pitches used to describe musical scales modes chords or other groupings of pitches within an octave range 
it consists of circle with markings along the circumference or lines from the center which indicate pitches 
most pitch constellations use of subset of pitches chosen from the twelve pitch chromatic scale 
in this case the points on the circle are spaced like the twelve hour markings on an analog clock where each tick mark represents semitone 
scales and modes the pitch constellation provides an easy way to identify certain patterns and similarities between harmonic structures 
major scale consists of circle with markings at or and clock 
minor scale consists of circle with markings at or and clock 
the diagrams above show the two scales marked with scale degrees 
it can be observed that the tonic second fourth and fifth are shared while the minor scale flattens the third sixth and seventh notes relative to the major scale 
another observation is that the minor scale constellation is the same as the major scale but rotated degrees 
in the following drawing all of the major minor scales are drawn 
note that the constellation for all the major scales or all the minor scales are identical 
the different scales are generated by rotating the note overlay 
the notes that need to be sharpened flattened can be easily identified 
moreover if we draw all seven diatonic modes we can see them all as rotations of the ionian mode 
note also the significance of the clock point 
this corresponds to tritone 
the modes including pitches tritone from the tonic locrian and lydian are least used 
the clock and clock pitches are also important points corresponding to perfect fourth and perfect fifth respectively 
the most used scales modes major ionian mode minor aeolian mode and mixolydian include these pitches 
symmetric scales have simple representations in this scheme 
more exotic scales such as the pentatonic blues and octatonic can also be drawn and related to the common scales 
more complete list of musical scales and modes other overlays in previous sections we saw how various overlays scale degrees semi tone numbering notes can be used to notate the circumference of the constellation 
various other overlays can be laid around the constellation 
pitch ratios ratios of pitch frequencies 
note that once pitch constellation has been determined any number of overlays notes solf ge intervals etc 
may be placed on top for analysis comparison 
often generating one harmonic relationship from another is simply matter of rotating the overlay or constellation or shifting one or two pitch locations 
chords similarities between chords can also be observed as well as the significance of augmented diminished notes for triads we have the following and for seventh chords circle of fifths beginning with pitch constellation of chromatic scale the notes of circle of fifths can be easily generated 
starting at and moving across the circle and then one tick clockwise line is drawn with an arrow indicating the direction moved 
continuing from that point across the circle and one tick clockwise all points are connected 
moving through this pattern the notes of the circle of fifths can be determined 
one can also depict non tempered intervals on chromatic circle which allows one to depict commas small intervals particularly comma pumps 
for example using sequence of twelve just fifths ratio does not quite return to the starting point the size of the gap is the pythagorean comma resulting in broken circle of fifths 
technical note the ratio of the frequencies between two pitches in the constellation can be determined as follows 
take the length of the arc measured clockwise between the two points and divide by the circumference of the circle 
the frequency ratio is two raised to this power 
for example for fifth which is located at clock relative to the tonic the frequency ratio is references further reading brower candace cognitive theory of musical meaning journal of music theory duke university press doi jstor ku inskas darius symmetry in creative work of mikalojus konstantinas iurlionis pdf menotyra olson harry music physics and engineering dover publications isbn external links notenscheibe web application pitch constellations of scales triads intervals and the circle of fifths with basic audio on line app illustrating pitch constellations scaletapper iphone app which utilizes pitch constellations 
pdf of musical scales
among alternative tunings for guitar major thirds tuning is regular tuning in which each interval between successive open strings is major third in musical abbreviation
other names for major thirds tuning include major third tuning tuning all thirds tuning and augmented tuning
by definition major third interval separates two notes that differ by exactly four semitones one third of the twelve note octave
the spanish guitar tuning mixes four perfect fourths five semitones and one major third the latter occurring between the and strings this tuning which is used for acoustic and electric guitars is called standard in english convention that is followed in this article
while standard tuning is irregular mixing four fourths and one major third tunings are regular only major third intervals occur between the successive strings of the tunings for example the open augmented tuning
for each tuning the open strings form an augmented triad in two octaves
for guitars with six strings every major third tuning repeats its three open notes in two octaves so providing many options for fingering chords
by repeating open string notes and by having uniform intervals between strings major thirds tuning simplifies learning by beginners
these features also facilitate advanced guitarists improvisation precisely the aim of jazz guitarist ralph patt when he began popularizing major thirds tuning between and
avoiding standard tuning irregular intervals in standard tuning the successive open strings mix two types of intervals four perfect fourths and the major third between the and strings only major thirds occur as open string intervals for major thirds tuning which is also called major third tuning all thirds tuning and tuning
the most viable tunings are all of these tunings reduce the overall range of the instrument bit the first takes off the top of the range and the last takes off the bottom of the range
one popular tuning has the open strings which some guitarists have applied to the top six strings of seven string guitar with the low seventh string tuned to the low to restore the standard range
while tuning can use standard sets of guitar strings specialized string gauges have been recommended
the middle tunings are compromise each losing note or two off both the top and the bottom of the range
for example for six string guitars the tuning loses the two lowest semitones on the low string and the two highest semitones from the high string in standard tuning it can use string sets for standard tuning note that regardless of which note is chosen to start the tuning sequence there are only four distinct sets of open note pitch classes
the major thirds tunings respectively have the open notes properties major thirds tunings require less hand stretching than other tunings because each tuning packs the octave twelve notes into four consecutive frets
the major third intervals allow major chords and minor chords to be played with two three consecutive fingers on two consecutive frets
every major thirds tuning is regular and repetitive two properties that facilitate learning by beginners and improvisation by advanced guitarists
four frets for the four fingers in major thirds tuning the chromatic scale is arranged on three consecutive strings in four consecutive frets
this four fret arrangement facilitates the left hand technique for classical spanish guitar for each hand position of four frets the hand is stationary and the fingers move each finger being responsible for one fret
consequently three hand positions covering frets and partition the fingerboard of classical guitar which has exactly frets only two or three frets are needed for the guitar chords major minor and dominant sevenths which are emphasized in introductions to guitar playing and to the fundamentals of music
each major and minor chord can be played on two successive frets on three successive strings and therefore each needs only two fingers
other chords seconds fourths sevenths and ninths are played on only three successive frets
for fundamental chord fingerings major thirds tuning simplicity and consistency are not shared by standard tuning whose seventh chord fingering is discussed at the end of this section
repetition each major thirds tuning repeats its open notes after every two strings which results in two copies of the three open strings notes each in different octave
this repetition again simplifies the learning of chords and improvisation
this advantage is not shared by two popular regular tunings all fourths and all fifths tuning chord inversion is especially simple in major thirds tuning
chords are inverted simply by raising one or two notes three strings
the raised notes are played with the same finger as the original notes
thus major and minor chords are played on two frets in tuning even when they are inverted
in contrast inversions of chords in standard tuning require three fingers on span of four frets in standard tuning the shape of inversions depends on the involvement of the irregular major third
regular musical intervals in each regular tuning the musical intervals are the same for each pair of consecutive strings
other regular tunings include all fourths augmented fourths and all fifths tunings
for each regular tuning chord patterns may be moved around the fretboard property that simplifies beginners learning of chords and advanced players improvisation in contrast chords cannot be shifted around the fretboard in standard tuning which requires four chord shapes for the major chords there are separate fingerings for chords having root notes on one of the four strings three six
shifting chords vertical and diagonal the repetition of the major thirds tuning enables notes and chords to be raised one octave by being vertically shifted by three strings
notes and chords may be shifted diagonally in major thirds tuning by combining vertical shift of one string with horizontal shift of four frets like all regular tunings chords in the major third tuning can be moved across the fretboard ascending or descending major third for each string in standard tuning playing scales of one octave requires three patterns which depend on the string of the root note
chords cannot be shifted diagonally without changing finger patterns
standard tuning has four finger patterns for musical intervals four forms for basic major chords and three forms for the inversion of the basic major chords
open chords and beginning players major thirds tunings are unconventional open tunings in which the open strings form an augmented triad
in tunings the augmented fifth replaces the perfect fifth of the major triad which is used in conventional open tunings
for example the augmented triad has in place of the major triad the note is enharmonically equivalent to as noted above
consequently tunings are also called open augmented fifth tunings in french la guitare majeure quinte augment instructional literature uses standard tuning
traditionally course begins with the hand in first position that is with the left hand covering frets
beginning players first learn open chords belonging to the major keys and guitarists who play mainly open chords in these three major keys and their relative minor keys am em bm may prefer standard tuning over an tuning
in particular hobbyists playing folk music around campfire are well served by standard tuning
such hobbyists may also play major thirds tuning which also has many open chords with notes on five or six strings chords with five six strings have greater volume than chords with three four strings and so are useful for acoustic guitars for example acoustic electric guitars without amplification
intermediate guitarists do not limit themselves to one hand position and consequently open chords are only part of their chordal repertoire
in contemporary music master guitarists think diagonally and move up and down the strings fluency on the entire fretboard is needed particularly by guitarists playing jazz
according to its inventor ralph patt major thirds tuning makes the hard things easy and the easy things hard
this is never going to take the place of folk guitar and it not meant to
for difficult music and for where we are going in free jazz and even the old be bop jazz this is much easier way to play
left handed chords major thirds tuning is closely related to minor sixths tuning which is the regular tuning that is based on the minor sixth the interval of eight semitones
either ascending by major third or by descending by minor sixth one arrives at the same pitch class the same note representing pitches in different octaves
intervals paired like the pair of major third and minor sixth intervals are termed inverse intervals in the theory of music
consequently chord charts for minor sixths tunings may be used for left handed major thirds tunings conversely chord charts for major thirds tunings may be used for left handed minor sixths tunings
fingering of seventh chords major thirds tuning facilitates playing chords with closed voicings
in contrast standard tuning would require more hand stretching to play closed voice seventh chords and so standard tuning uses open voicings for many four note chords for example of dominant seventh chords
by definition dominant seventh is four note chord combining major chord and minor seventh
for example the seventh chord combines the major chord with
in standard tuning extending the root bass major chord to chord would span six frets such seventh chords contain some pretty serious stretches in the left hand
an illustration shows this voicing which would be extremely difficult to play in standard tuning besides the openly voiced chord that is conventional in standard tuning this open position chord is termed second inversion drop chord because the second highest note in the second inversion chord is lowered by an octave
disadvantages while major thirds tuning confers the numerous advantages detailed above it also introduces certain disadvantages as compared to the instrument standard tuning tuning decreases the overall range of the guitar this is why some players eventually resorted to and string instruments to regain that lost range simplifies the voicing of chords in close harmony but it makes certain common voicings in open harmony more difficult or even impossible facilitates moving and note chords up or down an octave but it makes the fingerings for and note multi octave chords more complex and awkward
history major thirds tuning was introduced in by jazz guitarist ralph patt
he was studying with gunther schuller whose twelve tone technique was invented for atonal composition by his teacher arnold schoenberg
patt was also inspired by the free jazz of ornette coleman and john coltrane
seeking guitar tuning that would facilitate improvisation using twelve tones he introduced major thirds tuning by perhaps in to achieve the open string range of standard spanish tuning patt started using seven string guitars in before settling on eight string guitars with high equivalently as their highest open notes
patt used major thirds tuning during all of his work as session musician after in new york
patt developed webpage with extensive information about major thirds tuning
see also minor thirds tuning repetitive open tunings approximate tunings non spanish classical guitars english its open tuning approximates russian its string open tuning approximates other open tunings open tuning approximates open tuning approximates open tuning approximates open tuning approximates open tuning approximates open tuning approximates open tuning approximates references footnotes citations bibliography further reading sethares bill january
alternate tuning guide pdf
madison wisconsin university of wisconsin department of electrical engineering
external links wolfowitz kiefer may august
chord diagrams for major thirds tuning pdf
dictionary of chords major minor dominant sevenths diagrams of sevenths major minor dominant half diminished arising in the tertian harmonization of the major scale on etc
retrieved april professors andreas griewank and william sethares each recommend discussions of major thirds tuning by two jazz guitarists sethares regular tunings and griewank ole kirkeby for and string guitars charts of intervals major minor and dominant chords recommended gauges for strings
ralph patt for and string guitars charts of scales chords and chord progressions string gauges
three other jazz guitar websites oberlin alexandre october
tuning your guitar in major thirds tune afresh and improvise
archived from the original on march retrieved december zemb patrick august
sommaire du site musical french summary of the musical site
archived from the original on june retrieved august corman tony august
free downloadable method book
archived from the original on august retrieved august guitar tunings database
tuner scales and chords for tunings most popular and for beginners
retrieved december video tutorial on major and minor chords in major thirds tuning on youtube bromley keith october
chord shapes for major thirds tuning on string guitar pdf
new standard tuning nst is an alternative tuning for the guitar that approximates all fifths tuning
the guitar strings are assigned the notes from lowest to highest the five lowest open strings are each tuned to an interval of perfect fifth the two highest strings are minor third apart
all fifths tuning is typically used for mandolins cellos violas and violins
on guitar tuning the strings in fifths would mean the first string would be high something that was impractical until recently
nst provides good approximation to all fifths tuning
like other regular tunings nst allows chord fingerings to be shifted from one set of strings to another
nst range is wider both lower and higher than the range of standard tuning in which the strings are tuned to the open notes
the greater range allows nst guitars to play repertoire that would be impractical if not impossible on standard tuned guitar
nst was developed by robert fripp the guitarist for king crimson
fripp taught the new standard tuning in guitar craft courses beginning in and thousands of guitar craft students continue to use the tuning
like other alternative tunings for guitar nst provides challenges and new opportunities to guitarists who have developed music especially suited to nst
nst places the guitar strings under greater tension than standard tuning
standard sets of guitar strings do not work well with the tuning as the lowest strings are too loose and the highest string may snap under the increased tension
special sets of nst strings have been available for decades and some guitarists have assembled nst sets from individual strings
history new standard tuning nst was invented by robert fripp of the band king crimson in september
was in the apple health spa on bleecker and thompson back in september in the sauna at half past in the morning almost asleep and the tuning flew over my head
at the time couldn understand what it was for
was asked to give guitar seminar at claymont court in december to raise funds for the running of the estate and the children school
there was click and realized the tuning was for the guitar class
fripp began using the tuning in before beginning his guitar craft seminars which have taught the tuning to three thousand guitarists
the tuning is from low to high
the original version of nst was all fifths tuning
however in the fripp never attained the all fifth high while he could attain the string lifetime distribution was too short
experimenting with string fripp succeeded
originally seen in ths all the way the top string would not go to so as on tenor banjo adopted an on the first string
these kept breaking so was adopted
in fripp suggested that guitar circle members experiment with an string from octave plus of gary goodman if successful the experiment could lead to the nst according to fripp
in fripp suggested renaming the tuning as guitar craft standard tuning or pentatonic tuning
properties the lowest five strings are tuned in perfect fifths from low the first string is minor third up from the to since the lowest five strings are tuned in fifths guitars with nst can be played with the fingerings for chords and scales used on the violin cello and mandolin the first five strings of nst have all fifths tuning and so its all fifths chords are movable around the fretboard
in contrast standard tuning has an irregular major third interjected among its perfect fourths which complicates the learning of chords by beginners the distinct open notes are the notes of the major pentatonic scale on which contains only consonant intervals
the pentatonic scale omits the open of standard tuning and all fifths tuning which forms dissonant second interval with with the king crimson fripp had used pentatonic harmony in discipline thela hun ginjeet and sartori in tangier
harmonics overtones with note of music one strikes the fundamental and in addition to the root note other notes are generated these are the harmonic series as one fundamental note contains within it other notes in the octave two fundamentals produce remarkable array of harmonics and the number of possible combinations between all the notes increases phenomenally
with triad affairs stand good chance of getting severely out of hand
new standard tuning lists four notes from the harmonic sequence overtones for the note when the low open note string is struck its harmonic sequence begins with the notes to strengthen given chord vincent persichetti twentieth century harmony recommends adding perfect fifths above the initial overtones rather than adding higher overtones such as and the higher persichetti book influenced fripp
in new standard tuning is the fundamental overtone as fifth reinforces as fifth reinforces as fifth reinforces both as fifth reinforces and as the fifth overtone reinforces and as the sixth overtone reinforces range like all fifths tuning nst has greater range than the standard tuning perfect fifth greater major third lower and minor third higher
chords perfect intervals rather than thirds asked whether nst facilitates new intervals or harmonies that aren readily available in standard tuning fripp responded yes that part of it
it more rational system but it also better sounding better for chords better for single notes
to build chords fripp uses perfect intervals in fourths fifths and octaves so avoiding minor and major thirds
quartal and quintal harmony was stressed from the beginning of fripp teaching of guitar craft
fripp began course with these directions now pick note from the following series it was series of fourths or fifths
when you are ready do not be in any hurry but when you are ready play your note then pick others and play them as the situation demands it
your first note will be the first intentional note you have played in week
it is challenge to adapt conventional guitar chords to new standard tuning
nst has wider intervals between consecutive strings than standard tuning
most songs that is music which has both words and instrumental accompaniments written in the nst have quality of walking on long stilts
there are rarely many intervals harmonic or melodic in these guitar accompaniments that are closer than major third except in the top of the voicing
close voicings from single guitar in nst are possible thanks to the minor third between the first and second string and this is often the only practical place where close voicings occur with any regularity
historical background modern quartal and quintal harmony revives the polyphonic traditions of medieval europe
before the common practice period european polyphony emphasized unison intervals and octaves and also perfect fifths
from the renaissance to western symphonic music was diatonic emphasizing the tertian harmony of major and minor scales keys and chords
much popular music especially rock retains diatonic harmony
string gauges with traditional guitar strings the low may be loose and the high may be too tight
special gauges are therefore more suitable for nst
for steel stringed acoustic guitars many guitar craft participants use either an inch set or an inch set string sets may be purchased as set from manufacturer or purchased singly and assembled by the guitarist
in inch gauge was being evaluated by fripp and other members of guitar circle who are considering replacing the first string note with an note the better to approximate the note of all fifths tuning
the inch gauge was produced by octave plus of gary goodman
robert fripp uses lighter strings for electric guitar
artists who use nst robert fripp has used the new standard tuning since fripp has taught nst in his guitar craft courses
in guitar craft and since in the successor guitar circles students use only new standard tuning
having to use new tuning the students are challenged to approach their playing with greater mindfulness putting to rest their habitual use of automatic chords or licks
with the new tuning guitarists have to find new ways of musical expression the tuning is used by students of guitar craft of which there have been three thousand
guitar craft alumni who continue to practice nst are called crafty guitarists or crafties
some crafty guitarists formed the league of crafty guitarists lcg which toured with robert fripp and released several albums
the guitar craft experience and the league of crafty guitarists trained guitarists who went on to form new bands such as the california guitar trio and trey gunn the california guitar trio and gunn toured with fripp as the robert fripp string quintet
other alumni of the league of crafty guitarists include members of los gauchos alemanes such as guitarist steve ball ball is associated with the seattle guitar circle along with lcg alumnus curt golden
the collection plague of crafty guitarists features the following guitar craft alumni who were listed in review by barry cleveland tobin buttram nigel gavin geary street quartet bill hibbits janssen and jensen alejandro miniaci and from power trio project sur pacifico playmovil and santos luminosos
nst has been adapted for instruments besides guitar
italian guitarist fabio mittino plays in nst
trey gunn crimson warr guitar player from to and markus reuter tuner with crimson drummer pat mastelotto have adapted nst for their and string instruments in reuter used tuning
finnish musician heikki malmberg uses string guitar tuned in nst with an additional low additionally british musician michael linden west plays the oud in nst
see also notes references cleveland barry august
the plague of crafty guitarists volume one
archived from the original on february retrieved march cleveland barry december
california guitar trio interview pdf
guitar player subscription required
retrieved march fripp robert
seven guitar craft themes definitive scores for guitar ensemble
original transcriptions by curt golden layout scores and tablatures ariel rzezak and theo morresi first limited ed
ismn dgm sku partitas
on the discipline of craft and art an interview with robert fripp
retrieved january persichetti vincent
twentieth century harmony creative aspects and practice
isbn oclc sethares william
madison wisconsin university of wisconsin department of electrical engineering
pdf version by bill sethares
retrieved may tamm eric robert fripp from crimson king to crafty master progressive ears ed
faber and faber isbn zipped microsoft word document archived from the original on october retrieved march zwerdling daniel september
all things considered npr weekend ed
washington dc national public radio
html transcription subscription required
archived from the original on october retrieved march
further reading drozdowski ted february
robert fripp plectral purist answers the dumb questions
external links courses in new standard tuning are offered by guitar circle the successor of guitar craft guitar circle of europe guitar circle of latin america guitar circle of north america the frakctured zone is king crimson fan website with notation and tabs to songs in nst with acknowledgment to trey gunn for permission
harmonization of diatonic major scale on progressions of chords triads and sevenths
new standard tuning of robert fripp guitar craft pdf
in statistics mediation model seeks to identify and explain the mechanism or process that underlies an observed relationship between an independent variable and dependent variable via the inclusion of third hypothetical variable known as mediator variable also mediating variable intermediary variable or intervening variable 
rather than direct causal relationship between the independent variable and the dependent variable mediation model proposes that the independent variable influences the mediator variable which in turn influences the dependent variable 
thus the mediator variable serves to clarify the nature of the relationship between the independent and dependent variables mediation analyses are employed to understand known relationship by exploring the underlying mechanism or process by which one variable influences another variable through mediator variable 
in particular mediation analysis can contribute to better understanding the relationship between an independent variable and dependent variable when these variables do not have an obvious direct connection 
baron and kenny steps for mediation analysis baron and kenny laid out several requirements that must be met to form true mediation relationship 
they are outlined below using real world example 
see the diagram above for visual representation of the overall mediating relationship to be explained 
note hayes critiqued baron and kenny mediation steps approach and as of david kenny on his website stated that mediation can exist in the absence of significant total effect and therefore step below may not be needed 
this situation is sometimes referred to as inconsistent mediation 
later publications by hayes also questioned the concepts of full or partial mediation and advocated for these terms along with the classical mediation steps approach outlined below to be abandoned 
step regress the dependent variable on the independent variable to confirm that the independent variable is significant predictor of the dependent variable independent variable dependent variable is significant step regress the mediator on the independent variable to confirm that the independent variable is significant predictor of the mediator 
if the mediator is not associated with the independent variable then it couldn possibly mediate anything independent variable mediator is significant step regress the dependent variable on both the mediator and independent variable to confirm that the mediator is significant predictor of the dependent variable and the strength of the coefficient of the previously significant independent variable in step is now greatly reduced if not rendered nonsignificant 
is significant should be smaller in absolute value than the original effect for the independent variable above example the following example drawn from howell explains each step of baron and kenny requirements to understand further how mediation effect is characterized 
step and step use simple regression analysis whereas step uses multiple regression analysis 
how you were parented independent variable predicts how confident you feel about parenting your own children dependent variable 
how you were parented independent variable predicts your feelings of competence and self esteem mediator 
your feelings of competence and self esteem mediator predict how confident you feel about parenting your own children dependent variable while controlling for how you were parented independent variable such findings would lead to the conclusion implying that your feelings of competence and self esteem mediate the relationship between how you were parented and how confident you feel about parenting your own children 
if step does not yield significant result one may still have grounds to move to step sometimes there is actually significant relationship between independent and dependent variables but because of small sample sizes or other extraneous factors there could not be enough power to predict the effect that actually exists 
direct versus indirect effects in the diagram shown above the indirect effect is the product of path coefficients and 
the direct effect is the coefficient 
the direct effect measures the extent to which the dependent variable changes when the independent variable increases by one unit and the mediator variable remains unaltered 
in contrast the indirect effect measures the extent to which the dependent variable changes when the independent variable is held constant and the mediator variable changes by the amount it would have changed had the independent variable increased by one unit 
in linear systems the total effect is equal to the sum of the direct and indirect ab in the model above 
in nonlinear models the total effect is not generally equal to the sum of the direct and indirect effects but to modified combination of the two 
full versus partial mediation mediator variable can either account for all or some of the observed relationship between two variables 
full mediation maximum evidence for mediation also called full mediation would occur if inclusion of the mediation variable drops the relationship between the independent variable and dependent variable see pathway in diagram above to zero 
partial mediation partial mediation maintains that the mediating variable accounts for some but not all of the relationship between the independent variable and dependent variable 
partial mediation implies that there is not only significant relationship between the mediator and the dependent variable but also some direct relationship between the independent and dependent variable 
in order for either full or partial mediation to be established the reduction in variance explained by the independent variable must be significant as determined by one of several tests such as the sobel test 
the effect of an independent variable on the dependent variable can become nonsignificant when the mediator is introduced simply because trivial amount of variance is explained not true mediation 
thus it is imperative to show significant reduction in variance explained by the independent variable before asserting either full or partial mediation 
it is possible to have statistically significant indirect effects in the absence of total effect 
this can be explained by the presence of several mediating paths that cancel each other out and become noticeable when one of the cancelling mediators is controlled for 
this implies that the terms partial and full mediation should always be interpreted relative to the set of variables that are present in the model 
in all cases the operation of fixing variable must be distinguished from that of controlling for variable which has been inappropriately used in the literature 
the former stands for physically fixing while the latter stands for conditioning on adjusting for or adding to the regression model 
the two notions coincide only when all error terms not shown in the diagram are statistically uncorrelated 
when errors are correlated adjustments must be made to neutralize those correlations before embarking on mediation analysis see bayesian networks 
sobel test sobel test is performed to determine if the relationship between the independent variable and dependent variable has been significantly reduced after inclusion of the mediator variable 
in other words this test assesses whether mediation effect is significant 
it examines the relationship between the independent variable and the dependent variable compared to the relationship between the independent variable and dependent variable including the mediation factor 
the sobel test is more accurate than the baron and kenny steps explained above however it does have low statistical power 
as such large sample sizes are required in order to have sufficient power to detect significant effects 
this is because the key assumption of sobel test is the assumption of normality 
because sobel test evaluates given sample on the normal distribution small sample sizes and skewness of the sampling distribution can be problematic see normal distribution for more details 
thus the rule of thumb as suggested by mackinnon et al is that sample size of is required to detect small effect sample size of is sufficient in detecting medium effect and sample size of is required to detect large effect 
the equation for sobel is preacher hayes bootstrap method the bootstrapping method provides some advantages to the sobel test primarily an increase in power 
the preacher and hayes bootstrapping method is non parametric test 
as such the bootstrap method does not violate assumptions of normality and is therefore recommended for small sample sizes 
bootstrapping involves repeatedly randomly sampling observations with replacement from the data set to compute the desired statistic in each resample 
computing over hundreds or thousands of bootstrap resamples provide an approximation of the sampling distribution of the statistic of interest 
the preacher hayes method provides point estimates and confidence intervals by which one can assess the significance or nonsignificance of mediation effect 
point estimates reveal the mean over the number of bootstrapped samples and if zero does not fall between the resulting confidence intervals of the bootstrapping method one can confidently conclude that there is significant mediation effect to report 
significance of mediation as outlined above there are few different options one can choose from to evaluate mediation model 
bootstrapping is becoming the most popular method of testing mediation because it does not require the normality assumption to be met and because it can be effectively utilized with smaller sample sizes 
however mediation continues to be most frequently determined using the logic of baron and kenny or the sobel test 
it is becoming increasingly more difficult to publish tests of mediation based purely on the baron and kenny method or tests that make distributional assumptions such as the sobel test 
thus it is important to consider your options when choosing which test to conduct 
approaches to mediation while the concept of mediation as defined within psychology is theoretically appealing the methods used to study mediation empirically have been challenged by statisticians and epidemiologists and interpreted formally 
experimental causal chain design an experimental causal chain design is used when the proposed mediator is experimentally manipulated 
such design implies that one manipulates some controlled third variable that they have reason to believe could be the underlying mechanism of given relationship 
measurement of mediation design measurement of mediation design can be conceptualized as statistical approach 
such design implies that one measures the proposed intervening variable and then uses statistical analyses to establish mediation 
this approach does not involve manipulation of the hypothesized mediating variable but only involves measurement 
criticisms of mediation measurement experimental approaches to mediation must be carried out with caution 
first it is important to have strong theoretical support for the exploratory investigation of potential mediating variable 
criticism of mediation approach rests on the ability to manipulate and measure mediating variable 
thus one must be able to manipulate the proposed mediator in an acceptable and ethical fashion 
as such one must be able to measure the intervening process without interfering with the outcome 
the mediator must also be able to establish construct validity of manipulation 
one of the most common criticisms of the measurement of mediation approach is that it is ultimately correlational design 
consequently it is possible that some other third variable independent from the proposed mediator could be responsible for the proposed effect 
however researchers have worked hard to provide counter evidence to this disparagement 
specifically the following counter arguments have been put forward temporal precedence for example if the independent variable precedes the dependent variable in time this would provide evidence suggesting directional and potentially causal link from the independent variable to the dependent variable 
nonspuriousness and or no confounds for example should one identify other third variables and prove that they do not alter the relationship between the independent variable and the dependent variable he she would have stronger argument for their mediation effect 
see other rd variables below mediation can be an extremely useful and powerful statistical test however it must be used properly 
it is important that the measures used to assess the mediator and the dependent variable are theoretically distinct and that the independent variable and mediator cannot interact 
should there be an interaction between the independent variable and the mediator one would have grounds to investigate moderation 
other third variables confounding another model that is often tested is one in which competing variables in the model are alternative potential mediators or an unmeasured cause of the dependent variable 
an additional variable in causal model may obscure or confound the relationship between the independent and dependent variables 
potential confounders are variables that may have causal impact on both the independent variable and dependent variable 
they include common sources of measurement error as discussed above as well as other influences shared by both the independent and dependent variables 
in experimental studies there is special concern about aspects of the experimental manipulation or setting that may account for study effects rather than the motivating theoretical factor 
any of these problems may produce spurious relationships between the independent and dependent variables as measured 
ignoring confounding variable may bias empirical estimates of the causal effect of the independent variable 
suppression suppressor variable increases the predictive validity of another variable when included in regression equation 
suppression can occur when single causal variable is related to an outcome variable through two separate mediator variables and when one of those mediated effects is positive and one is negative 
in such case each mediator variable suppresses or conceals the effect that is carried through the other mediator variable 
for example higher intelligence scores causal variable may cause an increase in error detection mediator variable which in turn may cause decrease in errors made at work on an assembly line an outcome variable at the same time intelligence could also cause an increase in boredom which in turn may cause an increase in errors 
thus in one causal path intelligence decreases errors and in the other it increases them 
when neither mediator is included in the analysis intelligence appears to have no effect or weak effect on errors 
however when boredom is controlled intelligence will appear to decrease errors and when error detection is controlled intelligence will appear to increase errors 
if intelligence could be increased while only boredom was held constant errors would decrease if intelligence could be increased while holding only error detection constant errors would increase 
in general the omission of suppressors or confounders will lead to either an underestimation or an overestimation of the effect of on thereby either reducing or artificially inflating the magnitude of relationship between two variables 
moderators other important third variables are moderators 
moderators are variables that can make the relationship between two variables either stronger or weaker 
such variables further characterize interactions in regression by affecting the direction and or strength of the relationship between and moderating relationship can be thought of as an interaction 
it occurs when the relationship between variables and depends on the level of see moderation for further discussion 
moderated mediation mediation and moderation can co occur in statistical models 
it is possible to mediate moderation and moderate mediation 
moderated mediation is when the effect of the treatment on the mediator and or the partial effect on the dependent variable depend in turn on levels of another variable moderator 
essentially in moderated mediation mediation is first established and then one investigates if the mediation effect that describes the relationship between the independent variable and dependent variable is moderated by different levels of another variable moderator 
this definition has been outlined by muller judd and yzerbyt and preacher rucker and hayes 
models of moderated mediation there are five possible models of moderated mediation as illustrated in the diagrams below 
in the first model the independent variable also moderates the relationship between the mediator and the dependent variable 
the second possible model of moderated mediation involves new variable which moderates the relationship between the independent variable and the mediator the path 
the third model of moderated mediation involves new moderator variable which moderates the relationship between the mediator and the dependent variable the path 
moderated mediation can also occur when one moderating variable affects both the relationship between the independent variable and the mediator the path and the relationship between the mediator and the dependent variable the path 
the fifth and final possible model of moderated mediation involves two new moderator variables one moderating the path and the other moderating the path 
in addition to the models mentioned above new variable can also exist which moderates the relationship between the independent variable and mediator the path while at the same time have the new variable moderate the relationship between the independent variable and dependent variable the path 
mediated moderation mediated moderation is variant of both moderation and mediation 
this is where there is initially overall moderation and the direct effect of the moderator variable on the outcome is mediated 
the main difference between mediated moderation and moderated mediation is that for the former there is initial overall moderation and this effect is mediated and for the latter there is no moderation but the effect of either the treatment on the mediator path is moderated or the effect of the mediator on the outcome path is moderated in order to establish mediated moderation one must first establish moderation meaning that the direction and or the strength of the relationship between the independent and dependent variables path differs depending on the level of third variable the moderator variable 
researchers next look for the presence of mediated moderation when they have theoretical reason to believe that there is fourth variable that acts as the mechanism or process that causes the relationship between the independent variable and the moderator path or between the moderator and the dependent variable path 
example the following is published example of mediated moderation in psychological research 
participants were presented with an initial stimulus prime that made them think of morality or made them think of might 
they then participated in the prisoner dilemma game pdg in which participants pretend that they and their partner in crime have been arrested and they must decide whether to remain loyal to their partner or to compete with their partner and cooperate with the authorities 
the researchers found that prosocial individuals were affected by the morality and might primes whereas proself individuals were not 
thus social value orientation proself vs prosocial moderated the relationship between the prime independent variable morality vs might and the behaviour chosen in the pdg dependent variable competitive vs cooperative 
the researchers next looked for the presence of mediated moderation effect 
regression analyses revealed that the type of prime morality vs might mediated the moderating relationship of participants social value orientation on pdg behaviour 
prosocial participants who experienced the morality prime expected their partner to cooperate with them so they chose to cooperate themselves 
prosocial participants who experienced the might prime expected their partner to compete with them which made them more likely to compete with their partner and cooperate with the authorities 
in contrast participants with pro self social value orientation always acted competitively 
regression equations for moderated mediation and mediated moderation muller judd and yzerbyt outline three fundamental models that underlie both moderated mediation and mediated moderation 
mo represents the moderator variable me represents the mediator variable and represents the measurement error of each regression equation 
step moderation of the relationship between the independent variable and the dependent variable also called the overall treatment effect path in the diagram 
to establish overall moderation the regression weight must be significant first step for establishing mediated moderation 
establishing moderated mediation requires that there be no moderation effect so the regression weight must not be significant 
step moderation of the relationship between the independent variable and the mediator path 
if the regression weight is significant the moderator affects the relationship between the independent variable and the mediator 
step moderation of both the relationship between the independent and dependent variables path and the relationship between the mediator and the dependent variable path 
if both in step and in step are significant the moderator affects the relationship between the independent variable and the mediator path 
if both in step and in step are significant the moderator affects the relationship between the mediator and the dependent variable path 
either or both of the conditions above may be true 
causal mediation analysis fixing versus conditioning mediation analysis quantifies the extent to which variable participates in the transmittance of change from cause to its effect 
it is inherently causal notion hence it cannot be defined in statistical terms 
traditionally however the bulk of mediation analysis has been conducted within the confines of linear regression with statistical terminology masking the causal character of the relationships involved 
this led to difficulties biases and limitations that have been alleviated by modern methods of causal analysis based on causal diagrams and counterfactual logic 
the source of these difficulties lies in defining mediation in terms of changes induced by adding third variables into regression equation 
such statistical changes are epiphenomena which sometimes accompany mediation but in general fail to capture the causal relationships that mediation analysis aims to quantify 
the basic premise of the causal approach is that it is not always appropriate to control for the mediator when we seek to estimate the direct effect of on see the figure above 
the classical rationale for controlling for is that if we succeed in preventing from changing then whatever changes we measure in are attributable solely to variations in and we are justified then in proclaiming the effect observed as direct effect of on 
unfortunately controlling for does not physically prevent from changing it merely narrows the analyst attention to cases of equal values 
moreover the language of probability theory does not possess the notation to express the idea of preventing from changing or physically holding constant 
the only operator probability provides is conditioning which is what we do when we control for or add as regressor in the equation for the result is that instead of physically holding constant say at and comparing for units under to those under we allow to vary but ignore all units except those in which achieves the value these two operations are fundamentally different and yield different results except in the case of no omitted variables 
to illustrate assume that the error terms of and are correlated 
under such conditions the structural coefficient and between and and between and can no longer be estimated by regressing on and in fact the regression slopes may both be nonzero even when is zero 
this has two consequences 
first new strategies must be devised for estimating the structural coefficients and second the basic definitions of direct and indirect effects must go beyond regression analysis and should invoke an operation that mimics fixing rather than conditioning on definitions such an operator denoted do was defined in pearl and it operates by removing the equation of and replacing it by constant for example if the basic mediation model consists of the equations then after applying the operator do the model becomes and after applying the operator do the model becomes where the functions and as well as the distributions of the error terms and remain unaltered 
if we further rename the variables and resulting from do as and respectively we obtain what came to be known as potential outcomes or structural counterfactuals 
these new variables provide convenient notation for defining direct and indirect effects 
in particular four types of effects have been defined for the transition from to total effect controlled direct effect natural direct effect natural indirect effect where stands for expectation taken over the error terms 
these effects have the following interpretations te measures the expected increase in the outcome as changes from to while the mediator is allowed to track the change in as dictated by the function 
cde measures the expected increase in the outcome as changes from to while the mediator is fixed at pre specified level uniformly over the entire population nde measures the expected increase in as changes from to while setting the mediator variable to whatever value it would have obtained under before the change 
nie measures the expected increase in when the is held constant at and changes to whatever value it would have attained for each individual under the difference te nde measures the extent to which mediation is necessary for explaining the effect while the nie measures the extent to which mediation is sufficient for sustaining it controlled version of the indirect effect does not exist because there is no way of disabling the direct effect by fixing variable to constant 
according to these definitions the total effect can be decomposed as sum where nier stands for the reverse transition from to it becomes additive in linear systems where reversal of transitions entails sign reversal 
the power of these definitions lies in their generality they are applicable to models with arbitrary nonlinear interactions arbitrary dependencies among the disturbances and both continuous and categorical variables 
the mediation formula in linear analysis all effects are determined by sums of products of structural coefficients giving independent of therefore all effects are estimable whenever the model is identified 
in non linear systems more stringent conditions are needed for estimating the direct and indirect effects 
for example if no confounding exists and are mutually independent the following formulas can be derived 
the last two equations are called mediation formulas and have become the target of estimation in many studies of mediation 
they give distribution free expressions for direct and indirect effects and demonstrate that despite the arbitrary nature of the error distributions and the functions and mediated effects can nevertheless be estimated from data using regression 
the analyses of moderated mediation and mediating moderators fall as special cases of the causal mediation analysis and the mediation formulas identify how various interactions coefficients contribute to the necessary and sufficient components of mediation 
example assume the model takes the form where the parameter quantifies the degree to which modifies the effect of on even when all parameters are estimated from data it is still not obvious what combinations of parameters measure the direct and indirect effect of on or more practically how to assess the fraction of the total effect that is explained by mediation and the fraction of that is owed to mediation 
in linear analysis the former fraction is captured by the product the latter by the difference and the two quantities coincide 
in the presence of interaction however each fraction demands separate analysis as dictated by the mediation formula which yields thus the fraction of output response for which mediation would be sufficient is while the fraction for which mediation would be necessary is 
these fractions involve non obvious combinations of the model parameters and can be constructed mechanically with the help of the mediation formula 
significantly due to interaction direct effect can be sustained even when the parameter vanishes and moreover total effect can be sustained even when both the direct and indirect effects vanish 
this illustrates that estimating parameters in isolation tells us little about the effect of mediation and more generally mediation and moderation are intertwined and cannot be assessed separately 
references as of june this article is derived in whole or in part from causal analysis in theory and practice 
the copyright holder has licensed the content in manner that permits reuse under cc by sa and gfdl 
all relevant terms must be followed 
notes bibliographypreacher kristopher hayes andrew 
spss and sas procedures for estimating indirect effects in simple mediation models 
behavior research methods instruments and computers 
pmid preacher kristopher hayes andrew 
asymptotic and resampling strategies for assessing and comparing indirect effects in multiple mediator models 
pmid preacher zyphur zhang 
general multilevel sem framework for assessing multilevel mediation 
pmid baron and kenny 
the moderator mediator variable distinction in social psychological research conceptual strategic and statistical considerations journal of personality and social psychology vol 
statistical power analysis for the behavioral sciences nd ed 
new york ny academic press 
beyond baron and kenny statistical mediation analysis in the new millennium 
statistical methods for psychology th ed 
belmot ca cengage learning 
advances in statistical methods for substance abuse prevention research 
doi pmc pmid preacher kelley 
effect sizes measures for mediation models quantitative strategies for communicating indirect effects 
pmid rucker preacher tormala 
mediation analysis in social psychology current practices and new recommendations 
social and personality psychology compass 
asymptotic confidence intervals for indirect effects in structural equation models 
jstor spencer zanna fong 
establishing causal chain why experiments are often more effective than mediational analyses in examining psychological processes 
journal of personality and social psychology 
the mediation formula guide to the assessment of causal pathways in nonlinear models 
in berzuini dawid bernardinelli 
causality statistical perspectives and applications 
chichester uk john wiley and sons ltd pp 
research methods in psychology th ed pp 
new york mcgraw hill 
the determiners of behavior at choice point 
degrees of hunger reward and nonreward and maze learning in rats 
university of california publications in psychology 
explanation in causal inference 
external links summary of mediation methods at psychwiki example of causal mediation using propensity scores the methodology center penn state university book on moderation and mediation analysis including an introduction to the process macro for spss and sas andrew hayes ohio state university online text of the determiner of behavior at choice point kenneth maccorquodale and paul meehl on distinction between hypothetical constructs and intervening variables classics in the history of psychology retr
in music closely related key or close key is one sharing many common tones with an original key as opposed to distantly related key or distant key 
in music harmony there are six of them five share all or all except one pitches with key with which it is being compared and is adjacent to it on the circle of fifths and its relative major or minor and one shares the same tonic 
such keys are the most commonly used destinations or transpositions in modulation because of their strong structural links with the home key 
distant keys may be reached sequentially through closely related keys by chain modulation for example to to for example one principle that every composer of haydn day classical music era kept in mind was over all unity of tonality 
no piece dared wander too far from its tonic key and no piece in four movement form dared to present tonality not closely related to the key of the whole series 
for example the first movement of mozart piano sonata no 
modulates only to closely related keys the dominant supertonic and submediant given major key tonic the related keys are ii supertonic the relative minor of the subdominant iii mediant the relative minor of the dominant iv subdominant one less sharp or one more flat around circle of fifths dominant one more sharp or one fewer flat around circle of fifths vi submediant or relative minor different tonic same key signature parallel minor same tonic different key signature specifically starting from minor key the closely related keys are the mediant or relative major iii the subdominant iv the minor dominant the submediant vi the subtonic vii and the parallel major 
in the key of minor when we translate them to keys we get major minor minor major major majoranother view of closely related keys is that there are six closely related keys based on the tonic and the remaining triads of the diatonic scale excluding the dissonant diminished triads 
four of the five differ by one accidental one has the same key signature and one uses the parallel modal form 
in the key of major these would be minor minor major major minor and minor 
despite being three sharps or flats away from the original key in the circle of fifths parallel keys are also considered as closely related keys as the tonal center is the same and this makes this key have an affinity with the original key 
in modern music the closeness of relation between any two keys or sets of pitches may be determined by the number of tones they share in common which allows one to consider modulations not occurring in standard major minor tonality 
for example in music based on the pentatonic scale containing pitches and modulating fifth higher gives the collection of pitches and having four of five tones in common 
however modulating up tritone would produce which shares no common tones with the original scale 
thus the scale fifth higher is very closely related while the scale tritone higher is not 
other modulations may be placed in order from closest to most distant depending upon the number of common tones 
another view in modern music notably in bart common tonic produces closely related keys the other scales being the six other modes 
this usage can be found in several of the mikrokosmos piano pieces 
when modulation causes the new key to traverse the bottom of the circle of fifths this may give rise to theoretical key containing eight or more sharps or flats in its notated key signature in such case notational conventions require recasting the new section in its enharmonically equivalent key 
andranik tangian suggests and visualizations of key chord proximity for both all major and all minor keys chords by locating them along single subdominant dominant axis which wraps torus that is then unfolded 
see also chromatic mediant common chord music monotonality parallel and counter parallel pitch space references further reading howard hanson harmonic materials of modern music 
appleton century crofts inc
in the field of multivariate statistics kernel principal component analysis kernel pca is an extension of principal component analysis pca using techniques of kernel methods 
using kernel the originally linear operations of pca are performed in reproducing kernel hilbert space 
background linear pca recall that conventional pca operates on zero centered data that is where is one of the multivariate observations 
it operates by diagonalizing the covariance matrix in other words it gives an eigendecomposition of the covariance matrix which can be rewritten as for see also covariance matrix as linear operator introduction of the kernel to pca to understand the utility of kernel pca particularly for clustering observe that while points cannot in general be linearly separated in dimensions they can almost always be linearly separated in dimensions 
that is given points if we map them to an dimensional space with where it is easy to construct hyperplane that divides the points into arbitrary clusters 
of course this creates linearly independent vectors so there is no covariance on which to perform eigendecomposition explicitly as we would in linear pca 
instead in kernel pca non trivial arbitrary function is chosen that is never calculated explicitly allowing the possibility to use very high dimensional if we never have to actually evaluate the data in that space 
since we generally try to avoid working in the space which we will call the feature space we can create the by kernel which represents the inner product space see gramian matrix of the otherwise intractable feature space 
the dual form that arises in the creation of kernel allows us to mathematically formulate version of pca in which we never actually solve the eigenvectors and eigenvalues of the covariance matrix in the space see kernel trick 
the elements in each column of represent the dot product of one point of the transformed data with respect to all the transformed points points 
some well known kernels are shown in the example below 
because we are never working directly in the feature space the kernel formulation of pca is restricted in that it computes not the principal components themselves but the projections of our data onto those components 
to evaluate the projection from point in the feature space onto the kth principal component where superscript means the component not powers of we note that denotes dot product which is simply the elements of the kernel it seems all that left is to calculate and normalize the which can be done by solving the eigenvector equation where is the number of data points in the set and and are the eigenvalues and eigenvectors of then to normalize the eigenvectors we require that care must be taken regarding the fact that whether or not has zero mean in its original space it is not guaranteed to be centered in the feature space which we never compute explicitly 
since centered data is required to perform an effective principal component analysis we centralize to become where denotes by matrix for which each element takes value we use to perform the kernel pca algorithm described above 
one caveat of kernel pca should be illustrated here 
in linear pca we can use the eigenvalues to rank the eigenvectors based on how much of the variation of the data is captured by each principal component 
this is useful for data dimensionality reduction and it could also be applied to kpca 
however in practice there are cases that all variations of the data are same 
this is typically caused by wrong choice of kernel scale 
large datasets in practice large data set leads to large and storing may become problem 
one way to deal with this is to perform clustering on the dataset and populate the kernel with the means of those clusters 
since even this method may yield relatively large it is common to compute only the top eigenvalues and eigenvectors of the eigenvalues are calculated in this way 
example consider three concentric clouds of points shown we wish to use kernel pca to identify these groups 
the color of the points does not represent information involved in the algorithm but only shows how the transformation relocates the data points 
first consider the kernel applying this to kernel pca yields the next image 
now consider gaussian kernel that is this kernel is measure of closeness equal to when the points coincide and equal to at infinity 
note in particular that the first principal component is enough to distinguish the three different groups which is impossible using only linear pca because linear pca operates only in the given in this case two dimensional space in which these concentric point clouds are not linearly separable 
applications kernel pca has been demonstrated to be useful for novelty detection and image de noising 
see also cluster analysis nonlinear dimensionality reduction spectral clustering references
guitar tunings are the assignment of pitches to the open strings of guitars including acoustic guitars electric guitars and classical guitars
tunings are described by the particular pitches that are made by notes in western music
by convention the notes are ordered and arranged from the lowest pitched string the deepest bass sounding note to the highest pitched string the highest sounding note or the thickest string to thinnest or the lowest frequency to the highest
this sometimes confuses beginner guitarists since the highest pitched string is referred to as the st string and the lowest pitched is the th string
standard tuning defines the string pitches as and from the lowest pitch low to the highest pitch high
standard tuning is used by most guitarists and frequently used tunings can be understood as variations on standard tuning
to aid in memorising these notes mnemonics are used for example elephants and donkeys grow big ears the term guitar tunings may refer to pitch sets other than standard tuning also called nonstandard alternative or alternate
there are hundreds of these tunings often with small variants of established tunings
communities of guitarists who share common musical tradition often use the same or similar tuning styles
standard and alternatives standard standard tuning is the tuning most frequently used on six string guitar and musicians assume this tuning by default if specific alternate or scordatura is not mentioned
in scientific pitch notation the guitar standard tuning consists of the following notes
the guitar is transposing instrument that is music for guitars is notated one octave higher than the true pitch
this is to reduce the need for ledger lines in music written for the instrument and thus simplify the reading of notes when playing the guitar standard tuning provides reasonably simple fingering fret hand movement for playing standard scales and basic chords in all major and minor keys
separation of the first high and second string as well as the separation between the third fourth fifth and sixth low strings by five semitone interval perfect fourth allows the guitarist to play chromatic scale with each of the four fingers of the fretting hand controlling one of the first four frets index finger on fret little finger on fret etc
only when the hand is in the first position the open notes of the second and third strings are separated by four semitones major third
this tuning pattern of low fourths one major third and one fourth was inherited by the guitar from its predecessor instrument the viol
the irregular major third breaks the fingering patterns of scales and chords so that guitarists have to memorize multiple chord shapes for each chord
scales and chords are simplified by major thirds tuning and all fourths tuning which are regular tunings maintaining the same musical interval between consecutive open string notes
alternative alternative alternate tuning refers to any open string note arrangement other than standard tuning
these offer different kinds of deep or ringing sounds chord voicings and fingerings on the guitar
alternative tunings are common in folk music where the guitar may be called upon to produce sustained note or chord known as drone
this often gives folk music its haunting and lamenting ambience due to the atmosphere and mood that the notes make
alternative tunings change the fingering of common chords when playing the guitar and this can ease the playing of certain chords while simultaneously increase the difficulty of playing other chords
some tunings are used for particular songs and may be named after the song title
there are hundreds of these tunings although many are slight variations of other alternate tunings
several alternative tunings are used regularly by communities of guitarists who share common musical tradition such as american folk or celtic folk music the various alternative tunings have been grouped into the following categories dropped open both major and minor cross note modal instrumental based on other stringed instruments miscellaneous special joni mitchell is known for developing shorthand descriptive method of noting guitar tuning where the first letter documents the note of the lowest string and is followed by the relative fret half step offsets required to obtain the pitch of the next higher string
this scheme highlights pitch relationships and simplifies the process of comparing different tuning schemes
string gauges string gauge refers to the thickness and diameter of guitar string which influences the overall sound and pitch of the guitar depending on the guitar string used
some alternative tunings are difficult or even impossible to achieve with conventional guitars due to the sets of guitar strings which have gauges optimized for standard tuning
with conventional sets of guitar strings some higher tunings increase the string tension until playing the guitar requires significantly more finger strength and stamina or even until string snaps or the guitar is warped
however with lower tunings the sets of guitar strings may be loose and buzz
the tone of the guitar strings is also negatively affected by using unsuitable string gauges on the guitar
generally alternative tunings benefit from re stringing of the guitar with string gauges purposefully chosen to optimize particular tunings by using lighter strings for higher pitched notes to lower the tension of the strings and heavier strings for lower pitched notes to prevent string buzz and vibration
dropped tunings dropped tuning is one of the categories of alternative tunings and the process starts with standard tuning and typically lowers the pitch of drops only single string almost always the lowest pitched string on the guitar
the drop tuning is common in electric guitar and heavy metal music
the low string is tuned down one whole step to and the rest of the strings remain in standard tuning
this creates an open power chord three note fifth with the low three strings dad
there also exists double drop tuning in which both strings are down tuned whole step to
the rest of the strings keep their original pitch
although the drop tuning was introduced and developed by blues and classical guitarists it is well known from its usage in contemporary heavy metal and hard rock bands
early hard rock songs tuned in drop include the beatles want you she so heavy and led zeppelin moby dick both first released in tuning the lowest string one tone down from to allowed these musicians to acquire heavier and darker sound than in standard tuning
without needing to tune all strings standard tuning they could tune just one in order to lower the key
drop is also convenient tuning because it expands the scale of an instrument by two semitones and
in the mid three alternative rock bands king soundgarden and melvins influenced by led zeppelin and black sabbath made extensive use of drop tuning
while playing power chords chord that includes the prime fifth and octave in standard tuning requires player to use two or three fingers drop tuning needs just one similar in technique to playing barre chords
it allowed them to use different methods of articulating power chords legato for example and more importantly it allowed guitarists to change chords faster
this new technique of playing power chords introduced by these early grunge bands was great influence on many artists such as rage against the machine and tool
the same drop tuning then became common practice among alternative metal acts such as the band helmet who used the tuning great deal throughout their career and would later influence many alternative metal and nu metal bands
open tunings an open tuning allows the guitarist to play chord by strumming the open strings no strings fretted
open tunings may be chordal or modal
in chordal open tunings the open chord consists of at least three different pitch classes
in given key these are the root note its rd and its th and may include all the strings or subset
the tuning is named for the base chord when played open typically major chord and all similar chords in the chromatic scale are played by barring all strings across single fret
open tunings are common in blues and folk music
these tunings are frequently used in the playing of slide and lap slide hawaiian guitars and hawaiian slack key music
musician who is well known for using open tuning in his music is ry cooder who uses open tunings when playing the slide guitar most modern music uses equal temperament because it facilitates the ability to play the guitar in any key as compared to just intonation which favors certain keys and makes the other keys sound less in tune repetitive open tunings are used for two classical non spanish guitars
for the english guitar the open chord is major for the russian guitar which has seven strings it is major when the open strings constitute minor chord the open tuning may sometimes be called cross note tuning
major key tunings major open tunings give major chord with the open strings
open tunings often tune the lowest open note to or and they often tune the highest open note to or tuning down the open string from to or avoids the risk of breaking strings which is associated with tuning up strings
open the open tuning also called vestapol tuning is common open tuning used by european and american western guitarists working with alternative tunings
the allman brothers instrumental little martha used an open tuning raised one half step giving an open tuning with the same intervallic relationships as open open the english guitar used repetitive open tuning with distinct open notes that approximated major thirds tuning
this tuning is evident in william ackerman song townsend shuffle as well as by john fahey for his tribute to mississippi john hurt the tuning uses some of the harmonic sequence overtones of the note this overtone series tuning was modified by mick ralphs who used high note rather than the high note for can get enough on bad company
ralphs said it needs the open to have that ring and it never really sounds right in standard tuning
open mick ralphs open tuning was originally an open tuning which listed the initial six overtones of the note namely ralphs used this open tuning for hey hey and while writing the demo of can get enough open tuning usually refers to
the open tuning variant was used by joni mitchell for electricity for the roses and hunter the good samaritan
truncating this tuning to for his five string guitar keith richards uses this overtones tuning on the rolling stones honky tonk women brown sugar and start me up the seven string russian guitar uses the open tuning which contains mostly major and minor thirds
creating any kind of open tuning any kind of chordal tuning can be achieved simply by using the notes in the chord and tuning the strings to those notes
for example asus has the notes by tuning the strings to only those notes it creates chordal asus tuning
bass players may omit the last two strings
minor or cross note tunings cross note tunings include minor third so giving minor chord with open strings
fretting the minor third string at the first fret produces major third so allowing one finger fretting of major chord
by contrast it is more difficult to fret minor chord using an open major chord tuning
bukka white and skip james are well known for using cross note minor in their music
other open chordal tunings some guitarists choose open tunings that use more complex chords which gives them more available intervals on the open strings
and other such tunings are common among lap steel players such as hawaiian slack key guitarists and country guitarists and are also sometimes applied to the regular guitar by bottleneck slide repurposed from glass bottle players striving to emulate these styles
common tuning for example is which provides open major and minor thirds open major and minor sixths fifths and octaves
by contrast most open major or open minor tunings provide only octaves fifths and either major third sixth or minor third sixth but not both
don helms of hank williams band favored tuning slack key artist henry kaleialoha allen uses modified tuning with on the bottom harmon davis favored tuning david gilmour has used an open tuning
modal tunings modal tunings are open tunings in which the open strings of the guitar do not produce tertian major or minor or variants thereof chord
the strings may be tuned to exclusively present single interval all fourths all fifths etc
or they may be tuned to non tertian chord unresolved suspensions such as for example
modal open tunings may use only one or two pitch classes across all strings as for example some metal guitarists who tune each string to either or forming power chords of ambiguous major minor tonality
popular modal tunings include modal and modal
lowered standard derived from standard eadgbe all the strings are tuned lower by the same interval thus providing the same chord positions transposed to lower key
lower tunings are popular among rock and heavy metal bands
the reason for tuning down below the standard pitch is usually either to accommodate singer vocal range or to get deeper heavier sound or pitch
common examples include tuning rock guitarists such as jimi hendrix on the songs voodoo child slight return and little wing occasionally tune all their strings down by one semitone to obtain tuning
this makes the strings easier to bend when playing and with standard fingering results in lower key
it also facilitates shape fingerings when playing with horn instruments
tuning tuning also called one step lower whole step down full step or standard is another alternative
each string is lowered by whole tone two semitones resulting in it is used mostly by heavy metal bands to achieve heavier deeper sound and by blues guitarists who use it to accommodate string bending and by string guitar players to reduce the mechanical load on their instrument
among musicians elliott smith was known to use tuning as his main tuning for his music
it was also used for several songs on the velvet underground album the velvet underground nico
regular tunings in standard tuning there is an interval of major third between the second and third strings and all the other intervals are fourths
the irregularity has price
chords cannot be shifted around the fretboard in the standard tuning which requires four chord shapes for the major chords
there are separate chord forms for chords having their root note on the third fourth fifth and sixth strings
these are called inversions
in contrast regular tunings have equal intervals between the strings and so they have symmetrical scales all along the fretboard
this makes it simpler to translate chords
for the regular tunings chords may be moved diagonally around the fretboard
the diagonal movement of chords is especially simple for the regular tunings that are repetitive in which case chords can be moved vertically chords can be moved three strings up or down in major thirds tuning and chords can be moved two strings up or down in augmented fourths tuning
regular tunings thus appeal to new guitarists and also to jazz guitarists whose improvisation is simplified by regular intervals
on the other hand five and six string open chords cowboy chords are more difficult to play in regular tuning than in standard tuning
instructional literature uses standard tuning
traditionally course begins with the hand in first position that is with the left hand covering frets
beginning players first learn open chords belonging to the major keys and guitarists who play mainly open chords in these three major keys and their relative minor keys am em bm may prefer standard tuning over many regular tunings on the other hand minor thirds tuning features many barre chords with repeated notes properties that appeal to acoustic guitarists and beginners
major thirds and perfect fourths standard tuning mixes major third with its perfect fourths
regular tunings that are based on either major thirds or perfect fourths are used for example in jazz
all fourths tuning keeps the lowest four strings of standard tuning changing the major third to perfect fourth
jazz musician stanley jordan stated that all fourths tuning simplifies the fingerboard making it logical major thirds tuning tuning is regular tuning in which the musical intervals between successive strings are each major thirds for example
unlike all fourths and all fifths tuning tuning repeats its octave after three strings which simplifies the learning of chords and improvisation
this repetition provides the guitarist with many possibilities for fingering chords
with six strings major thirds tuning has smaller range than standard tuning with seven strings the major thirds tuning covers the range of standard tuning on six strings major thirds tunings require less hand stretching than other tunings because each tuning packs the octave twelve notes into four consecutive frets
the major third intervals let the guitarist play major chords and minor chords with two three consecutive fingers on two consecutive frets chord inversion is especially simple in major thirds tuning
the guitarist can invert chords by raising one or two notes on three strings playing the raised notes with the same finger as the original notes
in contrast inverting triads in standard and all fourths tuning requires three fingers on span of four frets
in standard tuning the shape of an inversion depends on the involvement of the major third between the nd and rd strings
all fifths and new standard tuning all fifths tuning is tuning in intervals of perfect fifths like that of mandolin or violin other names include perfect fifths and fifths
it has wide range
its implementation has been impossible with nylon strings and has been difficult with conventional steel strings
the high makes the first string very taut and consequently conventionally gauged string easily breaks
jazz guitarist carl kress used variation of all fifths tuning with the bottom four strings in fifths and the top two strings in thirds resulting in
this facilitated tenor banjo chord shapes on the bottom four strings and plectrum banjo chord shapes on the top four strings
contemporary new york jazz guitarist marty grosz uses this tuning
all fifths tuning has been approximated by the so called new standard tuning nst of king crimson robert fripp which nst replaces all fifths high with high
to build chords fripp uses perfect intervals in fourths fifths and octaves so avoiding minor thirds and especially major thirds which are slightly sharp in equal temperament tuning in comparison to thirds in just intonation
it is challenge to adapt conventional guitar chords to new standard tuning which is based on all fifths tuning
some closely voiced jazz chords become impractical in nst and all fifths tuning
instrumental tunings these are tunings in which some or all strings are retuned to emulate the standard tuning of some other instrument such as lute banjo cittern mandolin etc
many of these tunings overlap other categories especially open and modal tunings
miscellaneous or special tunings this category includes everything that does not fit into any of the other categories for example but not limited to tunings designated only for particular piece non western intervals and modes micro or macro tones half sharps flats etc
and hybrid tunings combining features of major alternate tuning categories most commonly an open tuning with the lowest string dropped
see also bass guitar tuning list of guitar tunings mathematics and music open tuning stringed instrument tunings dadgad notes citation references references allen warren september december
wa encyclopedia of guitar tunings
recommended by marcus gary
guitar zero the science of learning to be musical
archived from the original on july retrieved june annala hannu tlik heiki
composers for other plucked instruments rudolf straube
handbook of guitar and lute composers
translated by katarina backman
the illustrated history of the guitar
playing the guitar how the guitar is tuned pp
and alternative tunings pp
special contributors isaac guillory and alastair crawford fully revised and updated ed
london and sydney pan books
in casabona helen belew adrian eds
new directions in modern guitar
guitar player basic library
isbn griewank andreas january tuning guitars and reading music in major thirds matheon preprints vol
berlin germany dfg research center matheon mathematics for key technologies berlin urn nbn de matheon
postscript file and pdf file archived from the original on november grossman stefan
the book of guitar tunings
new york amsco publishing company
isbn lccn persichetti vincent
twentieth century harmony creative aspects and practice
isbn oclc peterson jonathon
tuning in thirds new approach to playing leads to new kind of guitar
american lutherie the quarterly journal of the guild of american luthiers
tacoma wa the guild of american luthiers
issn archived from the original on october retrieved october roche eric
thinking outside the box
the acoustic guitar bible
london bobcat books limited smt
madison wisconsin university of wisconsin department of electrical engineering
retrieved may sethares bill
alternate tuning guide pdf
madison wisconsin university of wisconsin department of electrical engineering
retrieved may sethares william
madison wisconsin university of wisconsin department of electrical engineering
retrieved may tamm eric
chapter ten guitar craft
robert fripp from crimson king to crafty master
isbn archived from the original on october retrieved march via progressive ears
zipped microsoft word document weissman dick
guitar tunings comprehensive guide
alternate tunings guitar essentials
acoustic guitar magazine private lessons
hal leonard publishing corporation
isbn lccn hanson mark
the complete book of alternate tunings
alternate tunings picture chords
mastering alternate tunings revolutionary system of fretboard navigation for fingerstyle guitarists
alternate tuning chord dictionary
isbn lccn maloof richard
alternate tunings for guitar
cherry lane music company
isbn lccn shark mark
the tao of tunings map to the world of alternate tunings
external links allen warren september december
wa encyclopedia of guitar tunings
recommended by marcus gary
guitar zero the science of learning to be musical
retrieved june sethares william
alternate tuning guide interactive
uses wolfram cdf player
the circle of fifths text table shows the number of flats or sharps in each of the diatonic musical scales and keys 
both major and minor keys have no flats or sharps 
in the table minor keys are written with lowercase letters for brevity 
however in common guitar tabs notation minor key is designated with lowercase 
for example minor is am and sharp minor is 
the small interval between equivalent notes such as sharp and flat is the pythagorean comma 
minor scales start with major scales start with 
see also circle of fifths key signature musical notation notes
searchable symmetric encryption sse is form of encryption that allows one to efficiently search over collection of encrypted documents or files without the ability to decrypt them 
sse can be used to outsource files to an untrusted cloud storage server without ever revealing the files in the clear but while preserving the server ability to search over them 
description searchable symmetric encryption scheme is symmetric key encryption scheme that encrypts collection of documents where each document is viewed as set of keywords from keyword space given the encryption key and keyword one can generate search token with which the encrypted data collection can be searched for the result of the search is the subset of encrypted documents that contain the keyword static sse static sse scheme consists of three algorithms that work as follows takes as input security parameter and document collection and outputs symmetric key and an encrypted document collection takes as input the secret key and keyword and outputs search token takes as input the encrypted document collection and search token and outputs set of encrypted documents static sse scheme is used by client and an untrusted server as follows 
the client encrypts its data collection using the algorithm which returns secret key and an encrypted document collection the client keeps secret and sends to the untrusted server 
to search for keyword the client runs the algorithm on and to generate search token which it sends to the server 
the server runs search with and and returns the resulting encrypted documents back to the server 
dynamic sse dynamic sse scheme supports in addition to search the insertion and deletion of documents 
dynamic sse scheme consists of seven algorithms where and are as in the static case and the remaining algorithms work as follows takes as input the secret key and new document and outputs an insert token takes as input the encrypted document collection edc and an insert token and outputs an updated encrypted document collection takes as input the secret key and document identifier and outputs delete token takes as input the encrypted data collection and delete token and outputs an updated encrypted data collection to add new document the client runs on and to generate an insert token which it sends to the server 
the server runs with and and stores the updated encrypted document collection 
to delete document with identifier the client runs the algorithm with and to generate delete token which it sends to the server 
the server runs with and and stores the updated encrypted document collection 
an sse scheme that does not support and is called semi dynamic 
history of searchable symmetric encryption the problem of searching on encrypted data was considered by song wagner and perrig though previous work on oblivious ram by goldreich and ostrovsky could be used in theory to address the problem 
this work proposed an sse scheme with search algorithm that runs in time where 
goh and chang and mitzenmacher gave new sse constructions with search algorithms that run in time where is the number of documents 
curtmola garay kamara and ostrovsky later proposed two static constructions with search time where is the number of documents that contain which is optimal 
this work also proposed semi dynamic construction with log search time where is the number of updates 
an optimal dynamic sse construction was later proposed by kamara papamanthou and roeder goh and chang and mitzenmacher proposed security definitions for sse 
these were strengthened and extended by curtmola garay kamara and ostrovsky who proposed the notion of adaptive security for sse 
this work also was the first to observe leakage in sse and to formally capture it as part of the security definition 
leakage was further formalized and generalized by chase and kamara 
islam kuzu and kantarcioglu described the first leakage attack all the previously mentioned constructions support single keyword search 
cash jarecki jutla krawczyk rosu and steiner proposed an sse scheme that supports conjunctive search in sub linear time in the construction can also be extended to support disjunctive and boolean searches that can be expressed in searchable normal form snf in sub linear time 
at the same time pappas krell vo kolesnikov malkin choi george keromytis and bellovin described construction that supports conjunctive and all disjunctive and boolean searches in sub linear time 
security sse schemes are designed to guarantee that the untrusted server cannot learn any partial information about the documents or the search queries beyond some well defined and reasonable leakage 
the leakage of scheme is formally described using leakage profile which itself can consists of several leakage patterns 
sse constructions attempt to minimize leakage while achieving the best possible search efficiency 
sse security can be analyzed in several adversarial models but the most common are the persistent model where an adversary is given the encrypted data collection and transcript of all the operations executed on the collection the snapshot model where an adversary is only given the encrypted data collection but possibly after each operation 
security in the persistent model in the persistent model there are sse schemes that achieve wide variety of leakage profiles 
the most common leakage profile for static schemes that achieve single keyword search in optimal time is which reveals the number of documents in the collection the size of each document in the collection if and when query was repeated and which encrypted documents match the search query 
it is known however how to construct schemes that leak considerably less at an additional cost in search time and storage when considering dynamic sse schemes the state of the art constructions with optimal time search have leakage profiles that guarantee forward privacy which means that inserts cannot be correlated with past search queries 
security in the snapshot model in the snapshot model efficient dynamic sse schemes with no leakage beyond the number of documents and the size of the collection can be constructed 
when using an sse construction that is secure in the snapshot model one has to carefully consider how the scheme will be deployed because some systems might cache previous search queries 
cryptanalysis leakage profile only describes the leakage of an sse scheme but it says nothing about whether that leakage can be exploited or not 
cryptanalysis is therefore used to better understand the real world security of leakage profile 
there is wide variety of attacks working in different adversarial models based on variety of assumptions and attacking different leakage profiles 
see also homomorphic encryption oblivious ram structured encryption deterministic encryption references
in music guitar chord is set of notes played on guitar
chord notes are often played simultaneously but they can be played sequentially in an arpeggio
the implementation of guitar chords depends on the guitar tuning
most guitars used in popular music have six strings with the standard tuning of the spanish classical guitar namely from the lowest pitched string to the highest in standard tuning the intervals present among adjacent strings are perfect fourths except for the major third
standard tuning requires four chord shapes for the major triads
there are separate chord forms for chords having their root note on the third fourth fifth and sixth strings
for six string guitar in standard tuning it may be necessary to drop or omit one or more tones from the chord this is typically the root or fifth
the layout of notes on the fretboard in standard tuning often forces guitarists to permute the tonal order of notes in chord
the playing of conventional chords is simplified by open tunings which are especially popular in folk blues guitar and non spanish classical guitar such as english and russian guitar
for example the typical twelve bar blues uses only three chords each of which can be played in every open tuning by fretting six strings with one finger
open tunings are used especially for steel guitar and slide guitar
open tunings allow one finger chords to be played with greater consonance than do other tunings which use equal temperament at the cost of increasing the dissonance in other chords
the playing of to string guitar chords is simplified by the class of alternative tunings called regular tunings in which the musical intervals are the same for each pair of consecutive strings
regular tunings include major thirds tuning all fourths and all fifths tunings
for each regular tuning chord patterns may be diagonally shifted down the fretboard property that simplifies beginners learning of chords and that simplifies advanced players improvisation
on the other hand in regular tunings string chords in the keys of and are more difficult to play
conventionally guitarists double notes in chord to increase its volume an important technique for players without amplification doubling notes and changing the order of notes also changes the timbre of chords
it can make possible chord which is composed of the all same note on different strings
many chords can be played with the same notes in more than one place on the fretboard
musical fundamentals the theory of guitar chords respects harmonic conventions of western music
discussions of basic guitar chords rely on fundamental concepts in music theory the twelve notes of the octave musical intervals chords and chord progressions
intervals the octave consists of twelve notes
its natural notes constitute the major scale and
the intervals between the notes of chromatic scale are listed in table in which only the emboldened intervals are discussed in this article section on fundamental chords those intervals and other seventh intervals are discussed in the section on intermediate chords
the unison and octave intervals have perfect consonance
octave intervals were popularized by the jazz playing of wes montgomery
the perfect fifth interval is highly consonant which means that the successive playing of the two notes from the perfect fifth sounds harmonious
semitone is the distance between two adjacent notes on the chromatic circle which displays the twelve notes of an octave
as indicated by their having been emboldened in the table handful of intervals thirds minor and major perfect fifths and minor sevenths are used in the following discussion of fundamental guitar chords
as already stated the perfect fifths interval is the most harmonious after the unison and octave intervals
an explanation of human perception of harmony relates the mechanics of vibrating string to the musical acoustics of sound waves using the harmonic analysis of fourier series
when string is struck with finger or pick plectrum it vibrates according to its harmonic series
when an open note string is struck its harmonic series begins with the terms
the root note is associated with sequence of intervals beginning with the unison interval the octave interval the perfect fifth the perfect fourth and the major third
in particular this sequence of intervals contains the thirds of the major chord
with note of music one strikes the fundamental and in addition to the root note other notes are generated these are the harmonic series as one fundamental note contains within it other notes in the octave two fundamentals produce remarkable array of harmonics and the number of possible combinations between all the notes increases phenomenally
with triad affairs stand good chance of getting severely out of hand
perfect fifths the perfect fifth interval is featured in guitar playing and in sequences of chords
the sequence of fifth intervals built on the major scale is used in the construction of triads which is discussed below
cycle of fifths concatenating the perfect fifths yields the sequence of fifths this sequence of fifths displays all the notes of the octave
this sequence of fifths shall be used in the discussions of chord progressions below
power chord the perfect fifth interval is called power chord by guitarists who play them especially in blues and rock music
the who guitarist peter townshend performed power chords with theatrical windmill strum
power chords are often played with the notes repeated in higher octaves although established the term power chord is inconsistent with the usual definition of chord in musical theory which requires three or more distinct notes in each chord
chords in music theory brief overview the musical theory of chords is reviewed to provide terminology for discussion of guitar chords
three kinds of chords which are emphasized in introductions to guitar playing are discussed
these basic chords arise in chord triples that are conventional in western music triples that are called three chord progressions
after each type of chord is introduced its role in three chord progressions is noted
intermediate discussions of chords derive both chords and their progressions simultaneously from the harmonization of scales
the basic guitar chords can be constructed by stacking thirds that is by concatenating two or three third intervals where all of the lowest notes come from the scale
triads major both major and minor chords are examples of musical triads which contain three distinct notes
triads are often introduced as an ordered triplet the root the third which is above the root by either major third for major chord or minor third for minor chord the fifth which is perfect fifth above the root consequently the fifth is third above the third either minor third above major third or major third above minor third
the major triad has root major third and fifth
the major chord major third interval is replaced by minor third interval in the minor chord which shall be discussed in the next subsection
for example major triad consists of the root third fifth notes
the three notes of major triad have been introduced as an ordered triplet namely root third fifth where the major third is four semitones above the root and where the perfect fifth is seven semitones above the root
this type of triad is in closed position
triads are quite commonly played in open position for example the major triad is often played with the third and fifth an octave higher respectively sixteen and nineteen semitones above the root
another variation of the major triad changes the order of the notes for example the major triad is often played as where is perfect fifth and is raised an octave above the perfect third
alternative orderings of the notes in triad are discussed below in the discussions of chord inversions and drop chords
in popular music subset of triads is emphasized those with notes from the three major keys which also contain the notes of their relative minor keys am em bm
progressions the major chords are highlighted by the three chord theory of chord progressions which describes the three chord song that is archetypal in popular music
when played sequentially in any order the chords from three chord progression sound harmonious good together the most basic three chord progressions of western harmony have only major chords
in each key three chords are designated with the roman numerals of musical notation the tonic the subdominant iv and the dominant
while the chords of each three chord progression are numbered iv and they appear in other orders
in the the iv chord progression was used in hound dog elvis presley and in chantilly lace the big bopper major chord progressions are constructed in the harmonization of major scales in triads
for example stacking the major scale with thirds creates chord progression which is traditionally enumerated with the roman numerals ii iii iv vi viio its sub progression iv is used in popular music as already discussed
further chords are constructed by stacking additional thirds
stacking the dominant major triad with minor third creates the dominant seventh chord which shall be discussed after minor chords
minor minor chord has the root and the fifth of the corresponding major chord but its first interval is minor third rather than major third minor chords arise in the harmonization of the major scale in thirds which was already discussed the minor chords have the degree positions ii iii and vi
minor chords arise as the tonic notes of minor keys that share the same key signature with major keys
from the major key ii iii iv vi viio progression the secondary minor triads ii iii vi appear in the relative minor key corresponding chord progression as iv or iv or iv for example from vi ii iii progression am dm em the chord em is often played as or in minor chord progression
among basic chords the minor chords are the tonic chords of the relative minors of the three major keys the technique of changing among relative keys pairs of relative majors and relative minors is form of modulation
minor chords are constructed by the harmonization of minor scales in triads
seventh chords major minor chords with dominant function adding minor seventh to major triad creates dominant seventh denoted
in music theory the dominant seventh described here is called major minor seventh emphasizing the chord construction rather than its usual function
dominant sevenths are often the dominant chords in three chord progressions in which they increase the tension with the tonic already inherent in the dominant triad
the dominant seventh discussed is the most commonly played seventh chord
an major iv chord progression was used by paul mccartney in the song legs on his album ram these progressions with seventh chords arise in the harmonization of major scales in seventh chords
twelve bar blues be they in major key or minor key such iv chord progressions are extended over twelve bars in popular music especially in jazz blues and rock music
for example twelve bar blues progression of chords in the key of has three sets of four bars this progression is simplified by playing the sevenths as major chords
the twelve bar blues structure is used by mccartney legs which was noted earlier
playing chords open strings inversion and note doubling the implementation of musical chords on guitars depends on the tuning
since standard tuning is most commonly used expositions of guitar chords emphasize the implementation of musical chords on guitars with standard tuning
the implementation of chords using particular tunings is defining part of the literature on guitar chords which is omitted in the abstract musical theory of chords for all instruments
for example in the guitar like other stringed instruments but unlike the piano open string notes are not fretted and so require less hand motion
thus chords that contain open notes are more easily played and hence more frequently played in popular music such as folk music
many of the most popular tunings standard tuning open tunings and new standard tuning are rich in the open notes used by popular chords
open tunings allow major triads to be played by barring one fret with only one finger using the finger like capo
on guitars without zeroth fret after the nut the intonation of an open note may differ from then note when fretted on other strings consequently on some guitars the sound of an open note may be inferior to that of fretted note unlike the piano the guitar has the same notes on different strings
consequently guitar players often double notes in chord so increasing the volume of sound
doubled notes also changes the chordal timbre having different string widths tensions and tunings the doubled notes reinforce each other like the doubled strings of twelve string guitar add chorusing and depth
notes can be doubled at identical pitches or in different octaves
for triadic chords doubling the third interval which is either major third or minor third clarifies whether the chord is major or minor unlike piano or the voices of choir the guitar in standard tuning has difficulty playing the chords as stacks of thirds which would require the left hand to span too many frets particularly for dominant seventh chords as explained below
if in particular tuning chords cannot be played in closed position then they often can be played in open position similarly if in particular tuning chords cannot be played in root position they can often be played in inverted positions
chord is inverted when the bass note is not the root note
additional chords can be generated with drop or drop voicing which are discussed for standard tuning implementation of dominant seventh chords below
when providing harmony in accompanying melody guitarists may play chords all at once or as arpeggios
arpeggiation was the traditional method of playing chords for guitarists for example in the time of mozart
contemporary guitarists using arpeggios include johnny marr of the smiths
fundamental chords standard tuning six string guitar has five musical intervals between its consecutive strings
in standard tuning the intervals are four perfect fourths and one major third the comparatively irregular interval for the pair
consequently standard tuning requires four chord shapes for the major chords
there are separate chord forms for chords having their root note on the third fourth fifth and sixth strings
of course beginner learns guitar by learning notes and chords and irregularities make learning the guitar difficult even more difficult than learning the formation of plural nouns in german according to gary marcus
nonetheless most beginners use standard tuning another feature of standard tuning is that the ordering of notes often differs from root position
notes are often inverted or otherwise permuted particularly with seventh chords in standard tuning as discussed below
power chords fingerings as previously discussed each power chord has only one interval perfect fifth between the root note and the fifth
in standard tuning the following fingerings are conventional triads triads are usually played with doubled notes as the following examples illustrate
major commonly used major chords are convenient to play in standard tuning in which fundamental chords are available in open position that is the first three frets and additional open strings
for the major chord the conventional left hand fingering doubles the and notes in the next octave this fingering uses two open notes and on the first string on the second string on the third string on the fourth string on the fifth string sixth string is not played major chords guide for guitar chord charts xx movable remember that no sharps or flats are between bc and ef normal nashville style for the other commonly used chords the conventional fingerings also double notes and feature open string notes besides doubling the fifth note the conventional major chord features tripled bass note
the major and major chords are commonly played as barre chords with the first finger depressing five six strings
major chord has the same shape as the major chord but it is located two frets further up the fretboard
the major chord is the same shape as major but it is located one fret further up the fretboard
minor minor chords commonly notated as cm cmi or cmin are the same as major chords except that they have minor third instead of major third
this is difference of one semitone
to create minor from the major chord in major shape the second finger should be lifted so that the third string plays onto the barre
compare the major to minor the other shapes can be modified as well suspended movable suspended chords guide for chord charts in standard tuning sus sus sus sus sus sus sus sus sus these chords are used extensively by my bloody valentine on the album loveless
they are also used on the who song pinball wizard and many many more songs
dominant sevenths drop two as previously stated dominant seventh is four note chord combining major chord and minor seventh
for example the dominant seventh chord adds to the major chord
the naive chord spans six frets from fret to fret such seventh chords contain some pretty serious stretches in the left hand
an illustration shows naive chord which would be extremely difficult to play besides the open position chord that is conventional in standard tuning
the standard tuning implementation of chord is second inversion drop chord in which the second highest note in second inversion of the chord is lowered by an octave
drop two chords are used for sevenths chords besides the major minor seventh with dominant function which are discussed in the section on intermediate chords below
drop two chords are used particularly in jazz guitar
drop two second inversions are examples of openly voiced chords which are typical of standard tuning and other popular guitar tunings
alternatively voiced seventh chords are commonly played with standard tuning
list of fret number configurations for some common chords follows this requires no barre unlike the major
xx other chord inversions already in basic guitar playing inversion is important for sevenths chords in standard tuning
it is also important for playing major chords
in standard tuning chord inversion depends on the bass note string and so there are three different forms for the inversion of each major chord depending on the position of the irregular major thirds interval between the and strings
for example if the note the open sixth string is played over the minor chord then the chord would be
this has the note as its lowest tone instead of it is often written as am where the letter following the slash indicates the new bass note
however in popular music it is usual to play inverted chords on the guitar when they are not part of the harmony since the bass guitar can play the root pitch
alternate tunings there are many alternate tunings
these change the way chords are played making some chords easier to play and others harder
open tunings each allow chord to be played by strumming the strings when open or while fretting no strings
open tunings are common in blues and folk music and they are used in the playing of slide guitar
drop tunings are common in hard rock and heavy metal music
in drop tuning the standard tuning string is tuned down to note
with drop tuning the bottom three strings are tuned to root fifth octave tuning which simplifies the playing of power chords
regular tunings allow chord note forms to be shifted all around the fretboard on all six strings unlike standard or other non regular tunings
knowing few note patterns for example of the major minor and chords enables guitarist to play all such chords sethares learn handful of chord forms in regular tuning and you ll know hundreds of chords
ref open tunings an open tuning allows chord to be played by strumming the strings when open or while fretting no strings
the base chord consists of at least three notes and may include all the strings or subset
the tuning is named for the base chord when played open typically major triad and each major triad can be played by barring exactly one fret
open tunings are common in blues and folk music and they are used in the playing of slide and lap slide hawaiian guitars
ry cooder uses open tunings when he plays slide guitar open tunings improve the intonation of major chords by reducing the error of third intervals in equal temperaments
for example in the open overtones tuning the interval is major third and of course each successive pair of notes on the and strings is also major third similarly the open string minor third induces minor thirds among all the frets of the strings
the thirds of equal temperament have audible deviations from the thirds of just intonation equal temperaments is used in modern music because it facilitates music in all keys while on piano and other instruments just intonation provided better sounding major third intervals for only subset of keys
sonny landreth keith richards and other open masters often lower the second string slightly so the major third is in tune with the overtone series
this adjustment dials out the dissonance and makes those big one finger major chords come alive
repetitive open tunings are used for two non spanish classical guitars
for the english guitar the open chord is major for the russian guitar which has seven strings major
mixing perfect fourth and minor third along with major third these tunings are on average major thirds regular tunings
while on average major thirds tunings are conventional open tunings properly major thirds tunings are unconventional open tunings because they have augmented triads as their open chords
regular tunings guitar chords are dramatically simplified by the class of alternative tunings called regular tunings
in each regular tuning the musical intervals are the same for each pair of consecutive strings
regular tunings include major thirds all fourths augmented fourths and all fifths tunings
for each regular tuning chord patterns may be diagonally shifted down the fretboard property that simplifies beginners learning of chords and that simplifies advanced players improvisation
the diagonal shifting of major chord in tuning appears in diagram
further simplifications occur for the regular tunings that are repetitive that is which repeat their strings
for example the tuning repeats its octave after every two strings
such repetition further simplifies the learning of chords and improvisation this repetition results in two copies of the three open strings notes each in different octave
similarly the augmented fourths tuning repeats itself after one string
chord is inverted when the bass note is not the root note
chord inversion is especially simple in tuning
chords are inverted simply by raising one or two notes by three strings each raised note is played with the same finger as the original note
inverted major and minor chords can be played on two frets in tuning
in standard tuning the shape of inversions depends on the involvement of the irregular major third and can involve four frets
it is challenge to adapt conventional guitar chords to new standard tuning which is based on all fifths tuning
intermediate chords after major and minor triads are learned intermediate guitarists play seventh chords
tertian harmonization stacking of third intervals the fundamental guitar chords major and minor triads and dominant sevenths are tertian chords which concatenate third intervals with each such third being either major or minor
more triads diminished and augmented as discussed above major and minor triads are constructed by stacking thirds the major triad concatenates supplementing with perfect fifth interval and the minor triad concatenates supplementing with interval similar tertian harmonization yields the remaining two triads the diminished triad concatenates supplementing with diminished fifth interval and the augmented triad concatenates supplementing with an augmented fifth interval
more sevenths major minor and half diminished stacking thirds also constructs the most used seventh chords
the most important seventh chords concatenate major triad with third interval supplementing it with seventh interval the dominant major minor seventh concatenates major triad with another minor third supplementing it with minor seventh interval
the major seventh concatenates major triad with major third supplementing it with major seventh interval
the minor seventh concatenates minor triad with minor third supplementing it with minor seventh interval
the half diminished seventh concatenates diminished triad with major third supplementing it with diminished seventh interval
the fully diminished seventh concatenates diminished triad with minor third supplementing it with diminished seventh interval four of these five seventh chords all but the diminished seventh are constructed via the tertian harmonization of major scale
as already stated the major minor seventh has the dominant function
the major seventh plays the tonic and subdominant iv roles the minor seventh plays the ii iii and vi roles
the half diminished seventh plays the vii role while absent from the tertian harmonization of the major scale the diminished seventh plays the viio role in the tertian harmonization of the harmonic minor scale besides these five types there are many more seventh chords which are less used in the tonal harmony of the common practice period
when playing seventh chords guitarists often play only subset of notes from the chord
the fifth is often omitted
when guitar is accompanied by bass the guitarist may omit the bass note from chord
as discussed earlier the third of triad is doubled to emphasize its major or minor quality similarly the third of seventh is doubled to emphasize its major or minor quality
the most frequent seventh is the dominant seventh the minor half diminished and major sevenths are also popular
chord progression circle of fifths the previously discussed iv chord progressions of major triads is subsequence of the circle progression which ascends by perfect fourths and descends by perfect fifths perfect fifths and perfect fourths are inverse intervals because one reaches the same pitch class by either ascending by perfect fourth five semitones or descending by perfect fifth seven semitones
for example the jazz standard autumn leaves contains the iv vii vim ii circle of fifths chord progression its sevenths occur in the tertian harmonization in sevenths of the minor scale
other subsequences of the fifths circle chord progression are used in music
in particular the ii progression is the most important chord progression in jazz music
chord chart guide for major inversions major inversions for guitar in standard tuning
the low is on the left
the demonstrates three of the different movable shapes
xxx xxx xxx xxx xxx xxx xxx xxx xxx specific tunings standard tuning minor and major sevenths besides the dominant seventh chords discussed above other seventh chords especially minor seventh chords and major seventh chords are used in guitar music
minor seventh chords have the following fingerings in standard tuning dm xx em am bm or xx also an chord major seventh chords have the following fingerings in standard tuning cmaj dmaj xx emaj fmaj gmaj amaj major thirds tuning in major thirds tuning the chromatic scale is arranged on three consecutive strings in four consecutive frets
this four fret arrangement facilitates the left hand technique for classical spanish guitar for each hand position of four frets the hand is stationary and the fingers move each finger being responsible for exactly one fret
consequently three hand positions covering frets and partition the fingerboard of classical guitar which has exactly frets only two or three frets are needed for the guitar chords major minor and dominant sevenths which are emphasized in introductions to guitar playing and to the fundamentals of music
each major and minor chord can be played on exactly two successive frets on exactly three successive strings and therefore each needs only two fingers
other chords seconds fourths sevenths and ninths are played on only three successive frets
advanced chords and harmony sequences of thirds and seconds the circle of fifths was discussed in the section on intermediate guitar chords
other progressions are also based on sequences of third intervals progressions are occasionally based on sequences of second intervals
extended chords as their categorical name suggests extended chords indeed extend seventh chords by stacking one or more additional third intervals successively constructing ninth eleventh and finally thirteenth chords thirteenth chords contain all seven notes of the diatonic scale
in closed position extended chords contain dissonant intervals or may sound supersaturated particularly thirteenth chords with their seven notes
consequently extended chords are often played with the omission of one or more tones especially the fifth and often the third as already noted for seventh chords similarly eleventh chords often omit the ninth and thirteenth chords the ninth or eleventh
often the third is raised an octave mimicking its position in the root sequence of harmonics dominant ninth chords were used by beethoven and eleventh chords appeared in impressionist music
thirteenth chords appeared in the twentieth century
extended chords appear in many musical genres including jazz funk rhythm and blues and progressive rock
chord guide for major and minor chords standard tuning read from left to right low to high major am xx bbm xx bm xx cm xx xx dm xx em fm xx gm xx minor am bm cm dm em fm gm alternative harmonies scales and modes conventional music uses diatonic harmony the major and minor keys and major and minor scales as sketched above
jazz guitarists must be fluent with jazz chords and also with many scales and modes of all the forms of music jazz demands the highest level of musicianship in terms of both theory and technique whole tone scales were used by king crimson for the title track on its red album of whole tone scales were also used by king crimson guitarist robert fripp on fractured
beyond tertian harmony in popular music chords are often extended also with added tones especially added sixths
quartal and quintal harmony chords are also systematically constructed by stacking not only thirds but also fourths and fifths supplementing tertian major minor harmony with quartal and quintal harmonies
quartal and quintal harmonies are used by guitarists who play jazz folk and rock music
quartal harmony has been used in jazz by guitarists such as jim hall especially on sonny rollins the bridge george benson skydive kenny burrell so what and wes montgomery little sunflower harmonies based on fourths and fifths also appear in folk guitar
on her debut album song to seagull joni mitchell used both quartal and quintal harmony in dawntreader and she used quintal harmony in seagull quartal and quintal harmonies also appear in alternate tunings
it is easier to finger the chords that are based on perfect fifths in new standard tuning than in standard tuning
new standard tuning was invented by robert fripp guitarist for king crimson
preferring to base chords on perfect intervals especially octaves fifths and fourths fripp often avoids minor thirds and especially major thirds which are sharp in equal temperament tuning in comparison to thirds in just intonation
alternative harmonies can also be generated by stacking second intervals major or minor
see also chord diagram guitar mel bay deluxe encyclopedia of guitar chords voice leading references footnotes citations bibliography further reading bay william
deluxe guitar chord encyclopedia case size edition
isbn kirkeby ole august
welcome to guitar version
archived from the original on april retrieved june patt ralph
berklee college of music professors at the department of guitar at the berklee college of music wrote the following books which like their colleagues chapman and willmott are berklee course textbooks goodrick mick
the advancing guitarist applying guitar concepts and techniques
hal leonard corp isbn goodrick mick
mr goodchord almanac of guitar voice leading name that chord
mr goodchord almanac of guitar voice leading for the year and beyond
isbn goodrick mick miller tim
creative chordal harmony for guitar using generic modality compression
berklee jazz guitar dictionary
berklee college of music
berklee rock guitar dictionary
berklee college of music
external links guitar at curlie guitar lessons at curlie
in statistics the logistic model or logit model is statistical model that models the probability of an event taking place by having the log odds for the event be linear combination of one or more independent variables 
in regression analysis logistic regression or logit regression is estimating the parameters of logistic model the coefficients in the linear combination 
formally in binary logistic regression there is single binary dependent variable coded by an indicator variable where the two values are labeled and while the independent variables can each be binary variable two classes coded by an indicator variable or continuous variable any real value 
the corresponding probability of the value labeled can vary between certainly the value and certainly the value hence the labeling the function that converts log odds to probability is the logistic function hence the name 
the unit of measurement for the log odds scale is called logit from logistic unit hence the alternative names 
see background and definition for formal mathematics and example for worked example 
binary variables are widely used in statistics to model the probability of certain class or event taking place such as the probability of team winning of patient being healthy etc 
see applications and the logistic model has been the most commonly used model for binary regression since about binary variables can be generalized to categorical variables when there are more than two possible values 
whether an image is of cat dog lion etc 
and the binary logistic regression generalized to multinomial logistic regression 
if the multiple categories are ordered one can use the ordinal logistic regression for example the proportional odds ordinal logistic model 
see extensions for further extensions 
the logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification it is not classifier though it can be used to make classifier for instance by choosing cutoff value and classifying inputs with probability greater than the cutoff as one class below the cutoff as the other this is common way to make binary classifier 
analogous linear models for binary variables with different sigmoid function instead of the logistic function to convert the linear combination to probability can also be used most notably the probit model see alternatives 
the defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at constant rate with each independent variable having its own parameter for binary dependent variable this generalizes the odds ratio 
more abstractly the logistic function is the natural parameter for the bernoulli distribution and in this sense is the simplest way to convert real number to probability 
in particular it maximizes entropy minimizes added information and in this sense makes the fewest assumptions of the data being modeled see maximum entropy 
the parameters of logistic regression are most commonly estimated by maximum likelihood estimation mle 
this does not have closed form expression unlike linear least squares see model fitting 
logistic regression by mle plays similarly basic role for binary or categorical responses as linear regression by ordinary least squares ols plays for scalar responses it is simple well analyzed baseline model see comparison with linear regression for discussion 
the logistic regression as general statistical model was originally developed and popularized primarily by joseph berkson beginning in berkson where he coined logit see history 
applications logistic regression is used in various fields including machine learning most medical fields and social sciences 
for example the trauma and injury severity score triss which is widely used to predict mortality in injured patients was originally developed by boyd et al 
many other medical scales used to assess severity of patient have been developed using logistic regression 
logistic regression may be used to predict the risk of developing given disease 
diabetes coronary heart disease based on observed characteristics of the patient age sex body mass index results of various blood tests etc 
another example might be to predict whether nepalese voter will vote nepali congress or communist party of nepal or any other party based on age income sex race state of residence votes in previous elections etc 
the technique can also be used in engineering especially for predicting the probability of failure of given process system or product 
it is also used in marketing applications such as prediction of customer propensity to purchase product or halt subscription etc 
in economics it can be used to predict the likelihood of person ending up in the labor force and business application would be to predict the likelihood of homeowner defaulting on mortgage 
conditional random fields an extension of logistic regression to sequential data are used in natural language processing 
example problem as simple example we can use logistic regression with one explanatory variable and two categories to answer the following question group of students spends between and hours studying for an exam 
how does the number of hours spent studying affect the probability of the student passing the exam 
the reason for using logistic regression for this problem is that the values of the dependent variable pass and fail while represented by and are not cardinal numbers 
if the problem was changed so that pass fail was replaced with the grade cardinal numbers then simple regression analysis could be used 
the table shows the number of hours each student spent studying and whether they passed or failed 
we wish to fit logistic function to the data consisting of the hours studied xk and the outcome of the test yk for pass for fail 
the data points are indexed by the subscript which runs from to the variable is called the explanatory variable and the variable is called the categorical variable consisting of two categories pass or fail corresponding to the categorical values and respectively 
model the logistic function is of the form where is location parameter the midpoint of the curve where and is scale parameter 
this expression may be rewritten as where and is known as the intercept it is the vertical intercept or intercept of the line and inverse scale parameter or rate parameter these are the intercept and slope of the log odds as function of conversely and 
fit the usual measure of goodness of fit for logistic regression uses logistic loss or log loss the negative log likelihood 
for given xk and yk write 
the are the probabilities that the corresponding will be unity and are the probabilities that they will be zero see bernoulli distribution 
we wish to find the values of and which give the best fit to the data 
in the case of linear regression the sum of the squared deviations of the fit from the data points yk the squared error loss is taken as measure of the goodness of fit and the best fit is obtained when that function is minimized 
the log loss for the th point is ln if ln if the log loss can be interpreted as the surprisal of the actual outcome relative to the prediction and is measure of information content 
note that log loss is always greater than or equal to equals only in case of perfect prediction when and or and and approaches infinity as the prediction gets worse when and or and meaning the actual outcome is more surprising 
since the value of the logistic function is always strictly between zero and one the log loss is always greater than zero and less than infinity 
note that unlike in linear regression where the model can have zero loss at point by passing through data point and zero loss overall if all points are on line in logistic regression it is not possible to have zero loss at any points since is either or but these can be combined into single expression ln ln 
this expression is more formally known as the cross entropy of the predicted distribution from the actual distribution as probability distributions on the two element space of pass fail 
the sum of these the total loss is the overall negative log likelihood and the best fit is obtained for those choices of and for which is minimized 
alternatively instead of minimizing the loss one can maximize its inverse the positive log likelihood ln ln ln ln or equivalently maximize the likelihood function itself which is the probability that the given data set is produced by particular logistic function this method is known as maximum likelihood estimation 
parameter estimation since is nonlinear in and determining their optimum values will require numerical methods 
note that one method of maximizing is to require the derivatives of with respect to and to be zero and the maximization procedure can be accomplished by solving the above two equations for and which again will generally require the use of numerical methods 
the values of and which maximize and using the above data are found to be which yields value for and of predictions the and coefficients may be entered into the logistic regression equation to estimate the probability of passing the exam 
for example for student who studies hours entering the value into the equation gives the estimated probability of passing the exam of probability of passing exam similarly for student who studies hours the estimated probability of passing the exam is probability of passing exam this table shows the estimated probability of passing the exam for several values of hours studying 
model evaluation the logistic regression analysis gives the following output 
by the wald test the output indicates that hours studying is significantly associated with the probability of passing the exam 
rather than the wald method the recommended method to calculate the value for logistic regression is the likelihood ratio test lrt which for this data gives see error and significance of the fit below 
generalizations this simple model is an example of binary logistic regression and has one explanatory variable and binary categorical variable which can assume one of two categorical values 
multinomial logistic regression is the generalization of binary logistic regression to include any number of explanatory variables and any number of categories 
background definition of the logistic function an explanation of logistic regression can begin with an explanation of the standard logistic function 
the logistic function is sigmoid function which takes any real input and outputs value between zero and one 
for the logit this is interpreted as taking input log odds and having output probability 
the standard logistic function is defined as follows graph of the logistic function on the interval is shown in figure let us assume that is linear function of single explanatory variable the case where is linear combination of multiple explanatory variables is treated similarly 
we can then express as follows and the general logistic function can now be written as in the logistic model is interpreted as the probability of the dependent variable equaling success case rather than failure non case 
it clear that the response variables are not identically distributed differs from one data point to another though they are independent given design matrix and shared parameters 
definition of the inverse of the logistic function we can now define the logit log odds function as the inverse of the standard logistic function 
it is easy to see that it satisfies logit ln and equivalently after exponentiating both sides we have the odds 
interpretation of these terms in the above equations the terms are as follows is the logit function 
the equation for illustrates that the logit log odds or natural logarithm of the odds is equivalent to the linear regression expression 
ln denotes the natural logarithm 
is the probability that the dependent variable equals case given some linear combination of the predictors 
the formula for illustrates that the probability of the dependent variable equaling case is equal to the value of the logistic function of the linear regression expression 
this is important in that it shows that the value of the linear regression expression can vary from negative to positive infinity and yet after transformation the resulting expression for the probability ranges between and is the intercept from the linear regression equation the value of the criterion when the predictor is equal to zero 
is the regression coefficient multiplied by some value of the predictor 
base denotes the exponential function 
definition of the odds the odds of the dependent variable equaling case given some linear combination of the predictors is equivalent to the exponential function of the linear regression expression 
this illustrates how the logit serves as link function between the probability and the linear regression expression 
given that the logit ranges between negative and positive infinity it provides an adequate criterion upon which to conduct linear regression and the logit is easily converted back into the odds so we define odds of the dependent variable equaling case given some linear combination of the predictors as follows odds 
the odds ratio for continuous independent variable the odds ratio can be defined as odds odds this exponential relationship provides an interpretation for the odds multiply by for every unit increase in for binary independent variable the odds ratio is defined as where and are cells in contingency table 
multiple explanatory variables if there are multiple explanatory variables the above expression can be revised to then when this is used in the equation relating the log odds of success to the values of the predictors the linear regression will be multiple regression with explanators the parameters for all are all estimated 
again the more traditional equations are log and where usually definition the basic setup of logistic regression is as follows 
we are given dataset containing points 
each point consists of set of input variables xm also called independent variables explanatory variables predictor variables features or attributes and binary outcome variable yi also known as dependent variable response variable output variable or class 
it can assume only the two possible values often meaning no or failure or often meaning yes or success 
the goal of logistic regression is to use the dataset to create predictive model of the outcome variable 
as in linear regression the outcome variables yi are assumed to depend on the explanatory variables xm 
explanatory variablesthe explanatory variables may be of any type real valued binary categorical etc 
the main distinction is between continuous variables and discrete variables 
discrete variables referring to more than two possible choices are typically coded using dummy variables or indicator variables that is separate explanatory variables taking the value or are created for each possible value of the discrete variable with meaning variable does have the given value and meaning variable does not have that value 
outcome variablesformally the outcomes yi are described as being bernoulli distributed data where each outcome is determined by an unobserved probability pi that is specific to the outcome at hand but related to the explanatory variables 
this can be expressed in any of the following equivalent forms bernoulli pr if if pr the meanings of these four lines are the first line expresses the probability distribution of each yi conditioned on the explanatory variables it follows bernoulli distribution with parameters pi the probability of the outcome of for trial as noted above each separate trial has its own probability of success just as each trial has its own explanatory variables 
the probability of success pi is not observed only the outcome of an individual bernoulli trial using that probability 
the second line expresses the fact that the expected value of each yi is equal to the probability of success pi which is general property of the bernoulli distribution 
in other words if we run large number of bernoulli trials using the same probability of success pi then take the average of all the and outcomes then the result would be close to pi 
this is because doing an average this way simply computes the proportion of successes seen which we expect to converge to the underlying probability of success 
the third line writes out the probability mass function of the bernoulli distribution specifying the probability of seeing each of the two possible outcomes 
the fourth line is another way of writing the probability mass function which avoids having to write separate cases and is more convenient for certain types of calculations 
this relies on the fact that yi can take only the value or in each case one of the exponents will be choosing the value under it while the other is canceling out the value under it 
hence the outcome is either pi or pi as in the previous line linear predictor functionthe basic idea of logistic regression is to use the mechanism already developed for linear regression by modeling the probability pi using linear predictor function 
linear combination of the explanatory variables and set of regression coefficients that are specific to the model at hand but the same for all trials 
the linear predictor function for particular data point is written as where are regression coefficients indicating the relative effect of particular explanatory variable on the outcome 
the model is usually put into more compact form as follows the regression coefficients are grouped into single vector of size for each data point an additional explanatory pseudo variable is added with fixed value of corresponding to the intercept coefficient 
the resulting explanatory variables xm are then grouped into single vector xi of size this makes it possible to write the linear predictor function as follows using the notation for dot product between two vectors 
many explanatory variables two categories the above example of binary logistic regression on one explanatory variable can be generalized to binary logistic regression on any number of explanatory variables and any number of categorical values 
to begin with we may consider logistic model with explanatory variables xm and as in the example above two categorical values and 
for the simple binary logistic regression model we assumed linear relationship between the predictor variable and the log odds also called logit of the event that this linear relationship may be extended to the case of explanatory variables log where is the log odds and are parameters of the model 
an additional generalization has been introduced in which the base of the model is not restricted to the euler number in most applications the base of the logarithm is usually taken to be however in some cases it can be easier to communicate results by working in base or base for more compact notation we will specify the explanatory variables and the coefficients as dimensional vectors with an added explanatory variable 
the logit may now be written as solving for the probability that yields where is the sigmoid function with base the above formula shows that once the are fixed we can easily compute either the log odds that for given observation or the probability that for given observation 
the main use case of logistic model is to be given an observation and estimate the probability that the optimum beta coefficients may again be found by maximizing the log likelihood 
for measurements defining xk as the explanatory vector of the th measurement and yk as the categorical outcome of that measurement the log likelihood may be written in form very similar to the simple case above log log as in the simple example above finding the optimum parameters will require numerical methods 
one useful technique is to equate the derivatives of the log likelihood with respect to each of the parameters to zero yielding set of equations which will hold at the maximum of the log likelihood where xmk is the value of the xm explanatory variable from the th measurement 
consider an example with explanatory variables and coefficients and which have been determined by the above method 
to be concrete the model is log where is the probability of the event that this can be interpreted as follows is the intercept 
it is the log odds of the event that when the predictors by exponentiating we can see that when the odds of the event that are to or similarly the probability of the event that when can be computed as means that increasing by increases the log odds by so if increases by the odds that increase by factor of note that the probability of has also increased but it has not increased by as much as the odds have increased 
means that increasing by increases the log odds by so if increases by the odds that increase by factor of note how the effect of on the log odds is twice as great as the effect of but the effect on the odds is times greater 
but the effect on the probability of is not as much as times greater it only the effect on the odds that is times greater 
multinomial logistic regression many explanatory variables and many categories in the above cases of two categories binomial logistic regression the categories were indexed by and and we had two probability distributions the probability that the outcome was in category was given by and the probability that the outcome was in category was given by 
the sum of both probabilities is equal to unity as they must be 
in general if we have explanatory variables including and categories we will need separate probability distributions one for each category indexed by which describe the probability that the categorical outcome for explanatory vector will be in category 
it will also be required that the sum of these probabilities over all categories be equal to unity 
using the mathematically convenient base these probabilities are for each of the probabilities except will have their own set of regression coefficients it can be seen that as required the sum of the over all categories is unity 
note that the selection of to be defined in terms of the other probabilities is artificial 
any of the probabilities could have been selected to be so defined 
this special value of is termed the pivot index and the log odds tn are expressed in terms of the pivot probability and are again expressed as linear combination of the explanatory variables ln note also that for the simple case of the two category case is recovered with and 
the log likelihood that particular set of measurements or data points will be generated by the above probabilities can now be calculated 
indexing each measurement by let the th set of measured explanatory variables be denoted by and their categorical outcomes be denoted by which can be equal to any integer in 
the log likelihood is then ln where is an indicator function which is equal to unity if yk and zero otherwise 
in the case of two explanatory variables this indicator function was defined as yk when and yk when this was convenient but not necessary 
again the optimum beta coefficients may be found by maximizing the log likelihood function generally using numerical methods 
possible method of solution is to set the derivatives of the log likelihood with respect to each beta coefficient equal to zero and solve for the beta coefficients where is the th coefficient of the vector and is the th explanatory variable of the th measurement 
once the beta coefficients have been estimated from the data we will be able to estimate the probability that any subsequent set of explanatory variables will result in any of the possible outcome categories 
interpretations there are various equivalent specifications and interpretations of logistic regression which fit into different types of more general models and allow different generalizations 
as generalized linear model the particular model used by logistic regression which distinguishes it from standard linear regression and from other types of regression analysis used for binary valued outcomes is the way the probability of particular outcome is linked to the linear predictor function logit logit ln written using the more compact notation described above this is logit logit ln this formulation expresses logistic regression as type of generalized linear model which predicts variables with various types of probability distributions by fitting linear predictor function of the above form to some sort of arbitrary transformation of the expected value of the variable 
the intuition for transforming using the logit function the natural log of the odds was explained above 
it also has the practical effect of converting the probability which is bounded to be between and to variable that ranges over thereby matching the potential range of the linear prediction function on the right side of the equation 
note that both the probabilities pi and the regression coefficients are unobserved and the means of determining them is not part of the model itself 
they are typically determined by some sort of optimization procedure 
maximum likelihood estimation that finds values that best fit the observed data 
that give the most accurate predictions for the data already observed usually subject to regularization conditions that seek to exclude unlikely values 
extremely large values for any of the regression coefficients 
the use of regularization condition is equivalent to doing maximum posteriori map estimation an extension of maximum likelihood 
regularization is most commonly done using squared regularizing function which is equivalent to placing zero mean gaussian prior distribution on the coefficients but other regularizers are also possible 
whether or not regularization is used it is usually not possible to find closed form solution instead an iterative numerical method must be used such as iteratively reweighted least squares irls or more commonly these days quasi newton method such as the bfgs method the interpretation of the parameter estimates is as the additive effect on the log of the odds for unit change in the the explanatory variable 
in the case of dichotomous explanatory variable for instance gender is the estimate of the odds of having the outcome for say males compared with females 
an equivalent formula uses the inverse of the logit function which is the logistic function 
logit the formula can also be written as probability distribution specifically using probability mass function pr as latent variable model the logistic model has an equivalent formulation as latent variable model 
this formulation is common in the theory of discrete choice models and makes it easier to extend to certain more complicated models with multiple correlated choices as well as to compare logistic regression to the closely related probit model 
imagine that for each trial there is continuous latent variable yi 
an unobserved random variable that is distributed as follows where logistic 
the latent variable can be written directly in terms of the linear predictor function and an additive random error variable that is distributed according to standard logistic distribution 
then yi can be viewed as an indicator for whether this latent variable is positive if 
the choice of modeling the error variable specifically with standard logistic distribution rather than general logistic distribution with the location and scale set to arbitrary values seems restrictive but in fact it is not 
it must be kept in mind that we can choose the regression coefficients ourselves and very often can use them to offset changes in the parameters of the error variable distribution 
for example logistic error variable distribution with non zero location parameter which sets the mean is equivalent to distribution with zero location parameter where has been added to the intercept coefficient 
both situations produce the same value for yi regardless of settings of explanatory variables 
similarly an arbitrary scale parameter is equivalent to setting the scale parameter to and then dividing all regression coefficients by in the latter case the resulting value of yi will be smaller by factor of than in the former case for all sets of explanatory variables but critically it will always remain on the same side of and hence lead to the same yi choice 
note that this predicts that the irrelevancy of the scale parameter may not carry over into more complex models where more than two choices are available 
it turns out that this formulation is exactly equivalent to the preceding one phrased in terms of the generalized linear model and without any latent variables 
this can be shown as follows using the fact that the cumulative distribution function cdf of the standard logistic distribution is the logistic function which is the inverse of the logit function 
pr logit then pr pr pr pr pr because the logistic distribution is symmetric logit see above this formulation which is standard in discrete choice models makes clear the relationship between logistic regression the logit model and the probit model which uses an error variable distributed according to standard normal distribution instead of standard logistic distribution 
both the logistic and normal distributions are symmetric with basic unimodal bell curve shape 
the only difference is that the logistic distribution has somewhat heavier tails which means that it is less sensitive to outlying data and hence somewhat more robust to model mis specifications or erroneous data 
two way latent variable model yet another formulation uses two separate latent variables where ev ev where ev is standard type extreme value distribution 
pr pr then if otherwise 
this model has separate latent variable and separate set of regression coefficients for each possible outcome of the dependent variable 
the reason for this separation is that it makes it easy to extend logistic regression to multi outcome categorical variables as in the multinomial logit model 
in such model it is natural to model each possible outcome using different set of regression coefficients 
it is also possible to motivate each of the separate latent variables as the theoretical utility associated with making the associated choice and thus motivate logistic regression in terms of utility theory 
in terms of utility theory rational actor always chooses the choice with the greatest associated utility 
this is the approach taken by economists when formulating discrete choice models because it both provides theoretically strong foundation and facilitates intuitions about the model which in turn makes it easy to consider various sorts of extensions 
see the example below 
the choice of the type extreme value distribution seems fairly arbitrary but it makes the mathematics work out and it may be possible to justify its use through rational choice theory 
it turns out that this model is equivalent to the previous model although this seems non obvious since there are now two sets of regression coefficients and error variables and the error variables have different distribution 
in fact this model reduces directly to the previous one with the following substitutions an intuition for this comes from the fact that since we choose based on the maximum of two values only their difference matters not the exact values and this effectively removes one degree of freedom 
another critical fact is that the difference of two type extreme value distributed variables is logistic distribution 
we can demonstrate the equivalent as follows pr pr pr pr pr pr pr substitute as above pr substitute as above pr now same as above model pr logit example as an example consider province level election where the choice is between right of center party left of center party and secessionist party 
the parti qu cois which wants quebec to secede from canada 
we would then use three latent variables one for each choice 
then in accordance with utility theory we can then interpret the latent variables as expressing the utility that results from making each of the choices 
we can also interpret the regression coefficients as indicating the strength that the associated factor 
explanatory variable has in contributing to the utility or more correctly the amount by which unit change in an explanatory variable changes the utility of given choice 
voter might expect that the right of center party would lower taxes especially on rich people 
this would give low income people no benefit 
no change in utility since they usually don pay taxes would cause moderate benefit 
somewhat more money or moderate utility increase for middle incoming people would cause significant benefits for high income people 
on the other hand the left of center party might be expected to raise taxes and offset it with increased welfare and other assistance for the lower and middle classes 
this would cause significant positive benefit to low income people perhaps weak benefit to middle income people and significant negative benefit to high income people 
finally the secessionist party would take no direct actions on the economy but simply secede 
low income or middle income voter might expect basically no clear utility gain or loss from this but high income voter might expect negative utility since he she is likely to own companies which will have harder time doing business in such an environment and probably lose money 
these intuitions can be expressed as follows this clearly shows that separate sets of regression coefficients need to exist for each choice 
when phrased in terms of utility this can be seen very easily 
different choices have different effects on net utility furthermore the effects vary in complex ways that depend on the characteristics of each individual so there need to be separate sets of coefficients for each characteristic not simply single extra per choice characteristic 
even though income is continuous variable its effect on utility is too complex for it to be treated as single variable 
either it needs to be directly split up into ranges or higher powers of income need to be added so that polynomial regression on income is effectively done 
as log linear model yet another formulation combines the two way latent variable formulation above with the original formulation higher up without latent variables and in the process provides link to one of the standard formulations of the multinomial logit 
here instead of writing the logit of the probabilities pi as linear predictor we separate the linear predictor into two one for each of the two outcomes ln pr ln ln pr ln two separate sets of regression coefficients have been introduced just as in the two way latent variable model and the two equations appear form that writes the logarithm of the associated probability as linear predictor with an extra term ln at the end 
this term as it turns out serves as the normalizing factor ensuring that the result is distribution 
this can be seen by exponentiating both sides pr pr in this form it is clear that the purpose of is to ensure that the resulting distribution over yi is in fact probability distribution 
it sums to this means that is simply the sum of all un normalized probabilities and by dividing each probability by the probabilities become normalized 
that is and the resulting equations are pr pr or generally pr this shows clearly how to generalize this formulation to more than two outcomes as in multinomial logit 
note that this general formulation is exactly the softmax function as in pr softmax 
in order to prove that this is equivalent to the previous model note that the above model is overspecified in that pr and pr cannot be independently specified rather pr pr so knowing one automatically determines the other 
as result the model is nonidentifiable in that multiple combinations of and will produce the same probabilities for all possible explanatory variables 
in fact it can be seen that adding any constant vector to both of them will produce the same probabilities pr as result we can simplify matters and restore identifiability by picking an arbitrary value for one of the two vectors 
we choose to set then and so pr which shows that this formulation is indeed equivalent to the previous formulation 
as in the two way latent variable formulation any settings where will produce equivalent results 
note that most treatments of the multinomial logit model start out either by extending the log linear formulation presented here or the two way latent variable formulation presented above since both clearly show the way that the model could be extended to multi way outcomes 
in general the presentation with latent variables is more common in econometrics and political science where discrete choice models and utility theory reign while the log linear formulation here is more common in computer science 
machine learning and natural language processing 
as single layer perceptron the model has an equivalent formulation 
this functional form is commonly called single layer perceptron or single layer artificial neural network 
single layer neural network computes continuous output instead of step function 
the derivative of pi with respect to xk is computed from the general form where is an analytic function in with this choice the single layer neural network is identical to the logistic regression model 
this function has continuous derivative which allows it to be used in backpropagation 
this function is also preferred because its derivative is easily calculated 
in terms of binomial data closely related model assumes that each is associated not with single bernoulli trial but with ni independent identically distributed trials where the observation yi is the number of successes observed the sum of the individual bernoulli distributed random variables and hence follows binomial distribution bin for an example of this distribution is the fraction of seeds pi that germinate after ni are planted 
in terms of expected values this model is expressed as follows so that logit logit ln or equivalently pr this model can be fit using the same sorts of methods as the above more basic model 
model fitting maximum likelihood estimation mle the regression coefficients are usually estimated using maximum likelihood estimation 
unlike linear regression with normally distributed residuals it is not possible to find closed form expression for the coefficient values that maximize the likelihood function so that an iterative process must be used instead for example newton method 
this process begins with tentative solution revises it slightly to see if it can be improved and repeats this revision until no more improvement is made at which point the process is said to have converged in some instances the model may not reach convergence 
non convergence of model indicates that the coefficients are not meaningful because the iterative process was unable to find appropriate solutions 
failure to converge may occur for number of reasons having large ratio of predictors to cases multicollinearity sparseness or complete separation 
having large ratio of variables to cases results in an overly conservative wald statistic discussed below and can lead to non convergence 
regularized logistic regression is specifically intended to be used in this situation 
multicollinearity refers to unacceptably high correlations between predictors 
as multicollinearity increases coefficients remain unbiased but standard errors increase and the likelihood of model convergence decreases 
to detect multicollinearity amongst the predictors one can conduct linear regression analysis with the predictors of interest for the sole purpose of examining the tolerance statistic used to assess whether multicollinearity is unacceptably high 
sparseness in the data refers to having large proportion of empty cells cells with zero counts 
zero cell counts are particularly problematic with categorical predictors 
with continuous predictors the model can infer values for the zero cell counts but this is not the case with categorical predictors 
the model will not converge with zero cell counts for categorical predictors because the natural logarithm of zero is an undefined value so that the final solution to the model cannot be reached 
to remedy this problem researchers may collapse categories in theoretically meaningful way or add constant to all cells 
another numerical problem that may lead to lack of convergence is complete separation which refers to the instance in which the predictors perfectly predict the criterion all cases are accurately classified and the likelihood maximized with infinite coefficients 
in such instances one should re examine the data as there may be some kind of error 
one can also take semi parametric or non parametric approaches via local likelihood or nonparametric quasi likelihood methods which avoid assumptions of parametric form for the index function and is robust to the choice of the link function probit or logit 
iteratively reweighted least squares irls binary logistic regression or can for example be calculated using iteratively reweighted least squares irls which is equivalent to maximizing the log likelihood of bernoulli distributed process using newton method 
if the problem is written in vector matrix form with parameters explanatory variables and expected value of the bernoulli distribution the parameters can be found using the following iterative algorithm where diag is diagonal weighting matrix the vector of expected values the regressor matrix and the vector of response variables 
more details can be found in the literature 
bayesian in bayesian statistics context prior distributions are normally placed on the regression coefficients for example in the form of gaussian distributions 
there is no conjugate prior of the likelihood function in logistic regression 
when bayesian inference was performed analytically this made the posterior distribution difficult to calculate except in very low dimensions 
now though automatic software such as openbugs jags pymc stan or turing jl allows these posteriors to be computed using simulation so lack of conjugacy is not concern 
however when the sample size or the number of parameters is large full bayesian simulation can be slow and people often use approximate methods such as variational bayesian methods and expectation propagation 
rule of ten widely used rule of thumb the one in ten rule states that logistic regression models give stable values for the explanatory variables if based on minimum of about events per explanatory variable epv where event denotes the cases belonging to the less frequent category in the dependent variable 
thus study designed to use explanatory variables for an event 
myocardial infarction expected to occur in proportion of participants in the study will require total of participants 
however there is considerable debate about the reliability of this rule which is based on simulation studies and lacks secure theoretical underpinning 
according to some authors the rule is overly conservative in some circumstances with the authors stating if we somewhat subjectively regard confidence interval coverage less than percent type error greater than percent or relative bias greater than percent as problematic our results indicate that problems are fairly frequent with epv uncommon with epv and still observed with epv 
the worst instances of each problem were not severe with epv and usually comparable to those with epv others have found results that are not consistent with the above using different criteria 
useful criterion is whether the fitted model will be expected to achieve the same predictive discrimination in new sample as it appeared to achieve in the model development sample 
for that criterion events per candidate variable may be required 
also one can argue that observations are needed only to estimate the model intercept precisely enough that the margin of error in predicted probabilities is with confidence level 
error and significance of fit deviance and likelihood ratio test simple case in any fitting procedure the addition of another fitting parameter to model 
the beta parameters in logistic regression model will almost always improve the ability of the model to predict the measured outcomes 
this will be true even if the additional term has no predictive value since the model will simply be overfitting to the noise in the data 
the question arises as to whether the improvement gained by the addition of another fitting parameter is significant enough to recommend the inclusion of the additional term or whether the improvement is simply that which may be expected from overfitting 
in short for logistic regression statistic known as the deviance is defined which is measure of the error between the logistic model fit and the outcome data 
in the limit of large number of data points the deviance is chi squared distributed which allows chi squared test to be implemented in order to determine the significance of the explanatory variables 
linear regression and logistic regression have many similarities 
for example in simple linear regression set of data points xk yk are fitted to proposed model function of the form the fit is obtained by choosing the parameters which minimize the sum of the squares of the residuals the squared error term for each data point the minimum value which constitutes the fit will be denoted by the idea of null model may be introduced in which it is assumed that the variable is of no use in predicting the yk outcomes the data points are fitted to null model function of the form with squared error term the fitting process consists of choosing value of which minimizes of the fit to the null model denoted by where the subscript denotes the null model 
it is seen that the null model is optimized by where is the mean of the yk values and the optimized is which is proportional to the square of the uncorrected sample standard deviation of the yk data points 
we can imagine case where the yk data points are randomly assigned to the various xk and then fitted using the proposed model 
specifically we can consider the fits of the proposed model to every permutation of the yk outcomes 
it can be shown that the optimized error of any of these fits will never be less than the optimum error of the null model and that the difference between these minimum error will follow chi squared distribution distribution with degrees of freedom equal those of the proposed model minus those of the null model which in this case will be 
using the chi squared test we may then estimate how many of these permuted sets of yk will yield an minimum error less than or equal to the minimum error using the original yk and so we can estimate how significant an improvement is given by the inclusion of the variable in the proposed model 
for logistic regression the measure of goodness of fit is the likelihood function or its logarithm the log likelihood the likelihood function is analogous to the in the linear regression case except that the likelihood is maximized rather than minimized 
denote the maximized log likelihood of the proposed model by 
in the case of simple binary logistic regression the set of data points are fitted in probabilistic sense to function of the form where is the probability that the log odds are given by and the log likelihood is ln ln for the null model the probability that is given by the log odds for the null model are given by and the log likelihood is ln ln since we have at the maximum of the maximum log likelihood for the null model is ln ln the optimum is ln where is again the mean of the yk values 
again we can conceptually consider the fit of the proposed model to every permutation of the yk and it can be shown that the maximum log likelihood of these permutation fits will never be smaller than that of the null model also as an analog to the error of the linear regression case we may define the deviance of logistic regression fit as ln which will always be positive or zero 
the reason for this choice is that not only is the deviance good measure of the goodness of fit it is also approximately chi squared distributed with the approximation improving as the number of data points increases becoming exactly chi square distributed in the limit of an infinite number of data points 
as in the case of linear regression we may use this fact to estimate the probability that random set of data points will give better fit than the fit obtained by the proposed model and so have an estimate how significantly the model is improved by including the xk data points in the proposed model 
for the simple model of student test scores described above the maximum value of the log likelihood of the null model is the maximum value of the log likelihood for the simple model is so that the deviance is using the chi squared test of significance the integral of the chi squared distribution with one degree of freedom from to infinity is equal to this effectively means that about out of fits to random yk can be expected to have better fit smaller deviance than the given yk and so we can conclude that the inclusion of the variable and data in the proposed model is very significant improvement over the null model 
in other words we reject the null hypothesis with confidence 
goodness of fit summary goodness of fit in linear regression models is generally measured using 
since this has no direct analog in logistic regression various methods ch including the following can be used instead 
deviance and likelihood ratio tests in linear regression analysis one is concerned with partitioning variance via the sum of squares calculations variance in the criterion is essentially divided into variance accounted for by the predictors and residual variance 
in logistic regression analysis deviance is used in lieu of sum of squares calculations 
deviance is analogous to the sum of squares calculations in linear regression and is measure of the lack of fit to the data in logistic regression model 
when saturated model is available model with theoretically perfect fit deviance is calculated by comparing given model with the saturated model 
this computation gives the likelihood ratio test ln likelihood of the fitted model likelihood of the saturated model 
in the above equation represents the deviance and ln represents the natural logarithm 
the log of this likelihood ratio the ratio of the fitted model to the saturated model will produce negative value hence the need for negative sign 
can be shown to follow an approximate chi squared distribution 
smaller values indicate better fit as the fitted model deviates less from the saturated model 
when assessed upon chi square distribution nonsignificant chi square values indicate very little unexplained variance and thus good model fit 
conversely significant chi square value indicates that significant amount of the variance is unexplained 
when the saturated model is not available common case deviance is calculated simply as log likelihood of the fitted model and the reference to the saturated model log likelihood can be removed from all that follows without harm 
two measures of deviance are particularly important in logistic regression null deviance and model deviance 
the null deviance represents the difference between model with only the intercept which means no predictors and the saturated model 
the model deviance represents the difference between model with at least one predictor and the saturated model 
in this respect the null model provides baseline upon which to compare predictor models 
given that deviance is measure of the difference between given model and the saturated model smaller values indicate better fit 
thus to assess the contribution of predictor or set of predictors one can subtract the model deviance from the null deviance and assess the difference on chi square distribution with degrees of freedom equal to the difference in the number of parameters estimated 
let null ln likelihood of null model likelihood of the saturated model fitted ln likelihood of fitted model likelihood of the saturated model 
then the difference of both is null fitted ln likelihood of null model likelihood of the saturated model ln likelihood of fitted model likelihood of the saturated model ln likelihood of null model likelihood of the saturated model likelihood of fitted model likelihood of the saturated model ln likelihood of the null model likelihood of fitted model 
if the model deviance is significantly smaller than the null deviance then one can conclude that the predictor or set of predictors significantly improve the model fit 
this is analogous to the test used in linear regression analysis to assess the significance of prediction 
pseudo squared in linear regression the squared multiple correlation is used to assess goodness of fit as it represents the proportion of variance in the criterion that is explained by the predictors 
in logistic regression analysis there is no agreed upon analogous measure but there are several competing measures each with limitations four of the most commonly used indices and one less commonly used one are examined on this page likelihood ratio cox and snell cs nagelkerke mcfadden mcf tjur hosmer lemeshow test the hosmer lemeshow test uses test statistic that asymptotically follows distribution to assess whether or not the observed event rates match expected event rates in subgroups of the model population 
this test is considered to be obsolete by some statisticians because of its dependence on arbitrary binning of predicted probabilities and relative low power 
coefficient significance after fitting the model it is likely that researchers will want to examine the contribution of individual predictors 
to do so they will want to examine the regression coefficients 
in linear regression the regression coefficients represent the change in the criterion for each unit change in the predictor 
in logistic regression however the regression coefficients represent the change in the logit for each unit change in the predictor 
given that the logit is not intuitive researchers are likely to focus on predictor effect on the exponential function of the regression coefficient the odds ratio see definition 
in linear regression the significance of regression coefficient is assessed by computing test 
in logistic regression there are several different tests designed to assess the significance of an individual predictor most notably the likelihood ratio test and the wald statistic 
likelihood ratio test the likelihood ratio test discussed above to assess model fit is also the recommended procedure to assess the contribution of individual predictors to given model 
in the case of single predictor model one simply compares the deviance of the predictor model with that of the null model on chi square distribution with single degree of freedom 
if the predictor model has significantly smaller deviance 
chi square using the difference in degrees of freedom of the two models then one can conclude that there is significant association between the predictor and the outcome 
although some common statistical packages 
spss do provide likelihood ratio test statistics without this computationally intensive test it would be more difficult to assess the contribution of individual predictors in the multiple logistic regression case 
to assess the contribution of individual predictors one can enter the predictors hierarchically comparing each new model with the previous to determine the contribution of each predictor 
there is some debate among statisticians about the appropriateness of so called stepwise procedures 
the fear is that they may not preserve nominal statistical properties and may become misleading 
wald statistic alternatively when assessing the contribution of individual predictors in given model one may examine the significance of the wald statistic 
the wald statistic analogous to the test in linear regression is used to assess the significance of coefficients 
the wald statistic is the ratio of the square of the regression coefficient to the square of the standard error of the coefficient and is asymptotically distributed as chi square distribution 
although several statistical packages spss sas report the wald statistic to assess the contribution of individual predictors the wald statistic has limitations 
when the regression coefficient is large the standard error of the regression coefficient also tends to be larger increasing the probability of type ii error 
the wald statistic also tends to be biased when data are sparse 
case control sampling suppose cases are rare 
then we might wish to sample them more frequently than their prevalence in the population 
for example suppose there is disease that affects person in and to collect our data we need to do complete physical 
it may be too expensive to do thousands of physicals of healthy people in order to obtain data for only few diseased individuals 
thus we may evaluate more diseased individuals perhaps all of the rare outcomes 
this is also retrospective sampling or equivalently it is called unbalanced data 
as rule of thumb sampling controls at rate of five times the number of cases will produce sufficient control data logistic regression is unique in that it may be estimated on unbalanced data rather than randomly sampled data and still yield correct coefficient estimates of the effects of each independent variable on the outcome 
that is to say if we form logistic model from such data if the model is correct in the general population the parameters are all correct except for we can correct if we know the true prevalence as follows log log where is the true prevalence and is the prevalence in the sample 
discussion like other forms of regression analysis logistic regression makes use of one or more predictor variables that may be either continuous or categorical 
unlike ordinary linear regression however logistic regression is used for predicting dependent variables that take membership in one of limited number of categories treating the dependent variable in the binomial case as the outcome of bernoulli trial rather than continuous outcome 
given this difference the assumptions of linear regression are violated 
in particular the residuals cannot be normally distributed 
in addition linear regression may make nonsensical predictions for binary dependent variable 
what is needed is way to convert binary variable into continuous one that can take on any real value negative or positive 
to do that binomial logistic regression first calculates the odds of the event happening for different levels of each independent variable and then takes its logarithm to create continuous criterion as transformed version of the dependent variable 
the logarithm of the odds is the logit of the probability the logit is defined as follows although the dependent variable in logistic regression is bernoulli the logit is on an unrestricted scale 
the logit function is the link function in this kind of generalized linear model 
is the bernoulli distributed response variable and is the predictor variable the values are the linear parameters 
the logit of the probability of success is then fitted to the predictors 
the predicted value of the logit is converted back into predicted odds via the inverse of the natural logarithm the exponential function 
thus although the observed dependent variable in binary logistic regression is or variable the logistic regression estimates the odds as continuous variable that the dependent variable is success 
in some applications the odds are all that is needed 
in others specific yes or no prediction is needed for whether the dependent variable is or is not success this categorical prediction can be based on the computed odds of success with predicted odds above some chosen cutoff value being translated into prediction of success 
maximum entropy of all the functional forms used for estimating the probabilities of particular categorical outcome which optimize the fit by maximizing the likelihood function 
probit regression poisson regression etc 
the logistic regression solution is unique in that it is maximum entropy solution 
this is case of general property an exponential family of distributions maximizes entropy given an expected value 
in the case of the logistic model the logistic function is the natural parameter of the bernoulli distribution it is in canonical form and the logistic function is the canonical link function while other sigmoid functions are non canonical link functions this underlies its mathematical elegance and ease of optimization 
see exponential family maximum entropy derivation for details 
proof in order to show this we use the method of lagrange multipliers 
the lagrangian is equal to the entropy plus the sum of the products of lagrange multipliers times various constraint expressions 
the general multinomial case will be considered since the proof is not made that much simpler by considering simpler cases 
equating the derivative of the lagrangian with respect to the various probabilities to zero yields functional form for those probabilities which corresponds to those used in logistic regression as in the above section on multinomial logistic regression we will consider explanatory variables denoted and which include there will be total of data points indexed by and the data points are given by and the xmk will also be represented as an dimensional vector 
there will be possible values of the categorical variable ranging from to let pn be the probability given explanatory variable vector that the outcome will be define which is the probability that for the th measurement the categorical outcome is the lagrangian will be expressed as function of the probabilities pnk and will minimized by equating the derivatives of the lagrangian with respect to these probabilities to zero 
an important point is that the probabilities are treated equally and the fact that they sum to unity is part of the lagrangian formulation rather than being assumed from the beginning 
the first contribution to the lagrangian is the entropy ln the log likelihood is ln assuming the multinomial logistic function the derivative of the log likelihood with respect the beta coefficients was found to be very important point here is that this expression is remarkably not an explicit function of the beta coefficients 
it is only function of the probabilities pnk and the data 
rather than being specific to the assumed multinomial logistic case it is taken to be general statement of the condition at which the log likelihood is maximized and makes no reference to the functional form of pnk 
there are then fitting constraints and the fitting constraint term in the lagrangian is then where the nm are the appropriate lagrange multipliers 
there are normalization constraints which may be written so that the normalization term in the lagrangian is where the are the appropriate lagrange multipliers 
the lagrangian is then the sum of the above three terms setting the derivative of the lagrangian with respect to one of the probabilities to zero yields ln using the more condensed vector notation and dropping the primes on the and indices and then solving for yields where imposing the normalization constraint we can solve for the zk and write the probabilities as the are not all independent 
we can add any constant dimensional vector to each of the without changing the value of the probabilities so that there are only rather than independent in the multinomial logistic regression section above the was subtracted from each which set the exponential term involving to unity and the beta coefficients were given by 
other approaches in machine learning applications where logistic regression is used for binary classification the mle minimises the cross entropy loss function 
logistic regression is an important machine learning algorithm 
the goal is to model the probability of random variable being or given experimental data consider generalized linear model function parameterized by pr therefore pr and since we see that pr is given by pr 
we now calculate the likelihood function assuming that all the observations in the sample are independently bernoulli distributed pr pr typically the log likelihood is maximized log log pr which is maximized using optimization techniques such as gradient descent 
assuming the pairs are drawn uniformly from the underlying distribution then in the limit of large lim log pr pr log pr pr log pr pr log pr kl where is the conditional entropy and kl is the kullback leibler divergence 
this leads to the intuition that by maximizing the log likelihood of model you are minimizing the kl divergence of your model from the maximal entropy distribution 
intuitively searching for the model that makes the fewest assumptions in its parameters 
comparison with linear regression logistic regression can be seen as special case of the generalized linear model and thus analogous to linear regression 
the model of logistic regression however is based on quite different assumptions about the relationship between the dependent and independent variables from those of linear regression 
in particular the key differences between these two models can be seen in the following two features of logistic regression 
first the conditional distribution is bernoulli distribution rather than gaussian distribution because the dependent variable is binary 
second the predicted values are probabilities and are therefore restricted to through the logistic distribution function because logistic regression predicts the probability of particular outcomes rather than the outcomes themselves 
alternatives common alternative to the logistic model logit model is the probit model as the related names suggest 
from the perspective of generalized linear models these differ in the choice of link function the logistic model uses the logit function inverse logistic function while the probit model uses the probit function inverse error function 
equivalently in the latent variable interpretations of these two methods the first assumes standard logistic distribution of errors and the second standard normal distribution of errors 
other sigmoid functions or error distributions can be used instead 
logistic regression is an alternative to fisher method linear discriminant analysis 
if the assumptions of linear discriminant analysis hold the conditioning can be reversed to produce logistic regression 
the converse is not true however because logistic regression does not require the multivariate normal assumption of discriminant analysis the assumption of linear predictor effects can easily be relaxed using techniques such as spline functions 
history detailed history of the logistic regression is given in cramer 
the logistic function was developed as model of population growth and named logistic by pierre fran ois verhulst in the and under the guidance of adolphe quetelet see logistic function history for details 
in his earliest paper verhulst did not specify how he fit the curves to the data 
in his more detailed paper verhulst determined the three parameters of the model by making the curve pass through three observed points which yielded poor predictions the logistic function was independently developed in chemistry as model of autocatalysis wilhelm ostwald 
an autocatalytic reaction is one in which one of the products is itself catalyst for the same reaction while the supply of one of the reactants is fixed 
this naturally gives rise to the logistic equation for the same reason as population growth the reaction is self reinforcing but constrained 
the logistic function was independently rediscovered as model of population growth in by raymond pearl and lowell reed published as pearl reed which led to its use in modern statistics 
they were initially unaware of verhulst work and presumably learned about it from gustave du pasquier but they gave him little credit and did not adopt his terminology 
verhulst priority was acknowledged and the term logistic revived by udny yule in and has been followed since 
pearl and reed first applied the model to the population of the united states and also initially fitted the curve by making it pass through three points as with verhulst this again yielded poor results in the the probit model was developed and systematized by chester ittner bliss who coined the term probit in bliss and by john gaddum in gaddum and the model fit by maximum likelihood estimation by ronald fisher in fisher as an addendum to bliss work 
the probit model was principally used in bioassay and had been preceded by earlier work dating to see probit model history 
the probit model influenced the subsequent development of the logit model and these models competed with each other the logistic model was likely first used as an alternative to the probit model in bioassay by edwin bidwell wilson and his student jane worcester in wilson worcester 
however the development of the logistic model as general alternative to the probit model was principally due to the work of joseph berkson over many decades beginning in berkson where he coined logit by analogy with probit and continuing through berkson and following years 
the logit model was initially dismissed as inferior to the probit model but gradually achieved an equal footing with the logit particularly between and by the logit model achieved parity with the probit model in use in statistics journals and thereafter surpassed it 
this relative popularity was due to the adoption of the logit outside of bioassay rather than displacing the probit within bioassay and its informal use in practice the logit popularity is credited to the logit model computational simplicity mathematical properties and generality allowing its use in varied fields various refinements occurred during that time notably by david cox as in cox the multinomial logit model was introduced independently in cox and thiel which greatly increased the scope of application and the popularity of the logit model 
in daniel mcfadden linked the multinomial logit to the theory of discrete choice specifically luce choice axiom showing that the multinomial logit followed from the assumption of independence of irrelevant alternatives and interpreting odds of alternatives as relative preferences this gave theoretical foundation for the logistic regression 
extensions there are large numbers of extensions multinomial logistic regression or multinomial logit handles the case of multi way categorical dependent variable with unordered values also called classification 
note that the general case of having dependent variables with more than two values is termed polytomous regression 
ordered logistic regression or ordered logit handles ordinal dependent variables ordered values 
mixed logit is an extension of multinomial logit that allows for correlations among the choices of the dependent variable 
an extension of the logistic model to sets of interdependent variables is the conditional random field 
conditional logistic regression handles matched or stratified data when the strata are small 
it is mostly used in the analysis of observational studies 
software most statistical software can do binary logistic regression 
spss for basic logistic regression 
stata sas proc logistic for basic logistic regression 
proc catmod when all the variables are categorical 
proc glimmix for multilevel model logistic regression 
glm in the stats package using family binomial lrm in the rms package glmnet package for an efficient implementation regularized logistic regression lmer for mixed effects logistic regression rfast package command gm logistic for fast and heavy calculations involving large scale data 
arm package for bayesian logistic regression python logit in the statsmodels module 
logisticregression in the scikit learn module 
logisticregressor in the tensorflow module 
full example of logistic regression in the theano tutorial bayesian logistic regression with ard prior code tutorial variational bayes logistic regression with ard prior code tutorial bayesian logistic regression code tutorial ncss logistic regression in ncss matlab mnrfit in the statistics and machine learning toolbox with incorrect coded as instead of fminunc fmincon fitglm mnrfit fitclinear mle can all do logistic regression 
java jvm liblinear apache flink apache spark sparkml supports logistic regression fpga logistic regresesion ip core in hls for fpga notably microsoft excel statistics extension package does not include it 
see also logistic function discrete choice jarrow turnbull model limited dependent variable multinomial logit model ordered logit hosmer lemeshow test brier score mlpack contains implementation of logistic regression local case control sampling logistic model tree references further reading external links media related to logistic regression at wikimedia commons econometrics lecture topic logit model on youtube by mark thoma logistic regression tutorial mlelr software in for teaching purposes
batch normalization also known as batch norm is method used to make training of artificial neural networks faster and more stable through normalization of the layers inputs by re centering and re scaling
it was proposed by sergey ioffe and christian szegedy in while the effect of batch normalization is evident the reasons behind its effectiveness remain under discussion
it was believed that it can mitigate the problem of internal covariate shift where parameter initialization and changes in the distribution of the inputs of each layer affect the learning rate of the network
recently some scholars have argued that batch normalization does not reduce internal covariate shift but rather smooths the objective function which in turn improves the performance
however at initialization batch normalization in fact induces severe gradient explosion in deep networks which is only alleviated by skip connections in residual networks
others maintain that batch normalization achieves length direction decoupling and thereby accelerates neural networks
more recently normalize gradient clipping technique and smart hyperparameter tuning has been introduced in normalizer free nets so called nf nets which mitigates the need for batch normalization
internal covariate shift each layer of neural network has inputs with corresponding distribution which is affected during the training process by the randomness in the parameter initialization and the randomness in the input data
the effect of these sources of randomness on the distribution of the inputs to internal layers during training is described as internal covariate shift
although clear cut precise definition seems to be missing the phenomenon observed in experiments is the change on means and variances of the inputs to internal layers during training
batch normalization was initially proposed to mitigate internal covariate shift
during the training stage of networks as the parameters of the preceding layers change the distribution of inputs to the current layer changes accordingly such that the current layer needs to constantly readjust to new distributions
this problem is especially severe for deep networks because small changes in shallower hidden layers will be amplified as they propagate within the network resulting in significant shift in deeper hidden layers
therefore the method of batch normalization is proposed to reduce these unwanted shifts to speed up training and to produce more reliable models
besides reducing internal covariate shift batch normalization is believed to introduce many other benefits
with this additional operation the network can use higher learning rate without vanishing or exploding gradients
furthermore batch normalization seems to have regularizing effect such that the network improves its generalization properties and it is thus unnecessary to use dropout to mitigate overfitting
it has been observed also that with batch norm the network becomes more robust to different initialization schemes and learning rates
procedures transformation in neural network batch normalization is achieved through normalization step that fixes the means and variances of each layer inputs
ideally the normalization would be conducted over the entire training set but to use this step jointly with stochastic optimization methods it is impractical to use the global information
thus normalization is restrained to each mini batch in the training process
let us use to denote mini batch of size of the entire training set
the empirical mean and variance of could thus be denoted as and for layer of the network with dimensional input
each dimension of its input is then normalized
re centered and re scaled separately where and and are the per dimension mean and standard deviation respectively
is added in the denominator for numerical stability and is an arbitrarily small constant
the resulting normalized activation have zero mean and unit variance if is not taken into account
to restore the representation power of the network transformation step then follows as where the parameters and are subsequently learned in the optimization process
formally the operation that implements batch normalization is transform called the batch normalizing transform
the output of the bn transform is then passed to other network layers while the normalized output remains internal to the current layer
backpropagation the described bn transform is differentiable operation and the gradient of the loss with respect to the different parameters can be computed directly with the chain rule
specifically depends on the choice of activation function and the gradient against other parameters could be expressed as function of and inference during the training stage the normalization steps depend on the mini batches to ensure efficient and reliable training
however in the inference stage this dependence is not useful any more
instead the normalization step in this stage is computed with the population statistics such that the output could depend on the input in deterministic manner
the population mean and variance var are computed as and var
the population statistics thus is complete representation of the mini batches
the bn transform in the inference step thus becomes inf var where is passed on to future layers instead of
since the parameters are fixed in this transformation the batch normalization procedure is essentially applying linear transform to the activation
theoretical understanding although batch normalization has become popular due to its strong empirical performance the working mechanism of the method is not yet well understood
the explanation made in the original paper was that batch norm works by reducing internal covariate shift but this has been challenged by more recent work
one experiment trained vgg network under different training regimes standard no batch norm batch norm and batch norm with noise added to each layer during training
in the third model the noise has non zero mean and non unit variance
it explicitly introduces covariate shift
despite this it showed similar accuracy to the second model and both performed better than the first suggesting that covariate shift is not the reason that batch norm improves performance
smoothness one alternative explanation is that the improvement with batch normalization is instead due to it producing smoother parameter space and smoother gradients as formalized by smaller lipschitz constant
consider two identical networks one contains batch normalization layers and the other doesn the behaviors of these two networks are then compared
denote the loss functions as and respectively
let the input to both networks be and the output be for which where is the layer weights
for the second network additionally goes through batch normalization layer
denote the normalized activation as which has zero mean and unit variance
let the transformed activation be and suppose and are constants
finally denote the standard deviation over mini batch as first it can be shown that the gradient magnitude of batch normalized network is bounded with the bound expressed as
since the gradient magnitude represents the lipschitzness of the loss this relationship indicates that batch normalized network could achieve greater lipschitzness comparatively
notice that the bound gets tighter when the gradient correlates with the activation which is common phenomena
the scaling of is also significant since the variance is often large
secondly the quadratic form of the loss hessian with respect to activation in the gradient direction can be bounded as the scaling of indicates that the loss hessian is resilient to the mini batch variance whereas the second term on the right hand side suggests that it becomes smoother when the hessian and the inner product are non negative
if the loss is locally convex then the hessian is positive semi definite while the inner product is positive if is in the direction towards the minimum of the loss
it could thus be concluded from this inequality that the gradient generally becomes more predictive with the batch normalization layer
it then follows to translate the bounds related to the loss with respect to the normalized activation to bound on the loss with respect to the network weights where and in addition to the smoother landscape it is further shown that batch normalization could result in better initialization with the following inequality where and are the local optimal weights for the two networks respectively
some scholars argue that the above analysis cannot fully capture the performance of batch normalization because the proof only concerns the largest eigenvalue or equivalently one direction in the landscape at all points
it is suggested that the complete eigenspectrum needs to be taken into account to make conclusive analysis
measure since it is hypothesized that batch normalization layers could reduce internal covariate shift an experiment is set up to measure quantitatively how much covariate shift is reduced
first the notion of internal covariate shift needs to be defined mathematically
specifically to quantify the adjustment that layer parameters make in response to updates in previous layers the correlation between the gradients of the loss before and after all previous layers are updated is measured since gradients could capture the shifts from the first order training method
if the shift introduced by the changes in previous layers is small then the correlation between the gradients would be close to the correlation between the gradients are computed for four models standard vgg network vgg network with batch normalization layers layer deep linear network dln trained with full batch gradient descent and dln network with batch normalization layers
interestingly it is shown that the standard vgg and dln models both have higher correlations of gradients compared with their counterparts indicating that the additional batch normalization layers are not reducing internal covariate shift
vanishing exploding gradients even though batchnorm was originally introduced to alleviate gradient vanishing or explosion problems deep batchnorm network in fact suffers from gradient explosion at initialization time no matter what it uses for nonlinearity
thus the optimization landscape is very far from smooth for randomly initialized deep batchnorm network
more precisely if the network has layers then the gradient of the first layer weights has norm for some depending only on the nonlinearity
for any fixed nonlinearity decreases as the batch size increases
for example for relu decreases to as the batch size tends to infinity
practically this means deep batchnorm networks are untrainable
this is only relieved by skip connections in the fashion of residual networks this gradient explosion on the surface contradicts the smoothness property explained in the previous section but in fact they are consistent
the previous section studies the effect of inserting single batchnorm in network while the gradient explosion depends on stacking batchnorms typical of modern deep neural networks
decoupling another possible reason for the success of batch normalization is that it decouples the length and direction of the weight vectors and thus facilitates better training
by interpreting batch norm as reparametrization of weight space it can be shown that the length and the direction of the weights are separated and can thus be trained separately
for particular neural network unit with input and weight vector denote its output as where is the activation function and denote
assume that and that the spectrum of the matrix is bounded as such that is symmetric positive definite
adding batch normalization to this unit thus results in by definition
the variance term can be simplified such that assume that has zero mean and can be omitted then it follows that where is the induced norm of hence it could be concluded that where and and accounts for its length and direction separately
this property could then be used to prove the faster convergence of problems with batch normalization
linear convergence least square problem with the reparametrization interpretation it could then be proved that applying batch normalization to the ordinary least squares problem achieves linear convergence rate in gradient descent which is faster than the regular gradient descent with only sub linear convergence
denote the objective of minimizing an ordinary least squares problem as where
since the objective thus becomes where is excluded to avoid in the denominator
since the objective is convex with respect to its optimal value could be calculated by setting the partial derivative of the objective against to the objective could be further simplified to be
note that this objective is form of the generalized rayleigh quotient where is symmetric matrix and is symmetric positive definite matrix
it is proven that the gradient descent convergence rate of the generalized rayleigh quotient is where is the largest eigenvalue of is the second largest eigenvalue of and is the smallest eigenvalue of in our case is rank one matrix and the convergence result can be simplified accordingly
specifically consider gradient descent steps of the form with step size and starting from then
learning halfspace problem the problem of learning halfspaces refers to the training of the perceptron which is the simplest form of neural network
the optimization problem in this case is where and is an arbitrary loss function
suppose that is infinitely differentiable and has bounded derivative
assume that the objective function is smooth and that solution exists and is bounded such that
also assume is multivariate normal random variable
with the gaussian assumption it can be shown that all critical points lie on the same line for any choice of loss function specifically the gradient of could be represented as where and is the th derivative of by setting the gradient to it thus follows that the bounded critical points can be expressed as where depends on and combining this global property with length direction decoupling it could thus be proved that this optimization problem converges linearly
first variation of gradient descent with batch normalization gradient descent in normalized parameterization gdnp is designed for the objective function such that the direction and length of the weights are updated separately
denote the stopping criterion of gdnp as let the step size be
for each step if then update the direction as
then update the length according to where is the classical bisection algorithm and is the total iterations ran in the bisection step
denote the total number of iterations as then the final output of gdnp is the gdnp algorithm thus slightly modifies the batch normalization step for the ease of mathematical analysis
it can be shown that in gdnp the partial derivative of against the length component converges to zero at linear rate such that where and are the two starting points of the bisection algorithm on the left and on the right correspondingly
further for each iteration the norm of the gradient of with respect to converges linearly such that
combining these two inequalities bound could thus be obtained for the gradient with respect to such that the algorithm is guaranteed to converge linearly
although the proof stands on the assumption of gaussian input it is also shown in experiments that gdnp could accelerate optimization without this constraint
neural networks consider multilayer perceptron mlp with one hidden layer and hidden units with mapping from input to scalar output described as where and are the input and output weights of unit correspondingly and is the activation function and is assumed to be tanh function
the input and output weights could then be optimized with where is loss function
consider fixed and optimizing only it can be shown that the critical points of of particular hidden unit all align along one line depending on incoming information into the hidden layer such that where is scalar
this result could be proved by setting the gradient of to zero and solving the system of equations
apply the gdnp algorithm to this optimization problem by alternating optimization over the different hidden units
specifically for each hidden unit run gdnp to find the optimal and with the same choice of stopping criterion and stepsize it follows that since the parameters of each hidden unit converge linearly the whole optimization problem has linear rate of convergence
references ioffe sergey szegedy christian
batch normalization accelerating deep network training by reducing internal covariate shift icml proceedings of the nd international conference on international conference on machine learning volume july pages simonyan karen zisserman andrew
very deep convolutional networks for large scale image recognition
linear combination of atomic orbitals or lcao is quantum superposition of atomic orbitals and technique for calculating molecular orbitals in quantum chemistry 
in quantum mechanics electron configurations of atoms are described as wavefunctions 
in mathematical sense these wave functions are the basis set of functions the basis functions which describe the electrons of given atom 
in chemical reactions orbital wavefunctions are modified 
the electron cloud shape is changed according to the type of atoms participating in the chemical bond 
it was introduced in by sir john lennard jones with the description of bonding in the diatomic molecules of the first main row of the periodic table but had been used earlier by linus pauling for 
mathematical description an initial assumption is that the number of molecular orbitals is equal to the number of atomic orbitals included in the linear expansion 
in sense atomic orbitals combine to form molecular orbitals which can be numbered to and which may not all be the same 
the expression linear expansion for the th molecular orbital would be or where is molecular orbital represented as the sum of atomic orbitals each multiplied by corresponding coefficient and numbered to represents which atomic orbital is combined in the term 
the coefficients are the weights of the contributions of the atomic orbitals to the molecular orbital 
the hartree fock method is used to obtain the coefficients of the expansion 
the orbitals are thus expressed as linear combinations of basis functions and the basis functions are single electron functions which may or may not be centered on the nuclei of the component atoms of the molecule 
in either case the basis functions are usually also referred to as atomic orbitals even though only in the former case this name seems to be adequate 
the atomic orbitals used are typically those of hydrogen like atoms since these are known analytically 
slater type orbitals but other choices are possible such as the gaussian functions from standard basis sets or the pseudo atomic orbitals from plane wave pseudopotentials 
by minimizing the total energy of the system an appropriate set of coefficients of the linear combinations is determined 
this quantitative approach is now known as the hartree fock method 
however since the development of computational chemistry the lcao method often refers not to an actual optimization of the wave function but to qualitative discussion which is very useful for predicting and rationalizing results obtained via more modern methods 
in this case the shape of the molecular orbitals and their respective energies are deduced approximately from comparing the energies of the atomic orbitals of the individual atoms or molecular fragments and applying some recipes known as level repulsion and the like 
the graphs that are plotted to make this discussion clearer are called correlation diagrams 
the required atomic orbital energies can come from calculations or directly from experiment via koopmans theorem 
this is done by using the symmetry of the molecules and orbitals involved in bonding and thus is sometimes called symmetry adapted linear combination salc 
the first step in this process is assigning point group to the molecule 
each operation in the point group is performed upon the molecule 
the number of bonds that are unmoved is the character of that operation 
this reducible representation is decomposed into the sum of irreducible representations 
these irreducible representations correspond to the symmetry of the orbitals involved 
molecular orbital diagrams provide simple qualitative lcao treatment 
the ckel method the extended ckel method and the pariser parr pople method provide some quantitative theories 
see also quantum chemistry computer programs hartree fock method basis set chemistry tight binding holstein herring method external links lcao chemistry umeche maine edu link references
in cryptography related key attack is any form of cryptanalysis where the attacker can observe the operation of cipher under several different keys whose values are initially unknown but where some mathematical relationship connecting the keys is known to the attacker 
for example the attacker might know that the last bits of the keys are always the same even though they don know at first what the bits are 
this appears at first glance to be an unrealistic model it would certainly be unlikely that an attacker could persuade human cryptographer to encrypt plaintexts under numerous secret keys related in some way 
kasumi kasumi is an eight round bit block cipher with bit key 
it is based upon misty and was designed to form the basis of the confidentiality and integrity algorithms 
mark blunden and adrian escott described differential related key attacks on five and six rounds of kasumi 
differential attacks were introduced by biham and shamir 
related key attacks were first introduced by biham 
differential related key attacks are discussed in kelsey et al 
wep an important example of cryptographic protocol that failed because of related key attack is wired equivalent privacy wep used in wi fi wireless networks 
each client wi fi network adapter and wireless access point in wep protected network shares the same wep key 
encryption uses the rc algorithm stream cipher 
it is essential that the same key never be used twice with stream cipher 
to prevent this from happening wep includes bit initialization vector iv in each message packet 
the rc key for that packet is the iv concatenated with the wep key 
wep keys have to be changed manually and this typically happens infrequently 
an attacker therefore can assume that all the keys used to encrypt packets share single wep key 
this fact opened up wep to series of attacks which proved devastating 
the simplest to understand uses the fact that the bit iv only allows little under million possibilities 
because of the birthday paradox it is likely that for every packets two will share the same iv and hence the same rc key allowing the packets to be attacked 
more devastating attacks take advantage of certain weak keys in rc and eventually allow the wep key itself to be recovered 
in agents from the federal bureau of investigation publicly demonstrated the ability to do this with widely available software tools in about three minutes 
preventing related key attacks one approach to preventing related key attacks is to design protocols and applications so that encryption keys will never have simple relationship with each other 
for example each encryption key can be generated from the underlying key material using key derivation function 
for example replacement for wep wi fi protected access wpa uses three levels of keys master key working key and rc key 
the master wpa key is shared with each client and access point and is used in protocol called temporal key integrity protocol tkip to create new working keys frequently enough to thwart known attack methods 
the working keys are then combined with longer bit iv to form the rc key for each packet 
this design mimics the wep approach enough to allow wpa to be used with first generation wi fi network cards some of which implemented portions of wep in hardware 
however not all first generation access points can run wpa 
another more conservative approach is to employ cipher designed to prevent related key attacks altogether usually by incorporating strong key schedule 
newer version of wi fi protected access wpa uses the aes block cipher instead of rc in part for this reason 
there are related key attacks against aes but unlike those against rc they re far from practical to implement and wpa key generation functions may provide some security against them 
many older network cards cannot run wpa
in statistics ordinary least squares ols is type of linear least squares method for choosing the unknown parameters in linear regression model with fixed level one effects of linear function of set of explanatory variables by the principle of least squares minimizing the sum of the squares of the differences between the observed dependent variable values of the variable being observed in the input dataset and the output of the linear function of the independent variable 
geometrically this is seen as the sum of the squared distances parallel to the axis of the dependent variable between each data point in the set and the corresponding point on the regression surface the smaller the differences the better the model fits the data 
the resulting estimator can be expressed by simple formula especially in the case of simple linear regression in which there is single regressor on the right side of the regression equation 
the ols estimator is consistent for the level one fixed effects when the regressors are exogenous and forms perfect colinearity rank condition consistent for the variance estimate of the residuals when regressors have finite fourth moments and by the gauss markov theorem optimal in the class of linear unbiased estimators when the errors are homoscedastic and serially uncorrelated 
under these conditions the method of ols provides minimum variance mean unbiased estimation when the errors have finite variances 
under the additional assumption that the errors are normally distributed with zero mean ols is the maximum likelihood estimator that outperforms any non linear unbiased estimator 
linear model suppose the data consists of observations each observation includes scalar response and column vector of parameters regressors in linear regression model the response variable is linear function of the regressors or in vector form where as introduced previously is column vector of the th observation of all the explanatory variables is vector of unknown parameters and the scalar represents unobserved random variables errors of the th observation 
accounts for the influences upon the responses from sources other than the explanators this model can also be written in matrix notation as where and are vectors of the response variables and the errors of the observations and is an matrix of regressors also sometimes called the design matrix whose row is and contains the th observations on all the explanatory variables 
as rule the constant term is always included in the set of regressors say by taking for all the coefficient corresponding to this regressor is called the intercept 
regressors do not have to be independent there can be any desired relationship between the regressors so long as it is not linear relationship 
for instance we might suspect the response depends linearly both on value and its square in which case we would include one regressor whose value is just the square of another regressor 
in that case the model would be quadratic in the second regressor but none the less is still considered linear model because the model is still linear in the parameters 
matrix vector formulation consider an overdetermined system of linear equations in unknown coefficients with note for linear model as above not all elements in contains information on the data points 
the first column is populated with ones only the other columns contain actual data 
so here is equal to the number of regressors plus one 
this can be written in matrix form as where 
such system usually has no exact solution so the goal is instead to find the coefficients which fit the equations best in the sense of solving the quadratic minimization problem where the objective function is given by justification for choosing this criterion is given in properties below 
this minimization problem has unique solution provided that the columns of the matrix are linearly independent given by solving the so called normal equations the matrix is known as the normal matrix or gram matrix and the matrix is known as the moment matrix of regressand by regressors 
finally is the coefficient vector of the least squares hyperplane expressed as or 
estimation suppose is candidate value for the parameter vector the quantity yi xitb called the residual for the th observation measures the vertical distance between the data point xi yi and the hyperplane xtb and thus assesses the degree of fit between the actual data and the model 
the sum of squared residuals ssr also called the error sum of squares ess or residual sum of squares rss is measure of the overall model fit where denotes the matrix transpose and the rows of denoting the values of all the independent variables associated with particular value of the dependent variable are xi xit 
the value of which minimizes this sum is called the ols estimator for the function is quadratic in with positive definite hessian and therefore this function possesses unique global minimum at which can be given by the explicit formula proof argmin the product xt is gram matrix and its inverse is the cofactor matrix of closely related to its covariance matrix 
the matrix xt xt xt is called the moore penrose pseudoinverse matrix of this formulation highlights the point that estimation can be carried out if and only if there is no perfect multicollinearity between the explanatory variables which would cause the gram matrix to have no inverse 
after we have estimated the fitted values or predicted values from the regression will be where xtx xt is the projection matrix onto the space spanned by the columns of this matrix is also sometimes called the hat matrix because it puts hat onto the variable another matrix closely related to is the annihilator matrix in this is projection matrix onto the space orthogonal to both matrices and are symmetric and idempotent meaning that and and relate to the data matrix via identities px and mx matrix creates the residuals from the regression using these residuals we can estimate the value of using the reduced chi squared statistic the denominator is the statistical degrees of freedom 
the first quantity is the ols estimate for whereas the second is the mle estimate for 
the two estimators are quite similar in large samples the first estimator is always unbiased while the second estimator is biased but has smaller mean squared error 
in practice is used more often since it is more convenient for the hypothesis testing 
the square root of is called the regression standard error standard error of the regression or standard error of the equation it is common to assess the goodness of fit of the ols regression by comparing how much the initial variation in the sample can be reduced by regressing onto the coefficient of determination is defined as ratio of explained variance to the total variance of the dependent variable in the cases where the regression sum of squares equals the sum of squares of residuals where tss is the total sum of squares for the dependent variable and is an matrix of ones 
is centering matrix which is equivalent to regression on constant it simply subtracts the mean from variable 
in order for to be meaningful the matrix of data on regressors must contain column vector of ones to represent the constant whose coefficient is the regression intercept 
in that case will always be number between and with values close to indicating good degree of fit 
the variance in the prediction of the independent variable as function of the dependent variable is given in the article polynomial least squares 
simple linear regression model if the data matrix contains only two variables constant and scalar regressor xi then this is called the simple regression model 
this case is often considered in the beginner statistics classes as it provides much simpler formulas even suitable for manual calculation 
the parameters are commonly denoted as the least squares estimates in this case are given by simple formulas alternative derivations in the previous section the least squares estimator was obtained as value that minimizes the sum of squared residuals of the model 
however it is also possible to derive the same estimator from other approaches 
in all cases the formula for ols estimator remains the same xtx xty the only difference is in how we interpret this result 
projection for mathematicians ols is an approximate solution to an overdetermined system of linear equations where is the unknown 
assuming the system cannot be solved exactly the number of equations is much larger than the number of unknowns we are looking for solution that could provide the smallest discrepancy between the right and left hand sides 
in other words we are looking for the solution that satisfies min where is the standard norm in the dimensional euclidean space rn 
the predicted quantity is just certain linear combination of the vectors of regressors 
thus the residual vector will have the smallest length when is projected orthogonally onto the linear subspace spanned by the columns of the ols estimator in this case can be interpreted as the coefficients of vector decomposition of py along the basis of in other words the gradient equations at the minimum can be written as geometrical interpretation of these equations is that the vector of residuals is orthogonal to the column space of since the dot product is equal to zero for any conformal vector this means that is the shortest of all possible vectors that is the variance of the residuals is the minimum possible 
this is illustrated at the right 
introducing and matrix with the assumption that matrix is non singular and kt cf 
orthogonal projections the residual vector should satisfy the following equation 
the equation and solution of linear least squares are thus described as follows another way of looking at it is to consider the regression line to be weighted average of the lines passing through the combination of any two points in the dataset 
although this way of calculation is more computationally expensive it provides better intuition on ols 
maximum likelihood the ols estimator is identical to the maximum likelihood estimator mle under the normality assumption for the error terms 
proof this normality assumption has historical importance as it provided the basis for the early work in linear regression analysis by yule and pearson 
from the properties of mle we can infer that the ols estimator is asymptotically efficient in the sense of attaining the cram rao bound for variance if the normality assumption is satisfied 
generalized method of moments in iid case the ols estimator can also be viewed as gmm estimator arising from the moment conditions these moment conditions state that the regressors should be uncorrelated with the errors 
since xi is vector the number of moment conditions is equal to the dimension of the parameter vector and thus the system is exactly identified 
this is the so called classical gmm case when the estimator does not depend on the choice of the weighting matrix 
note that the original strict exogeneity assumption xi implies far richer set of moment conditions than stated above 
in particular this assumption implies that for any vector function the moment condition xi will hold 
however it can be shown using the gauss markov theorem that the optimal choice of function is to take which results in the moment equation posted above 
properties assumptions there are several different frameworks in which the linear regression model can be cast in order to make the ols technique applicable 
each of these settings produces the same formulas and same results 
the only difference is the interpretation and the assumptions which have to be imposed in order for the method to give meaningful results 
the choice of the applicable framework depends mostly on the nature of data in hand and on the inference task which has to be performed 
one of the lines of difference in interpretation is whether to treat the regressors as random variables or as predefined constants 
in the first case random design the regressors xi are random and sampled together with the yi from some population as in an observational study 
this approach allows for more natural study of the asymptotic properties of the estimators 
in the other interpretation fixed design the regressors are treated as known constants set by design and is sampled conditionally on the values of as in an experiment 
for practical purposes this distinction is often unimportant since estimation and inference is carried out while conditioning on all results stated in this article are within the random design framework 
classical linear regression model the classical model focuses on the finite sample estimation and inference meaning that the number of observations is fixed 
this contrasts with the other approaches which study the asymptotic behavior of ols and in which the number of observations is allowed to grow to infinity 
the linear functional form must coincide with the form of the actual data generating process 
the errors in the regression should have conditional mean zero the immediate consequence of the exogeneity assumption is that the errors have mean zero and that the regressors are uncorrelated with the errors xt the exogeneity assumption is critical for the ols theory 
if it holds then the regressor variables are called exogenous 
if it doesn then those regressors that are correlated with the error term are called endogenous and the ols estimator becomes biased 
in such case the method of instrumental variables may be used to carry out inference no linear dependence 
the regressors in must all be linearly independent 
mathematically this means that the matrix must have full column rank almost surely pr rank usually it is also assumed that the regressors have finite moments up to at least the second moment 
then the matrix qxx xtx is finite and positive semi definite 
when this assumption is violated the regressors are called linearly dependent or perfectly multicollinear 
in such case the value of the regression coefficient cannot be learned although prediction of values is still possible for new values of the regressors that lie in the same linearly dependent subspace spherical errors var where in is the identity matrix in dimension and is parameter which determines the variance of each observation 
this is considered nuisance parameter in the model although usually it is also estimated 
if this assumption is violated then the ols estimates are still valid but no longer efficient 
it is customary to split this assumption into two parts homoscedasticity which means that the error term has the same variance in each observation 
when this requirement is violated this is called heteroscedasticity in such case more efficient estimator would be weighted least squares 
if the errors have infinite variance then the ols estimates will also have infinite variance although by the law of large numbers they will nonetheless tend toward the true values so long as the errors have zero mean 
in this case robust estimation techniques are recommended 
no autocorrelation the errors are uncorrelated between observations for this assumption may be violated in the context of time series data panel data cluster samples hierarchical data repeated measures data longitudinal data and other data with dependencies 
in such cases generalized least squares provides better alternative than the ols 
another expression for autocorrelation is serial correlation normality 
it is sometimes additionally assumed that the errors have normal distribution conditional on the regressors 
this assumption is not needed for the validity of the ols method although certain additional finite sample properties can be established in case when it does especially in the area of hypotheses testing 
also when the errors are normal the ols estimator is equivalent to the maximum likelihood estimator mle and therefore it is asymptotically efficient in the class of all regular estimators 
importantly the normality assumption applies only to the error terms contrary to popular misconception the response dependent variable is not required to be normally distributed 
independent and identically distributed iid in some applications especially with cross sectional data an additional assumption is imposed that all observations are independent and identically distributed 
this means that all observations are taken from random sample which makes all the assumptions listed earlier simpler and easier to interpret 
also this framework allows one to state asymptotic results as the sample size which are understood as theoretical possibility of fetching new independent observations from the data generating process 
the list of assumptions in this case is iid observations xi yi is independent from and has the same distribution as xj yj for all no perfect multicollinearity qxx xi xit is positive definite matrix exogeneity xi homoscedasticity var xi 
time series model the stochastic process is co integrating 
the regressors are predetermined xi for all the matrix qxx xi xit is of full rank and hence positive definite is martingale difference sequence with finite matrix of second moments qxx xi xit 
finite sample properties first of all under the strict exogeneity assumption the ols estimators and are unbiased meaning that their expected values coincide with the true values of the parameters proof if the strict exogeneity does not hold as is the case with many time series models where exogeneity is assumed only with respect to the past shocks but not the future ones then these estimators will be biased in finite samples 
the variance covariance matrix or simply covariance matrix of is equal to var in particular the standard error of each coefficient is equal to square root of the th diagonal element of this matrix 
the estimate of this standard error is obtained by replacing the unknown quantity with its estimate 
thus it can also be easily shown that the estimator is uncorrelated with the residuals from the model cov the gauss markov theorem states that under the spherical errors assumption that is the errors should be uncorrelated and homoscedastic the estimator is efficient in the class of linear unbiased estimators 
this is called the best linear unbiased estimator blue 
efficiency should be understood as if we were to find some other estimator which would be linear in and unbiased then var var in the sense that this is nonnegative definite matrix 
this theorem establishes optimality only in the class of linear unbiased estimators which is quite restrictive 
depending on the distribution of the error terms other non linear estimators may provide better results than ols 
assuming normality the properties listed so far are all valid regardless of the underlying distribution of the error terms 
however if you are willing to assume that the normality assumption holds that is that in then additional properties of the ols estimators can be stated 
the estimator is normally distributed with mean and variance as given before 
this estimator reaches the cram rao bound for the model and thus is optimal in the class of all unbiased estimators 
note that unlike the gauss markov theorem this result establishes optimality among both linear and non linear estimators but only in the case of normally distributed error terms 
the estimator will be proportional to the chi squared distribution the variance of this estimator is equal to which does not attain the cram rao bound of 
however it was shown that there are no unbiased estimators of with variance smaller than that of the estimator 
if we are willing to allow biased estimators and consider the class of estimators that are proportional to the sum of squared residuals ssr of the model then the best in the sense of the mean squared error estimator in this class will be ssr which even beats the cram rao bound in case when there is only one regressor moreover the estimators and are independent the fact which comes in useful when constructing the and tests for the regression 
influential observations as was mentioned before the estimator is linear in meaning that it represents linear combination of the dependent variables yi 
the weights in this linear combination are functions of the regressors and generally are unequal 
the observations with high weights are called influential because they have more pronounced effect on the value of the estimator 
to analyze which observations are influential we remove specific th observation and consider how much the estimated quantities are going to change similarly to the jackknife method 
it can be shown that the change in the ols estimator for will be equal to where hj xjt xtx xj is the th diagonal element of the hat matrix and xj is the vector of regressors corresponding to the th observation 
similarly the change in the predicted value for th observation resulting from omitting that observation from the dataset will be equal to from the properties of the hat matrix hj and they sum up to so that on average hj 
these quantities hj are called the leverages and observations with high hj are called leverage points 
usually the observations with high leverage ought to be scrutinized more carefully in case they are erroneous or outliers or in some other way atypical of the rest of the dataset 
partitioned regression sometimes the variables and corresponding parameters in the regression can be logically split into two groups so that the regression takes form where and have dimensions and are and vectors with the frisch waugh lovell theorem states that in this regression the residuals and the ols estimate will be numerically identical to the residuals and the ols estimate for in the following regression where is the annihilator matrix for regressors 
the theorem can be used to establish number of theoretical results 
for example having regression with constant and another regressor is equivalent to subtracting the means from the dependent variable and the regressor and then running the regression for the de meaned variables but without the constant term 
constrained estimation suppose it is known that the coefficients in the regression satisfy system of linear equations where is matrix of full rank and is vector of known constants where in this case least squares estimation is equivalent to minimizing the sum of squared residuals of the model subject to the constraint the constrained least squares cls estimator can be given by an explicit formula 
this expression for the constrained estimator is valid as long as the matrix xtx is invertible 
it was assumed from the beginning of this article that this matrix is of full rank and it was noted that when the rank condition fails will not be identifiable 
however it may happen that adding the restriction makes identifiable in which case one would like to find the formula for the estimator 
the estimator is equal to where is matrix such that the matrix is non singular and rtq such matrix can always be found although generally it is not unique 
the second formula coincides with the first in case when xtx is invertible 
large sample properties the least squares estimators are point estimates of the linear regression model parameters however generally we also want to know how close those estimates might be to the true values of parameters 
in other words we want to construct the interval estimates 
since we haven made any assumption about the distribution of error term it is impossible to infer the distribution of the estimators and nevertheless we can apply the central limit theorem to derive their asymptotic properties as sample size goes to infinity 
while the sample size is necessarily finite it is customary to assume that is large enough so that the true distribution of the ols estimator is close to its asymptotic limit 
we can show that under the model assumptions the least squares estimator for is consistent that is converges in probability to and asymptotically normal proof where 
intervals using this asymptotic distribution approximate two sided confidence intervals for the th component of the vector can be constructed as at the confidence level where denotes the quantile function of standard normal distribution and jj is the th diagonal element of matrix 
similarly the least squares estimator for is also consistent and asymptotically normal provided that the fourth moment of exists with limiting distribution 
these asymptotic distributions can be used for prediction testing hypotheses constructing other estimators etc as an example consider the problem of prediction 
suppose is some point within the domain of distribution of the regressors and one wants to know what the response variable would have been at that point 
the mean response is the quantity whereas the predicted response is 
clearly the predicted response is random variable its distribution can be derived from that of which allows construct confidence intervals for mean response to be constructed at the confidence level 
hypothesis testing two hypothesis tests are particularly widely used 
first one wants to know if the estimated regression equation is any better than simply predicting that all values of the response variable equal its sample mean if not it is said to have no explanatory power 
the null hypothesis of no explanatory value of the estimated regression is tested using an test 
if the calculated value is found to be large enough to exceed its critical value for the pre chosen level of significance the null hypothesis is rejected and the alternative hypothesis that the regression has explanatory power is accepted 
otherwise the null hypothesis of no explanatory power is accepted 
second for each explanatory variable of interest one wants to know whether its estimated coefficient differs significantly from zero that is whether this particular explanatory variable in fact has explanatory power in predicting the response variable 
here the null hypothesis is that the true coefficient is zero 
this hypothesis is tested by computing the coefficient statistic as the ratio of the coefficient estimate to its standard error 
if the statistic is larger than predetermined value the null hypothesis is rejected and the variable is found to have explanatory power with its coefficient significantly different from zero 
otherwise the null hypothesis of zero value of the true coefficient is accepted 
in addition the chow test is used to test whether two subsamples both have the same underlying true coefficient values 
the sum of squared residuals of regressions on each of the subsets and on the combined data set are compared by computing an statistic if this exceeds critical value the null hypothesis of no difference between the two subsets is rejected otherwise it is accepted 
example with real data the following data set gives average heights and weights for american women aged source the world almanac and book of facts 
when only one dependent variable is being modeled scatterplot will suggest the form and strength of the relationship between the dependent variable and regressors 
it might also reveal outliers heteroscedasticity and other aspects of the data that may complicate the interpretation of fitted regression model 
the scatterplot suggests that the relationship is strong and can be approximated as quadratic function 
ols can handle non linear relationships by introducing the regressor height 
the regression model then becomes multiple linear model the output from most popular statistical packages will look similar to this in this table the value column gives the least squares estimates of parameters the std error column shows standard errors of each coefficient estimate the statistic and value columns are testing whether any of the coefficients might be equal to zero 
the statistic is calculated simply as if the errors follow normal distribution follows student distribution 
under weaker conditions is asymptotically normal 
large values of indicate that the null hypothesis can be rejected and that the corresponding coefficient is not zero 
the second column value expresses the results of the hypothesis test as significance level 
conventionally values smaller than are taken as evidence that the population coefficient is nonzero 
squared is the coefficient of determination indicating goodness of fit of the regression 
this statistic will be equal to one if fit is perfect and to zero when regressors have no explanatory power whatsoever 
this is biased estimate of the population squared and will never decrease if additional regressors are added even if they are irrelevant 
adjusted squared is slightly modified version of designed to penalize for the excess number of regressors which do not add to the explanatory power of the regression 
this statistic is always smaller than can decrease as new regressors are added and even be negative for poorly fitting models log likelihood is calculated under the assumption that errors follow normal distribution 
even though the assumption is not very reasonable this statistic may still find its use in conducting lr tests 
durbin watson statistic tests whether there is any evidence of serial correlation between the residuals 
as rule of thumb the value smaller than will be an evidence of positive correlation 
akaike information criterion and schwarz criterion are both used for model selection 
generally when comparing two alternative models smaller values of one of these criteria will indicate better model 
standard error of regression is an estimate of standard error of the error term 
total sum of squares model sum of squared and residual sum of squares tell us how much of the initial variation in the sample were explained by the regression 
statistic tries to test the hypothesis that all coefficients except the intercept are equal to zero 
this statistic has distribution under the null hypothesis and normality assumption and its value indicates probability that the hypothesis is indeed true 
note that when errors are not normal this statistic becomes invalid and other tests such as wald test or lr test should be used 
ordinary least squares analysis often includes the use of diagnostic plots designed to detect departures of the data from the assumed form of the model 
these are some of the common diagnostic plots residuals against the explanatory variables in the model 
non linear relation between these variables suggests that the linearity of the conditional mean function may not hold 
different levels of variability in the residuals for different levels of the explanatory variables suggests possible heteroscedasticity 
residuals against explanatory variables not in the model 
any relation of the residuals to these variables would suggest considering these variables for inclusion in the model 
residuals against the fitted values 
residuals against the preceding residual 
this plot may identify serial correlations in the residuals an important consideration when carrying out statistical inference using regression models is how the data were sampled 
in this example the data are averages rather than measurements on individual women 
the fit of the model is very good but this does not imply that the weight of an individual woman can be predicted with high accuracy based only on her height 
sensitivity to rounding this example also demonstrates that coefficients determined by these calculations are sensitive to how the data is prepared 
the heights were originally given rounded to the nearest inch and have been converted and rounded to the nearest centimetre 
since the conversion factor is one inch to cm this is not an exact conversion 
the original inches can be recovered by round and then re converted to metric without rounding 
if this is done the results become using either of these equations to predict the weight of woman gives similar values kg with rounding vs kg without rounding 
thus seemingly small variation in the data has real effect on the coefficients but small effect on the results of the equation 
while this may look innocuous in the middle of the data range it could become significant at the extremes or in the case where the fitted model is used to project outside the data range extrapolation 
this highlights common error this example is an abuse of ols which inherently requires that the errors in the independent variable in this case height are zero or at least negligible 
the initial rounding to nearest inch plus any actual measurement errors constitute finite and non negligible error 
as result the fitted parameters are not the best estimates they are presumed to be 
though not totally spurious the error in the estimation will depend upon relative size of the and errors 
another example with less real data problem statement we can use the least square mechanism to figure out the equation of two body orbit in polar base co ordinates 
the equation typically used is cos where is the radius of how far the object is from one of the bodies 
in the equation the parameters and are used to determine the path of the orbit 
we have measured the following data 
we need to find the least squares approximation of and for the given data 
solution first we need to represent and in linear form 
so we are going to rewrite the equation as cos 
now we can use this form to represent our observational data as where is and is and is constructed by the first column being the coefficient of and the second column being the coefficient of and is the values for the respective so and 
on solving we get so and see also bayesian least squares fama macbeth regression nonlinear least squares numerical methods for linear least squares nonlinear system identification references further reading dougherty christopher 
introduction to econometrics nd ed 
new york oxford university press 
isbn gujarati damodar porter dawn 
basic econometics fifth ed 
boston mcgraw hill irwin 
isbn heij christiaan boer paul franses philip kloek teun van dijk herman 
econometric methods with applications in business and economics st ed 
oxford oxford university press 
isbn hill carter griffiths william lim guay 
principles of econometrics rd ed 
hoboken nj john wiley sons 
the simple regression model 
introductory econometrics modern approach th ed 
mason oh cengage learning
virginity is the state of person who has never engaged in sexual intercourse
the term virgin originally only referred to sexually inexperienced women but has evolved to encompass range of definitions as found in traditional modern and ethical concepts
heterosexual individuals may or may not consider loss of virginity to occur only through penile vaginal penetration while people of other sexual orientations often include oral sex anal sex or mutual masturbation in their definitions of losing one virginity there are cultural and religious traditions that place special value and significance on this state predominantly towards unmarried females associated with notions of personal purity honor and worth
like chastity the concept of virginity has traditionally involved sexual abstinence
the concept of virginity usually involves moral or religious issues and can have consequences in terms of social status and in interpersonal relationships
although virginity has social implications and had significant legal implications in some societies in the past it has no legal consequences in most societies today
the social implications of virginity still remain in many societies and can have varying effects on an individual social agency
etymology the word virgin comes via old french virgine from the root form of latin virgo genitive virginis meaning literally maiden or virgin sexually intact young woman or sexually inexperienced woman
as in latin the english word is also often used with wider reference by relaxing the age gender or sexual criteria
in this case more mature women can be virgins the virgin queen men can be virgins and potential initiates into many fields can be colloquially termed virgins for example skydiving virgin
in the latter usage virgin means uninitiated
the latin word likely arose by analogy with suit of lexemes based on vireo meaning to be green fresh or flourishing mostly with botanic reference in particular virga meaning strip of wood the first known use of virgin in english is found in middle english manuscript held at trinity college cambridge of about ar haue martirs and confessors and uirgines maked faier bode inne to women
in this and many later contexts the reference is specifically christian alluding to members of the ordo virginum order of virgins which applies to the consecrated virgins known to have existed since the early church from the writings of the church fathers by about the word was expanded to apply also to mary the mother of jesus hence to sexual virginity explicitly conceiud hali gast born virgine marie
further expansion of the word to include virtuous or na ve young women irrespective of religious connection occurred over about another century until by about we find voide vacand of vices as virgyns it ware
these are three of the eighteen definitions of virgin from the first edition of the oxford english dictionary oed pages
most of the oed definitions however are similar
the german word for virgin is jungfrau
jungfrau literally means young woman but is not used in this sense anymore
instead junge frau can be used
jungfrau is the word reserved specifically for sexual inexperience
as frau means woman it suggests female referent
unlike english german also has specific word for male virgin ngling youngling
it is however dated and rarely used
jungfrau with some masculine modifier is more typical as evidenced by the film the year old virgin about year old male virgin titled in german jungfrau nnlich sucht
german also distinguishes between young women and girls who are denoted by the word dchen
the english cognate maid was often used to imply virginity especially in poetry
maid marian the love interest of the legendary outlaw robin hood in english folklore
german is not the only language to have specific name for male virginity in french male virgins are called puceau
the greek word for virgin is parthenos see parthenon
although typically applied to women like english it is also applied to men in both cases specifically denoting absence of sexual experience
when used of men it does not carry strong association of never married status
however in reference to women historically it was sometimes used to refer to an engaged woman parthenos autou his virgin his fianc as opposed to gun autou his woman his wife
this distinction is necessary due to there being no specific word for wife or husband in greek
by extension from its primary sense the idea that virgin has sexual blank slate unchanged by any past intimate connection or experience can imply that the person is of unadulterated purity
culture concept the concept of virginity has significance only in particular social cultural or moral context
according to hanne blank virginity reflects no known biological imperative and grants no demonstrable evolutionary advantage
medieval bestiaries stated that the only way to capture or tame unicorn was by way of using virgin as lure due to her implied purity
the topic is popular in renaissance paintings
although virginity has historically been correlated with purity and worth many feminist scholars believe that virginity itself is myth
they argue that no standardized medical definition of virginity exists that there is no scientifically verifiable proof of virginity loss and that sexual intercourse results in no change in personality
jessica valenti feminist writer and author of the purity myth reasons that the concept of virginity is also dubious because of the many individual definitions of virginity loss and that valuing virginity has placed woman morality between her legs
she critiques the notion that sexual activity has any influence on morality or ethics the urge of wanting one spouse or partner to have never engaged in sexual activities is called virgin complex
person may also have virgin complex directed towards oneself
definitions of virginity loss there are varying understandings as to which types of sexual activities result in loss of virginity
the traditional view is that virginity is only lost through vaginal penetration by the penis consensual or non consensual and that acts of oral sex anal sex mutual masturbation or other forms of non penetrative sex do not result in loss of virginity
person who engages in such acts without having engaged in vaginal intercourse is often regarded among heterosexuals and researchers as technically virgin
by contrast gay or lesbian individuals often describe such acts as resulting in loss of virginity
some gay males regard penile anal penetration as resulting in loss of virginity but not oral sex or non penetrative sex and lesbians may regard oral sex or fingering as loss of virginity
some lesbians who debate the traditional definition consider whether or not non penile forms of vaginal penetration constitute virginity loss while other gay men and lesbians assert that the term virginity is meaningless to them because of the prevalence of the traditional definition whether person can lose their virginity through rape is also subject to debate with the belief that virginity can only be lost through consensual sex being prevalent in some studies
in study by researcher and author laura carpenter many men and women discussed how they felt virginity could not be taken through rape
they described losing their virginities in one of three ways as gift stigma or part of the process
carpenter states that despite perceptions of what determines virginity loss being as varied among gay men and lesbians as they are among heterosexuals and in some cases more varied among the former that the matter has been described to her as people viewing sexual acts relating to virginity loss as acts that correspond to your sexual orientation which suggests the following so if you re gay male you re supposed to have anal sex because that what gay men do
and if you re gay woman then you re supposed to have oral sex because that what gay women do
and so those become like markers for when virginity is lost
the concept of technical virginity or sexual abstinence through oral sex is popular among teenagers
for example oral sex is common among adolescent girls who fellate their boyfriends not only to preserve their virginity but also to create and maintain intimacy or to avoid pregnancy
in study published in jama the journal of the american medical association the definition of sex was examined based on random sample of college students from us states it found that said oral genital contact like fellatio cunnilingus did not constitute having sex
stephanie sanders of the kinsey institute co author of the study stated that the technical virginity thing that going on
she and other researchers titled their findings would you say you had sex if
by contrast in study released in by the guttmacher institute author of the findings laura lindberg stated that there is widespread belief that teens engage in nonvaginal forms of sex especially oral sex as way to be sexually active while still claiming that technically they are virgins but that her study drew the conclusion that research shows that this supposed substitution of oral sex for vaginal sex is largely myth study published in the canadian journal of human sexuality focusing on definitions of having sex and noting studies concerning university students from the united states the united kingdom and australia reported that hile the vast majority of respondents more than in these three studies included penile vaginal intercourse in their definition of sex fewer between and respondents considered penile anal intercourse to constitute having sex and that oral genital behaviours were defined as sex by between and of respondents
different study by the kinsey institute sampled people ranging in ages
nearly percent of people in the study agreed that penile vaginal intercourse meant had sex
but the numbers changed as the questions got more specific
percent of respondents based had sex on whether the man had achieved an orgasm concluding that absence of an orgasm does not constitute having had sex
about percent of respondents said penile anal intercourse meant had sex
about percent of people believed oral sex was sex
virginity pledges or abstinence pledges made by heterosexual teenagers and young adults may also include the practice of technical virginity
in peer reviewed study by sociologists peter bearman and hannah brueckner which looked at virginity pledgers five years after their pledge they found that the pledgers have similar proportions of sexually transmitted diseases stds and at least as high proportions of anal and oral sex as those who have not made virginity pledge and deduced that there was substitution of oral and anal sex for vaginal sex among the pledgers
however the data for anal sex without vaginal sex reported by males did not reflect this directly
early loss of virginity early loss of virginity has been shown to be linked to factors such as level of education independence biological factors like age and gender and social factors such as parental supervision or religious affiliation with the most common being sociodemographic variables
along with this sexual abuse has also been shown to have link to later risky sexual behaviors and younger age of voluntary sexual intercourse
sexual initiation at an earlier age has been associated with less frequency of condom use less satisfaction and more frequency of non autonomous reasons for that first sexual encounter
adverse effects for losing virginity at an early age include lower chance of economic stability lower level of education social isolation marital disruption and greater medical consequences
these medical consequences consist of an increase in stds cervical cancer pelvic inflammatory disease fertility and unwanted pregnancies
female virginity cultural value the first act of sexual intercourse by female is commonly considered within many cultures to be an important personal milestone
its significance is reflected in expressions such as saving oneself losing one virginity taking someone virginity and sometimes as deflowering
the occasion is at times seen as the end of innocence integrity or purity and the sexualization of the individual traditionally there was cultural expectation that female would not engage in premarital sex and would come to her wedding virgin and that she would give up her virginity to her new husband in the act of consummation of the marriage
feminine sexual practices have revolved around the idea of females waiting to have sex until they are married some females who have been previously sexually active or their hymen has been otherwise damaged may undergo surgical procedure called hymenorrhaphy or hymenoplasty to repair or replace her hymen and cause vaginal bleeding on the next intercourse as proof of virginity see below
in some cultures an unmarried female who is found not to be virgin whether by choice or as result of rape can be subject to shame ostracism or even an honor killing
in those cultures female virginity is closely interwoven with personal or even family honor especially those known as shame societies in which the loss of virginity before marriage is matter of deep shame
in some parts of africa the myth that sex with virgin can cure hiv aids continues to prevail leading to girls and women being raped
in other societies such as many modern day western cultures lack of sexual abstinence before marriage is not as socially stigmatized as it may be in the formerly mentioned cultures virginity is regarded as valuable commodity in some cultures
in the past within most societies woman options for marriage were largely dependent upon her status as virgin
those women who were not virgins experienced dramatic decrease in opportunities for socially advantageous marriage and in some instances the premarital loss of virginity eliminated their chances of marriage entirely
modern virginity auctions like that of natalie dylan are discussed in the documentary how to lose your virginity
the bible required man who seduced or raped virgin to pay her bride price to her father and marry the girl
in some countries until the late th century woman could sue man who had taken her virginity but did not marry her
in some languages the compensation for these damages are called wreath money
proof of virginity some cultures require proof of bride virginity before her marriage
this has traditionally been tested by the presence of an intact hymen which was verified by either physical examination usually by physician who provided certificate of virginity or by proof of blood which refers to vaginal bleeding that results from the tearing of the hymen after the first sanctioned sexual contact
in some cultures the nuptial blood spotted bed sheet would be displayed as proof of both consummation of marriage and that the bride had been virgin
coerced medical virginity tests are practiced in many regions of the world but are today condemned as form of abuse of women
according to the world health organization who sexual violence encompasses wide range of acts including violent acts against the sexual integrity of women including female genital mutilation and obligatory inspections for virginity researchers stress that the presence or absence of hymen is not reliable indicator of whether or not female has been vaginally penetrated
the hymen is thin film of membrane situated just inside the vulva which can partially occlude the entrance to the vaginal canal
it is flexible and can be stretched or torn during first engagement in vaginal intercourse
however hymen may also be broken during physical activity
many women possess such thin fragile hymens easily stretched and already perforated at birth that the hymen can be broken in childhood without the girl even being aware of it often through athletic activities
for example slip while riding bicycle may on occasion result in the bicycle saddle horn entering the introitus just far enough to break the hymen
further there is the case of women with damaged hymens undergoing hymenorrhaphy or hymenoplasty to repair or replace their hymens and cause vaginal bleeding on the next intercourse as proof of virginity
others consider the practice to be virginity fraud or unnecessary
some call themselves born again virgins
there is common belief that some women are born without hymen but some doubt has been cast on this by recent study
it is likely that almost all women are born with hymen but not necessarily ones that will experience measurable change during first experience of vaginal intercourse
some medical procedures occasionally may require woman hymen to be opened hymenotomy
male virginity historically and in modern times female virginity has been regarded as more significant than male virginity the perception that sexual prowess is fundamental to masculinity has lowered the expectation of male virginity without lowering the social status
for example in some islamic cultures unmarried women who have been sexually active or raped may be subject to name calling shunning or family shame while unmarried men who have lost their virginities are not though premarital sex is forbidden in the quran with regard to both men and women
among various countries or cultures males are expected or encouraged to want to engage in sexual activity and to be more sexually experienced
not following these standards often leads to teasing and other such ridicule from their male peers
study by the guttmacher institute showed that in the countries surveyed most men have experienced sexual intercourse by their th birthdays male sexuality is seen as something that is innate and competitive and displays different set of cultural values and stigmas from female sexuality and virginity
in one study scholars wenger and berger found that male virginity is understood to be real by society but it has been ignored by sociological studies
within british and american culture in particular male virginity has been made an object of embarrassment and ridicule in films such as summer of american pie the inbetweeners movie and the year old virgin with the male virgin typically being presented as socially inept
such attitudes have resulted in some men keeping their status as virgin secret
prevalence of virginity the prevalence of virginity varies from culture to culture
in cultures which place importance on female virginity at marriage the age at which virginity is lost is in effect determined by the age at which marriages would normally take place in those cultures as well as the minimum marriage age set by the laws of the country where the marriage takes place in cross cultural study at what age do women and men have their first sexual intercourse
michael bozon of the french institut national tudes mographiques found that contemporary cultures fall into three broad categories
in the first group the data indicated families arranging marriage for daughters as close to puberty as possible with significantly older men
age of men at sexual initiation in these societies is at later ages than that of women but is often extra marital
this group included sub saharan africa the study listed mali senegal and ethiopia
the study considered the indian subcontinent to also fall into this group although data was only available from nepal in the second group the data indicated families encouraged daughters to delay marriage and to abstain from sexual activity before that time
however sons are encouraged to gain experience with older women or prostitutes before marriage
age of men at sexual initiation in these societies is at lower ages than that of women
this group includes latin cultures both from southern europe portugal greece and romania are noted and from latin america brazil chile and the dominican republic
the study considered many asian societies to also fall into this group although matching data was only available from thailand in the third group age of men and women at sexual initiation was more closely matched
there were two sub groups however
in non latin catholic countries poland and lithuania are mentioned age at sexual initiation was higher suggesting later marriage and reciprocal valuing of male and female virginity
the same pattern of late marriage and reciprocal valuing of virginity was reflected in singapore and sri lanka
the study considered china and vietnam to also fall into this group although data were not available finally in northern and eastern european countries age at sexual initiation was lower with both men and women involved in sexual activity before any union formation
the study listed switzerland germany and the czech republic as members of this group according to unicef survey in out of developed nations with available data more than two thirds of young people have had sexual intercourse while still in their teens
in australia the united kingdom and the united states approximately of year olds and of year olds have had sex
international survey sought to study the sexual behavior of teenagers
students aged from countries completed self administered anonymous classroom survey consisting of standard questionnaire developed by the hbsc health behaviour in school aged children international research network
the survey revealed that the majority of the students were still virgins they had no experience of sexual intercourse and among those who were sexually active the majority used contraception
in kaiser family foundation study of us teenagers of teens reported feeling pressure to have sex of sexually active teens reported being in relationship where they felt things were moving too fast sexually and had done something sexual they didn really want to do
several polls have indicated peer pressure as factor in encouraging both girls and boys to have sex some studies suggest that people commence sexual activity at an earlier age than previous generations
the durex global sex survey found that people worldwide are having sex for the first time at an average age of ranging from in iceland to in india though evidence has shown that the average age is not good indicator of sexual initiation and that percentages of sexually initiated youth at each age are preferred
survey of uk teenagers between the ages of and conducted by yougov for channel showed that only of these teenagers intended to wait until marriage before having sex
according to cdc study in the to year old age group percent of males and percent of females in the united states reported never having an opposite sex partner the rates of teenage pregnancy vary and range from per girls in some sub saharan african countries to per in south korea
the rate for the united states is per the highest in the developed world and about four times the european union average
the teenage pregnancy rates between countries must take into account the level of general sex education available and access to contraceptive options
many western countries have instituted sex education programs the main objective of which is to reduce such pregnancies and stds
in the united states federal government shifted the objective of sex education towards abstinence only sex education programs promoting sexual abstinence before marriage virginity and prohibiting information on birth control and contraception
in president george bush announced five year global hiv aids strategy also known as the president emergency plan for aids relief pepfar which committed the to provide billion over five years toward aids relief in countries in africa and the caribbean and in vietnam
part of the funding was earmarked specifically for abstinence only until marriage programs
in one study about virginity pledges male pledgers were times more likely to remain virgins by age than those who did not pledge vs and estimated that female pledgers were times more likely to remain virgins by age than those who did not pledge vs
social psychology some cultural anthropologists argue that romantic love and sexual jealousy are universal features of human relationships
social values related to virginity reflect both sexual jealousy and ideals of romantic love and appear to be deeply embedded in human nature psychology explores the connection between thought and behavior
seeking understanding of social or anti social behaviors includes sexual behavior
joan kahn and kathryn london studied women married between and to see if virginity at marriage influenced risk of divorce
in this study women who were virgins at the time of marriage were shown to have less marital upset
it was shown that when observable characteristics were controlled women who were non virgins at the time of marriage had higher risk for divorce
however it was also shown that the link between premarital sex and the risk of divorce were attributed to prior unobserved differences such as deviating from norms study conducted by smith and schaffer found that someone first sexual experience has been linked to their sexual performance for years to come
participants whose first intercourse was pleasant showed more satisfaction in their current sex lives
different study showed that when compared with virgins nonvirgins have been shown to have higher levels of independence less desire for achievement more criticism from society and greater level of deviance
ethics social norms and legal implications human sexual activity like many other kinds of activity engaged in by humans is generally influenced by social rules that are culturally specific and vary widely
these social rules are referred to as sexual morality what can and can not be done by society rules and sexual norms what is and is not expected
there are number of groups within societies promoting their views of sexual morality in variety of ways including through sex education religious teachings seeking commitments or virginity pledges and other means
most countries have laws which set minimum marriage age with the most common age being years reduced to in special circumstances typically when the female partner is pregnant but the actual age at first marriage can be considerably higher
laws also prescribe the minimum age at which person is permitted to engage in sex commonly called the age of consent
social and legal attitudes toward the appropriate age of consent have drifted upwards in modern times
for example while ages from to were typically acceptable in western countries during the mid th century the end of the th century and the beginning of the th century were marked by changing attitudes resulting in raising the ages of consent to ages generally ranging from to today the age of consent varies from years or onset of puberty to but is the most common age of consent though some jurisdictions also having close in age exception allowing two adolescents as young as years of age to have sex with each other provided their ages are not more than specified number of years apart typical no more then to years age difference depending on the jurisdiction
some countries outlaw any sex outside marriage entirely
historically and still in many countries and jurisdictions today female sexual experience is sometimes considered relevant factor in the prosecution of perpetrator of rape
also historically man who took female virginity could be forced to marry her
in addition children born as result of premarital sex were subject to various legal and social disabilities such as being considered illegitimate and thus barred from inheriting from the putative father estate from bearing the father surname or title and support from the putative father
many of these legal disabilities on children born from extramarital relationships have been abolished by law in most western countries though social ostracism may still apply
religious views all major religions have moral codes covering issues of sexuality morality and ethics
though these moral codes do not address issues of sexuality directly they seek to regulate the situations which can give rise to sexual interest and to influence people sexual activities and practices
however the impact of religious teaching has at times been limited
for example though most religions disapprove of premarital sexual relations it has always been widely practiced
nevertheless these religious codes have always had strong influence on peoples attitudes to sexual issues
ancient greece and rome virginity was often considered virtue denoting purity and physical self restraint and is an important characteristic in greek mythology
in ancient greek literature such as the homeric hymns there are references to the parthenon goddesses artemis athena and hestia proclaiming pledges to eternal virginity greek
however it has been argued maiden state of parthenia greek as invoked by these deities carries slightly different meaning from what is normally understood as virginity in modern western religions
rather parthenia focused more on marriageability and abstract concepts without strict physical requirements which would be adversely affected but not entirely relinquished by pre marital sexual intercourse
for these reasons other goddesses not eternally committed to parthenia within the homeric hymns are able to renew theirs through ritual such as hera or choose an appearance which implies the possession of it such as aphrodite
although accounts vary the goddess of witchcraft known as hecate has been portrayed as virgin as well in roman times the vestal virgins were the highly respected strictly celibate although not necessarily virginal priestesses of vesta and keepers of the sacred fire of vesta
the vestals were committed to the priesthood before puberty when years old and sworn to celibacy for period of years
the chastity of the vestals was considered to have direct bearing on the health of the roman state
allowing the sacred fire of vesta to die out suggesting that the goddess had withdrawn her protection from the city was serious offence and was punishable by scourging
because vestal chastity was thought to be directly correlated to the sacred burning of the fire if the fire were extinguished it might be assumed that vestal had been unchaste
the penalty for vestal virgin found to have had sexual relations while in office was being buried alive
buddhism the most common formulation of buddhist ethics for lay followers are the five precepts and the eightfold path
these precepts take the form of voluntary personal undertakings not divine mandate or instruction
the third of the five precepts is to refrain from committing sensual misconduct
sensual misconduct is defined in the pali canon as follows abandoning sensual misconduct man abstains from sensual misconduct
he does not get sexually involved with those who are protected by their mothers their fathers their brothers their sisters their relatives or their dhamma those with husbands those who entail punishments or even those crowned with flowers by another man
virginity specifically is not mentioned in the canon
on the other hand buddhist monks and nuns of most traditions are expected to refrain from all sexual activity and the buddha is said to have admonished his followers to avoid unchastity as if it were pit of burning cinders
the rd of the precepts in buddhism warns against any sensual misconduct though the exact definition of it is unclear
buddhists have been more open compared to other religions about the subject of sex and that has expanded over time
as with christianity although traditionalist would assume that one should not have sex before marriage many buddhists do
there are different branches of buddhism like tantric and puritan and they have very different views on the subject of sex yet managed to get along
tantric is sanskrit word it is typically translated as two things or person being bound together
in the time of gotama the man who came to be known as buddha sex was not taboo
the world the prince lived in was filled with earthly pleasures
women naked from the waist above were in the court solely to serve the prince
gotama father even constructed chamber of love
prince gotama and founded the beginnings of buddhism which included the denial of earthly pleasures in order to follow the middle way
the stark contrast between the way buddha lived his life before and after rejecting the material world may arguably be one of the reasons buddhism evolved the way it did
in the present the mother of buddha does not have to be virgin she must have never had child however
hinduism in hinduism premarital virginity on the part of the bride is considered ideal
the prevailing hindu marriage ceremony or the vedic wedding centers around the kanyadan ritual which literally means gift of virgin by father of the maiden through which the hindus believe they gain greatest spiritual merit and marriages of the daughters are considered spiritual obligation
the purity of women is especially valued in south asia where hinduism is most commonly practiced
sex had never been taboo in ancient india and intactness of the hymen had nothing to do with virginity
judaism premarital sex is forbidden in judaism
in fact the precedent for the mitzvot which are related in deuteronomy which regard what happens when man rapes virgin may well have been set at shechem after the rape of dinah cf
there are other references in the torah to virginity
in the first reference in genesis lot offers his virgin daughters to the people of sodom for sexual purposes in an attempt to protect his guests cf
genesis with the implication that the people of sodom would be more likely to accept the offer in view of the girls virginity than they would otherwise
this also sets the precedent for israelites to avoid homosexual activity cf
the next reference is at genesis where eliezer is seeking wife for his master abraham son
he meets rebecca and the narrative tells us the damsel was very fair to look upon virgin neither had any man known her in biblical terms to know is euphemism for sexual relations
children born to single woman are not regarded as illegitimate mamzer or subject to social or religious disabilities perez and zerach for example and although their mother was widow who was willingly impregnated by her father in law were not counted as mamzerim cf
halakhah also contains rules related to protecting female virgins and rules regarding pre marital sex rape and the effects of each
in torah damsel who has not the sign of virginity in the early marriage shall be punished by death penalty since the unvirgin woman among israel is equal with defiled whore in her father house
christianity paul the apostle expressed the view that person body belongs to god and is god temple corinthians and that premarital sex is immoral corinthians on an equal level as adultery
corinthians paul also expressed the view in corinthians that sexual abstinence is the preferred state for both men and women
however he stated that sexual relations are expected between married couple
according to classicist evelyn stagg and new testament scholar frank stagg the new testament holds that sex is reserved for marriage
they maintain that the new testament teaches that sex outside of marriage is sin of adultery if either of the participants is married otherwise the sin of fornication if neither of the participants are married
an imperative given in corinthians says flee from sexual immorality
all other sins people commit are outside their bodies but those who sin sexually sin against their own bodies
cor those who are sexually immoral or adulterers are listed in corinthians in list of wrongdoers who will not inherit the kingdom of god
galatians and corinthians also address fornication
the apostolic decree of the council of jerusalem also includes prohibition on fornication
aquinas went further emphasizing that acts other than copulation destroy virginity and clarifying that involuntary sexual pleasure does not destroy virginity
from his summa theologica pleasure resulting from resolution of semen may arise in two ways
if this be the result of the mind purpose it destroys virginity whether copulation takes place or not
augustine however mentions copulation because such like resolution is the ordinary and natural result thereof
on another way this may happen beside the purpose of the mind either during sleep or through violence and without the mind consent although the flesh derives pleasure from it or again through weakness of nature as in the case of those who are subject to flow of semen
on such cases virginity is not forfeit because such like pollution is not the result of impurity which excludes virginity
some have theorized that the new testament was not against sex before marriage
the discussion turns on two greek words moicheia adultery and porneia fornication see also pornography
the first word is restricted to contexts involving sexual betrayal of spouse however the second word is used as generic term for illegitimate sexual activity
elsewhere in corinthians incest homosexual intercourse according to some interpretations and prostitution are all explicitly forbidden by name however the septuagint uses porneia to refer to male temple prostitution
paul is preaching about activities based on sexual prohibitions in leviticus in the context of achieving holiness
the theory suggests it is these and only these behaviors that are intended by paul prohibition in chapter seven
the strongest argument against this theory is that the modern interpretation of the new testament outside corinthians speaks against premarital sex christian orthodoxy accepts that mary the mother of jesus was virgin at the time jesus was conceived based on the accounts in the gospel of matthew and the gospel of luke
roman catholics eastern orthodox and oriental orthodox as well as many lutherans and anglicans hold to the dogma of the perpetual virginity of mary
however other christians reject the dogma citing sources such as mark isn this the carpenter the son of mary and the brother of james joses judas and simon
and aren his sisters here with us
the catholic church holds that in semitic usage the terms brother sister are applied not only to children of the same parents but to nephews nieces cousins half brothers and half sisters
catholics orthodox christians lutherans and other groups such as high church anglicans may refer to mary as the virgin mary or the blessed virgin mary the catholic encyclopedia says there are two elements in virginity the material element that is to say the absence in the past and in the present of all complete and voluntary delectation whether from lust or from the lawful use of marriage and the formal element that is the firm resolution to abstain forever from sexual pleasure and that virginity is irreparably lost by sexual pleasure voluntarily and completely experienced
however for the purposes of consecrated virgins it is canonically enough that they have never been married or lived in open violation of chastity
islam islam considers extramarital sex to be sinful and forbidden
though islamic law prescribes punishments for muslim men and women for the act of zin though in western cultures premarital sex and loss of virginity may be considered shameful to the individual in some muslim societies an act of premarital sex even if not falling within the legal standards of proof may result in personal shame and loss of family honor in some modern day largely muslim societies such as turkey vaginal examinations for verifying woman virginity are clinical practice which are at times state enforced
these types of examinations are typically ordered for women who go against traditional societal notions of public morality and rules of modesty though in the turkish penal code was altered to require woman consent prior to performing such an examination
sikhism in sikhism sexual activity is supposed to occur only between married individuals
sikhism advises against premarital sex as it has high potential of being an indulgence of lust kaam or extreme sexual desire
sikhism teaches that young women must have decent modesty sharam because the honor izzat of her family could be jeopardized
sexual activity and even living together prior to marriage is not allowed in sikhism
virginity is an important aspect of spirituality and it has to be preserved before marriage or when one is ready to move into another sacred state of being with their significant other
see also almah artificial hymen brahmacharya confraternity of the cord of saint thomas purity ball purity ring references further reading journal articlesarmour stacy and dana haynie
adolescent sexual debut and later delinquency
journal of youth and adolescence
abstract only cooksey elizabeth mott frank neubauer stefanie
friendships and early relationships links to sexual initiation among american adolescents born to young mothers pdf
perspectives on sexual and reproductive health
jstor pmid goodson evans edmundson
female adolescents and onset of sexual intercourse theory based review of research from to
journal of adolescent health
doi pmid rich lauren kim sun bin
employment and the sexual and reproductive behavior of female adolescents
perspectives on sexual and reproductive health
jstor pmid archived from the original on rosenberg
age at first sex and human papillomavirus infection linked through behavioral factors and partner traits
perspectives on sexual and reproductive health
jstor archived from the original on monographsbently thomas
the monument of matrones conteining seven severall lamps of virginitie
thomas dawson carpenter laura
virginity lost an intimate portrait of first sexual experiences
new york university press isbn external links
in mathematics the linear span also called the linear hull or just span of set of vectors from vector space denoted span is defined as the set of all linear combinations of the vectors in it can be characterized either as the intersection of all linear subspaces that contain or as the smallest subspace containing the linear span of set of vectors is therefore vector space itself 
spans can be generalized to matroids and modules 
to express that vector space is linear span of subset one commonly uses the following phrases either spans is spanning set of is spanned generated by or is generator or generator set of definition given vector space over field the span of set of vectors not necessarily infinite is defined to be the intersection of all subspaces of that contain is referred to as the subspace spanned by or by the vectors in conversely is called spanning set of and we say that spans alternatively the span of may be defined as the set of all finite linear combinations of elements vectors of which follows from the above definition 
in the case of infinite infinite linear combinations 
where combination may involve an infinite sum assuming that such sums are defined somehow as in say banach space are excluded by the definition generalization that allows these is not equivalent 
examples the real vector space has as spanning set 
this particular spanning set is also basis 
if were replaced by it would also form the canonical basis of another spanning set for the same space is given by but this set is not basis because it is linearly dependent 
the set is not spanning set of since its span is the space of all vectors in whose last component is zero 
that space is also spanned by the set as is linear combination of and 
it does however span 
when interpreted as subset of 
the empty set is spanning set of since the empty set is subset of all possible vector spaces in and is the intersection of all of these vector spaces 
the set of functions xn where is non negative integer spans the space of polynomials 
theorems equivalence of definitions the set of all linear combinations of subset of vector space over is the smallest linear subspace of containing proof 
we first prove that span is subspace of since is subset of we only need to prove the existence of zero vector in span that span is closed under addition and that span is closed under scalar multiplication 
letting it is trivial that the zero vector of exists in span since adding together two linear combinations of also produces linear combination of where all and multiplying linear combination of by scalar will produce another linear combination of thus is subspace of suppose that is linear subspace of containing it follows that span since every vi is linear combination of trivially 
since is closed under addition and scalar multiplication then every linear combination must be contained in thus span is contained in every subspace of containing and the intersection of all such subspaces or the smallest such subspace is equal to the set of all linear combinations of size of spanning set is at least size of linearly independent set every spanning set of vector space must contain at least as many elements as any linearly independent set of vectors from proof 
let be spanning set and be linearly independent set of vectors from we want to show that since spans then must also span and must be linear combination of thus is linearly dependent and we can remove one vector from that is linear combination of the other elements 
this vector cannot be any of the wi since is linearly indepedent 
the resulting set is which is spanning set of we repeat this step times where the resulting set after the pth step is the union of and vectors of it is ensured until the nth step that there will always be some to remove out of for every adjoint of and thus there are at least as many vi as there are wi 
to verify this we assume by way of contradiction that then at the mth step we have the set and we can adjoin another vector but since is spanning set of is linear combination of 
this is contradiction since is linearly independent 
spanning set can be reduced to basis let be finite dimensional vector space 
any set of vectors that spans can be reduced to basis for by discarding vectors if necessary 
if there are linearly dependent vectors in the set 
if the axiom of choice holds this is true without the assumption that has finite dimension 
this also indicates that basis is minimal spanning set when is finite dimensional 
generalizations generalizing the definition of the span of points in space subset of the ground set of matroid is called spanning set if the rank of equals the rank of the entire ground set 
the vector space definition can also be generalized to modules 
given an module and collection of elements an of the submodule of spanned by an is the sum of cyclic modules consisting of all linear combinations of the elements ai 
as with the case of vector spaces the submodule of spanned by any subset of is the intersection of all submodules containing that subset 
closed linear span functional analysis in functional analysis closed linear span of set of vectors is the minimal closed set which contains the linear span of that set 
suppose that is normed vector space and let be any non empty subset of the closed linear span of denoted by sp or span is the intersection of all the closed linear subspaces of which contain one mathematical formulation of this is sp sp 
the closed linear span of the set of functions xn on the interval where is non negative integer depends on the norm used 
if the norm is used then the closed linear span is the hilbert space of square integrable functions on the interval 
but if the maximum norm is used the closed linear span will be the space of continuous functions on the interval 
in either case the closed linear span contains functions that are not polynomials and so are not in the linear span itself 
however the cardinality of the set of functions in the closed linear span is the cardinality of the continuum which is the same cardinality as for the set of polynomials 
notes the linear span of set is dense in the closed linear span 
moreover as stated in the lemma below the closed linear span is indeed the closure of the linear span 
closed linear spans are important when dealing with closed linear subspaces which are themselves highly important see riesz lemma 
useful lemma let be normed space and let be any non empty subset of then so the usual way to find the closed linear span is to find the linear span first and then the closure of that linear span 
see also affine hull conical combination convex hull citations sources textbooks axler sheldon jay 
linear algebra done right rd ed 
linear algebra th ed 
isbn lane saunders mac birkhoff garrett 
advanced linear algebra nd ed 
isbn rynne brian youngson martin 
isbn lay david linear algebra and its applications th edition 
web lankham isaiah nachtergaele bruno schilling anne february 
linear algebra as an introduction to abstract mathematics pdf 
university of california davis 
retrieved september weisstein eric wolfgang 
retrieved feb cs maint url status link linear hull 
april retrieved feb cs maint url status link external links linear combinations and span understanding linear combinations and spans of vectors khanacademy org 
linear combinations span and basis vectors 
essence of linear algebra 
archived from the original on via youtube
in statistics principal component regression pcr is regression analysis technique that is based on principal component analysis pca 
more specifically pcr is used for estimating the unknown regression coefficients in standard linear regression model 
in pcr instead of regressing the dependent variable on the explanatory variables directly the principal components of the explanatory variables are used as regressors 
one typically uses only subset of all the principal components for regression making pcr kind of regularized procedure and also type of shrinkage estimator 
often the principal components with higher variances the ones based on eigenvectors corresponding to the higher eigenvalues of the sample variance covariance matrix of the explanatory variables are selected as regressors 
however for the purpose of predicting the outcome the principal components with low variances may also be important in some cases even more important one major use of pcr lies in overcoming the multicollinearity problem which arises when two or more of the explanatory variables are close to being collinear 
pcr can aptly deal with such situations by excluding some of the low variance principal components in the regression step 
in addition by usually regressing on only subset of all the principal components pcr can result in dimension reduction through substantially lowering the effective number of parameters characterizing the underlying model 
this can be particularly useful in settings with high dimensional covariates 
also through appropriate selection of the principal components to be used for regression pcr can lead to efficient prediction of the outcome based on the assumed model 
the principle the pcr method may be broadly divided into three major steps perform pca on the observed data matrix for the explanatory variables to obtain the principal components and then usually select subset based on some appropriate criteria of the principal components so obtained for further use 
now regress the observed vector of outcomes on the selected principal components as covariates using ordinary least squares regression linear regression to get vector of estimated regression coefficients with dimension equal to the number of selected principal components now transform this vector back to the scale of the actual covariates using the selected pca loadings the eigenvectors corresponding to the selected principal components to get the final pcr estimator with dimension equal to the total number of covariates for estimating the regression coefficients characterizing the original model 
details of the method data representation let denote the vector of observed outcomes and denote the corresponding data matrix of observed covariates where and denote the size of the observed sample and the number of covariates respectively with each of the rows of denotes one set of observations for the dimensional covariate and the respective entry of denotes the corresponding observed outcome 
data pre processing assume that and each of the columns of have already been centered so that all of them have zero empirical means 
this centering step is crucial at least for the columns of since pcr involves the use of pca on and pca is sensitive to centering of the data 
underlying model following centering the standard gauss markov linear regression model for on can be represented as where denotes the unknown parameter vector of regression coefficients and denotes the vector of random errors with and var for some unknown variance parameter objective the primary goal is to obtain an efficient estimator for the parameter based on the data 
one frequently used approach for this is ordinary least squares regression which assuming is full column rank gives the unbiased estimator of pcr is another technique that may be used for the same purpose of estimating pca step pcr starts by performing pca on the centered data matrix for this let denote the singular value decomposition of where diag with denoting the non negative singular values of while the columns of and are both orthonormal sets of vectors denoting the left and right singular vectors of respectively 
the principal components gives spectral decomposition of where diag diag with denoting the non negative eigenvalues also known as the principal values of while the columns of denote the corresponding orthonormal set of eigenvectors 
then and respectively denote the principal component and the principal component direction or pca loading corresponding to the th largest principal value for each 
derived covariates for any let denote the matrix with orthonormal columns consisting of the first columns of let denote the matrix having the first principal components as its columns 
may be viewed as the data matrix obtained by using the transformed covariates instead of using the original covariates the pcr estimator let denote the vector of estimated regression coefficients obtained by ordinary least squares regression of the response vector on the data matrix then for any the final pcr estimator of based on using the first principal components is given by fundamental characteristics and applications of the pcr estimator two basic properties the fitting process for obtaining the pcr estimator involves regressing the response vector on the derived data matrix which has orthogonal columns for any since the principal components are mutually orthogonal to each other 
thus in the regression step performing multiple linear regression jointly on the selected principal components as covariates is equivalent to carrying out independent simple linear regressions or univariate regressions separately on each of the selected principal components as covariate 
when all the principal components are selected for regression so that then the pcr estimator is equivalent to the ordinary least squares estimator 
thus this is easily seen from the fact that and also observing that is an orthogonal matrix 
variance reduction for any the variance of is given by var diag in particular var var hence for all we have var var thus for all we have var var where indicates that square symmetric matrix is non negative definite 
consequently any given linear form of the pcr estimator has lower variance compared to that of the same linear form of the ordinary least squares estimator 
addressing multicollinearity under multicollinearity two or more of the covariates are highly correlated so that one can be linearly predicted from the others with non trivial degree of accuracy 
consequently the columns of the data matrix that correspond to the observations for these covariates tend to become linearly dependent and therefore tends to become rank deficient losing its full column rank structure 
more quantitatively one or more of the smaller eigenvalues of get very close or become exactly equal to under such situations 
the variance expressions above indicate that these small eigenvalues have the maximum inflation effect on the variance of the least squares estimator thereby destabilizing the estimator significantly when they are close to this issue can be effectively addressed through using pcr estimator obtained by excluding the principal components corresponding to these small eigenvalues 
dimension reduction pcr may also be used for performing dimension reduction 
to see this let denote any matrix having orthonormal columns for any 
suppose now that we want to approximate each of the covariate observations through the rank linear transformation for some 
then it can be shown that is minimized at the matrix with the first principal component directions as columns and the corresponding dimensional derived covariates 
thus the dimensional principal components provide the best linear approximation of rank to the observed data matrix the corresponding reconstruction error is given by thus any potential dimension reduction may be achieved by choosing the number of principal components to be used through appropriate thresholding on the cumulative sum of the eigenvalues of since the smaller eigenvalues do not contribute significantly to the cumulative sum the corresponding principal components may be continued to be dropped as long as the desired threshold limit is not exceeded 
the same criteria may also be used for addressing the multicollinearity issue whereby the principal components corresponding to the smaller eigenvalues may be ignored as long as the threshold limit is maintained 
regularization effect since the pcr estimator typically uses only subset of all the principal components for regression it can be viewed as some sort of regularized procedure 
more specifically for any the pcr estimator denotes the regularized solution to the following constrained minimization problem min subject to 
the constraint may be equivalently written as where 
thus when only proper subset of all the principal components are selected for regression the pcr estimator so obtained is based on hard form of regularization that constrains the resulting solution to the column space of the selected principal component directions and consequently restricts it to be orthogonal to the excluded directions 
optimality of pcr among class of regularized estimators given the constrained minimization problem as defined above consider the following generalized version of it min subject to where denotes any full column rank matrix of order with let denote the corresponding solution 
thus arg min subject to then the optimal choice of the restriction matrix for which the corresponding estimator achieves the minimum prediction error is given by where diag 
quite clearly the resulting optimal estimator is then simply given by the pcr estimator based on the first principal components 
efficiency since the ordinary least squares estimator is unbiased for we have var mse where mse denotes the mean squared error 
now if for some we additionally have then the corresponding is also unbiased for and therefore var mse 
we have already seen that var var which then implies mse mse for that particular thus in that case the corresponding would be more efficient estimator of compared to based on using the mean squared error as the performance criteria 
in addition any given linear form of the corresponding would also have lower mean squared error compared to that of the same linear form of now suppose that for given then the corresponding is biased for however since var var it is still possible that mse mse especially if is such that the excluded principal components correspond to the smaller eigenvalues thereby resulting in lower bias 
in order to ensure efficient estimation and prediction performance of pcr as an estimator of park proposes the following guideline for selecting the principal components to be used for regression drop the principal component if and only if practical implementation of this guideline of course requires estimates for the unknown model parameters and in general they may be estimated using the unrestricted least squares estimates obtained from the original full model 
park however provides slightly modified set of estimates that may be better suited for this purpose unlike the criteria based on the cumulative sum of the eigenvalues of which is probably more suited for addressing the multicollinearity problem and for performing dimension reduction the above criteria actually attempts to improve the prediction and estimation efficiency of the pcr estimator by involving both the outcome as well as the covariates in the process of selecting the principal components to be used in the regression step 
alternative approaches with similar goals include selection of the principal components based on cross validation or the mallow cp criteria 
often the principal components are also selected based on their degree of association with the outcome 
shrinkage effect of pcr in general pcr is essentially shrinkage estimator that usually retains the high variance principal components corresponding to the higher eigenvalues of as covariates in the model and discards the remaining low variance components corresponding to the lower eigenvalues of 
thus it exerts discrete shrinkage effect on the low variance components nullifying their contribution completely in the original model 
in contrast the ridge regression estimator exerts smooth shrinkage effect through the regularization parameter or the tuning parameter inherently involved in its construction 
while it does not completely discard any of the components it exerts shrinkage effect over all of them in continuous manner so that the extent of shrinkage is higher for the low variance components and lower for the high variance components 
frank and friedman conclude that for the purpose of prediction itself the ridge estimator owing to its smooth shrinkage effect is perhaps better choice compared to the pcr estimator having discrete shrinkage effect 
in addition the principal components are obtained from the eigen decomposition of that involves the observations for the explanatory variables only 
therefore the resulting pcr estimator obtained from using these principal components as covariates need not necessarily have satisfactory predictive performance for the outcome 
somewhat similar estimator that tries to address this issue through its very construction is the partial least squares pls estimator 
similar to pcr pls also uses derived covariates of lower dimensions 
however unlike pcr the derived covariates for pls are obtained based on using both the outcome as well as the covariates 
while pcr seeks the high variance directions in the space of the covariates pls seeks the directions in the covariate space that are most useful for the prediction of the outcome 
variant of the classical pcr known as the supervised pcr was proposed 
in spirit similar to that of pls it attempts at obtaining derived covariates of lower dimensions based on criterion that involves both the outcome as well as the covariates 
the method starts by performing set of simple linear regressions or univariate regressions wherein the outcome vector is regressed separately on each of the covariates taken one at time 
then for some the first covariates that turn out to be the most correlated with the outcome based on the degree of significance of the corresponding estimated regression coefficients are selected for further use 
conventional pcr as described earlier is then performed but now it is based on only the data matrix corresponding to the observations for the selected covariates 
the number of covariates used and the subsequent number of principal components used are usually selected by cross validation 
generalization to kernel settings the classical pcr method as described above is based on classical pca and considers linear regression model for predicting the outcome based on the covariates 
however it can be easily generalized to kernel machine setting whereby the regression function need not necessarily be linear in the covariates but instead it can belong to the reproducing kernel hilbert space associated with any arbitrary possibly non linear symmetric positive definite kernel 
the linear regression model turns out to be special case of this setting when the kernel function is chosen to be the linear kernel 
in general under the kernel machine setting the vector of covariates is first mapped into high dimensional potentially infinite dimensional feature space characterized by the kernel function chosen 
the mapping so obtained is known as the feature map and each of its coordinates also known as the feature elements corresponds to one feature may be linear or non linear of the covariates 
the regression function is then assumed to be linear combination of these feature elements 
thus the underlying regression model in the kernel machine setting is essentially linear regression model with the understanding that instead of the original set of covariates the predictors are now given by the vector potentially infinite dimensional of feature elements obtained by transforming the actual covariates using the feature map 
however the kernel trick actually enables us to operate in the feature space without ever explicitly computing the feature map 
it turns out that it is only sufficient to compute the pairwise inner products among the feature maps for the observed covariate vectors and these inner products are simply given by the values of the kernel function evaluated at the corresponding pairs of covariate vectors 
the pairwise inner products so obtained may therefore be represented in the form of symmetric non negative definite matrix also known as the kernel matrix 
pcr in the kernel machine setting can now be implemented by first appropriately centering this kernel matrix say with respect to the feature space and then performing kernel pca on the centered kernel matrix say whereby an eigendecomposition of is obtained 
kernel pcr then proceeds by usually selecting subset of all the eigenvectors so obtained and then performing standard linear regression of the outcome vector on these selected eigenvectors 
the eigenvectors to be used for regression are usually selected using cross validation 
the estimated regression coefficients having the same dimension as the number of selected eigenvectors along with the corresponding selected eigenvectors are then used for predicting the outcome for future observation 
in machine learning this technique is also known as spectral regression 
clearly kernel pcr has discrete shrinkage effect on the eigenvectors of quite similar to the discrete shrinkage effect of classical pcr on the principal components as discussed earlier 
however the feature map associated with the chosen kernel could potentially be infinite dimensional and hence the corresponding principal components and principal component directions could be infinite dimensional as well 
therefore these quantities are often practically intractable under the kernel machine setting 
kernel pcr essentially works around this problem by considering an equivalent dual formulation based on using the spectral decomposition of the associated kernel matrix 
under the linear regression model which corresponds to choosing the kernel function as the linear kernel this amounts to considering spectral decomposition of the corresponding kernel matrix and then regressing the outcome vector on selected subset of the eigenvectors of so obtained 
it can be easily shown that this is the same as regressing the outcome vector on the corresponding principal components which are finite dimensional in this case as defined in the context of the classical pcr 
thus for the linear kernel the kernel pcr based on dual formulation is exactly equivalent to the classical pcr based on primal formulation 
however for arbitrary and possibly non linear kernels this primal formulation may become intractable owing to the infinite dimensionality of the associated feature map 
thus classical pcr becomes practically infeasible in that case but kernel pcr based on the dual formulation still remains valid and computationally scalable 
see also principal component analysis partial least squares regression ridge regression canonical correlation deming regression total sum of squares references further reading amemiya takeshi
in probability theory and statistics collection of random variables is independent and identically distributed if each random variable has the same probability distribution as the others and all are mutually independent 
this property is usually abbreviated as iid or iid 
iid was first defined in statistics and finds application in different fields such as data mining and signal processing 
introduction in statistics we commonly deal with random samples 
random sample can be thought of as set of objects that are chosen randomly 
or more formally it sequence of independent identically distributed iid random variables 
in other words the terms random sample and iid are basically one and the same 
in statistics we usually say random sample but in probability it more common to say iid identically distributed means that there are no overall trends the distribution doesn fluctuate and all items in the sample are taken from the same probability distribution 
independent means that the sample items are all independent events 
in other words they aren connected to each other in any way 
in other words knowledge of the value of one variable gives no information about the value of the other and vice versa 
application independent and identically distributed random variables are often used as an assumption which tends to simplify the underlying mathematics 
in practical applications of statistical modeling however the assumption may or may not be realistic the 
assumption is also used in central limit theorem which states that the probability distribution of the sum or average of 
variables with finite variance approaches normal distribution often the 
assumption arises in the context of sequences of random variables 
then independent and identically distributed implies that an element in the sequence is independent of the random variables that came before it 
in this way an 
sequence is different from markov sequence where the probability distribution for the nth random variable is function of the previous random variable in the sequence for first order markov sequence 
sequence does not imply the probabilities for all elements of the sample space or event space must be the same 
for example repeated throws of loaded dice will produce sequence that is despite the outcomes being biased 
definition definition for two random variables suppose that the random variables and are defined to assume values in let and be the cumulative distribution functions of and respectively and denote their joint cumulative distribution function by 
two random variables and are identically distributed if and only if two random variables and are independent if and only if 
see further independence probability theory two random variables 
two random variables and are 
if they are independent and identically distributed 
if and only if definition for more than two random variables the definition extends naturally to more than two random variables 
we say that random variables are 
if they are independent see further independence probability theory more than two random variables and identically distributed 
if and only if where denotes the joint cumulative distribution function of definition for independence in probability theory two events are called independent if and only if and 
in the following ab is short for and 
suppose there are the two events of the experiment if there is possibility 
generally the occurrence of has an effect on the probability of which is called conditional probability and only when the occurrence of has no effect on the occurrence of there is 
note if then are mutually independent which cannot be established with mutually incompatible at the same time that is independence must be compatible and mutual exclusion must be related 
suppose are three events 
if ab bc ac abc are satisfied then the events are independent of each other 
more general definition is there are events an 
if the probabilities of the product events for any events are equal to the product of the probabilities of each event then the events an are independent of each other 
examples example sequence of outcomes of spins of fair or unfair roulette wheel is 
one implication of this is that if the roulette ball lands on red for example times in row the next spin is no more or less likely to be black than on any other spin see the gambler fallacy 
sequence of fair or loaded dice rolls is 
sequence of fair or unfair coin flips is 
in signal processing and image processing the notion of transformation to 
implies two specifications the 
the signal level must be balanced on the time axis 
the signal spectrum must be flattened 
transformed by filtering such as deconvolution to white noise signal 
signal where all frequencies are equally present 
example toss coin times and record how many times does the coin lands on head 
independent each outcome of landing will not affect the other outcome which means the results are independent from each other 
identically distributed if the coin is homogeneous material each time the probability for head is which means the probability is identical for each time 
example roll dice times and record how many time the result is independent each outcome of the dice will not affect the next one which means the results are independent from each other 
identically distributed if the dice is homogeneous material each time the probability for the number is which means the probability is identical for each time 
example choose card from standard deck of cards containing cards then place the card back in the deck 
repeat it for times 
record the number of king appears independent each outcome of the card will not affect the next one which means the results are independent from each other 
identically distributed after drawing one card from it each time the probability for king is which means the probability is identical for each time 
generalizations many results that were first proven under the assumption that the random variables are 
have been shown to be true even under weaker distributional assumption 
exchangeable random variables the most general notion which shares the main properties of 
variables are exchangeable random variables introduced by bruno de finetti 
exchangeability means that while variables may not be independent future ones behave like past ones formally any value of finite sequence is as likely as any permutation of those values the joint probability distribution is invariant under the symmetric group 
this provides useful generalization for example sampling without replacement is not independent but is exchangeable 
vy process in stochastic calculus 
variables are thought of as discrete time vy process each variable gives how much one changes from one time to another 
for example sequence of bernoulli trials is interpreted as the bernoulli process 
one may generalize this to include continuous time vy processes and many vy processes can be seen as limits of 
variables for instance the wiener process is the limit of the bernoulli process 
in machine learning why assume the data in machine learning are independent and identically distributed 
machine learning uses currently acquired massive quantities of data to deliver faster more accurate results 
therefore we need to use historical data with overall representativeness 
if the data obtained is not representative of the overall situation then the rules will be summarized badly or wrongly 
hypothesis the number of individual cases in the training sample can be greatly reduced 
this assumption makes maximization very easy to calculate mathematically 
observing the assumption of independent and identical distribution in mathematics simplifies the calculation of the likelihood function in optimization problems 
because of the assumption of independence the likelihood function can be written like this 
in order to maximize the probability of the observed event take the log function and maximize the parameter that is to say to compute log where log log log log 
log the computer is very efficient to calculate multiple additions but it is not efficient to calculate the multiplication 
this simplification is the core reason for the increase in computational efficiency 
and this log transformation is also in the process of maximizing turning many exponential functions into linear functions 
for two reasons this hypothesis is easy to use the central limit theorem in practical applications 
even if the sample comes from more complex non gaussian distribution it can also approximate well 
because it can be simplified from the central limit theorem to gaussian distribution 
for large number of observable samples the sum of many random variables will have an approximately normal distribution 
the second reason is that the accuracy of the model depends on the simplicity and representative power of the model unit as well as the data quality 
because the simplicity of the unit makes it easy to interpret and scale and the representative power scale out of the unit improves the model accuracy 
like in deep neural network each neuron is very simple but has strong representative power layer by layer to represent more complex features to improve model accuracy 
see also de finetti theorem pairwise independent variables central limit theorem references further reading
in music theory the circle of fifths is way of organizing the chromatic pitches as sequence of perfect fifths 
this is strictly true in the standard tone equal temperament system using different system requires one interval of diminished sixth to be treated as fifth 
if is chosen as starting point the sequence is continuing the pattern from returns the sequence to its starting point of this order places the most closely related key signatures adjacent to one another 
it is usually illustrated in the form of circle 
definition the circle of fifths organizes pitches in sequence of perfect fifths generally shown as circle with the pitches and their corresponding keys in clockwise progression 
musicians and composers often use the circle of fifths to describe the musical relationships between pitches 
its design is helpful in composing and harmonizing melodies building chords and modulating to different keys within composition using the system of just intonation perfect fifth consists of two pitches with frequency ratio of but generating twelve successive perfect fifths in this way does not result in return to the pitch class of the starting note 
to adjust for this instruments are generally tuned with the equal temperament system 
twelve equal temperament fifths lead to note exactly seven octaves above the initial tone this results in perfect fifth that is equivalent to seven equal temperament semitones 
the top of the circle shows the key of major with no sharps or flats 
proceeding clockwise the pitches ascend by fifths 
the key signatures associated with those pitches also change the key of has one sharp the key of has sharps and so on 
similarly proceeding counterclockwise from the top of the circle the notes change by descending fifths and the key signatures change accordingly the key of has one flat the key of has flats and so on 
some keys at the bottom of the circle can be notated either in sharps or in flats 
starting at any pitch and ascending by fifth generates all twelve tones before returning to the beginning pitch class pitch class consists of all of the notes indicated by given letter regardless of octave all for example belong to the same pitch class 
moving counterclockwise the pitches descend by fifth but ascending by perfect fourth will lead to the same note an octave higher therefore in the same pitch class 
moving counter clockwise from could be thought of as descending by fifth to or ascending by fourth to structure and use diatonic key signatures each of the twelve pitches can serve as the tonic of major or minor key and each of these keys will have diatonic scale associated with it 
the circle diagram shows the number of sharps or flats in each key signature with the major key indicated by capital letter and the minor key indicated by lower case letter 
major and minor keys that have the same key signature are referred to as relative major and relative minor of one another 
modulation and chord progression tonal music often modulates to new tonal center whose key signature differs from the original by only one flat or sharp 
these closely related keys are fifth apart from each other and are therefore adjacent in the circle of fifths 
chord progressions also often move between chords whose roots are related by perfect fifth making the circle of fifths useful in illustrating the harmonic distance between chords 
the circle of fifths is used to organize and describe the harmonic function of chords 
chords can progress in pattern of ascending perfect fourths alternately viewed as descending perfect fifths in functional succession 
this can be shown by the circle of fifths in which therefore scale degree ii is closer to the dominant than scale degree iv 
in this view the tonic is considered the end point of chord progression derived from the circle of fifths 
according to richard franko goldman harmony in western music the iv chord is in the simplest mechanisms of diatonic relationships at the greatest distance from in terms of the descending circle of fifths it leads away from rather than toward it 
he states that the progression ii an authentic cadence would feel more final or resolved than iv plagal cadence 
goldman concurs with nattiez who argues that the chord on the fourth degree appears long before the chord on ii and the subsequent final in the progression iv viio iii vi ii and is farther from the tonic there as well 
in this and related articles upper case roman numerals indicate major triads while lower case roman numerals indicate minor triads 
circle closure in non equal tuning systems using the exact ratio of frequencies to define perfect fifth just intonation does not quite result in return to the pitch class of the starting note after going around the circle of fifths 
equal temperament tuning produces fifths that return to tone exactly seven octaves above the initial tone and makes the frequency ratio of each half step the same 
an equal tempered fifth has frequency ratio of or about approximately two cents narrower than justly tuned fifth at ratio of ascending by justly tuned fifths fails to close the circle by an excess of approximately cents roughly quarter of semitone an interval known as the pythagorean comma 
in pythagorean tuning this problem is solved by markedly shortening the width of one of the twelve fifths which makes it severely dissonant 
this anomalous fifth is called the wolf fifth humorous reference to wolf howling an off pitch note 
the quarter comma meantone tuning system uses eleven fifths slightly narrower than the equally tempered fifth and requires much wider and even more dissonant wolf fifth to close the circle 
more complex tuning systems based on just intonation such as limit tuning use at most eight justly tuned fifths and at least three non just fifths some slightly narrower and some slightly wider than the just fifth to close the circle 
other tuning systems use up to tones the original tones and more between them in order to close the circle of fifths 
history the circle of fifths developed in the late and early to theorize the modulation of the baroque era see baroque era 
the first circle of fifths diagram appears in the grammatika of the composer and theorist nikolay diletsky who intended to present music theory as tool for composition 
it was the first of its kind aimed at teaching russian audience how to write western style polyphonic compositions 
circle of fifths diagram was independently created by german composer and theorist johann david heinichen in his neu erfundene und gr ndliche anweisung which he called the musical circle german musicalischer circul 
this was also published in his der general bass in der composition 
heinichen placed the relative minor key next to the major key which did not reflect the actual proximity of keys 
johann mattheson and others attempted to improve this david kellner proposed having the major keys on one circle and the relative minor keys on second inner circle 
this was later developed into chordal space incorporating the parallel minor as well some sources imply that the circle of fifths was known in antiquity by pythagoras 
this is misunderstanding and an anachronism 
tuning by fifths so called pythagorean tuning dates to ancient mesopotamia see music of mesopotamia music theory though they did not extend this to twelve note scale stopping at seven 
the pythagorean comma was calculated by euclid and by chinese mathematicians in the huainanzi see pythagorean comma history 
thus it was known in antiquity that cycle of twelve fifths was almost exactly seven octaves more practically alternating ascending fifths and descending fourths was almost exactly an octave 
however this was theoretical knowledge and was not used to construct repeating twelve tone scale nor to modulate 
this was done later in meantone temperament and twelve tone equal temperament which allowed modulation while still being in tune but did not develop in europe until about 
use in musical pieces from the baroque music era and the classical era of music and in western popular music traditional music and folk music when pieces or songs modulate to new key these modulations are often associated with the circle of fifths 
in practice compositions rarely make use of the entire circle of fifths 
more commonly composers make use of the compositional idea of the cycle of ths when music moves consistently through smaller or larger segment of the tonal structural resources which the circle abstractly represents 
the usual practice is to derive the circle of fifths progression from the seven tones of the diatonic scale rather from the full range of twelve tones present in the chromatic scale 
in this diatonic version of the circle one of the fifths is not true fifth it is tritone or diminished fifth 
between and in the natural diatonic scale 
without sharps or flats 
here is how the circle of fifths derives through permutation from the diatonic major scale and from the natural minor scale the following is the basic sequence of chords that can be built over the major bass line and over the minor adding sevenths to the chords creates greater sense of forward momentum to the harmony baroque era according to richard taruskin arcangelo corelli was the most influential composer to establish the pattern as standard harmonic trope it was precisely in corelli time the late seventeenth century that the circle of fifths was being theorized as the main propellor of harmonic motion and it was corelli more than any one composer who put that new idea into telling practice 
the circle of fifths progression occurs frequently in the music of bach 
in the following from jauchzet gott in allen landen bwv even when the solo bass line implies rather than states the chords involved handel uses circle of fifths progression as the basis for the passacaglia movement from his harpsichord suite no 
baroque composers learnt to enhance the propulsive force of the harmony engendered by the circle of fifths by adding sevenths to most of the constituent chords 
these sevenths being dissonances create the need for resolution thus turning each progression of the circle into simultaneous reliever and re stimulator of harmonic tension hence harnessed for expressive purposes 
striking passages that illustrate the use of sevenths occur in the aria pena tiranna in handel opera amadigi di gaula and in bach keyboard arrangement of alessandro marcello concerto for oboe and strings 
nineteenth century during the nineteenth century composers made use of the circle of fifths to enhance the expressive character of their music 
franz schubert poignant impromptu in flat major contains such passage as does the intermezzo movement from mendelssohn string quartet no robert schumann evocative child falling asleep from his kinderszenen springs surprise at the end of the progression the piece ends on an minor chord instead of the expected tonic minor 
in wagner opera tterd mmerung cycle of fifths progression occurs in the music which transitions from the end of the prologue into the first scene of act set in the imposing hall of the wealthy gibichungs 
status and reputation are written all over the motifs assigned to gunther chief of the gibichung clan jazz and popular music the enduring popularity of the circle of fifths as both form building device and as an expressive musical trope is evident in the number of standard popular songs composed during the twentieth century 
it is also favored as vehicle for improvisation by jazz musicians 
bart howard fly me to the moon the song opens with pattern of descending phrases in essence the hook of the song presented with soothing predictability almost as if the future direction of the melody is dictated by the opening five notes 
the harmonic progression for its part rarely departs from the circle of fifths 
jerome kern all the things you are ray noble cherokee 
many jazz musicians have found this particularly challenging as the middle eight progresses so rapidly through the circle creating series of ii progressions that temporarily pass through several tonalities 
kosmo prevert and mercer autumn leaves the beatles you never give me your money mike oldfield incantations carlos santana europa earth cry heaven smile gloria gaynor will survive pet shop boys it sin donna summer love to love you baby related concepts diatonic circle of fifths the diatonic circle of fifths is the circle of fifths encompassing only members of the diatonic scale 
therefore it contains diminished fifth in major between and see structure implies multiplicity 
the circle progression is commonly circle of fifths through the diatonic chords including one diminished chord 
circle progression in major with chords iv viio iii vi ii is shown below 
chromatic circle the circle of fifths is closely related to the chromatic circle which also arranges the twelve equal tempered pitch classes in circular ordering 
key difference between the two circles is that the chromatic circle can be understood as continuous space where every point on the circle corresponds to conceivable pitch class and every conceivable pitch class corresponds to point on the circle 
by contrast the circle of fifths is fundamentally discrete structure and there is no obvious way to assign pitch classes to each of its points 
in this sense the two circles are mathematically quite different 
however the twelve equal tempered pitch classes can be represented by the cyclic group of order twelve or equivalently the residue classes modulo twelve the group has four generators which can be identified with the ascending and descending semitones and the ascending and descending perfect fifths 
the semitonal generator gives rise to the chromatic circle while the perfect fifth gives rise to the circle of fifths 
relation with chromatic scale the circle of fifths or fourths may be mapped from the chromatic scale by multiplication and vice versa 
to map between the circle of fifths and the chromatic scale in integer notation multiply by and for the circle of fourths multiply by 
here is demonstration of this procedure 
start off with an ordered tuple tone row of integers representing the notes of the chromatic scale 
now multiply the entire tuple by and then apply modulo reduction to each of the numbers subtract from each number as many times as necessary until the number becomes smaller than which is equivalent to which is the circle of fifths 
note that this is enharmonically equivalent to 
enharmonic equivalents theoretical keys and the spiral of fifths equal temperament tuning does not use the exact ratio of frequencies that defines perfect fifth wheras the system of just intonation uses this exact ratio 
ascending by fifths in equal temperament leads to return to the starting pitch class starting with and ascending by fifths leads to another after twelve iterations 
this does not occur if an exact ratio is used just intonation 
the adjustment made in equal temperament tuning is called the pythagorean comma 
because of this difference pitches that are enharmonically equivalent in equal temperament tuning and are not equivalent when using just intonation 
in just intonation the sequence of fifths can therefore be visualized as spiral not circle sequence of twelve fifths results in comma pump by the pythagorean comma visualized as going up level in the spiral 
see also circle closure in non equal tuning systems 
without enharmonic equivalence continuing sequence of fifths results in notes with double accidentals double sharps or double flats 
when using equal temperament these can be replaced by an enharmonically equivalent note 
keys with double sharps or flats in the key signatures are called theoretical keys their use is extremely rare 
notation in these cases is not standardized 
the default behaviour of lilypond pictured above writes single sharps or flats in the circle of fifths order before proceeding to double sharps or flats 
this is the format used in john foulds world requiem op 
which ends with the key signature of major as displayed above 
the sharps in the key signature of major here proceed single sharps or flats in the key signature are sometimes repeated as courtesy 
max reger supplement to the theory of modulation which contains minor key signatures on pp 
these have at the start and also at the end with double flat symbol going the convention of lilypond and foulds would suppress the initial 
sometimes the double signs are written at the beginning of the key signature followed by the single signs 
for example the key signature is notated as 
this convention is used by victor ewald by the program finale software and by some theoretical works 
see also approach chord sonata form well temperament circle of fifths text table pitch constellation multiplicative group of integers modulo notes references barnett gregory 
tonal organization in seventeenth century music theory 
in thomas christensen ed 
the cambridge history of western music theory 
cambridge cambridge university press 
the jazz standards guide to the repertoire 
isbn goldman richard franko 
harmony in western music 
theoretical work of late seventeenth century muscovy nikolai diletskii grammatika and the earliest circle of fifths 
journal of the american musicological society 
between modes and keys german theory 
prelude to musical geometry 
the college mathematics journal 
jstor archived from the original on retrieved nattiez jean jacques 
music and discourse toward semiology of music translated by carolyn abbate 
princeton new jersey princeton university press 
originally published in french as musicologie rale et miologie 
the oxford history of western music music in the seventeenth and eighteenth centuries 
further reading indy vincent 
cours de composition musicale 
paris durand et fils 
between modes and keys german theory 
the complete idiot guide to music theory nd ed 
indianapolis in alpha isbn purwins hendrik 
profiles of pitch classes circularity of relative pitch and key experiments models computational music analysis and perspectives 
berlin technische universit berlin 
purwins hendrik benjamin blankertz and klaus obermayer 
toroidal models in tonal theory and pitch class analysis 
in computing in musicology tonal theory for the digital age 
external links decoding the circle of vths interactive circle of fifths interactive circle of fifths for guitarists
principal component analysis pca is popular technique for analyzing large datasets containing high number of dimensions features per observation increasing the interpretability of data while preserving the maximum amount of information and enabling the visualization of multidimensional data 
formally pca is statistical technique for reducing the dimensionality of dataset 
this is accomplished by linearly transforming the data into new coordinate system where most of the variation in the data can be described with fewer dimensions than the initial data 
many studies use the first two principal components in order to plot the data in two dimensions and to visually identify clusters of closely related data points 
principal component analysis has applications in many fields such as population genetics microbiome studies and atmospheric science 
the principal components of collection of points in real coordinate space are sequence of unit vectors where the th vector is the direction of line that best fits the data while being orthogonal to the first vectors 
here best fitting line is defined as one that minimizes the average squared perpendicular distance from the points to the line 
these directions constitute an orthonormal basis in which different individual dimensions of the data are linearly uncorrelated 
principal component analysis pca is the process of computing the principal components and using them to perform change of basis on the data sometimes using only the first few principal components and ignoring the rest 
in data analysis the first principal component of set of variables presumed to be jointly normally distributed is the derived variable formed as linear combination of the original variables that explains the most variance 
the second principal component explains the most variance in what is left once the effect of the first component is removed and we may proceed through iterations until all the variance is explained 
pca is most commonly used when many of the variables are highly correlated with each other and it is desirable to reduce their number to an independent set 
pca is used in exploratory data analysis and for making predictive models 
it is commonly used for dimensionality reduction by projecting each data point onto only the first few principal components to obtain lower dimensional data while preserving as much of the data variation as possible 
the first principal component can equivalently be defined as direction that maximizes the variance of the projected data 
the th principal component can be taken as direction orthogonal to the first principal components that maximizes the variance of the projected data 
for either objective it can be shown that the principal components are eigenvectors of the data covariance matrix 
thus the principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix 
pca is the simplest of the true eigenvector based multivariate analyses and is closely related to factor analysis 
factor analysis typically incorporates more domain specific assumptions about the underlying structure and solves eigenvectors of slightly different matrix 
pca is also related to canonical correlation analysis cca 
cca defines coordinate systems that optimally describe the cross covariance between two datasets while pca defines new orthogonal coordinate system that optimally describes variance in single dataset 
robust and norm based variants of standard pca have also been proposed 
history pca was invented in by karl pearson as an analogue of the principal axis theorem in mechanics it was later independently developed and named by harold hotelling in the 
depending on the field of application it is also named the discrete karhunen lo ve transform klt in signal processing the hotelling transform in multivariate quality control proper orthogonal decomposition pod in mechanical engineering singular value decomposition svd of invented in the last quarter of the th century eigenvalue decomposition evd of xtx in linear algebra factor analysis for discussion of the differences between pca and factor analysis see ch 
of jolliffe principal component analysis eckart young theorem harman or empirical orthogonal functions eof in meteorological science empirical eigenfunction decomposition sirovich empirical component analysis lorenz quasiharmonic modes brooks et al spectral decomposition in noise and vibration and empirical modal analysis in structural dynamics 
intuition pca can be thought of as fitting dimensional ellipsoid to the data where each axis of the ellipsoid represents principal component 
if some axis of the ellipsoid is small then the variance along that axis is also small 
to find the axes of the ellipsoid we must first center the values of each variable in the dataset on by subtracting the mean of the variable observed values from each of those values 
these transformed values are used instead of the original observed values for each of the variables 
then we compute the covariance matrix of the data and calculate the eigenvalues and corresponding eigenvectors of this covariance matrix 
then we must normalize each of the orthogonal eigenvectors to turn them into unit vectors 
once this is done each of the mutually orthogonal unit eigenvectors can be interpreted as an axis of the ellipsoid fitted to the data 
this choice of basis will transform the covariance matrix into diagonalized form in which the diagonal elements represent the variance of each axis 
the proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues 
biplots and scree plots degree of explained variance are used to explain findings of the pca 
details pca is defined as an orthogonal linear transformation that transforms the data to new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate called the first principal component the second greatest variance on the second coordinate and so on consider an data matrix with column wise zero empirical mean the sample mean of each column has been shifted to zero where each of the rows represents different repetition of the experiment and each of the columns gives particular kind of feature say the results from particular sensor 
mathematically the transformation is defined by set of size of dimensional vectors of weights or coefficients that map each row vector of to new vector of principal component scores given by in such way that the individual variables of considered over the data set successively inherit the maximum possible variance from with each coefficient vector constrained to be unit vector where is usually selected to be strictly less than to reduce dimensionality 
first component in order to maximize variance the first weight vector thus has to satisfy arg max arg max equivalently writing this in matrix form gives arg max arg max since has been defined to be unit vector it equivalently also satisfies arg max the quantity to be maximised can be recognised as rayleigh quotient 
standard result for positive semidefinite matrix such as xtx is that the quotient maximum possible value is the largest eigenvalue of the matrix which occurs when is the corresponding eigenvector 
with found the first principal component of data vector can then be given as score in the transformed co ordinates or as the corresponding vector in the original variables 
further components the th component can be found by subtracting the first principal components from and then finding the weight vector which extracts the maximum variance from this new data matrix arg max it turns out that this gives the remaining eigenvectors of xtx with the maximum values for the quantity in brackets given by their corresponding eigenvalues 
thus the weight vectors are eigenvectors of xtx 
the th principal component of data vector can therefore be given as score tk in the transformed coordinates or as the corresponding vector in the space of the original variables where is the kth eigenvector of xtx 
the full principal components decomposition of can therefore be given as where is by matrix of weights whose columns are the eigenvectors of xtx 
the transpose of is sometimes called the whitening or sphering transformation 
columns of multiplied by the square root of corresponding eigenvalues that is eigenvectors scaled up by the variances are called loadings in pca or in factor analysis 
covariances xtx itself can be recognized as proportional to the empirical sample covariance matrix of the dataset xt 
the sample covariance between two of the different principal components over the dataset is given by where the eigenvalue property of has been used to move from line to line however eigenvectors and corresponding to eigenvalues of symmetric matrix are orthogonal if the eigenvalues are different or can be orthogonalised if the vectors happen to share an equal repeated value 
the product in the final line is therefore zero there is no sample covariance between different principal components over the dataset 
another way to characterise the principal components transformation is therefore as the transformation to coordinates which diagonalise the empirical sample covariance matrix 
in matrix form the empirical covariance matrix for the original variables can be written the empirical covariance matrix between the principal components becomes where is the diagonal matrix of eigenvalues of xtx 
is equal to the sum of the squares over the dataset associated with each component that is tk 
dimensionality reduction the transformation maps data vector from an original space of variables to new space of variables which are uncorrelated over the dataset 
however not all the principal components need to be kept 
keeping only the first principal components produced by using only the first eigenvectors gives the truncated transformation where the matrix tl now has rows but only columns 
in other words pca learns linear transformation where the columns of matrix form an orthogonal basis for the features the components of representation that are decorrelated 
by construction of all the transformed data matrices with only columns this score matrix maximises the variance in the original data that has been preserved while minimising the total squared reconstruction error or such dimensionality reduction can be very useful step for visualising and processing high dimensional datasets while still retaining as much of the variance in the dataset as possible 
for example selecting and keeping only the first two principal components finds the two dimensional plane through the high dimensional dataset in which the data is most spread out so if the data contains clusters these too may be most spread out and therefore most visible to be plotted out in two dimensional diagram whereas if two directions through the data or two of the original variables are chosen at random the clusters may be much less spread apart from each other and may in fact be much more likely to substantially overlay each other making them indistinguishable 
similarly in regression analysis the larger the number of explanatory variables allowed the greater is the chance of overfitting the model producing conclusions that fail to generalise to other datasets 
one approach especially when there are strong correlations between different possible explanatory variables is to reduce them to few principal components and then run the regression against them method called principal component regression 
dimensionality reduction may also be appropriate when the variables in dataset are noisy 
if each column of the dataset contains independent identically distributed gaussian noise then the columns of will also contain similarly identically distributed gaussian noise such distribution is invariant under the effects of the matrix which can be thought of as high dimensional rotation of the co ordinate axes 
however with more of the total variance concentrated in the first few principal components compared to the same noise variance the proportionate effect of the noise is less the first few components achieve higher signal to noise ratio 
pca thus can have the effect of concentrating much of the signal into the first few principal components which can usefully be captured by dimensionality reduction while the later principal components may be dominated by noise and so disposed of without great loss 
if the dataset is not too large the significance of the principal components can be tested using parametric bootstrap as an aid in determining how many principal components to retain 
singular value decomposition the principal components transformation can also be associated with another matrix factorization the singular value decomposition svd of here is an by rectangular diagonal matrix of positive numbers called the singular values of is an by matrix the columns of which are orthogonal unit vectors of length called the left singular vectors of and is by whose columns are orthogonal unit vectors of length and called the right singular vectors of in terms of this factorization the matrix xtx can be written where is the square diagonal matrix with the singular values of and the excess zeros chopped off that satisfies comparison with the eigenvector factorization of xtx establishes that the right singular vectors of are equivalent to the eigenvectors of xtx while the singular values of are equal to the square root of the eigenvalues of xtx 
using the singular value decomposition the score matrix can be written so each column of is given by one of the left singular vectors of multiplied by the corresponding singular value 
this form is also the polar decomposition of efficient algorithms exist to calculate the svd of without having to form the matrix xtx so computing the svd is now the standard way to calculate principal components analysis from data matrix unless only handful of components are required 
as with the eigen decomposition truncated score matrix tl can be obtained by considering only the first largest singular values and their singular vectors the truncation of matrix or using truncated singular value decomposition in this way produces truncated matrix that is the nearest possible matrix of rank to the original matrix in the sense of the difference between the two having the smallest possible frobenius norm result known as the eckart young theorem 
further considerations the singular values in are the square roots of the eigenvalues of the matrix xtx 
each eigenvalue is proportional to the portion of the variance more correctly of the sum of the squared distances of the points from their multidimensional mean that is associated with each eigenvector 
the sum of all the eigenvalues is equal to the sum of the squared distances of the points from their multidimensional mean 
pca essentially rotates the set of points around their mean in order to align with the principal components 
this moves as much of the variance as possible using an orthogonal transformation into the first few dimensions 
the values in the remaining dimensions therefore tend to be small and may be dropped with minimal loss of information see below 
pca is often used in this manner for dimensionality reduction 
pca has the distinction of being the optimal orthogonal transformation for keeping the subspace that has largest variance as defined above 
this advantage however comes at the price of greater computational requirements if compared for example and when applicable to the discrete cosine transform and in particular to the dct ii which is simply known as the dct 
nonlinear dimensionality reduction techniques tend to be more computationally demanding than pca 
pca is sensitive to the scaling of the variables 
if we have just two variables and they have the same sample variance and are completely correlated then the pca will entail rotation by and the weights they are the cosines of rotation for the two variables with respect to the principal component will be equal 
but if we multiply all values of the first variable by then the first principal component will be almost the same as that variable with small contribution from the other variable whereas the second component will be almost aligned with the second original variable 
this means that whenever the different variables have different units like temperature and mass pca is somewhat arbitrary method of analysis 
different results would be obtained if one used fahrenheit rather than celsius for example 
pearson original paper was entitled on lines and planes of closest fit to systems of points in space in space implies physical euclidean space where such concerns do not arise 
one way of making the pca less arbitrary is to use variables scaled so as to have unit variance by standardizing the data and hence use the autocorrelation matrix instead of the autocovariance matrix as basis for pca 
however this compresses or expands the fluctuations in all dimensions of the signal space to unit variance 
mean centering is necessary for performing classical pca to ensure that the first principal component describes the direction of maximum variance 
if mean subtraction is not performed the first principal component might instead correspond more or less to the mean of the data 
mean of zero is needed for finding basis that minimizes the mean square error of the approximation of the data mean centering is unnecessary if performing principal components analysis on correlation matrix as the data are already centered after calculating correlations 
correlations are derived from the cross product of two standard scores scores or statistical moments hence the name pearson product moment correlation 
also see the article by kromrey foster johnson on mean centering in moderated regression much ado about nothing 
since covariances are correlations of normalized variables or standard scores pca based on the correlation matrix of is equal to pca based on the covariance matrix of the standardized version of pca is popular primary technique in pattern recognition 
it is not however optimized for class separability 
however it has been used to quantify the distance between two or more classes by calculating center of mass for each class in principal component space and reporting euclidean distance between center of mass of two or more classes 
the linear discriminant analysis is an alternative which is optimized for class separability 
table of symbols and abbreviations properties and limitations of pca properties some properties of pca include property for any integer consider the orthogonal linear transformation where is element vector and is matrix and let be the variance covariance matrix for then the trace of denoted tr is maximized by taking where consists of the first columns of is the transpose of property consider again the orthonormal transformation with and defined as before 
then tr is minimized by taking where consists of the last columns of the statistical implication of this property is that the last few pcs are not simply unstructured left overs after removing the important pcs 
because these last pcs have variances as small as possible they are useful in their own right 
they can help to detect unsuspected near constant linear relationships between the elements of and they may also be useful in regression in selecting subset of variables from and in outlier detection 
property spectral decomposition of before we look at its usage we first look at diagonal elements var then perhaps the main statistical implication of the result is that not only can we decompose the combined variances of all the elements of into decreasing contributions due to each pc but we can also decompose the whole covariance matrix into contributions from each pc 
although not strictly decreasing the elements of will tend to become smaller as increases as is nonincreasing for increasing whereas the elements of tend to stay about the same size because of the normalization constraints limitations as noted above the results of pca depend on the scaling of the variables 
this can be cured by scaling each feature by its standard deviation so that one ends up with dimensionless features with unital variance the applicability of pca as described above is limited by certain tacit assumptions made in its derivation 
in particular pca can capture linear correlations between the features but fails when this assumption is violated see figure in the reference 
in some cases coordinate transformations can restore the linearity assumption and pca can then be applied see kernel pca 
another limitation is the mean removal process before constructing the covariance matrix for pca 
in fields such as astronomy all the signals are non negative and the mean removal process will force the mean of some astrophysical exposures to be zero which consequently creates unphysical negative fluxes and forward modeling has to be performed to recover the true magnitude of the signals 
as an alternative method non negative matrix factorization focusing only on the non negative elements in the matrices which is well suited for astrophysical observations 
see more at relation between pca and non negative matrix factorization 
pca is at disadvantage if the data has not been standardized before applying the algorithm to it 
pca transforms original data into data that is relevant to the principal components of that data which means that the new data variables cannot be interpreted in the same ways that the originals were 
they are linear interpretations of the original variables 
also if pca is not performed properly there is high likelihood of information loss pca relies on linear model 
if dataset has pattern hidden inside it that is nonlinear then pca can actually steer the analysis in the complete opposite direction of progress 
researchers at kansas state university discovered that the sampling error in their experiments impacted the bias of pca results 
if the number of subjects or blocks is smaller than and or the researcher is interested in pc beyond the first it may be better to first correct for the serial correlation before pca is conducted 
the researchers at kansas state also found that pca could be seriously biased if the autocorrelation structure of the data is not correctly handled 
pca and information theory dimensionality reduction results in loss of information in general 
pca based dimensionality reduction tends to minimize that information loss under certain signal and noise models 
under the assumption that that is that the data vector is the sum of the desired information bearing signal and noise signal one can show that pca can be optimal for dimensionality reduction from an information theoretic point of view 
in particular linsker showed that if is gaussian and is gaussian noise with covariance matrix proportional to the identity matrix the pca maximizes the mutual information between the desired information and the dimensionality reduced output if the noise is still gaussian and has covariance matrix proportional to the identity matrix that is the components of the vector are iid but the information bearing signal is non gaussian which is common scenario pca at least minimizes an upper bound on the information loss which is defined as 
the optimality of pca is also preserved if the noise is iid and at least more gaussian in terms of the kullback leibler divergence than the information bearing signal in general even if the above signal model holds pca loses its information theoretic optimality as soon as the noise becomes dependent 
computing pca using the covariance method the following is detailed description of pca using the covariance method see also here as opposed to the correlation method the goal is to transform given data set of dimension to an alternative data set of smaller dimension equivalently we are seeking to find the matrix where is the karhunen lo ve transform klt of matrix organize the data setsuppose you have data comprising set of observations of variables and you want to reduce the data so that each observation can be described with only variables suppose further that the data are arranged as set of data vectors with each representing single grouped observation of the variables 
write as row vectors each with elements 
place the row vectors into single matrix of dimensions calculate the empirical meanfind the empirical mean along each column place the calculated mean values into an empirical mean vector of dimensions calculate the deviations from the meanmean subtraction is an integral part of the solution towards finding principal component basis that minimizes the mean square error of approximating the data 
hence we proceed by centering the data as follows subtract the empirical mean vector from each row of the data matrix store mean subtracted data in the matrix where is an column vector of all for in some applications each variable column of may also be scaled to have variance equal to see score 
this step affects the calculated principal components but makes them independent of the units used to measure the different variables 
find the covariance matrixfind the empirical covariance matrix from matrix where is the conjugate transpose operator 
if consists entirely of real numbers which is the case in many applications the conjugate transpose is the same as the regular transpose 
the reasoning behind using instead of to calculate the covariance is bessel correction find the eigenvectors and eigenvalues of the covariance matrixcompute the matrix of eigenvectors which diagonalizes the covariance matrix where is the diagonal matrix of eigenvalues of this step will typically involve the use of computer based algorithm for computing eigenvectors and eigenvalues 
these algorithms are readily available as sub components of most matrix algebra systems such as sas matlab mathematica scipy idl interactive data language or gnu octave as well as opencv 
matrix will take the form of an diagonal matrix where is the jth eigenvalue of the covariance matrix and matrix also of dimension contains column vectors each of length which represent the eigenvectors of the covariance matrix the eigenvalues and eigenvectors are ordered and paired 
the jth eigenvalue corresponds to the jth eigenvector 
matrix denotes the matrix of right eigenvectors as opposed to left eigenvectors 
in general the matrix of right eigenvectors need not be the conjugate transpose of the matrix of left eigenvectors rearrange the eigenvectors and eigenvaluessort the columns of the eigenvector matrix and eigenvalue matrix in order of decreasing eigenvalue 
make sure to maintain the correct pairings between the columns in each matrix compute the cumulative energy content for each eigenvectorthe eigenvalues represent the distribution of the source data energy among each of the eigenvectors where the eigenvectors form basis for the data 
the cumulative energy content for the jth eigenvector is the sum of the energy content across all of the eigenvalues from through for select subset of the eigenvectors as basis vectorssave the first columns of as the matrix where use the vector as guide in choosing an appropriate value for the goal is to choose value of as small as possible while achieving reasonably high value of on percentage basis 
for example you may want to choose so that the cumulative energy is above certain threshold like percent 
in this case choose the smallest value of such that project the data onto the new basisthe projected data points are the rows of the matrix that is the first column of is the projection of the data points onto the first principal component the second column is the projection onto the second principal component etc 
derivation of pca using the covariance method let be dimensional random vector expressed as column vector 
without loss of generality assume has zero mean 
we want to find orthonormal transformation matrix so that px has diagonal covariance matrix that is px is random vector with all its distinct components pairwise uncorrelated 
quick computation assuming were unitary yields cov cov hence holds if and only if cov were diagonalisable by this is very constructive as cov is guaranteed to be non negative definite matrix and thus is guaranteed to be diagonalisable by some unitary matrix 
covariance free computation in practical implementations especially with high dimensional data large the naive covariance method is rarely used because it is not efficient due to high computational and memory costs of explicitly determining the covariance matrix 
the covariance free approach avoids the np operations of explicitly calculating and storing the covariance matrix xtx instead utilizing one of matrix free methods for example based on the function evaluating the product xt at the cost of np operations 
iterative computation one way to compute the first principal component efficiently is shown in the following pseudo code for data matrix with zero mean without ever computing its covariance matrix 
random vector of length norm do times vector of length for each row in rts is the eigenvalue error norm exit if error tolerance return this power iteration algorithm simply calculates the vector xt normalizes and places the result back in the eigenvalue is approximated by rt xtx which is the rayleigh quotient on the unit vector for the covariance matrix xtx 
if the largest singular value is well separated from the next largest one the vector gets close to the first principal component of within the number of iterations which is small relative to at the total cost cnp 
the power iteration convergence can be accelerated without noticeably sacrificing the small cost per iteration using more advanced matrix free methods such as the lanczos algorithm or the locally optimal block preconditioned conjugate gradient lobpcg method 
subsequent principal components can be computed one by one via deflation or simultaneously as block 
in the former approach imprecisions in already computed approximate principal components additively affect the accuracy of the subsequently computed principal components thus increasing the error with every new computation 
the latter approach in the block power method replaces single vectors and with block vectors matrices and every column of approximates one of the leading principal components while all columns are iterated simultaneously 
the main calculation is evaluation of the product xt 
implemented for example in lobpcg efficient blocking eliminates the accumulation of the errors allows using high level blas matrix matrix product functions and typically leads to faster convergence compared to the single vector one by one technique 
the nipals method non linear iterative partial least squares nipals is variant the classical power iteration with matrix deflation by subtraction implemented for computing the first few components in principal component or partial least squares analysis 
for very high dimensional datasets such as those generated in the omics sciences for example genomics metabolomics it is usually only necessary to compute the first few pcs 
the non linear iterative partial least squares nipals algorithm updates iterative approximations to the leading scores and loadings and by the power iteration multiplying on every iteration by on the left and on the right that is calculation of the covariance matrix is avoided just as in the matrix free implementation of the power iterations to xtx based on the function evaluating the product xt tx the matrix deflation by subtraction is performed by subtracting the outer product from leaving the deflated residual matrix used to calculate the subsequent leading pcs 
for large data matrices or matrices that have high degree of column collinearity nipals suffers from loss of orthogonality of pcs due to machine precision round off errors accumulated in each iteration and matrix deflation by subtraction 
gram schmidt re orthogonalization algorithm is applied to both the scores and the loadings at each iteration step to eliminate this loss of orthogonality 
nipals reliance on single vector multiplications cannot take advantage of high level blas and results in slow convergence for clustered leading singular values both these deficiencies are resolved in more sophisticated matrix free block solvers such as the locally optimal block preconditioned conjugate gradient lobpcg method 
online sequential estimation in an online or streaming situation with data arriving piece by piece rather than being stored in single batch it is useful to make an estimate of the pca projection that can be updated sequentially 
this can be done efficiently but requires different algorithms 
pca and qualitative variables in pca it is common that we want to introduce qualitative variables as supplementary elements 
for example many quantitative variables have been measured on plants 
for these plants some qualitative variables are available as for example the species to which the plant belongs 
these data were subjected to pca for quantitative variables 
when analyzing the results it is natural to connect the principal components to the qualitative variable species 
for this the following results are produced 
identification on the factorial planes of the different species for example using different colors 
representation on the factorial planes of the centers of gravity of plants belonging to the same species 
for each center of gravity and each axis value to judge the significance of the difference between the center of gravity and origin these results are what is called introducing qualitative variable as supplementary element 
this procedure is detailed in and husson pag and pag few software offer this option in an automatic way 
this is the case of spad that historically following the work of ludovic lebart was the first to propose this option and the package factominer 
applications intelligence the earliest application of factor analysis was in locating and measuring components of human intelligence 
it was believed that intelligence had various uncorrelated components such as spatial intelligence verbal intelligence induction deduction etc and that scores on these could be adduced by factor analysis from results on various tests to give single index known as the intelligence quotient iq 
the pioneering statistical psychologist spearman actually developed factor analysis in for his two factor theory of intelligence adding formal technique to the science of psychometrics 
in thurstone looked for factors of intelligence developing the notion of mental age 
standard iq tests today are based on this early work 
residential differentiation in shevky and williams introduced the theory of factorial ecology which dominated studies of residential differentiation from the to the 
neighbourhoods in city were recognizable or could be distinguished from one another by various characteristics which could be reduced to three by factor analysis 
these were known as social rank an index of occupational status familism or family size and ethnicity cluster analysis could then be applied to divide the city into clusters or precincts according to values of the three key factor variables 
an extensive literature developed around factorial ecology in urban geography but the approach went out of fashion after as being methodologically primitive and having little place in postmodern geographical paradigms 
one of the problems with factor analysis has always been finding convincing names for the various artificial factors 
in flood revived the factorial ecology approach to show that principal components analysis actually gave meaningful answers directly without resorting to factor rotation 
the principal components were actually dual variables or shadow prices of forces pushing people together or apart in cities 
the first component was accessibility the classic trade off between demand for travel and demand for space around which classical urban economics is based 
the next two components were disadvantage which keeps people of similar status in separate neighbourhoods mediated by planning and ethnicity where people of similar ethnic backgrounds try to co locate about the same time the australian bureau of statistics defined distinct indexes of advantage and disadvantage taking the first principal component of sets of key variables that were thought to be important 
these seifa indexes are regularly published for various jurisdictions and are used frequently in spatial analysis 
development indexes pca has been the only formal method available for the development of indexes which are otherwise hit or miss ad hoc undertaking 
the city development index was developed by pca from about indicators of city outcomes in survey of global cities 
the first principal component was subject to iterative regression adding the original variables singly until about of its variation was accounted for 
the index ultimately used about indicators but was good predictor of many more variables 
its comparative value agreed very well with subjective assessment of the condition of each city 
the coefficients on items of infrastructure were roughly proportional to the average costs of providing the underlying services suggesting the index was actually measure of effective physical and social investment in the city 
the country level human development index hdi from undp which has been published since and is very extensively used in development studies has very similar coefficients on similar indicators strongly suggesting it was originally constructed using pca 
population genetics in cavalli sforza and others pioneered the use of principal components analysis pca to summarise data on variation in human gene frequencies across regions 
the components showed distinctive patterns including gradients and sinusoidal waves 
they interpreted these patterns as resulting from specific ancient migration events 
since then pca has been ubiquitous in population genetics with thousands of papers using pca as display mechanism 
genetics varies largely according to proximity so the first two principal components actually show spatial distribution and may be used to map the relative geographical location of different population groups thereby showing individuals who have wandered from their original locations pca in genetics has been technically controversial in that the technique has been performed on discrete non normal variables and often on binary allele markers 
the lack of any measures of standard error in pca are also an impediment to more consistent usage 
in august the molecular biologist eran elhaik published theoretical paper in scientific reports analyzing pca applications 
he concluded that it was easy to manipulate the method which in his view generated results that were erroneous contradictory and absurd 
specifically he argued the results achieved in population genetics were characterized by cherry picking and circular reasoning 
market research and indexes of attitude market research has been an extensive user of pca 
it is used to develop customer satisfaction or customer loyalty scores for products and with clustering to develop market segments that may be targeted with advertising campaigns in much the same way as factorial ecology will locate geographical areas with similar characteristics pca rapidly transforms large amounts of data into smaller easier to digest variables that can be more rapidly and readily analyzed 
in any consumer questionnaire there are series of questions designed to elicit consumer attitudes and principal components seek out latent variables underlying these attitudes 
for example the oxford internet survey in asked people about their attitudes and beliefs and from these analysts extracted four principal component dimensions which they identified as escape social networking efficiency and problem creating another example from joe flood in extracted an attitudinal index toward housing from attitude questions in national survey of households in australia 
the first principal component represented general attitude toward property and home ownership 
the index or the attitude questions it embodied could be fed into general linear model of tenure choice 
the strongest determinant of private renting by far was the attitude index rather than income marital status or household type 
quantitative finance in quantitative finance principal component analysis can be directly applied to the risk management of interest rate derivative portfolios 
trading multiple swap instruments which are usually function of other market quotable swap instruments is sought to be reduced to usually or principal components representing the path of interest rates on macro basis 
converting risks to be represented as those to factor loadings or multipliers provides assessments and understanding beyond that available to simply collectively viewing risks to individual buckets 
pca has also been applied to equity portfolios in similar fashion both to portfolio risk and to risk return 
one application is to reduce portfolio risk where allocation strategies are applied to the principal portfolios instead of the underlying stocks 
second is to enhance portfolio return using the principal components to select stocks with upside potential 
neuroscience variant of principal components analysis is used in neuroscience to identify the specific properties of stimulus that increases neuron probability of generating an action potential 
this technique is known as spike triggered covariance analysis 
in typical application an experimenter presents white noise process as stimulus usually either as sensory input to test subject or as current injected directly into the neuron and records train of action potentials or spikes produced by the neuron as result 
presumably certain features of the stimulus make the neuron more likely to spike 
in order to extract these features the experimenter calculates the covariance matrix of the spike triggered ensemble the set of all stimuli defined and discretized over finite time window typically on the order of ms that immediately preceded spike 
the eigenvectors of the difference between the spike triggered covariance matrix and the covariance matrix of the prior stimulus ensemble the set of all stimuli defined over the same length time window then indicate the directions in the space of stimuli along which the variance of the spike triggered ensemble differed the most from that of the prior stimulus ensemble 
specifically the eigenvectors with the largest positive eigenvalues correspond to the directions along which the variance of the spike triggered ensemble showed the largest positive change compared to the varince of the prior 
since these were the directions in which varying the stimulus led to spike they are often good approximations of the sought after relevant stimulus features 
in neuroscience pca is also used to discern the identity of neuron from the shape of its action potential 
spike sorting is an important procedure because extracellular recording techniques often pick up signals from more than one neuron 
in spike sorting one first uses pca to reduce the dimensionality of the space of action potential waveforms and then performs clustering analysis to associate specific action potentials with individual neurons 
pca as dimension reduction technique is particularly suited to detect coordinated activities of large neuronal ensembles 
it has been used in determining collective variables that is order parameters during phase transitions in the brain 
relation with other methods correspondence analysis correspondence analysis ca was developed by jean paul benz cri and is conceptually similar to pca but scales the data which should be non negative so that rows and columns are treated equivalently 
it is traditionally applied to contingency tables 
ca decomposes the chi squared statistic associated to this table into orthogonal factors 
because ca is descriptive technique it can be applied to tables for which the chi squared statistic is appropriate or not 
several variants of ca are available including detrended correspondence analysis and canonical correspondence analysis 
one special extension is multiple correspondence analysis which may be seen as the counterpart of principal component analysis for categorical data 
factor analysis principal component analysis creates variables that are linear combinations of the original variables 
the new variables have the property that the variables are all orthogonal 
the pca transformation can be helpful as pre processing step before clustering 
pca is variance focused approach seeking to reproduce the total variable variance in which components reflect both common and unique variance of the variable 
pca is generally preferred for purposes of data reduction that is translating variable space into optimal factor space but not when the goal is to detect the latent construct or factors 
factor analysis is similar to principal component analysis in that factor analysis also involves linear combinations of variables 
different from pca factor analysis is correlation focused approach seeking to reproduce the inter correlations among variables in which the factors represent the common variance of variables excluding unique variance 
in terms of the correlation matrix this corresponds with focusing on explaining the off diagonal terms that is shared co variance while pca focuses on explaining the terms that sit on the diagonal 
however as side result when trying to reproduce the on diagonal terms pca also tends to fit relatively well the off diagonal correlations 
results given by pca and factor analysis are very similar in most situations but this is not always the case and there are some problems where the results are significantly different 
factor analysis is generally used when the research purpose is detecting data structure that is latent constructs or factors or causal modeling 
if the factor model is incorrectly formulated or the assumptions are not met then factor analysis will give erroneous results 
means clustering it has been asserted that the relaxed solution of means clustering specified by the cluster indicators is given by the principal components and the pca subspace spanned by the principal directions is identical to the cluster centroid subspace 
however that pca is useful relaxation of means clustering was not new result and it is straightforward to uncover counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions 
non negative matrix factorization non negative matrix factorization nmf is dimension reduction method where only non negative elements in the matrices are used which is therefore promising method in astronomy in the sense that astrophysical signals are non negative 
the pca components are orthogonal to each other while the nmf components are all non negative and therefore constructs non orthogonal basis 
in pca the contribution of each component is ranked based on the magnitude of its corresponding eigenvalue which is equivalent to the fractional residual variance frv in analyzing empirical data 
for nmf its components are ranked based only on the empirical frv curves 
the residual fractional eigenvalue plots that is as function of component number given total of components for pca has flat plateau where no data is captured to remove the quasi static noise then the curves dropped quickly as an indication of over fitting and captures random noise 
the frv curves for nmf is decreasing continuously when the nmf components are constructed sequentially indicating the continuous capturing of quasi static noise then converge to higher levels than pca indicating the less over fitting property of nmf 
iconography of correlations it is often difficult to interpret the principal components when the data include many variables of various origins or when some variables are qualitative 
this leads the pca user to delicate elimination of several variables 
if observations or variables have an excessive impact on the direction of the axes they should be removed and then projected as supplementary elements 
in addition it is necessary to avoid interpreting the proximities between the points close to the center of the factorial plane 
the iconography of correlations on the contrary which is not projection on system of axes does not have these drawbacks 
we can therefore keep all the variables 
the principle of the diagram is to underline the remarkable correlations of the correlation matrix by solid line positive correlation or dotted line negative correlation 
strong correlation is not remarkable if it is not direct but caused by the effect of third variable 
conversely weak correlations can be remarkable 
for example if variable depends on several independent variables the correlations of with each of them are weak and yet remarkable 
generalizations sparse pca particular disadvantage of pca is that the principal components are usually linear combinations of all input variables 
sparse pca overcomes this disadvantage by finding linear combinations that contain just few input variables 
it extends the classic method of principal component analysis pca for the reduction of dimensionality of data by adding sparsity constraint on the input variables 
several approaches have been proposed including regression framework convex relaxation semidefinite programming framework generalized power method framework an alternating maximization framework forward backward greedy search and exact methods using branch and bound techniques bayesian formulation framework the methodological and theoretical developments of sparse pca as well as its applications in scientific studies were recently reviewed in survey paper 
nonlinear pca most of the modern methods for nonlinear dimensionality reduction find their theoretical and algorithmic roots in pca or means 
pearson original idea was to take straight line or plane which will be the best fit to set of data points 
trevor hastie expanded on this concept by proposing principal curves as the natural extension for the geometric interpretation of pca which explicitly constructs manifold for data approximation followed by projecting the points onto it as is illustrated by fig 
see also the elastic map algorithm and principal geodesic analysis 
another popular generalization is kernel pca which corresponds to pca performed in reproducing kernel hilbert space associated with positive definite kernel 
in multilinear subspace learning pca is generalized to multilinear pca mpca that extracts features directly from tensor representations 
mpca is solved by performing pca in each mode of the tensor iteratively 
mpca has been applied to face recognition gait recognition etc 
mpca is further extended to uncorrelated mpca non negative mpca and robust mpca 
way principal component analysis may be performed with models such as tucker decomposition parafac multiple factor analysis co inertia analysis statis and distatis 
robust pca while pca finds the mathematically optimal method as in minimizing the squared error it is still sensitive to outliers in the data that produce large errors something that the method tries to avoid in the first place 
it is therefore common practice to remove outliers before computing pca 
however in some contexts outliers can be difficult to identify 
for example in data mining algorithms like correlation clustering the assignment of points to clusters and outliers is not known beforehand 
recently proposed generalization of pca based on weighted pca increases robustness by assigning different weights to data objects based on their estimated relevancy 
outlier resistant variants of pca have also been proposed based on norm formulations pca robust principal component analysis rpca via decomposition in low rank and sparse matrices is modification of pca that works well with respect to grossly corrupted observations 
similar techniques independent component analysis independent component analysis ica is directed to similar problems as principal component analysis but finds additively separable components rather than successive approximations 
network component analysis given matrix it tries to decompose it into two matrices such that key difference from techniques such as pca and ica is that some of the entries of are constrained to be here is termed the regulatory layer 
while in general such decomposition can have multiple solutions they prove that if the following conditions are satisfied has full column rank each column of must have at least zeroes where is the number of columns of or alternatively the number of rows of 
the justification for this criterion is that if node is removed from the regulatory layer along with all the output nodes connected to it the result must still be characterized by connectivity matrix with full column rank 
must have full row rank then the decomposition is unique up to multiplication by scalar 
discriminant analysis of principal components discriminant analysis of principal components dapc is multivariate method used to identify and describe clusters of genetically related individuals 
genetic variation is partitioned into two components variation between groups and within groups and it maximizes the former 
linear discriminants are linear combinations of alleles which best separate the clusters 
alleles that most contribute to this discrimination are therefore those that are the most markedly different across groups 
the contributions of alleles to the groupings identified by dapc can allow identifying regions of the genome driving the genetic divergence among groups in dapc data is first transformed using principal components analysis pca and subsequently clusters are identified using discriminant analysis da 
dapc can be realized on using the package adegenet 
more info adegenet on the web software source code alglib and library that implements pca and truncated pca analytica the built in eigendecomp function computes principal components 
elki includes pca for projection including robust variants of pca as well as pca based clustering algorithms 
gretl principal component analysis can be performed either via the pca command or via the princomp function 
julia supports pca with the pca function in the multivariatestats package knime java based nodal arranging software for analysis in this the nodes called pca pca compute pca apply pca inverse make it easily 
mathematica implements principal component analysis with the principalcomponents command using both covariance and correlation methods 
mathphp php mathematics library with support for pca 
matlab the svd function is part of the basic system 
in the statistics toolbox the functions princomp and pca give the principal components while the function pcares gives the residuals and reconstructed matrix for low rank pca approximation 
matplotlib python library have pca package in the mlab module 
mlpack provides an implementation of principal component analysis in 
nag library principal components analysis is implemented via the aa routine available in both the fortran versions of the library 
nmath proprietary numerical library containing pca for the net framework 
gnu octave free software computational environment mostly compatible with matlab the function princomp gives the principal component 
opencv oracle database implemented via dbms data mining svds scoring mode by specifying setting value svds scoring pca orange software integrates pca in its visual programming environment 
pca displays scree plot degree of explained variance where user can interactively select the number of principal components 
origin contains pca in its pro version 
qlucore commercial software for analyzing multivariate data with instant response using pca 
free statistical package the functions princomp and prcomp can be used for principal component analysis prcomp uses singular value decomposition which generally gives better numerical accuracy 
some packages that implement pca in include but are not limited to ade vegan exposition dimred and factominer 
sas proprietary software for example see scikit learn python library for machine learning which contains pca probabilistic pca kernel pca sparse pca and other techniques in the decomposition module 
spss proprietary software most commonly used by social scientists for pca factor analysis and associated cluster analysis 
weka java library for machine learning which contains modules for computing principal components 
see also references further reading jackson 
user guide to principal components wiley 
springer series in statistics 
springer series in statistics 
new york springer verlag 
isbn husson fran ois bastien pag me 
exploratory multivariate analysis by example using chapman hall crc the series london 
multiple factor analysis by example using chapman hall crc the series london external links university of copenhagen video by rasmus bro on youtube stanford university video by andrew ng on youtube tutorial on principal component analysis layman introduction to principal component analysis on youtube video of less than seconds 
statquest principal component analysis pca clearly explained on youtube see also the list of software implementations
in statistical modeling regression analysis is set of statistical processes for estimating the relationships between dependent variable often called the outcome or response variable or label in machine learning parlance and one or more independent variables often called predictors covariates explanatory variables or features 
the most common form of regression analysis is linear regression in which one finds the line or more complex linear combination that most closely fits the data according to specific mathematical criterion 
for example the method of ordinary least squares computes the unique line or hyperplane that minimizes the sum of squared differences between the true data and that line or hyperplane 
for specific mathematical reasons see linear regression this allows the researcher to estimate the conditional expectation or population average value of the dependent variable when the independent variables take on given set of values 
less common forms of regression use slightly different procedures to estimate alternative location parameters quantile regression or necessary condition analysis or estimate the conditional expectation across broader collection of non linear models nonparametric regression 
regression analysis is primarily used for two conceptually distinct purposes 
first regression analysis is widely used for prediction and forecasting where its use has substantial overlap with the field of machine learning 
second in some situations regression analysis can be used to infer causal relationships between the independent and dependent variables 
importantly regressions by themselves only reveal relationships between dependent variable and collection of independent variables in fixed dataset 
to use regressions for prediction or to infer causal relationships respectively researcher must carefully justify why existing relationships have predictive power for new context or why relationship between two variables has causal interpretation 
the latter is especially important when researchers hope to estimate causal relationships using observational data 
history the earliest form of regression was the method of least squares which was published by legendre in and by gauss in legendre and gauss both applied the method to the problem of determining from astronomical observations the orbits of bodies about the sun mostly comets but also later the then newly discovered minor planets 
gauss published further development of the theory of least squares in including version of the gauss markov theorem 
the term regression was coined by francis galton in the th century to describe biological phenomenon 
the phenomenon was that the heights of descendants of tall ancestors tend to regress down towards normal average phenomenon also known as regression toward the mean 
for galton regression had only this biological meaning but his work was later extended by udny yule and karl pearson to more general statistical context 
in the work of yule and pearson the joint distribution of the response and explanatory variables is assumed to be gaussian 
this assumption was weakened by fisher in his works of and fisher assumed that the conditional distribution of the response variable is gaussian but the joint distribution need not be 
in this respect fisher assumption is closer to gauss formulation of in the and economists used electromechanical desk calculators to calculate regressions 
before it sometimes took up to hours to receive the result from one regression regression methods continue to be an area of active research 
in recent decades new methods have been developed for robust regression regression involving correlated responses such as time series and growth curves regression in which the predictor independent variable or response variables are curves images graphs or other complex data objects regression methods accommodating various types of missing data nonparametric regression bayesian methods for regression regression in which the predictor variables are measured with error regression with more predictor variables than observations and causal inference with regression 
regression model in practice researchers first select model they would like to estimate and then use their chosen method ordinary least squares to estimate the parameters of that model 
regression models involve the following components the unknown parameters often denoted as scalar or vector the independent variables which are observed in data and are often denoted as vector where denotes row of data 
the dependent variable which are observed in data and often denoted using the scalar the error terms which are not directly observed in data and are often denoted using the scalar in various fields of application different terminologies are used in place of dependent and independent variables 
most regression models propose that is function of and with representing an additive error term that may stand in for un modeled determinants of or random statistical noise the researchers goal is to estimate the function that most closely fits the data 
to carry out regression analysis the form of the function must be specified 
sometimes the form of this function is based on knowledge about the relationship between and that does not rely on the data 
if no such knowledge is available flexible or convenient form for is chosen 
for example simple univariate regression may propose suggesting that the researcher believes to be reasonable approximation for the statistical process generating the data 
once researchers determine their preferred statistical model different forms of regression analysis provide tools to estimate the parameters for example least squares including its most common variant ordinary least squares finds the value of that minimizes the sum of squared errors given regression method will ultimately provide an estimate of usually denoted to distinguish the estimate from the true unknown parameter value that generated the data 
using this estimate the researcher can then use the fitted value for prediction or to assess the accuracy of the model in explaining the data 
whether the researcher is intrinsically interested in the estimate or the predicted value will depend on context and their goals 
as described in ordinary least squares least squares is widely used because the estimated function approximates the conditional expectation 
however alternative variants least absolute deviations or quantile regression are useful when researchers want to model other functions 
it is important to note that there must be sufficient data to estimate regression model 
for example suppose that researcher has access to rows of data with one dependent and two independent variables 
suppose further that the researcher wants to estimate bivariate linear model via least squares if the researcher only has access to data points then they could find infinitely many combinations that explain the data equally well any combination can be chosen that satisfies all of which lead to and are therefore valid solutions that minimize the sum of squared residuals 
to understand why there are infinitely many options note that the system of equations is to be solved for unknowns which makes the system underdetermined 
alternatively one can visualize infinitely many dimensional planes that go through fixed points 
more generally to estimate least squares model with distinct parameters one must have distinct data points 
if then there does not generally exist set of parameters that will perfectly fit the data 
the quantity appears often in regression analysis and is referred to as the degrees of freedom in the model 
moreover to estimate least squares model the independent variables 
must be linearly independent one must not be able to reconstruct any of the independent variables by adding and multiplying the remaining independent variables 
as discussed in ordinary least squares this condition ensures that is an invertible matrix and therefore that unique solution exists 
underlying assumptions by itself regression is simply calculation using the data 
in order to interpret the output of regression as meaningful statistical quantity that measures real world relationships researchers often rely on number of classical assumptions 
these assumptions often include the sample is representative of the population at large 
the independent variables are measured with no error 
deviations from the model have an expected value of zero conditional on covariates the variance of the residuals is constant across observations homoscedasticity 
the residuals are uncorrelated with one another 
mathematically the variance covariance matrix of the errors is diagonal handful of conditions are sufficient for the least squares estimator to possess desirable properties in particular the gauss markov assumptions imply that the parameter estimates will be unbiased consistent and efficient in the class of linear unbiased estimators 
practitioners have developed variety of methods to maintain some or all of these desirable properties in real world settings because these classical assumptions are unlikely to hold exactly 
for example modeling errors in variables can lead to reasonable estimates independent variables are measured with errors 
heteroscedasticity consistent standard errors allow the variance of to change across values of correlated errors that exist within subsets of the data or follow specific patterns can be handled using clustered standard errors geographic weighted regression or newey west standard errors among other techniques 
when rows of data correspond to locations in space the choice of how to model within geographic units can have important consequences 
the subfield of econometrics is largely focused on developing techniques that allow researchers to make reasonable real world conclusions in real world settings where classical assumptions do not hold exactly 
linear regression in linear regression the model specification is that the dependent variable is linear combination of the parameters but need not be linear in the independent variables 
for example in simple linear regression for modeling data points there is one independent variable and two parameters and straight line in multiple linear regression there are several independent variables or functions of independent variables 
adding term in to the preceding regression gives parabola this is still linear regression although the expression on the right hand side is quadratic in the independent variable it is linear in the parameters and in both cases is an error term and the subscript indexes particular observation 
returning our attention to the straight line case given random sample from the population we estimate the population parameters and obtain the sample linear regression model the residual is the difference between the value of the dependent variable predicted by the model and the true value of the dependent variable one method of estimation is ordinary least squares 
this method obtains parameter estimates that minimize the sum of squared residuals ssr minimization of this function results in set of normal equations set of simultaneous linear equations in the parameters which are solved to yield the parameter estimators in the case of simple regression the formulas for the least squares estimates are where is the mean average of the values and is the mean of the values 
under the assumption that the population error term has constant variance the estimate of that variance is given by this is called the mean square error mse of the regression 
the denominator is the sample size reduced by the number of model parameters estimated from the same data for regressors or if an intercept is used 
in this case so the denominator is the standard errors of the parameter estimates are given by under the further assumption that the population error term is normally distributed the researcher can use these estimated standard errors to create confidence intervals and conduct hypothesis tests about the population parameters 
general linear model in the more general multiple regression model there are independent variables where is the th observation on the th independent variable 
if the first independent variable takes the value for all then is called the regression intercept 
the least squares parameter estimates are obtained from normal equations 
the residual can be written as the normal equations are in matrix notation the normal equations are written as where the element of is the element of the column vector is and the element of is thus is is and is the solution is 
diagnostics once regression model has been constructed it may be important to confirm the goodness of fit of the model and the statistical significance of the estimated parameters 
commonly used checks of goodness of fit include the squared analyses of the pattern of residuals and hypothesis testing 
statistical significance can be checked by an test of the overall fit followed by tests of individual parameters 
interpretations of these diagnostic tests rest heavily on the model assumptions 
although examination of the residuals can be used to invalidate model the results of test or test are sometimes more difficult to interpret if the model assumptions are violated 
for example if the error term does not have normal distribution in small samples the estimated parameters will not follow normal distributions and complicate inference 
with relatively large samples however central limit theorem can be invoked such that hypothesis testing may proceed using asymptotic approximations 
limited dependent variables limited dependent variables which are response variables that are categorical variables or are variables constrained to fall only in certain range often arise in econometrics 
the response variable may be non continuous limited to lie on some subset of the real line 
for binary zero or one variables if analysis proceeds with least squares linear regression the model is called the linear probability model 
nonlinear models for binary dependent variables include the probit and logit model 
the multivariate probit model is standard method of estimating joint relationship between several binary dependent variables and some independent variables 
for categorical variables with more than two values there is the multinomial logit 
for ordinal variables with more than two values there are the ordered logit and ordered probit models 
censored regression models may be used when the dependent variable is only sometimes observed and heckman correction type models may be used when the sample is not randomly selected from the population of interest 
an alternative to such procedures is linear regression based on polychoric correlation or polyserial correlations between the categorical variables 
such procedures differ in the assumptions made about the distribution of the variables in the population 
if the variable is positive with low values and represents the repetition of the occurrence of an event then count models like the poisson regression or the negative binomial model may be used 
nonlinear regression when the model function is not linear in the parameters the sum of squares must be minimized by an iterative procedure 
this introduces many complications which are summarized in differences between linear and non linear least squares 
interpolation and extrapolation regression models predict value of the variable given known values of the variables 
prediction within the range of values in the dataset used for model fitting is known informally as interpolation 
prediction outside this range of the data is known as extrapolation 
performing extrapolation relies strongly on the regression assumptions 
the further the extrapolation goes outside the data the more room there is for the model to fail due to differences between the assumptions and the sample data or the true values 
it is generally advised that when performing extrapolation one should accompany the estimated value of the dependent variable with prediction interval that represents the uncertainty 
such intervals tend to expand rapidly as the values of the independent variable moved outside the range covered by the observed data 
for such reasons and others some tend to say that it might be unwise to undertake extrapolation however this does not cover the full set of modeling errors that may be made in particular the assumption of particular form for the relation between and properly conducted regression analysis will include an assessment of how well the assumed form is matched by the observed data but it can only do so within the range of values of the independent variables actually available 
this means that any extrapolation is particularly reliant on the assumptions being made about the structural form of the regression relationship 
best practice advice here is that linear in variables and linear in parameters relationship should not be chosen simply for computational convenience but that all available knowledge should be deployed in constructing regression model 
if this knowledge includes the fact that the dependent variable cannot go outside certain range of values this can be made use of in selecting the model even if the observed dataset has no values particularly near such bounds 
the implications of this step of choosing an appropriate functional form for the regression can be great when extrapolation is considered 
at minimum it can ensure that any extrapolation arising from fitted model is realistic or in accord with what is known 
power and sample size calculations there are no generally agreed methods for relating the number of observations versus the number of independent variables in the model 
one method conjectured by good and hardin is where is the sample size is the number of independent variables and is the number of observations needed to reach the desired precision if the model had only one independent variable 
for example researcher is building linear regression model using dataset that contains patients 
if the researcher decides that five observations are needed to precisely define straight line then the maximum number of independent variables the model can support is because log log 
other methods although the parameters of regression model are usually estimated using the method of least squares other methods which have been used include bayesian methods 
bayesian linear regression percentage regression for situations where reducing percentage errors is deemed more appropriate 
least absolute deviations which is more robust in the presence of outliers leading to quantile regression nonparametric regression requires large number of observations and is computationally intensive scenario optimization leading to interval predictor models distance metric learning which is learned by the search of meaningful distance metric in given input space 
software all major statistical software packages perform least squares regression analysis and inference 
simple linear regression and multiple regression using least squares can be done in some spreadsheet applications and on some calculators 
while many statistical software packages can perform various types of nonparametric and robust regression these methods are less standardized 
different software packages implement different methods and method with given name may be implemented differently in different packages 
specialized regression software has been developed for use in fields such as survey analysis and neuroimaging 
see also references further reading william kruskal and judith tanur ed 
linear hypotheses international encyclopedia of statistics 
free press evan williams regression pp 
analysis of variance pp 
regression and correlation analysis new palgrave dictionary of economics pp 
birkes david and dodge alternative methods of regression 
isbn chatfield calculating interval forecasts journal of business and economic statistics pp 
applied regression analysis rd ed 
applied regression analysis linear models and related methods 
sage hardle applied nonparametric regression isbn meade nigel islam towhidul 
prediction intervals for growth curve forecasts 
sen srivastava regression analysis theory methods and applications springer verlag berlin th printing 
strutz data fitting and uncertainty practical introduction to weighted least squares and beyond 
vieweg teubner isbn stulp freek and olivier sigaud 
many regression algorithms one unified model review 
https doi org neunet 
operations and production systems with multiple objectives 
external links regression analysis encyclopedia of mathematics ems press earliest uses regression basic history and references what is multiple regression used for 
multiple regression regression of weakly correlated data how linear regression mistakes can appear when range is much smaller than range
in machine learning hyperparameter is parameter whose value is used to control the learning process 
by contrast the values of other parameters typically node weights are derived via training 
hyperparameters can be classified as model hyperparameters that cannot be inferred while fitting the machine to the training set because they refer to the model selection task or algorithm hyperparameters that in principle have no influence on the performance of the model but affect the speed and quality of the learning process 
an example of model hyperparameter is the topology and size of neural network 
examples of algorithm hyperparameters are learning rate and batch size as well as mini batch size 
batch size can refer to the full data sample where mini batch size would be smaller sample set 
different model training algorithms require different hyperparameters some simple algorithms such as ordinary least squares regression require none 
given these hyperparameters the training algorithm learns the parameters from the data 
for instance lasso is an algorithm that adds regularization hyperparameter to ordinary least squares regression which has to be set before estimating the parameters through the training algorithm 
considerations the time required to train and test model can depend upon the choice of its hyperparameters 
hyperparameter is usually of continuous or integer type leading to mixed type optimization problems 
the existence of some hyperparameters is conditional upon the value of others 
the size of each hidden layer in neural network can be conditional upon the number of layers 
difficulty learnable parameters usually but not always hyperparameters cannot be learned using well known gradient based methods such as gradient descent lbfgs which are commonly employed to learn parameters 
these hyperparameters are those parameters describing model representation that cannot be learned by common optimization methods but nonetheless affect the loss function 
an example would be the tolerance hyperparameter for errors in support vector machines 
untrainable parameters sometimes hyperparameters cannot be learned from the training data because they aggressively increase the capacity of model and can push the loss function to an undesired minimum overfitting to and picking up noise in the data as opposed to correctly mapping the richness of the structure in the data 
for example if we treat the degree of polynomial equation fitting regression model as trainable parameter the degree would increase until the model perfectly fit the data yielding low training error but poor generalization performance 
tunability most performance variation can be attributed to just few hyperparameters 
the tunability of an algorithm hyperparameter or interacting hyperparameters is measure of how much performance can be gained by tuning it 
for an lstm while the learning rate followed by the network size are its most crucial hyperparameters batching and momentum have no significant effect on its performance although some research has advocated the use of mini batch sizes in the thousands other work has found the best performance with mini batch sizes between and 
robustness an inherent stochasticity in learning directly implies that the empirical hyperparameter performance is not necessarily its true performance 
methods that are not robust to simple changes in hyperparameters random seeds or even different implementations of the same algorithm cannot be integrated into mission critical control systems without significant simplification and robustification reinforcement learning algorithms in particular require measuring their performance over large number of random seeds and also measuring their sensitivity to choices of hyperparameters 
their evaluation with small number of random seeds does not capture performance adequately due to high variance 
some reinforcement learning methods 
ddpg deep deterministic policy gradient are more sensitive to hyperparameter choices than others 
optimization hyperparameter optimization finds tuple of hyperparameters that yields an optimal model which minimizes predefined loss function on given test data 
the objective function takes tuple of hyperparameters and returns the associated loss 
reproducibility apart from tuning hyperparameters machine learning involves storing and organizing the parameters and results and making sure they are reproducible 
in the absence of robust infrastructure for this purpose research code often evolves quickly and compromises essential aspects like bookkeeping and reproducibility 
online collaboration platforms for machine learning go further by allowing scientists to automatically share organize and discuss experiments data and algorithms 
reproducibility can be particularly difficult for deep learning models 
see also hyper heuristic replication crisis references
computerized batch processing is method of running software programs called jobs in batches automatically
while users are required to submit the jobs no other interaction by the user is required to process the batch
batches may automatically be run at scheduled times as well as being run contingent on the availability of computer resources
history the term batch processing originates in the traditional classification of methods of production as job production one off production batch production production of batch of multiple items at once one stage at time and flow production mass production all stages in process at once
early history early computers were capable of running only one program at time
each user had sole control of the machine for scheduled period of time
they would arrive at the computer with program and data often on punched paper cards and magnetic or paper tape and would load their program run and debug it and carry off their output when done
as computers became faster the setup and takedown time became larger percentage of available computer time
programs called monitors the forerunners of operating systems were developed which could process series or batch of programs often from magnetic tape prepared offline
the monitor would be loaded into the computer and run the first job of the batch
at the end of the job it would regain control and load and run the next until the batch was complete
often the output of the batch would be written to magnetic tape and printed or punched offline
examples of monitors were ibm fortran monitor system sos share operating system and finally ibsys for ibm systems in
third generation systems third generation computers capable of multiprogramming began to appear in the
instead of running one batch job at time these systems can have multiple batch programs running at the same time in order to keep the system as busy as possible
one or more programs might be awaiting input one actively running on the cpu and others generating output
instead of offline input and output programs called spoolers read jobs from cards disk or remote terminals and place them in job queue to be run
in order to prevent deadlocks the job scheduler needs to know each job resource requirements memory magnetic tapes mountable disks etc so various scripting languages were developed to supply this information in structured way
probably the most well known is ibm job control language jcl
job schedulers select jobs to run according to variety of criteria including priority memory size etc
remote batch is procedure for submitting batch jobs from remote terminals often equipped with punch card reader and line printer
sometimes asymmetric multiprocessing is used to spool batch input and output for one or more large computers using an attached smaller and less expensive system as in the ibm system attached support processor
later history the first general purpose time sharing system compatible time sharing system ctss was compatible with batch processing
this facilitated transitioning from batch processing to interactive computing from the late onwards interactive computing such as via text based computer terminal interfaces as in unix shells or read eval print loops and later graphical user interfaces became common
non interactive computation both one off jobs such as compilation and processing of multiple items in batches became retrospectively referred to as batch processing and the term batch job in early use often batch of jobs became common
early use is particularly found at the university of michigan around the michigan terminal system mts
although timesharing did exist its use was not robust enough for corporate data processing none of this was related to the earlier unit record equipment which was human operated
ongoing non interactive computation remains pervasive in computing both for general data processing and for system housekeeping tasks using system software
high level program executing multiple programs with some additional glue logic is today most often called script and written in scripting languages particularly shell scripts for system tasks in ibm pc dos and ms dos this is instead known as batch file
that includes unix based computers microsoft windows macos whose foundation is the bsd unix kernel and even smartphones
running script particularly one executed from an interactive login session is often known as job but that term is used very ambiguously
there is no direct counterpart to os batch processing in pc or unix systems
batch jobs are typically executed at scheduled time or on an as needed basis
perhaps the closest comparison is with processes run by an at or cron command in unix although the differences are significant
modern systems batch applications are still critical in most organizations in large part because many common business processes are amenable to batch processing
while online systems can also function when manual intervention is not desired they are not typically optimized to perform high volume repetitive tasks
therefore even new systems usually contain one or more batch applications for updating information at the end of the day generating reports printing documents and other non interactive tasks that must complete reliably within certain business deadlines
some applications are amenable to flow processing namely those that only need data from single input at once not totals for instance start the next step for each input as it completes the previous step
in this case flow processing lowers latency for individual inputs allowing them to be completed without waiting for the entire batch to finish
however many applications require data from all records notably computations such as totals
in this case the entire batch must be completed before one has usable result partial results are not usable
modern batch applications make use of modern batch frameworks such as jem the bee spring batch or implementations of jsr written for java and other frameworks for other programming languages to provide the fault tolerance and scalability required for high volume processing
in order to ensure high speed processing batch applications are often integrated with grid computing solutions to partition batch job over large number of processors although there are significant programming challenges in doing so
high volume batch processing places particularly heavy demands on system and application architectures as well
architectures that feature strong input output performance and vertical scalability including modern mainframe computers tend to provide better batch performance than alternatives
scripting languages became popular as they evolved along with batch processing
batch window batch window is period of less intensive online activity when the computer system is able to run batch jobs without interference from or with interactive online systems
bank end of day eod jobs require the concept of cutover where transaction and data are cut off for particular day batch activity deposits after pm will be processed the next day
as requirements for online systems uptime expanded to support globalization the internet and other business needs the batch window shrank and increasing emphasis was placed on techniques that would require online data to be available for maximum amount of time
batch size the batch size refers to the number of work units to be processed within one batch operation
some examples are the number of lines from file to load into database before committing the transaction
the number of messages to dequeue from queue
the number of requests to send within one payload
common batch processing usage efficient bulk database updates and automated transaction processing as contrasted to interactive online transaction processing oltp applications
the extract transform load etl step in populating data warehouses is inherently batch process in most implementations
performing bulk operations on digital images such as resizing conversion watermarking or otherwise editing group of image files
converting computer files from one format to another
for example batch job may convert proprietary and legacy files to common standard formats for end user queries and display
notable batch scheduling and execution environments the ibm mainframe os operating system or platform has arguably the most highly refined and evolved set of batch processing facilities owing to its origins long history and continuing evolution
today such systems commonly support hundreds or even thousands of concurrent online and batch tasks within single operating system image
technologies that aid concurrent batch and online processing include job control language jcl scripting languages such as rexx job entry subsystem jes and jes workload manager wlm automatic restart manager arm resource recovery services rrs ibm db data sharing parallel sysplex unique performance optimizations such as hiperdispatch channel architecture and several others
the unix programs cron at and batch today batch is variant of at allow for complex scheduling of jobs
windows has job scheduler
most high performance computing clusters use batch processing to maximize cluster usage
see also background process batch file batch renaming to rename lots of files automatically without human intervention in order to save time and effort batchpipes for utility that increases batch performance processing modes production support for batch job schedule stream support high throughput computing notes references
among alternative guitar tunings regular tunings have equal musical intervals between the paired notes of their successive open strings
guitar tunings assign pitches to the open strings of guitars
tunings can be described by the particular pitches that are denoted by notes in western music
by convention the notes are ordered from lowest to highest
the standard tuning defines the string pitches as and between the open strings of the standard tuning are three perfect fourths then the major third and the fourth perfect fourth
in contrast regular tunings have constant intervals between their successive open strings semitones minor third minor thirds or diminished tuning semitones major third major thirds or augmented tuning semitones perfect fourth all fourths tuning semitones augmented fourth tritone or diminished fifth augmented fourths tuning semitones perfect fifth all fifths tuningfor the regular tunings chords may be moved diagonally around the fretboard as well as vertically for the repetitive regular tunings minor thirds major thirds and augmented fourths
regular tunings thus often appeal to new guitarists and also to jazz guitarists as they facilitate key transpositions without requiring completely new set of fingerings for the new key
on the other hand some conventional major minor system chords are easier to play in standard tuning than in regular tuning
left handed guitarists may use the chord charts from one class of regular tunings for its left handed tuning for example the chord charts for all fifths tuning may be used for guitars strung with left handed all fourths tuning
the class of regular tunings has been named and described by professor william sethares
sethares chapter regular tunings in his revised alternate tuning guide is the leading source for this article
this article descriptions of particular regular tunings use other sources also
standard and alternative guitar tunings review this summary of standard tuning also introduces the terms for discussing alternative tunings
standard standard tuning has the following open string notes in standard tuning the separation of the second and third string is by major third interval which has width of four semitones
the irregularity has price
chords cannot be shifted around the fretboard in the standard tuning which requires four chord shapes for the major chords
there are separate chord forms for chords having their root note on the third fourth fifth and sixth strings
alternative alternative alternate tuning refers to any open string note arrangement other than standard tuning
such alternative tuning arrangements offer different chord voicing and sonorities
alternative tunings necessarily change the chord shapes associated with standard tuning which eases the playing of some often non standard chords at the cost of increasing the difficulty of some traditionally voiced chords
as with other scordatura tuning regular tunings may require re stringing the guitar with different string gauges
for example all fifths tuning has been difficult to implement on conventional guitars due to the extreme high pitch required from the top string
even common approximation to all fifths tuning new standard tuning requires special set of strings
properties with standard tuning and with all tunings chord patterns can be moved twelve frets down where the notes repeat in higher octave
for the standard tuning there is exactly one interval of third between the second and third strings and all the other intervals are fourths
working around the irregular third of standard tuning guitarists have to memorize chord patterns for at least three regions the first four strings tuned in perfect fourths two or more fourths and the third and one or more initial fourths the third and the last fourth
in contrast regular tunings have constant intervals between their successive open strings
in fact the class of each regular tuning is characterized by its musical interval as shown by the following list semitones minor third minor thirds tuning semitones major third major thirds tuning semitones perfect fourth all fourths tuning semitones augmented fourth tritone or diminished fifth augmented fourths tuning semitones perfect fifth all fifths tuningthe regular tunings whose number of semitones divides the number of notes in the octave repeat their open string notes raised one octave after strings for example having three semitones in its interval minor thirds tuning repeats its open notes after four strings having four semitones in its interval major thirds tuning repeats its open notes after three strings having six semitones in its interval augmented fourths tuning repeats its notes after two strings regular tunings have symmetrical scales all along the fretboard
this makes it simpler to translate chords into new keys
for the regular tunings chords may be moved diagonally around the fretboard
the shifting of chords is especially simple for the regular tunings that repeat their open strings in which case chords can be moved vertically chords can be moved three strings up or down in major thirds tuning and chords can be moved two strings up or down in augmented fourths tuning
regular tunings thus appeal to new guitarists and also to jazz guitarists whose improvisation is simplified by regular intervals
particular conventional chords are more difficult to play on the other hand particular traditional chords may be more difficult to play in regular tuning than in standard tuning
it can be difficult to play conventional chords especially in augmented fourths tuning and all fifths tuning in which the wide tritone and perfect fifth intervals require hand stretching
some chords that are conventional in folk music are difficult to play even in all fourths and major thirds tunings which do not require more hand stretching than standard tuning
on the other hand minor thirds tuning features many barre chords with repeated notes properties that appeal to beginners
frets covered by the hand the chromatic scale climbs from one string to the next after number of frets that is specific to each regular tuning
the chromatic scale climbs after exactly four frets in major thirds tuning so reducing the extensions of the little and index fingers hand stretching
for other regular tunings the successive strings have intervals that are minor thirds perfect fourths augmented fourths or perfect fifths thus the fretting hand covers three five six or seven frets respectively to play chromatic scale
of course the lowest chromatic scale uses the open strings and so requires one less fret to be covered
examples the following regular tunings are discussed by sethares who also mentions other regular tunings that are difficult to play or have had little musical interest to date
minor thirds or in each minor thirds tuning every interval between successive strings is minor third
thus each repeats its open notes after four strings
in the minor thirds tuning beginning with the open strings contain the notes gb of the diminished triad minor thirds tuning features many barre chords with repeated notes properties that appeal to acoustic guitarists and to beginners
doubled notes have different sounds because of differing string widths tensions and tunings and they reinforce each other like the doubled strings of twelve string guitar add chorusing and depth according to william sethares achieving the same range as standard tuned guitar using minor thirds tuning would require nine string guitar
major thirds major thirds tuning is regular tuning in which the musical intervals between successive strings are each major thirds
like minor thirds tuning and unlike all fourths and all fifths tuning major thirds tuning is repetitive tuning it repeats its octave after three strings which again simplifies the learning of chords and improvisation similarly minor thirds tuning repeats itself after four strings while augmented fourths tuning repeats itself after two strings
neighboring the standard tuning is the all thirds tuning that has the open strings or with six strings major thirds tuning has smaller range than standard tuning with seven strings the major thirds tuning covers the range of standard tuning on six strings
with the repetition of three open string notes each major thirds tuning provides the guitarist with many options for fingering chords
indeed the fingering of two successive frets suffices to play pure major and minor chords while the fingering of three successive frets suffices to play seconds fourths sevenths and ninths for the standard western guitar which has six strings major thirds tuning has smaller range than standard tuning on guitar with seven strings the major thirds tuning covers the range of standard tuning on six strings
even greater range is possible with guitars with eight strings major thirds tuning was heavily used in by the american jazz guitarist ralph patt to facilitate his style of improvisation
all fourths this tuning is like that of the lowest four strings in standard tuning
consequently of all the regular tunings it is the closest approximation to standard tuning and thus it best allows the transfer of knowledge of chords from standard tuning to regular tuning
jazz musician stanley jordan plays guitar in all fourths tuning he has stated that all fourths tuning simplifies the fingerboard making it logical for all fourths tuning all twelve major chords in the first or open positions are generated by two chords the open major chord and the major chord
the regularity of chord patterns reduces the number of finger positions that need to be memorized the left handed involute of an all fourths tuning is an all fifths tuning
all fourths tuning is based on the perfect fourth five semitones and all fifths tuning is based on the perfect fifth seven semitones
consequently chord charts for all fifths tunings may be used for left handed all fourths tuning
augmented fourths and etc
between the all fifths and all fourths tunings are augmented fourth tunings which are also called diminished fifths or tritone tunings
it is repetitive tuning that repeats its notes after two strings
with augmented fourths tunings the fretboard has greatest symmetry
in fact every augmented fourths tuning lists the notes of all the other augmented fourths tunings on the frets of its fretboard
professor sethares wrote that the augmented fourth interval is the only interval whose inverse is the same as itself
the augmented fourths tuning is the only tuning other than the trivial tuning for which all chords forms remain unchanged when the strings are reversed
thus the augmented fourths tuning is its own lefty tuning
of all the augmented fourths tunings the tuning is the closest approximation to the standard tuning and its fretboard is displayed next an augmented fourths tuning makes it very easy for playing half whole scales diminished licks and whole tone scales stated guitarist ron jarzombek
all fifths mandoguitar all fifths tuning is tuning in intervals of perfect fifths like that of mandolin cello or violin other names include perfect fifths and fifths
consequently classical compositions written for violin or guitar may be adapted to all fifths tuning more easily than to standard tuning
when he was asked whether tuning in fifths facilitates new intervals or harmonies that aren readily available in standard tuning robert fripp responded it more rational system but it also better sounding better for chords better for single notes
to build chords fripp uses perfect intervals in fourths fifths and octaves so avoiding minor thirds and especially major thirds which are sharp in equal temperament tuning in comparison to thirds in just intonation
it is challenge to adapt conventional guitar chords to new standard tuning which is based on all fifths tuning
some closely voiced jazz chords become impractical in nst and all fifths tuning it has wide range thus its implementation can be difficult
the high requires taut thin string and consequently is prone to breaking
this can be ameliorated by using shorter scale length guitar by shifting to different key or by shifting down fifth
all fifths tuning was used by the jazz guitarist carl kress
the left handed involute of an all fifths tuning is an all fourths tuning
all fifths tuning is based on the perfect fifth seven semitones and all fourths tuning is based on the perfect fourth five semitones
consequently chord charts for all fifths tunings are used for left handed all fourths tuning all fifths tuning has been approximated with tunings in the through the looking glass guitar of kei nakano which has been played by him since this new tuning is like mirror to all kinds of string instruments including guitar
also it can adapt to any other tunings of guitar
if tuned to normal guitar for the right handed person it is able to use for lefty guitar in general and vice versa
new standard tuning all fifths tuning has been approximated with tunings that avoid the high or the low
the has been replaced with in the new standard tuning nst of king crimson robert fripp
the original version of nst was all fifths tuning
however in the fripp never attained the all fifth high
while he could attain the string life time distribution was too short
experimenting with string fripp succeeded
originally seen in ths
all the way the top string would not go to so as on tenor banjo adopted an on the first string
these kept breaking so was adopted
in fripp experimented with string if successful the experiment could lead to the nst cgdae according to fripp
fripp nst has been taught in guitar craft courses
guitar craft and its successor guitar circle have taught fripp tuning to three thousand students
extreme intervals for regular tunings intervals wider than perfect fifth or narrower than minor third have thus far had limited interest
wide intervals two regular tunings based on sixths having intervals of minor sixths eight semitones and of major sixths nine semitones have received scholarly discussion
the chord charts for minor sixths tuning are useful for left handed guitarists playing in major thirds tuning the chord charts for major sixths tuning for left handed guitarists playing in minor thirds tuning the regular tunings with minor seventh ten semitones or major seventh eleven semitones intervals would make conventional major minor chord playing very difficult as would octave intervals
narrow intervals there are regular tunings that have as their intervals either zero semi tones unison one semi tone minor second or two semi tones major second
these tunings tend to increase the difficulty in playing the major minor system chords of conventionally tuned guitars the trivial class of unison tunings such as are each their own left handed tuning
unison tunings are briefly discussed in the article on ostrich tunings
having exactly one note unison tunings are also ostrich tunings which have exactly one pitch class but may have two or more octaves for example and non unison ostrich tunings are not regular
left handed involution the class of regular tunings is preserved under the involution from right handed to left handed tunings as observed by william sethares
the present discussion of left handed tunings is of interest to musical theorists mathematicians and left handed persons but may be skipped by other readers
for left handed guitars the ordering of the strings reverses the ordering of the strings for right handed guitars
for example the left handed involute of the standard tuning is the lefty tuning
similarly the left handed involute of the lefty tuning is the standard righty tuning the reordering of open strings in left handed tunings has an important consequence
the chord fingerings for the right handed tunings must be changed for left handed tunings
however the left handed involute of regular tuning is easily recognized it is another regular tuning
thus the chords for the involuted regular tuning may be used for the left handed involute of regular tuning
for example the left handed version of all fourths tuning is all fifths tuning and the left handed version of all fifths tuning is all fourths tuning
in general the left handed involute of the regular tuning based on the interval with semitones is the regular tuning based on its involuted interval with semitones all fourths tuning is based on the perfect fourth five semitones and all fifths tuning is based on the perfect fifth seven semitones as mentioned previously
the following table summarizes the lefty righty pairings discussed by sethares
the left handed involute of left handed involute is the original right handed tuning
the left handed version of the trivial tuning is also
among non trivial tunings only the class of augmented fourths tunings is fixed under the lefty involution
summary the principal regular tunings have their properties summarized in the following table notes references denyer ralph
playing the guitar how the guitar is tuned pp
and alternative tunings pp
special contributors isaac guillory and alastair crawford fully revised and updated ed
london and sydney pan books
griewank andreas january tuning guitars and reading music in major thirds matheon preprints vol
berlin germany dfg research center matheon mathematics for key technologies berlin msc classification arts
postscript file and pdf file archived from the original on november sethares bill
alternate tuning guide pdf
madison wisconsin university of wisconsin department of electrical engineering
retrieved may sethares bill january
alternate tuning guide pdf
madison wisconsin university of wisconsin department of electrical engineering
retrieved may sethares william
madison wisconsin university of wisconsin department of electrical engineering
further reading allen warren september december
wa encyclopedia of guitar tunings
archived from the original on july retrieved june
recommended by marcus gary
guitar zero the science of learning to be musical
alternate tuning guide interactive
retrieved june uses wolfram cdf player
guitar tunings comprehensive guide
external links major thirds professors andreas griewank and william sethares each recommend discussions of major thirds tuning by two jazz guitarists sethares regular tunings and griewank ole kirkeby for and string guitars charts of intervals major chords and minor chords and recommended gauges for strings
ralph patt for and string guitars charts of scales chords and chord progressions
all fourths yahoo group for all fourths tuning new standard tuning courses in new standard tuning are offered by guitar circle the successor of guitar craft guitar circle of europe guitar circle of latin america guitar circle of north america
in music theory major chord is chord that has root major third and perfect fifth
when chord comprises only these three notes it is called major triad
for example the major triad built on called major triad has pitches in harmonic analysis and on lead sheets major chord can be notated as cm or cmaj
major triad is represented by the integer notation
major triad can also be described by its intervals the interval between the bottom and middle notes is major third and the interval between the middle and top notes is minor third
by contrast minor triad has minor third interval on the bottom and major third interval on top
they both contain fifths because major third four semitones plus minor third three semitones equals perfect fifth seven semitones
chords that are constructed of consecutive or stacked thirds are called tertian
in western classical music from to and in western pop folk and rock music major chord is usually played as triad
along with the minor triad the major triad is one of the basic building blocks of tonal music in the western common practice period and western pop folk and rock music
it is considered consonant stable or not requiring resolution
in western music minor chord sounds darker than major chord giving off sense of sadness or somber feeling some major chords with additional notes such as the major seventh chord are also called major chords
major seventh chords are used in jazz and occasionally in rock music
in jazz major chords may also have other chord tones added such as the ninth and the thirteenth scale degrees
inversions given major chord may be voiced in many ways
for example the notes of major triad may be arranged in many different vertical orders and the chord will still be major triad
however if the lowest note
the bass note is not the root of the chord then the chord is said to be in an inversion it is in root position if the lowest note is the root of the chord it is in first inversion if the lowest note is its third and it is in second inversion if the lowest note is its fifth
these inversions of major triad are shown below
the additional notes above the bass note can be in any order and the chord still retains its inversion identity
for example major chord is considered to be in first inversion if its lowest note is regardless of how the notes above it are arranged or even doubled
major chord table in this table the chord names are in the leftmost column
the chords are given in root position
for given chord name the following three columns indicate the individual notes that make up this chord
thus in the first row the chord is major which is made up of the individual pitches and just intonation most western keyboard instruments are tuned to equal temperament
in equal temperament each semitone is the same distance apart and there are four semitones between the root and third three between the third and fifth and seven between the root and fifth
another tuning system that is used is just intonation
in just intonation major chord is tuned to the frequency ratio this may be found on iv vi iii and vi
in equal temperament the fifth is only two cents narrower than the just perfect fifth but the major third is noticeably different at about cents wider
see also major and minor musical tuning minor chord otonality and utonality references external links media related to major chords at wikimedia commons major triads explained on virtual piano major chords explained on virtual piano
in mathematics and statistics the arithmetic mean air ith met ik or arithmetic average or just the mean or the average when the context is clear is the sum of collection of numbers divided by the count of numbers in the collection 
the collection is often set of results of an experiment or an observational study or frequently set of results from survey 
the term arithmetic mean is preferred in some contexts in mathematics and statistics because it helps distinguish it from other means such as the geometric mean and the harmonic mean 
in addition to mathematics and statistics the arithmetic mean is used frequently in many diverse fields such as economics anthropology and history and it is used in almost every academic field to some extent 
for example per capita income is the arithmetic average income of nation population 
while the arithmetic mean is often used to report central tendencies it is not robust statistic meaning that it is greatly influenced by outliers values that are very much larger or smaller than most of the values 
for skewed distributions such as the distribution of income for which few people incomes are substantially greater than most people the arithmetic mean may not coincide with one notion of middle and robust statistics such as the median may provide better description of central tendency 
definition given data set the arithmetic mean or mean or average denoted read bar is the mean of the values the arithmetic mean is the most commonly used and readily understood measure of central tendency in data set 
in statistics the term average refers to any of the measures of central tendency 
the arithmetic mean of set of observed data is defined as being equal to the sum of the numerical values of each and every observation divided by the total number of observations 
symbolically if we have data set consisting of the values then the arithmetic mean is defined by the formula for an explanation of the summation operator see summation 
for example if the monthly salaries of employees of firm are then the arithmetic mean is if the data set is statistical population consists of every possible observation and not just subset of them then the mean of that population is called the population mean and denoted by the greek letter if the data set is statistical sample subset of the population it is called the sample mean which for data set is denoted as 
the arithmetic mean can be similarly defined for vectors in multiple dimension not only scalar values this is often referred to as centroid 
more generally because the arithmetic mean is convex combination coefficients sum to it can be defined on convex space not only vector space 
motivating properties the arithmetic mean has several properties that make it useful especially as measure of central tendency 
these include if numbers have mean then since is the distance from given number to the mean one way to interpret this property is as saying that the numbers to the left of the mean are balanced by the numbers to the right of the mean 
the mean is the only single number for which the residuals deviations from the estimate sum to zero 
if it is required to use single number as typical value for set of known numbers then the arithmetic mean of the numbers does this best in the sense of minimizing the sum of squared deviations from the typical value the sum of 
it follows that the sample mean is also the best single predictor in the sense of having the lowest root mean squared error 
if the arithmetic mean of population of numbers is desired then the estimate of it that is unbiased is the arithmetic mean of sample drawn from the population 
additional properties avg avg the arithmetic mean of any amount of equal sized number groups together is the arithmetic mean of the arithmetic means of each group 
contrast with median the arithmetic mean may be contrasted with the median 
the median is defined such that no more than half the values are larger than and no more than half are smaller than the median 
if elements in the data increase arithmetically when placed in some order then the median and arithmetic average are equal 
for example consider the data sample the average is as is the median 
however when we consider sample that cannot be arranged so as to increase arithmetically such as the median and arithmetic average can differ significantly 
in this case the arithmetic average is while the median is in general the average value can vary significantly from most values in the sample and can be larger or smaller than most of them 
there are applications of this phenomenon in many fields 
for example since the the median income in the united states has increased more slowly than the arithmetic average of income 
generalizations weighted average weighted average or weighted mean is an average in which some data points count more heavily than others in that they are given more weight in the calculation 
for example the arithmetic mean of and is or equivalently in contrast weighted mean in which the first number receives for example twice as much weight as the second perhaps because it is assumed to appear twice as often in the general population from which these numbers were sampled would be calculated as here the weights which necessarily sum to the value one are and the former being twice the latter 
the arithmetic mean sometimes called the unweighted average or equally weighted average can be interpreted as special case of weighted average in which all the weights are equal to each other equal to in the above example and equal to in situation with numbers being averaged 
continuous probability distributions if numerical property and any sample of data from it could take on any value from continuous range instead of for example just integers then the probability of number falling into some range of possible values can be described by integrating continuous probability distribution across this range even when the naive probability for sample number taking one certain value from infinitely many is zero 
the analog of weighted average in this context in which there are an infinite number of possibilities for the precise value of the variable in each range is called the mean of the probability distribution 
most widely encountered probability distribution is called the normal distribution it has the property that all measures of its central tendency including not just the mean but also the aforementioned median and the mode the three ms are equal to each other 
this equality does not hold for other probability distributions as illustrated for the log normal distribution here 
angles particular care is needed when using cyclic data such as phases or angles 
naively taking the arithmetic mean of and yields result of 
this is incorrect for two reasons firstly angle measurements are only defined up to an additive constant of or if measuring in radians 
thus these could easily be called and or and since each one of them gives different average 
secondly in this situation equivalently is geometrically better average value there is lower dispersion about it the points are both from it and from the putative average in general application such an oversight will lead to the average value artificially moving towards the middle of the numerical range 
solution to this problem is to use the optimization formulation viz define the mean as the central point the point about which one has the lowest dispersion and redefine the difference as modular distance the distance on the circle so the modular distance between and is not 
symbols and encoding the arithmetic mean is often denoted by bar 
vinculum or macron for example as in read bar some software text processors web browsers may not display the symbol properly 
for example the symbol in html is actually combination of two codes the base letter plus code for the line above or in some texts such as pdfs the symbol may be replaced by cent symbol unicode when copied to text processor such as microsoft word 
see also fr chet mean generalized mean geometric mean harmonic mean inequality of arithmetic and geometric means sample mean and covariance standard deviation standard error of the mean summary statistics references further reading huff darrell 
how to lie with statistics 
external links calculations and comparisons between arithmetic mean and geometric mean of two numbers calculate the arithmetic mean of series of numbers on fxsolver
the sample mean or empirical mean and the sample covariance are statistics computed from sample of data on one or more random variables 
the sample mean is the average value or mean value of sample of numbers taken from larger population of numbers where population indicates not number of people but the entirety of relevant data whether collected or not 
sample of companies sales from the fortune might be used for convenience instead of looking at the population all companies sales 
the sample mean is used as an estimator for the population mean the average value in the entire population where the estimate is more likely to be close to the population mean if the sample is large and representative 
the reliability of the sample mean is estimated using the standard error which in turn is calculated using the variance of the sample 
if the sample is random the standard error falls with the size of the sample and the sample mean distribution approaches the normal distribution as the sample size increases 
the term sample mean can also be used to refer to vector of average values when the statistician is looking at the values of several variables in the sample 
the sales profits and employees of sample of fortune companies 
in this case there is not just sample variance for each variable but sample variance covariance matrix or simply covariance matrix showing also the relationship between each pair of variables 
this would be matrix when variables are being considered 
the sample covariance is useful in judging the reliability of the sample means as estimators and is also useful as an estimate of the population covariance matrix 
due to their ease of calculation and other desirable characteristics the sample mean and sample covariance are widely used in statistics to represent the location and dispersion of the distribution of values in the sample and to estimate the values for the population 
definition of the sample mean the sample mean is the average of the values of variable in sample which is the sum of those values divided by the number of values 
using mathematical notation if sample of observations on variable is taken from the population the sample mean is under this definition if the sample is taken from the population then the sample mean is as compared to the population mean of even if sample is random it is rarely perfectly representative and other samples would have other sample means even if the samples were all from the same population 
the sample for example would have sample mean of if the statistician is interested in variables rather than one each observation having value for each of those variables the overall sample mean consists of sample means for individual variables 
let be the ith independently drawn observation on the jth random variable 
these observations can be arranged into column vectors each with entries with the column vector giving the th observations of all variables being denoted 
the sample mean vector is column vector whose th element is the average value of the observations of the jth variable thus the sample mean vector contains the average of the observations for each variable and is written definition of sample covariance the sample covariance matrix is by matrix with entries where is an estimate of the covariance between the jth variable and the kth variable of the population underlying the data 
in terms of the observation vectors the sample covariance is 
alternatively arranging the observation vectors as the columns of matrix so that which is matrix of rows and columns 
here the sample covariance matrix can be computed as where is an by vector of ones 
if the observations are arranged as rows instead of columns so is now row vector and is an matrix whose column is the vector of observations on variable then applying transposes in the appropriate places yields 
like covariance matrices for random vector sample covariance matrices are positive semi definite 
to prove it note that for any matrix the matrix is positive semi definite 
furthermore covariance matrix is positive definite if and only if the rank of the 
vectors is unbiasedness the sample mean and the sample covariance matrix are unbiased estimates of the mean and the covariance matrix of the random vector row vector whose jth element is one of the random variables 
the sample covariance matrix has in the denominator rather than due to variant of bessel correction in short the sample covariance relies on the difference between each observation and the sample mean but the sample mean is slightly correlated with each observation since it is defined in terms of all observations 
if the population mean is known the analogous unbiased estimate using the population mean has in the denominator 
this is an example of why in probability and statistics it is essential to distinguish between random variables upper case letters and realizations of the random variables lower case letters 
the maximum likelihood estimate of the covariance for the gaussian distribution case has in the denominator as well 
the ratio of to approaches for large so the maximum likelihood estimate approximately equals the unbiased estimate when the sample is large 
distribution of the sample mean for each random variable the sample mean is good estimator of the population mean where good estimator is defined as being efficient and unbiased 
of course the estimator will likely not be the true value of the population mean since different samples drawn from the same distribution will give different sample means and hence different estimates of the true mean 
thus the sample mean is random variable not constant and consequently has its own distribution 
for random sample of observations on the jth random variable the sample mean distribution itself has mean equal to the population mean and variance equal to where is the population variance 
the arithmetic mean of population or population mean is often denoted the sample mean the arithmetic mean of sample of values drawn from the population makes good estimator of the population mean as its expected value is equal to the population mean that is it is an unbiased estimator 
the sample mean is random variable not constant since its calculated value will randomly differ depending on which members of the population are sampled and consequently it will have its own distribution 
for random sample of independent observations the expected value of the sample mean is and the variance of the sample mean is var if the samples are not independent but correlated then special care has to be taken in order to avoid the problem of pseudoreplication 
if the population is normally distributed then the sample mean is normally distributed as follows 
if the population is not normally distributed the sample mean is nonetheless approximately normally distributed if is large and 
this is consequence of the central limit theorem 
weighted samples in weighted sample each vector each set of single observations on each of the random variables is assigned weight without loss of generality assume that the weights are normalized 
if they are not divide the weights by their sum 
then the weighted mean vector is given by and the elements of the weighted covariance matrix are 
if all weights are the same the weighted mean and covariance reduce to the biased sample mean and covariance mentioned above 
criticism the sample mean and sample covariance are not robust statistics meaning that they are sensitive to outliers 
as robustness is often desired trait particularly in real world applications robust alternatives may prove desirable notably quantile based statistics such as the sample median for location and interquartile range iqr for dispersion 
other alternatives include trimming and winsorising as in the trimmed mean and the winsorized mean 
see also estimation of covariance matrices scatter matrix unbiased estimation of standard deviation references
in music closely related key or close key is one sharing many common tones with an original key as opposed to distantly related key or distant key 
in music harmony there are six of them five share all or all except one pitches with key with which it is being compared and is adjacent to it on the circle of fifths and its relative major or minor and one shares the same tonic 
such keys are the most commonly used destinations or transpositions in modulation because of their strong structural links with the home key 
distant keys may be reached sequentially through closely related keys by chain modulation for example to to for example one principle that every composer of haydn day classical music era kept in mind was over all unity of tonality 
no piece dared wander too far from its tonic key and no piece in four movement form dared to present tonality not closely related to the key of the whole series 
for example the first movement of mozart piano sonata no 
modulates only to closely related keys the dominant supertonic and submediant given major key tonic the related keys are ii supertonic the relative minor of the subdominant iii mediant the relative minor of the dominant iv subdominant one less sharp or one more flat around circle of fifths dominant one more sharp or one fewer flat around circle of fifths vi submediant or relative minor different tonic same key signature parallel minor same tonic different key signature specifically starting from minor key the closely related keys are the mediant or relative major iii the subdominant iv the minor dominant the submediant vi the subtonic vii and the parallel major 
in the key of minor when we translate them to keys we get major minor minor major major majoranother view of closely related keys is that there are six closely related keys based on the tonic and the remaining triads of the diatonic scale excluding the dissonant diminished triads 
four of the five differ by one accidental one has the same key signature and one uses the parallel modal form 
in the key of major these would be minor minor major major minor and minor 
despite being three sharps or flats away from the original key in the circle of fifths parallel keys are also considered as closely related keys as the tonal center is the same and this makes this key have an affinity with the original key 
in modern music the closeness of relation between any two keys or sets of pitches may be determined by the number of tones they share in common which allows one to consider modulations not occurring in standard major minor tonality 
for example in music based on the pentatonic scale containing pitches and modulating fifth higher gives the collection of pitches and having four of five tones in common 
however modulating up tritone would produce which shares no common tones with the original scale 
thus the scale fifth higher is very closely related while the scale tritone higher is not 
other modulations may be placed in order from closest to most distant depending upon the number of common tones 
another view in modern music notably in bart common tonic produces closely related keys the other scales being the six other modes 
this usage can be found in several of the mikrokosmos piano pieces 
when modulation causes the new key to traverse the bottom of the circle of fifths this may give rise to theoretical key containing eight or more sharps or flats in its notated key signature in such case notational conventions require recasting the new section in its enharmonically equivalent key 
andranik tangian suggests and visualizations of key chord proximity for both all major and all minor keys chords by locating them along single subdominant dominant axis which wraps torus that is then unfolded 
see also chromatic mediant common chord music monotonality parallel and counter parallel pitch space references further reading howard hanson harmonic materials of modern music 
appleton century crofts inc
principal component analysis pca is popular technique for analyzing large datasets containing high number of dimensions features per observation increasing the interpretability of data while preserving the maximum amount of information and enabling the visualization of multidimensional data 
formally pca is statistical technique for reducing the dimensionality of dataset 
this is accomplished by linearly transforming the data into new coordinate system where most of the variation in the data can be described with fewer dimensions than the initial data 
many studies use the first two principal components in order to plot the data in two dimensions and to visually identify clusters of closely related data points 
principal component analysis has applications in many fields such as population genetics microbiome studies and atmospheric science 
the principal components of collection of points in real coordinate space are sequence of unit vectors where the th vector is the direction of line that best fits the data while being orthogonal to the first vectors 
here best fitting line is defined as one that minimizes the average squared perpendicular distance from the points to the line 
these directions constitute an orthonormal basis in which different individual dimensions of the data are linearly uncorrelated 
principal component analysis pca is the process of computing the principal components and using them to perform change of basis on the data sometimes using only the first few principal components and ignoring the rest 
in data analysis the first principal component of set of variables presumed to be jointly normally distributed is the derived variable formed as linear combination of the original variables that explains the most variance 
the second principal component explains the most variance in what is left once the effect of the first component is removed and we may proceed through iterations until all the variance is explained 
pca is most commonly used when many of the variables are highly correlated with each other and it is desirable to reduce their number to an independent set 
pca is used in exploratory data analysis and for making predictive models 
it is commonly used for dimensionality reduction by projecting each data point onto only the first few principal components to obtain lower dimensional data while preserving as much of the data variation as possible 
the first principal component can equivalently be defined as direction that maximizes the variance of the projected data 
the th principal component can be taken as direction orthogonal to the first principal components that maximizes the variance of the projected data 
for either objective it can be shown that the principal components are eigenvectors of the data covariance matrix 
thus the principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix 
pca is the simplest of the true eigenvector based multivariate analyses and is closely related to factor analysis 
factor analysis typically incorporates more domain specific assumptions about the underlying structure and solves eigenvectors of slightly different matrix 
pca is also related to canonical correlation analysis cca 
cca defines coordinate systems that optimally describe the cross covariance between two datasets while pca defines new orthogonal coordinate system that optimally describes variance in single dataset 
robust and norm based variants of standard pca have also been proposed 
history pca was invented in by karl pearson as an analogue of the principal axis theorem in mechanics it was later independently developed and named by harold hotelling in the 
depending on the field of application it is also named the discrete karhunen lo ve transform klt in signal processing the hotelling transform in multivariate quality control proper orthogonal decomposition pod in mechanical engineering singular value decomposition svd of invented in the last quarter of the th century eigenvalue decomposition evd of xtx in linear algebra factor analysis for discussion of the differences between pca and factor analysis see ch 
of jolliffe principal component analysis eckart young theorem harman or empirical orthogonal functions eof in meteorological science empirical eigenfunction decomposition sirovich empirical component analysis lorenz quasiharmonic modes brooks et al spectral decomposition in noise and vibration and empirical modal analysis in structural dynamics 
intuition pca can be thought of as fitting dimensional ellipsoid to the data where each axis of the ellipsoid represents principal component 
if some axis of the ellipsoid is small then the variance along that axis is also small 
to find the axes of the ellipsoid we must first center the values of each variable in the dataset on by subtracting the mean of the variable observed values from each of those values 
these transformed values are used instead of the original observed values for each of the variables 
then we compute the covariance matrix of the data and calculate the eigenvalues and corresponding eigenvectors of this covariance matrix 
then we must normalize each of the orthogonal eigenvectors to turn them into unit vectors 
once this is done each of the mutually orthogonal unit eigenvectors can be interpreted as an axis of the ellipsoid fitted to the data 
this choice of basis will transform the covariance matrix into diagonalized form in which the diagonal elements represent the variance of each axis 
the proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues 
biplots and scree plots degree of explained variance are used to explain findings of the pca 
details pca is defined as an orthogonal linear transformation that transforms the data to new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate called the first principal component the second greatest variance on the second coordinate and so on consider an data matrix with column wise zero empirical mean the sample mean of each column has been shifted to zero where each of the rows represents different repetition of the experiment and each of the columns gives particular kind of feature say the results from particular sensor 
mathematically the transformation is defined by set of size of dimensional vectors of weights or coefficients that map each row vector of to new vector of principal component scores given by in such way that the individual variables of considered over the data set successively inherit the maximum possible variance from with each coefficient vector constrained to be unit vector where is usually selected to be strictly less than to reduce dimensionality 
first component in order to maximize variance the first weight vector thus has to satisfy arg max arg max equivalently writing this in matrix form gives arg max arg max since has been defined to be unit vector it equivalently also satisfies arg max the quantity to be maximised can be recognised as rayleigh quotient 
standard result for positive semidefinite matrix such as xtx is that the quotient maximum possible value is the largest eigenvalue of the matrix which occurs when is the corresponding eigenvector 
with found the first principal component of data vector can then be given as score in the transformed co ordinates or as the corresponding vector in the original variables 
further components the th component can be found by subtracting the first principal components from and then finding the weight vector which extracts the maximum variance from this new data matrix arg max it turns out that this gives the remaining eigenvectors of xtx with the maximum values for the quantity in brackets given by their corresponding eigenvalues 
thus the weight vectors are eigenvectors of xtx 
the th principal component of data vector can therefore be given as score tk in the transformed coordinates or as the corresponding vector in the space of the original variables where is the kth eigenvector of xtx 
the full principal components decomposition of can therefore be given as where is by matrix of weights whose columns are the eigenvectors of xtx 
the transpose of is sometimes called the whitening or sphering transformation 
columns of multiplied by the square root of corresponding eigenvalues that is eigenvectors scaled up by the variances are called loadings in pca or in factor analysis 
covariances xtx itself can be recognized as proportional to the empirical sample covariance matrix of the dataset xt 
the sample covariance between two of the different principal components over the dataset is given by where the eigenvalue property of has been used to move from line to line however eigenvectors and corresponding to eigenvalues of symmetric matrix are orthogonal if the eigenvalues are different or can be orthogonalised if the vectors happen to share an equal repeated value 
the product in the final line is therefore zero there is no sample covariance between different principal components over the dataset 
another way to characterise the principal components transformation is therefore as the transformation to coordinates which diagonalise the empirical sample covariance matrix 
in matrix form the empirical covariance matrix for the original variables can be written the empirical covariance matrix between the principal components becomes where is the diagonal matrix of eigenvalues of xtx 
is equal to the sum of the squares over the dataset associated with each component that is tk 
dimensionality reduction the transformation maps data vector from an original space of variables to new space of variables which are uncorrelated over the dataset 
however not all the principal components need to be kept 
keeping only the first principal components produced by using only the first eigenvectors gives the truncated transformation where the matrix tl now has rows but only columns 
in other words pca learns linear transformation where the columns of matrix form an orthogonal basis for the features the components of representation that are decorrelated 
by construction of all the transformed data matrices with only columns this score matrix maximises the variance in the original data that has been preserved while minimising the total squared reconstruction error or such dimensionality reduction can be very useful step for visualising and processing high dimensional datasets while still retaining as much of the variance in the dataset as possible 
for example selecting and keeping only the first two principal components finds the two dimensional plane through the high dimensional dataset in which the data is most spread out so if the data contains clusters these too may be most spread out and therefore most visible to be plotted out in two dimensional diagram whereas if two directions through the data or two of the original variables are chosen at random the clusters may be much less spread apart from each other and may in fact be much more likely to substantially overlay each other making them indistinguishable 
similarly in regression analysis the larger the number of explanatory variables allowed the greater is the chance of overfitting the model producing conclusions that fail to generalise to other datasets 
one approach especially when there are strong correlations between different possible explanatory variables is to reduce them to few principal components and then run the regression against them method called principal component regression 
dimensionality reduction may also be appropriate when the variables in dataset are noisy 
if each column of the dataset contains independent identically distributed gaussian noise then the columns of will also contain similarly identically distributed gaussian noise such distribution is invariant under the effects of the matrix which can be thought of as high dimensional rotation of the co ordinate axes 
however with more of the total variance concentrated in the first few principal components compared to the same noise variance the proportionate effect of the noise is less the first few components achieve higher signal to noise ratio 
pca thus can have the effect of concentrating much of the signal into the first few principal components which can usefully be captured by dimensionality reduction while the later principal components may be dominated by noise and so disposed of without great loss 
if the dataset is not too large the significance of the principal components can be tested using parametric bootstrap as an aid in determining how many principal components to retain 
singular value decomposition the principal components transformation can also be associated with another matrix factorization the singular value decomposition svd of here is an by rectangular diagonal matrix of positive numbers called the singular values of is an by matrix the columns of which are orthogonal unit vectors of length called the left singular vectors of and is by whose columns are orthogonal unit vectors of length and called the right singular vectors of in terms of this factorization the matrix xtx can be written where is the square diagonal matrix with the singular values of and the excess zeros chopped off that satisfies comparison with the eigenvector factorization of xtx establishes that the right singular vectors of are equivalent to the eigenvectors of xtx while the singular values of are equal to the square root of the eigenvalues of xtx 
using the singular value decomposition the score matrix can be written so each column of is given by one of the left singular vectors of multiplied by the corresponding singular value 
this form is also the polar decomposition of efficient algorithms exist to calculate the svd of without having to form the matrix xtx so computing the svd is now the standard way to calculate principal components analysis from data matrix unless only handful of components are required 
as with the eigen decomposition truncated score matrix tl can be obtained by considering only the first largest singular values and their singular vectors the truncation of matrix or using truncated singular value decomposition in this way produces truncated matrix that is the nearest possible matrix of rank to the original matrix in the sense of the difference between the two having the smallest possible frobenius norm result known as the eckart young theorem 
further considerations the singular values in are the square roots of the eigenvalues of the matrix xtx 
each eigenvalue is proportional to the portion of the variance more correctly of the sum of the squared distances of the points from their multidimensional mean that is associated with each eigenvector 
the sum of all the eigenvalues is equal to the sum of the squared distances of the points from their multidimensional mean 
pca essentially rotates the set of points around their mean in order to align with the principal components 
this moves as much of the variance as possible using an orthogonal transformation into the first few dimensions 
the values in the remaining dimensions therefore tend to be small and may be dropped with minimal loss of information see below 
pca is often used in this manner for dimensionality reduction 
pca has the distinction of being the optimal orthogonal transformation for keeping the subspace that has largest variance as defined above 
this advantage however comes at the price of greater computational requirements if compared for example and when applicable to the discrete cosine transform and in particular to the dct ii which is simply known as the dct 
nonlinear dimensionality reduction techniques tend to be more computationally demanding than pca 
pca is sensitive to the scaling of the variables 
if we have just two variables and they have the same sample variance and are completely correlated then the pca will entail rotation by and the weights they are the cosines of rotation for the two variables with respect to the principal component will be equal 
but if we multiply all values of the first variable by then the first principal component will be almost the same as that variable with small contribution from the other variable whereas the second component will be almost aligned with the second original variable 
this means that whenever the different variables have different units like temperature and mass pca is somewhat arbitrary method of analysis 
different results would be obtained if one used fahrenheit rather than celsius for example 
pearson original paper was entitled on lines and planes of closest fit to systems of points in space in space implies physical euclidean space where such concerns do not arise 
one way of making the pca less arbitrary is to use variables scaled so as to have unit variance by standardizing the data and hence use the autocorrelation matrix instead of the autocovariance matrix as basis for pca 
however this compresses or expands the fluctuations in all dimensions of the signal space to unit variance 
mean centering is necessary for performing classical pca to ensure that the first principal component describes the direction of maximum variance 
if mean subtraction is not performed the first principal component might instead correspond more or less to the mean of the data 
mean of zero is needed for finding basis that minimizes the mean square error of the approximation of the data mean centering is unnecessary if performing principal components analysis on correlation matrix as the data are already centered after calculating correlations 
correlations are derived from the cross product of two standard scores scores or statistical moments hence the name pearson product moment correlation 
also see the article by kromrey foster johnson on mean centering in moderated regression much ado about nothing 
since covariances are correlations of normalized variables or standard scores pca based on the correlation matrix of is equal to pca based on the covariance matrix of the standardized version of pca is popular primary technique in pattern recognition 
it is not however optimized for class separability 
however it has been used to quantify the distance between two or more classes by calculating center of mass for each class in principal component space and reporting euclidean distance between center of mass of two or more classes 
the linear discriminant analysis is an alternative which is optimized for class separability 
table of symbols and abbreviations properties and limitations of pca properties some properties of pca include property for any integer consider the orthogonal linear transformation where is element vector and is matrix and let be the variance covariance matrix for then the trace of denoted tr is maximized by taking where consists of the first columns of is the transpose of property consider again the orthonormal transformation with and defined as before 
then tr is minimized by taking where consists of the last columns of the statistical implication of this property is that the last few pcs are not simply unstructured left overs after removing the important pcs 
because these last pcs have variances as small as possible they are useful in their own right 
they can help to detect unsuspected near constant linear relationships between the elements of and they may also be useful in regression in selecting subset of variables from and in outlier detection 
property spectral decomposition of before we look at its usage we first look at diagonal elements var then perhaps the main statistical implication of the result is that not only can we decompose the combined variances of all the elements of into decreasing contributions due to each pc but we can also decompose the whole covariance matrix into contributions from each pc 
although not strictly decreasing the elements of will tend to become smaller as increases as is nonincreasing for increasing whereas the elements of tend to stay about the same size because of the normalization constraints limitations as noted above the results of pca depend on the scaling of the variables 
this can be cured by scaling each feature by its standard deviation so that one ends up with dimensionless features with unital variance the applicability of pca as described above is limited by certain tacit assumptions made in its derivation 
in particular pca can capture linear correlations between the features but fails when this assumption is violated see figure in the reference 
in some cases coordinate transformations can restore the linearity assumption and pca can then be applied see kernel pca 
another limitation is the mean removal process before constructing the covariance matrix for pca 
in fields such as astronomy all the signals are non negative and the mean removal process will force the mean of some astrophysical exposures to be zero which consequently creates unphysical negative fluxes and forward modeling has to be performed to recover the true magnitude of the signals 
as an alternative method non negative matrix factorization focusing only on the non negative elements in the matrices which is well suited for astrophysical observations 
see more at relation between pca and non negative matrix factorization 
pca is at disadvantage if the data has not been standardized before applying the algorithm to it 
pca transforms original data into data that is relevant to the principal components of that data which means that the new data variables cannot be interpreted in the same ways that the originals were 
they are linear interpretations of the original variables 
also if pca is not performed properly there is high likelihood of information loss pca relies on linear model 
if dataset has pattern hidden inside it that is nonlinear then pca can actually steer the analysis in the complete opposite direction of progress 
researchers at kansas state university discovered that the sampling error in their experiments impacted the bias of pca results 
if the number of subjects or blocks is smaller than and or the researcher is interested in pc beyond the first it may be better to first correct for the serial correlation before pca is conducted 
the researchers at kansas state also found that pca could be seriously biased if the autocorrelation structure of the data is not correctly handled 
pca and information theory dimensionality reduction results in loss of information in general 
pca based dimensionality reduction tends to minimize that information loss under certain signal and noise models 
under the assumption that that is that the data vector is the sum of the desired information bearing signal and noise signal one can show that pca can be optimal for dimensionality reduction from an information theoretic point of view 
in particular linsker showed that if is gaussian and is gaussian noise with covariance matrix proportional to the identity matrix the pca maximizes the mutual information between the desired information and the dimensionality reduced output if the noise is still gaussian and has covariance matrix proportional to the identity matrix that is the components of the vector are iid but the information bearing signal is non gaussian which is common scenario pca at least minimizes an upper bound on the information loss which is defined as 
the optimality of pca is also preserved if the noise is iid and at least more gaussian in terms of the kullback leibler divergence than the information bearing signal in general even if the above signal model holds pca loses its information theoretic optimality as soon as the noise becomes dependent 
computing pca using the covariance method the following is detailed description of pca using the covariance method see also here as opposed to the correlation method the goal is to transform given data set of dimension to an alternative data set of smaller dimension equivalently we are seeking to find the matrix where is the karhunen lo ve transform klt of matrix organize the data setsuppose you have data comprising set of observations of variables and you want to reduce the data so that each observation can be described with only variables suppose further that the data are arranged as set of data vectors with each representing single grouped observation of the variables 
write as row vectors each with elements 
place the row vectors into single matrix of dimensions calculate the empirical meanfind the empirical mean along each column place the calculated mean values into an empirical mean vector of dimensions calculate the deviations from the meanmean subtraction is an integral part of the solution towards finding principal component basis that minimizes the mean square error of approximating the data 
hence we proceed by centering the data as follows subtract the empirical mean vector from each row of the data matrix store mean subtracted data in the matrix where is an column vector of all for in some applications each variable column of may also be scaled to have variance equal to see score 
this step affects the calculated principal components but makes them independent of the units used to measure the different variables 
find the covariance matrixfind the empirical covariance matrix from matrix where is the conjugate transpose operator 
if consists entirely of real numbers which is the case in many applications the conjugate transpose is the same as the regular transpose 
the reasoning behind using instead of to calculate the covariance is bessel correction find the eigenvectors and eigenvalues of the covariance matrixcompute the matrix of eigenvectors which diagonalizes the covariance matrix where is the diagonal matrix of eigenvalues of this step will typically involve the use of computer based algorithm for computing eigenvectors and eigenvalues 
these algorithms are readily available as sub components of most matrix algebra systems such as sas matlab mathematica scipy idl interactive data language or gnu octave as well as opencv 
matrix will take the form of an diagonal matrix where is the jth eigenvalue of the covariance matrix and matrix also of dimension contains column vectors each of length which represent the eigenvectors of the covariance matrix the eigenvalues and eigenvectors are ordered and paired 
the jth eigenvalue corresponds to the jth eigenvector 
matrix denotes the matrix of right eigenvectors as opposed to left eigenvectors 
in general the matrix of right eigenvectors need not be the conjugate transpose of the matrix of left eigenvectors rearrange the eigenvectors and eigenvaluessort the columns of the eigenvector matrix and eigenvalue matrix in order of decreasing eigenvalue 
make sure to maintain the correct pairings between the columns in each matrix compute the cumulative energy content for each eigenvectorthe eigenvalues represent the distribution of the source data energy among each of the eigenvectors where the eigenvectors form basis for the data 
the cumulative energy content for the jth eigenvector is the sum of the energy content across all of the eigenvalues from through for select subset of the eigenvectors as basis vectorssave the first columns of as the matrix where use the vector as guide in choosing an appropriate value for the goal is to choose value of as small as possible while achieving reasonably high value of on percentage basis 
for example you may want to choose so that the cumulative energy is above certain threshold like percent 
in this case choose the smallest value of such that project the data onto the new basisthe projected data points are the rows of the matrix that is the first column of is the projection of the data points onto the first principal component the second column is the projection onto the second principal component etc 
derivation of pca using the covariance method let be dimensional random vector expressed as column vector 
without loss of generality assume has zero mean 
we want to find orthonormal transformation matrix so that px has diagonal covariance matrix that is px is random vector with all its distinct components pairwise uncorrelated 
quick computation assuming were unitary yields cov cov hence holds if and only if cov were diagonalisable by this is very constructive as cov is guaranteed to be non negative definite matrix and thus is guaranteed to be diagonalisable by some unitary matrix 
covariance free computation in practical implementations especially with high dimensional data large the naive covariance method is rarely used because it is not efficient due to high computational and memory costs of explicitly determining the covariance matrix 
the covariance free approach avoids the np operations of explicitly calculating and storing the covariance matrix xtx instead utilizing one of matrix free methods for example based on the function evaluating the product xt at the cost of np operations 
iterative computation one way to compute the first principal component efficiently is shown in the following pseudo code for data matrix with zero mean without ever computing its covariance matrix 
random vector of length norm do times vector of length for each row in rts is the eigenvalue error norm exit if error tolerance return this power iteration algorithm simply calculates the vector xt normalizes and places the result back in the eigenvalue is approximated by rt xtx which is the rayleigh quotient on the unit vector for the covariance matrix xtx 
if the largest singular value is well separated from the next largest one the vector gets close to the first principal component of within the number of iterations which is small relative to at the total cost cnp 
the power iteration convergence can be accelerated without noticeably sacrificing the small cost per iteration using more advanced matrix free methods such as the lanczos algorithm or the locally optimal block preconditioned conjugate gradient lobpcg method 
subsequent principal components can be computed one by one via deflation or simultaneously as block 
in the former approach imprecisions in already computed approximate principal components additively affect the accuracy of the subsequently computed principal components thus increasing the error with every new computation 
the latter approach in the block power method replaces single vectors and with block vectors matrices and every column of approximates one of the leading principal components while all columns are iterated simultaneously 
the main calculation is evaluation of the product xt 
implemented for example in lobpcg efficient blocking eliminates the accumulation of the errors allows using high level blas matrix matrix product functions and typically leads to faster convergence compared to the single vector one by one technique 
the nipals method non linear iterative partial least squares nipals is variant the classical power iteration with matrix deflation by subtraction implemented for computing the first few components in principal component or partial least squares analysis 
for very high dimensional datasets such as those generated in the omics sciences for example genomics metabolomics it is usually only necessary to compute the first few pcs 
the non linear iterative partial least squares nipals algorithm updates iterative approximations to the leading scores and loadings and by the power iteration multiplying on every iteration by on the left and on the right that is calculation of the covariance matrix is avoided just as in the matrix free implementation of the power iterations to xtx based on the function evaluating the product xt tx the matrix deflation by subtraction is performed by subtracting the outer product from leaving the deflated residual matrix used to calculate the subsequent leading pcs 
for large data matrices or matrices that have high degree of column collinearity nipals suffers from loss of orthogonality of pcs due to machine precision round off errors accumulated in each iteration and matrix deflation by subtraction 
gram schmidt re orthogonalization algorithm is applied to both the scores and the loadings at each iteration step to eliminate this loss of orthogonality 
nipals reliance on single vector multiplications cannot take advantage of high level blas and results in slow convergence for clustered leading singular values both these deficiencies are resolved in more sophisticated matrix free block solvers such as the locally optimal block preconditioned conjugate gradient lobpcg method 
online sequential estimation in an online or streaming situation with data arriving piece by piece rather than being stored in single batch it is useful to make an estimate of the pca projection that can be updated sequentially 
this can be done efficiently but requires different algorithms 
pca and qualitative variables in pca it is common that we want to introduce qualitative variables as supplementary elements 
for example many quantitative variables have been measured on plants 
for these plants some qualitative variables are available as for example the species to which the plant belongs 
these data were subjected to pca for quantitative variables 
when analyzing the results it is natural to connect the principal components to the qualitative variable species 
for this the following results are produced 
identification on the factorial planes of the different species for example using different colors 
representation on the factorial planes of the centers of gravity of plants belonging to the same species 
for each center of gravity and each axis value to judge the significance of the difference between the center of gravity and origin these results are what is called introducing qualitative variable as supplementary element 
this procedure is detailed in and husson pag and pag few software offer this option in an automatic way 
this is the case of spad that historically following the work of ludovic lebart was the first to propose this option and the package factominer 
applications intelligence the earliest application of factor analysis was in locating and measuring components of human intelligence 
it was believed that intelligence had various uncorrelated components such as spatial intelligence verbal intelligence induction deduction etc and that scores on these could be adduced by factor analysis from results on various tests to give single index known as the intelligence quotient iq 
the pioneering statistical psychologist spearman actually developed factor analysis in for his two factor theory of intelligence adding formal technique to the science of psychometrics 
in thurstone looked for factors of intelligence developing the notion of mental age 
standard iq tests today are based on this early work 
residential differentiation in shevky and williams introduced the theory of factorial ecology which dominated studies of residential differentiation from the to the 
neighbourhoods in city were recognizable or could be distinguished from one another by various characteristics which could be reduced to three by factor analysis 
these were known as social rank an index of occupational status familism or family size and ethnicity cluster analysis could then be applied to divide the city into clusters or precincts according to values of the three key factor variables 
an extensive literature developed around factorial ecology in urban geography but the approach went out of fashion after as being methodologically primitive and having little place in postmodern geographical paradigms 
one of the problems with factor analysis has always been finding convincing names for the various artificial factors 
in flood revived the factorial ecology approach to show that principal components analysis actually gave meaningful answers directly without resorting to factor rotation 
the principal components were actually dual variables or shadow prices of forces pushing people together or apart in cities 
the first component was accessibility the classic trade off between demand for travel and demand for space around which classical urban economics is based 
the next two components were disadvantage which keeps people of similar status in separate neighbourhoods mediated by planning and ethnicity where people of similar ethnic backgrounds try to co locate about the same time the australian bureau of statistics defined distinct indexes of advantage and disadvantage taking the first principal component of sets of key variables that were thought to be important 
these seifa indexes are regularly published for various jurisdictions and are used frequently in spatial analysis 
development indexes pca has been the only formal method available for the development of indexes which are otherwise hit or miss ad hoc undertaking 
the city development index was developed by pca from about indicators of city outcomes in survey of global cities 
the first principal component was subject to iterative regression adding the original variables singly until about of its variation was accounted for 
the index ultimately used about indicators but was good predictor of many more variables 
its comparative value agreed very well with subjective assessment of the condition of each city 
the coefficients on items of infrastructure were roughly proportional to the average costs of providing the underlying services suggesting the index was actually measure of effective physical and social investment in the city 
the country level human development index hdi from undp which has been published since and is very extensively used in development studies has very similar coefficients on similar indicators strongly suggesting it was originally constructed using pca 
population genetics in cavalli sforza and others pioneered the use of principal components analysis pca to summarise data on variation in human gene frequencies across regions 
the components showed distinctive patterns including gradients and sinusoidal waves 
they interpreted these patterns as resulting from specific ancient migration events 
since then pca has been ubiquitous in population genetics with thousands of papers using pca as display mechanism 
genetics varies largely according to proximity so the first two principal components actually show spatial distribution and may be used to map the relative geographical location of different population groups thereby showing individuals who have wandered from their original locations pca in genetics has been technically controversial in that the technique has been performed on discrete non normal variables and often on binary allele markers 
the lack of any measures of standard error in pca are also an impediment to more consistent usage 
in august the molecular biologist eran elhaik published theoretical paper in scientific reports analyzing pca applications 
he concluded that it was easy to manipulate the method which in his view generated results that were erroneous contradictory and absurd 
specifically he argued the results achieved in population genetics were characterized by cherry picking and circular reasoning 
market research and indexes of attitude market research has been an extensive user of pca 
it is used to develop customer satisfaction or customer loyalty scores for products and with clustering to develop market segments that may be targeted with advertising campaigns in much the same way as factorial ecology will locate geographical areas with similar characteristics pca rapidly transforms large amounts of data into smaller easier to digest variables that can be more rapidly and readily analyzed 
in any consumer questionnaire there are series of questions designed to elicit consumer attitudes and principal components seek out latent variables underlying these attitudes 
for example the oxford internet survey in asked people about their attitudes and beliefs and from these analysts extracted four principal component dimensions which they identified as escape social networking efficiency and problem creating another example from joe flood in extracted an attitudinal index toward housing from attitude questions in national survey of households in australia 
the first principal component represented general attitude toward property and home ownership 
the index or the attitude questions it embodied could be fed into general linear model of tenure choice 
the strongest determinant of private renting by far was the attitude index rather than income marital status or household type 
quantitative finance in quantitative finance principal component analysis can be directly applied to the risk management of interest rate derivative portfolios 
trading multiple swap instruments which are usually function of other market quotable swap instruments is sought to be reduced to usually or principal components representing the path of interest rates on macro basis 
converting risks to be represented as those to factor loadings or multipliers provides assessments and understanding beyond that available to simply collectively viewing risks to individual buckets 
pca has also been applied to equity portfolios in similar fashion both to portfolio risk and to risk return 
one application is to reduce portfolio risk where allocation strategies are applied to the principal portfolios instead of the underlying stocks 
second is to enhance portfolio return using the principal components to select stocks with upside potential 
neuroscience variant of principal components analysis is used in neuroscience to identify the specific properties of stimulus that increases neuron probability of generating an action potential 
this technique is known as spike triggered covariance analysis 
in typical application an experimenter presents white noise process as stimulus usually either as sensory input to test subject or as current injected directly into the neuron and records train of action potentials or spikes produced by the neuron as result 
presumably certain features of the stimulus make the neuron more likely to spike 
in order to extract these features the experimenter calculates the covariance matrix of the spike triggered ensemble the set of all stimuli defined and discretized over finite time window typically on the order of ms that immediately preceded spike 
the eigenvectors of the difference between the spike triggered covariance matrix and the covariance matrix of the prior stimulus ensemble the set of all stimuli defined over the same length time window then indicate the directions in the space of stimuli along which the variance of the spike triggered ensemble differed the most from that of the prior stimulus ensemble 
specifically the eigenvectors with the largest positive eigenvalues correspond to the directions along which the variance of the spike triggered ensemble showed the largest positive change compared to the varince of the prior 
since these were the directions in which varying the stimulus led to spike they are often good approximations of the sought after relevant stimulus features 
in neuroscience pca is also used to discern the identity of neuron from the shape of its action potential 
spike sorting is an important procedure because extracellular recording techniques often pick up signals from more than one neuron 
in spike sorting one first uses pca to reduce the dimensionality of the space of action potential waveforms and then performs clustering analysis to associate specific action potentials with individual neurons 
pca as dimension reduction technique is particularly suited to detect coordinated activities of large neuronal ensembles 
it has been used in determining collective variables that is order parameters during phase transitions in the brain 
relation with other methods correspondence analysis correspondence analysis ca was developed by jean paul benz cri and is conceptually similar to pca but scales the data which should be non negative so that rows and columns are treated equivalently 
it is traditionally applied to contingency tables 
ca decomposes the chi squared statistic associated to this table into orthogonal factors 
because ca is descriptive technique it can be applied to tables for which the chi squared statistic is appropriate or not 
several variants of ca are available including detrended correspondence analysis and canonical correspondence analysis 
one special extension is multiple correspondence analysis which may be seen as the counterpart of principal component analysis for categorical data 
factor analysis principal component analysis creates variables that are linear combinations of the original variables 
the new variables have the property that the variables are all orthogonal 
the pca transformation can be helpful as pre processing step before clustering 
pca is variance focused approach seeking to reproduce the total variable variance in which components reflect both common and unique variance of the variable 
pca is generally preferred for purposes of data reduction that is translating variable space into optimal factor space but not when the goal is to detect the latent construct or factors 
factor analysis is similar to principal component analysis in that factor analysis also involves linear combinations of variables 
different from pca factor analysis is correlation focused approach seeking to reproduce the inter correlations among variables in which the factors represent the common variance of variables excluding unique variance 
in terms of the correlation matrix this corresponds with focusing on explaining the off diagonal terms that is shared co variance while pca focuses on explaining the terms that sit on the diagonal 
however as side result when trying to reproduce the on diagonal terms pca also tends to fit relatively well the off diagonal correlations 
results given by pca and factor analysis are very similar in most situations but this is not always the case and there are some problems where the results are significantly different 
factor analysis is generally used when the research purpose is detecting data structure that is latent constructs or factors or causal modeling 
if the factor model is incorrectly formulated or the assumptions are not met then factor analysis will give erroneous results 
means clustering it has been asserted that the relaxed solution of means clustering specified by the cluster indicators is given by the principal components and the pca subspace spanned by the principal directions is identical to the cluster centroid subspace 
however that pca is useful relaxation of means clustering was not new result and it is straightforward to uncover counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions 
non negative matrix factorization non negative matrix factorization nmf is dimension reduction method where only non negative elements in the matrices are used which is therefore promising method in astronomy in the sense that astrophysical signals are non negative 
the pca components are orthogonal to each other while the nmf components are all non negative and therefore constructs non orthogonal basis 
in pca the contribution of each component is ranked based on the magnitude of its corresponding eigenvalue which is equivalent to the fractional residual variance frv in analyzing empirical data 
for nmf its components are ranked based only on the empirical frv curves 
the residual fractional eigenvalue plots that is as function of component number given total of components for pca has flat plateau where no data is captured to remove the quasi static noise then the curves dropped quickly as an indication of over fitting and captures random noise 
the frv curves for nmf is decreasing continuously when the nmf components are constructed sequentially indicating the continuous capturing of quasi static noise then converge to higher levels than pca indicating the less over fitting property of nmf 
iconography of correlations it is often difficult to interpret the principal components when the data include many variables of various origins or when some variables are qualitative 
this leads the pca user to delicate elimination of several variables 
if observations or variables have an excessive impact on the direction of the axes they should be removed and then projected as supplementary elements 
in addition it is necessary to avoid interpreting the proximities between the points close to the center of the factorial plane 
the iconography of correlations on the contrary which is not projection on system of axes does not have these drawbacks 
we can therefore keep all the variables 
the principle of the diagram is to underline the remarkable correlations of the correlation matrix by solid line positive correlation or dotted line negative correlation 
strong correlation is not remarkable if it is not direct but caused by the effect of third variable 
conversely weak correlations can be remarkable 
for example if variable depends on several independent variables the correlations of with each of them are weak and yet remarkable 
generalizations sparse pca particular disadvantage of pca is that the principal components are usually linear combinations of all input variables 
sparse pca overcomes this disadvantage by finding linear combinations that contain just few input variables 
it extends the classic method of principal component analysis pca for the reduction of dimensionality of data by adding sparsity constraint on the input variables 
several approaches have been proposed including regression framework convex relaxation semidefinite programming framework generalized power method framework an alternating maximization framework forward backward greedy search and exact methods using branch and bound techniques bayesian formulation framework the methodological and theoretical developments of sparse pca as well as its applications in scientific studies were recently reviewed in survey paper 
nonlinear pca most of the modern methods for nonlinear dimensionality reduction find their theoretical and algorithmic roots in pca or means 
pearson original idea was to take straight line or plane which will be the best fit to set of data points 
trevor hastie expanded on this concept by proposing principal curves as the natural extension for the geometric interpretation of pca which explicitly constructs manifold for data approximation followed by projecting the points onto it as is illustrated by fig 
see also the elastic map algorithm and principal geodesic analysis 
another popular generalization is kernel pca which corresponds to pca performed in reproducing kernel hilbert space associated with positive definite kernel 
in multilinear subspace learning pca is generalized to multilinear pca mpca that extracts features directly from tensor representations 
mpca is solved by performing pca in each mode of the tensor iteratively 
mpca has been applied to face recognition gait recognition etc 
mpca is further extended to uncorrelated mpca non negative mpca and robust mpca 
way principal component analysis may be performed with models such as tucker decomposition parafac multiple factor analysis co inertia analysis statis and distatis 
robust pca while pca finds the mathematically optimal method as in minimizing the squared error it is still sensitive to outliers in the data that produce large errors something that the method tries to avoid in the first place 
it is therefore common practice to remove outliers before computing pca 
however in some contexts outliers can be difficult to identify 
for example in data mining algorithms like correlation clustering the assignment of points to clusters and outliers is not known beforehand 
recently proposed generalization of pca based on weighted pca increases robustness by assigning different weights to data objects based on their estimated relevancy 
outlier resistant variants of pca have also been proposed based on norm formulations pca robust principal component analysis rpca via decomposition in low rank and sparse matrices is modification of pca that works well with respect to grossly corrupted observations 
similar techniques independent component analysis independent component analysis ica is directed to similar problems as principal component analysis but finds additively separable components rather than successive approximations 
network component analysis given matrix it tries to decompose it into two matrices such that key difference from techniques such as pca and ica is that some of the entries of are constrained to be here is termed the regulatory layer 
while in general such decomposition can have multiple solutions they prove that if the following conditions are satisfied has full column rank each column of must have at least zeroes where is the number of columns of or alternatively the number of rows of 
the justification for this criterion is that if node is removed from the regulatory layer along with all the output nodes connected to it the result must still be characterized by connectivity matrix with full column rank 
must have full row rank then the decomposition is unique up to multiplication by scalar 
discriminant analysis of principal components discriminant analysis of principal components dapc is multivariate method used to identify and describe clusters of genetically related individuals 
genetic variation is partitioned into two components variation between groups and within groups and it maximizes the former 
linear discriminants are linear combinations of alleles which best separate the clusters 
alleles that most contribute to this discrimination are therefore those that are the most markedly different across groups 
the contributions of alleles to the groupings identified by dapc can allow identifying regions of the genome driving the genetic divergence among groups in dapc data is first transformed using principal components analysis pca and subsequently clusters are identified using discriminant analysis da 
dapc can be realized on using the package adegenet 
more info adegenet on the web software source code alglib and library that implements pca and truncated pca analytica the built in eigendecomp function computes principal components 
elki includes pca for projection including robust variants of pca as well as pca based clustering algorithms 
gretl principal component analysis can be performed either via the pca command or via the princomp function 
julia supports pca with the pca function in the multivariatestats package knime java based nodal arranging software for analysis in this the nodes called pca pca compute pca apply pca inverse make it easily 
mathematica implements principal component analysis with the principalcomponents command using both covariance and correlation methods 
mathphp php mathematics library with support for pca 
matlab the svd function is part of the basic system 
in the statistics toolbox the functions princomp and pca give the principal components while the function pcares gives the residuals and reconstructed matrix for low rank pca approximation 
matplotlib python library have pca package in the mlab module 
mlpack provides an implementation of principal component analysis in 
nag library principal components analysis is implemented via the aa routine available in both the fortran versions of the library 
nmath proprietary numerical library containing pca for the net framework 
gnu octave free software computational environment mostly compatible with matlab the function princomp gives the principal component 
opencv oracle database implemented via dbms data mining svds scoring mode by specifying setting value svds scoring pca orange software integrates pca in its visual programming environment 
pca displays scree plot degree of explained variance where user can interactively select the number of principal components 
origin contains pca in its pro version 
qlucore commercial software for analyzing multivariate data with instant response using pca 
free statistical package the functions princomp and prcomp can be used for principal component analysis prcomp uses singular value decomposition which generally gives better numerical accuracy 
some packages that implement pca in include but are not limited to ade vegan exposition dimred and factominer 
sas proprietary software for example see scikit learn python library for machine learning which contains pca probabilistic pca kernel pca sparse pca and other techniques in the decomposition module 
spss proprietary software most commonly used by social scientists for pca factor analysis and associated cluster analysis 
weka java library for machine learning which contains modules for computing principal components 
see also references further reading jackson 
user guide to principal components wiley 
springer series in statistics 
springer series in statistics 
new york springer verlag 
isbn husson fran ois bastien pag me 
exploratory multivariate analysis by example using chapman hall crc the series london 
multiple factor analysis by example using chapman hall crc the series london external links university of copenhagen video by rasmus bro on youtube stanford university video by andrew ng on youtube tutorial on principal component analysis layman introduction to principal component analysis on youtube video of less than seconds 
statquest principal component analysis pca clearly explained on youtube see also the list of software implementations
independence is fundamental notion in probability theory as in statistics and the theory of stochastic processes 
two events are independent statistically independent or stochastically independent if informally speaking the occurrence of one does not affect the probability of occurrence of the other or equivalently does not affect the odds 
similarly two random variables are independent if the realization of one does not affect the probability distribution of the other 
when dealing with collections of more than two events two notions of independence need to be distinguished 
the events are called pairwise independent if any two events in the collection are independent of each other while mutual independence or collective independence of events means informally speaking that each event is independent of any combination of other events in the collection 
similar notion exists for collections of random variables 
mutual independence implies pairwise independence but not the other way around 
in the standard literature of probability theory statistics and stochastic processes independence without further qualification usually refers to mutual independence 
definition for events two events two events and are independent often written as or where the latter symbol often is also used for conditional independence if and only if their joint probability equals the product of their probabilities indicates that two independent events and have common elements in their sample space so that they are not mutually exclusive mutually exclusive iff 
why this defines independence is made clear by rewriting with conditional probabilities as the probability at which the event occurs provided that the event has or is assumed to have occurred 
thus the occurrence of does not affect the probability of and vice versa 
in other words and are independent to each other 
although the derived expressions may seem more intuitive they are not the preferred definition as the conditional probabilities may be undefined if or are furthermore the preferred definition makes clear by symmetry that when is independent of is also independent of 
log probability and information content stated in terms of log probability two events are independent if and only if the log probability of the joint event is the sum of the log probability of the individual events log log log in information theory negative log probability is interpreted as information content and thus two events are independent if and only if the information content of the combined event equals the sum of information content of the individual events see information content additivity of independent events for details 
odds stated in terms of odds two events are independent if and only if the odds ratio of and is unity 
analogously with probability this is equivalent to the conditional odds being equal to the unconditional odds and or to the odds of one event given the other event being the same as the odds of the event given the other event not occurring and 
the odds ratio can be defined as or symmetrically for odds of given and thus is if and only if the events are independent 
more than two events finite set of events is pairwise independent if every pair of events is independent that is if and only if for all distinct pairs of indices finite set of events is mutually independent if every event is independent of any intersection of the other events that is if and only if for every and for every indices this is called the multiplication rule for independent events 
note that it is not single condition involving only the product of all the probabilities of all single events it must hold true for all subsets of events 
for more than two events mutually independent set of events is by definition pairwise independent but the converse is not necessarily true 
for real valued random variables two random variables two random variables and are independent if and only if iff the elements of the system generated by them are independent that is to say for every and the events and are independent events as defined above in eq 
that is and with cumulative distribution functions and are independent iff the combined random variable has joint cumulative distribution function or equivalently if the probability densities and and the joint probability density exist for all 
more than two random variables finite set of random variables is pairwise independent if and only if every pair of random variables is independent 
even if the set of random variables is pairwise independent it is not necessarily mutually independent as defined next 
finite set of random variables is mutually independent if and only if for any sequence of numbers the events are mutually independent events as defined above in eq 
this is equivalent to the following condition on the joint cumulative distribution function 
finite set of random variables is mutually independent if and only if notice that it is not necessary here to require that the probability distribution factorizes for all possible element subsets as in the case for events 
this is not required because 
the measure theoretically inclined may prefer to substitute events for events in the above definition where is any borel set 
that definition is exactly equivalent to the one above when the values of the random variables are real numbers 
it has the advantage of working also for complex valued random variables or for random variables taking values in any measurable space which includes topological spaces endowed by appropriate algebras 
for real valued random vectors two random vectors and are called independent if where and denote the cumulative distribution functions of and and denotes their joint cumulative distribution function 
independence of and is often denoted by written component wise and are called independent if for all for stochastic processes for one stochastic process the definition of independence may be extended from random vectors to stochastic process 
therefore it is required for an independent stochastic process that the random variables obtained by sampling the process at any times are independent random variables for any formally stochastic process is called independent if and only if for all and for all where 
independence of stochastic process is property within stochastic process not between two stochastic processes 
for two stochastic processes independence of two stochastic processes is property between two stochastic processes and that are defined on the same probability space 
formally two stochastic processes and are said to be independent if for all and for all the random vectors and are independent 
if independent algebras the definitions above eq and eq are both generalized by the following definition of independence for algebras 
let be probability space and let and be two sub algebras of and are said to be independent if whenever and 
likewise finite family of algebras where is an index set is said to be independent if and only if and an infinite family of algebras is said to be independent if all its finite subfamilies are independent 
the new definition relates to the previous ones very directly two events are independent in the old sense if and only if the algebras that they generate are independent in the new sense 
the algebra generated by an event is by definition 
two random variables and defined over are independent in the old sense if and only if the algebras that they generate are independent in the new sense 
the algebra generated by random variable taking values in some measurable space consists by definition of all subsets of of the form where is any measurable subset of using this definition it is easy to show that if and are random variables and is constant then and are independent since the algebra generated by constant random variable is the trivial algebra 
probability zero events cannot affect independence so independence also holds if is only pr almost surely constant 
properties self independence note that an event is independent of itself if and only if or thus an event is independent of itself if and only if it almost surely occurs or its complement almost surely occurs this fact is useful when proving zero one laws 
expectation and covariance if and are independent random variables then the expectation operator has the property and the covariance cov is zero as follows from cov 
the converse does not hold if two random variables have covariance of they still may be not independent 
similarly for two stochastic processes and if they are independent then they are uncorrelated 
characteristic function two random variables and are independent if and only if the characteristic function of the random vector satisfies 
in particular the characteristic function of their sum is the product of their marginal characteristic functions though the reverse implication is not true 
random variables that satisfy the latter condition are called subindependent 
examples rolling dice the event of getting the first time die is rolled and the event of getting the second time are independent 
by contrast the event of getting the first time die is rolled and the event that the sum of the numbers seen on the first and second trial is are not independent 
drawing cards if two cards are drawn with replacement from deck of cards the event of drawing red card on the first trial and that of drawing red card on the second trial are independent 
by contrast if two cards are drawn without replacement from deck of cards the event of drawing red card on the first trial and that of drawing red card on the second trial are not independent because deck that has had red card removed has proportionately fewer red cards 
pairwise and mutual independence consider the two probability spaces shown 
in both cases and the random variables in the first space are pairwise independent because and but the three random variables are not mutually independent 
the random variables in the second space are both pairwise independent and mutually independent 
to illustrate the difference consider conditioning on two events 
in the pairwise independent case although any one event is independent of each of the other two individually it is not independent of the intersection of the other two in the mutually independent case however triple independence but no pairwise independence it is possible to create three event example in which and yet no two of the three events are pairwise independent and hence the set of events are not mutually independent 
this example shows that mutual independence involves requirements on the products of probabilities of all combinations of events not just the single events as in this example 
conditional independence for events the events and are conditionally independent given an event when 
for random variables intuitively two random variables and are conditionally independent given if once is known the value of does not add any additional information about for instance two measurements and of the same underlying quantity are not independent but they are conditionally independent given unless the errors in the two measurements are somehow connected 
the formal definition of conditional independence is based on the idea of conditional distributions 
if and are discrete random variables then we define and to be conditionally independent given if for all and such that on the other hand if the random variables are continuous and have joint probability density function then and are conditionally independent given if for all real numbers and such that if discrete and are conditionally independent given then for any and with that is the conditional distribution for given and is the same as that given alone 
similar equation holds for the conditional probability density functions in the continuous case 
independence can be seen as special kind of conditional independence since probability can be seen as kind of conditional probability given no events 
see also copula statistics independent and identically distributed random variables mutually exclusive events pairwise independent events subindependence conditional independence normally distributed and uncorrelated does not imply independent mean dependence references external links media related to independence probability theory at wikimedia commons
in signal processing independent component analysis ica is computational method for separating multivariate signal into additive subcomponents 
this is done by assuming that at most one subcomponent is gaussian and that the subcomponents are statistically independent from each other 
ica is special case of blind source separation 
common example application is the cocktail party problem of listening in on one person speech in noisy room 
introduction independent component analysis attempts to decompose multivariate signal into independent non gaussian signals 
as an example sound is usually signal that is composed of the numerical addition at each time of signals from several sources 
the question then is whether it is possible to separate these contributing sources from the observed total signal 
when the statistical independence assumption is correct blind ica separation of mixed signal gives very good results 
it is also used for signals that are not supposed to be generated by mixing for analysis purposes 
simple application of ica is the cocktail party problem where the underlying speech signals are separated from sample data consisting of people talking simultaneously in room 
usually the problem is simplified by assuming no time delays or echoes 
note that filtered and delayed signal is copy of dependent component and thus the statistical independence assumption is not violated 
mixing weights for constructing the observed signals from the components can be placed in an matrix 
an important thing to consider is that if sources are present at least observations 
microphones if the observed signal is audio are needed to recover the original signals 
when there are an equal number of observations and source signals the mixing matrix is square 
other cases of underdetermined and overdetermined have been investigated 
that the ica separation of mixed signals gives very good results is based on two assumptions and three effects of mixing source signals 
two assumptions the source signals are independent of each other 
the values in each source signal have non gaussian distributions three effects of mixing source signals independence as per assumption the source signals are independent however their signal mixtures are not 
this is because the signal mixtures share the same source signals 
normality according to the central limit theorem the distribution of sum of independent random variables with finite variance tends towards gaussian distribution loosely speaking sum of two independent random variables usually has distribution that is closer to gaussian than any of the two original variables 
here we consider the value of each signal as the random variable 
complexity the temporal complexity of any signal mixture is greater than that of its simplest constituent source signal those principles contribute to the basic establishment of ica 
if the signals extracted from set of mixtures are independent and have non gaussian histograms or have low complexity then they must be source signals 
defining component independence ica finds the independent components also called factors latent variables or sources by maximizing the statistical independence of the estimated components 
we may choose one of many ways to define proxy for independence and this choice governs the form of the ica algorithm 
the two broadest definitions of independence for ica are minimization of mutual information maximization of non gaussianitythe minimization of mutual information mmi family of ica algorithms uses measures like kullback leibler divergence and maximum entropy 
the non gaussianity family of ica algorithms motivated by the central limit theorem uses kurtosis and negentropy 
typical algorithms for ica use centering subtract the mean to create zero mean signal whitening usually with the eigenvalue decomposition and dimensionality reduction as preprocessing steps in order to simplify and reduce the complexity of the problem for the actual iterative algorithm 
whitening and dimension reduction can be achieved with principal component analysis or singular value decomposition 
whitening ensures that all dimensions are treated equally priori before the algorithm is run 
well known algorithms for ica include infomax fastica jade and kernel independent component analysis among others 
in general ica cannot identify the actual number of source signals uniquely correct ordering of the source signals nor the proper scaling including sign of the source signals 
ica is important to blind signal separation and has many practical applications 
it is closely related to or even special case of the search for factorial code of the data new vector valued representation of each data vector such that it gets uniquely encoded by the resulting code vector loss free coding but the code components are statistically independent 
mathematical definitions linear independent component analysis can be divided into noiseless and noisy cases where noiseless ica is special case of noisy ica 
nonlinear ica should be considered as separate case 
general definition the data are represented by the observed random vector and the hidden components as the random vector the task is to transform the observed data using linear static transformation as into vector of maximally independent components measured by some function of independence 
generative model linear noiseless ica the components of the observed random vector are generated as sum of the independent components weighted by the mixing weights the same generative model can be written in vector form as where the observed random vector is represented by the basis vectors the basis vectors form the columns of the mixing matrix and the generative formula can be written as where given the model and realizations samples of the random vector the task is to estimate both the mixing matrix and the sources this is done by adaptively calculating the vectors and setting up cost function which either maximizes the non gaussianity of the calculated or minimizes the mutual information 
in some cases priori knowledge of the probability distributions of the sources can be used in the cost function 
the original sources can be recovered by multiplying the observed signals with the inverse of the mixing matrix also known as the unmixing matrix 
here it is assumed that the mixing matrix is square 
if the number of basis vectors is greater than the dimensionality of the observed vectors the task is overcomplete but is still solvable with the pseudo inverse 
linear noisy ica with the added assumption of zero mean and uncorrelated gaussian noise diag the ica model takes the form nonlinear ica the mixing of the sources does not need to be linear 
using nonlinear mixing function with parameters the nonlinear ica model is identifiability the independent components are identifiable up to permutation and scaling of the sources 
this identifiability requires that at most one of the sources is gaussian the number of observed mixtures must be at least as large as the number of estimated components it is equivalent to say that the mixing matrix must be of full rank for its inverse to exist 
binary ica special variant of ica is binary ica in which both signal sources and monitors are in binary form and observations from monitors are disjunctive mixtures of binary independent sources 
the problem was shown to have applications in many domains including medical diagnosis multi cluster assignment network tomography and internet resource management 
let be the set of binary variables from monitors and be the set of binary variables from sources 
source monitor connections are represented by the unknown mixing matrix where indicates that signal from the th source can be observed by the th monitor 
the system works as follows at any time if source is active and it is connected to the monitor then the monitor will observe some activity 
formally we have where is boolean and and is boolean or 
note that noise is not explicitly modelled rather can be treated as independent sources 
the above problem can be heuristically solved by assuming variables are continuous and running fastica on binary observation data to get the mixing matrix real values then apply round number techniques on to obtain the binary values 
this approach has been shown to produce highly inaccurate result another method is to use dynamic programming recursively breaking the observation matrix into its sub matrices and run the inference algorithm on these sub matrices 
the key observation which leads to this algorithm is the sub matrix of where corresponds to the unbiased observation matrix of hidden components that do not have connection to the th monitor 
experimental results from show that this approach is accurate under moderate noise levels 
the generalized binary ica framework introduces broader problem formulation which does not necessitate any knowledge on the generative model 
in other words this method attempts to decompose source into its independent components as much as possible and without losing any information with no prior assumption on the way it was generated 
although this problem appears quite complex it can be accurately solved with branch and bound search tree algorithm or tightly upper bounded with single multiplication of matrix with vector 
methods for blind source separation projection pursuit signal mixtures tend to have gaussian probability density functions and source signals tend to have non gaussian probability density functions 
each source signal can be extracted from set of signal mixtures by taking the inner product of weight vector and those signal mixtures where this inner product provides an orthogonal projection of the signal mixtures 
the remaining challenge is finding such weight vector 
one type of method for doing so is projection pursuit projection pursuit seeks one projection at time such that the extracted signal is as non gaussian as possible 
this contrasts with ica which typically extracts signals simultaneously from signal mixtures which requires estimating unmixing matrix 
one practical advantage of projection pursuit over ica is that fewer than signals can be extracted if required where each source signal is extracted from signal mixtures using an element weight vector 
we can use kurtosis to recover the multiple source signal by finding the correct weight vectors with the use of projection pursuit 
the kurtosis of the probability density function of signal for finite sample is computed as where is the sample mean of the extracted signals 
the constant ensures that gaussian signals have zero kurtosis super gaussian signals have positive kurtosis and sub gaussian signals have negative kurtosis 
the denominator is the variance of and ensures that the measured kurtosis takes account of signal variance 
the goal of projection pursuit is to maximize the kurtosis and make the extracted signal as non normal as possible 
using kurtosis as measure of non normality we can now examine how the kurtosis of signal extracted from set of mixtures varies as the weight vector is rotated around the origin 
given our assumption that each source signal is super gaussian we would expect the kurtosis of the extracted signal to be maximal precisely when the kurtosis of the extracted signal to be maximal when is orthogonal to the projected axes or because we know the optimal weight vector should be orthogonal to transformed axis or for multiple source mixture signals we can use kurtosis and gram schmidt orthogonalization gso to recover the signals 
given signal mixtures in an dimensional space gso project these data points onto an dimensional space by using the weight vector 
we can guarantee the independence of the extracted signals with the use of gso 
in order to find the correct value of we can use gradient descent method 
we first of all whiten the data and transform into new mixture which has unit variance and this process can be achieved by applying singular value decomposition to rescaling each vector and let the signal extracted by weighted vector is if the weight vector has unit length then the variance of is also that is the kurtosis can thus be written as the updating process for is 
where is small constant to guarantee that converges to the optimal solution 
after each update we normalize and set and repeat the updating process until convergence 
we can also use another algorithm to update the weight vector another approach is using negentropy instead of kurtosis 
using negentropy is more robust method than kurtosis as kurtosis is very sensitive to outliers 
the negentropy methods are based on an important property of gaussian distribution gaussian variable has the largest entropy among all continuous random variables of equal variance 
this is also the reason why we want to find the most nongaussian variables 
simple proof can be found in differential entropy 
is gaussian random variable of the same covariance matrix as log an approximation for negentropy is proof can be found in the original papers of comon it has been reproduced in the book independent component analysis by aapo hyv rinen juha karhunen and erkki oja this approximation also suffers from the same problem as kurtosis sensitivity to outliers 
other approaches have been developed 
choice of and are log cosh and exp based on infomax infomax ica is essentially multivariate parallel version of projection pursuit 
whereas projection pursuit extracts series of signals one at time from set of signal mixtures ica extracts signals in parallel 
this tends to make ica more robust than projection pursuit the projection pursuit method uses gram schmidt orthogonalization to ensure the independence of the extracted signal while ica use infomax and maximum likelihood estimate to ensure the independence of the extracted signal 
the non normality of the extracted signal is achieved by assigning an appropriate model or prior for the signal 
the process of ica based on infomax in short is given set of signal mixtures and set of identical independent model cumulative distribution functions cdfs we seek the unmixing matrix which maximizes the joint entropy of the signals where are the signals extracted by given the optimal the signals have maximum entropy and are therefore independent which ensures that the extracted signals are also independent 
is an invertible function and is the signal model 
note that if the source signal model probability density function matches the probability density function of the extracted signal then maximizing the joint entropy of also maximizes the amount of mutual information between and for this reason using entropy to extract independent signals is known as infomax 
consider the entropy of the vector variable where is the set of signals extracted by the unmixing matrix for finite set of values sampled from distribution with pdf the entropy of can be estimated as ln the joint pdf can be shown to be related to the joint pdf of the extracted signals by the multivariate form where is the jacobian matrix 
we have and is the pdf assumed for source signals therefore therefore ln we know that when is of uniform distribution and is maximized 
since where is the absolute value of the determinant of the unmixing matrix therefore ln so ln ln since ln and maximizing does not affect so we can maximize the function ln ln to achieve the independence of extracted signal 
if there are marginal pdfs of the model joint pdf are independent and use the commonly super gaussian model pdf for the source signals tanh then we have ln tanh ln in the sum given an observed signal mixture the corresponding set of extracted signals and source signal model we can find the optimal unmixing matrix and make the extracted signals independent and non gaussian 
like the projection pursuit situation we can use gradient descent method to find the optimal solution of the unmixing matrix 
based on maximum likelihood estimation maximum likelihood estimation mle is standard statistical tool for finding parameter values 
the unmixing matrix that provide the best fit of some data the extracted signals to given model the assumed joint probability density function pdf of source signals the ml model includes specification of pdf which in this case is the pdf of the unknown source signals using ml ica the objective is to find an unmixing matrix that yields extracted signals with joint pdf as similar as possible to the joint pdf of the unknown source signals mle is thus based on the assumption that if the model pdf and the model parameters are correct then high probability should be obtained for the data that were actually observed 
conversely if is far from the correct parameter values then low probability of the observed data would be expected 
using mle we call the probability of the observed data for given set of model parameter values pdf and matrix the likelihood of the model parameter values given the observed data 
we define likelihood function of det 
this equals to the probability density at since thus if we wish to find that is most likely to have generated the observed mixtures from the unknown source signals with pdf then we need only find that which maximizes the likelihood 
the unmixing matrix that maximizes equation is known as the mle of the optimal unmixing matrix 
it is common practice to use the log likelihood because this is easier to evaluate 
as the logarithm is monotonic function the that maximizes the function also maximizes its logarithm ln 
this allows us to take the logarithm of equation above which yields the log likelihood function ln ln ln det if we substitute commonly used high kurtosis model pdf for the source signals tanh then we have ln ln tanh ln det this matrix that maximizes this function is the maximum likelihood estimation 
history and background the early general framework for independent component analysis was introduced by jeanny rault and bernard ans from further developed by christian jutten in and and refined by pierre comon in and popularized in his paper of in tony bell and terry sejnowski introduced fast and efficient ica algorithm based on infomax principle introduced by ralph linsker in there are many algorithms available in the literature which do ica 
largely used one including in industrial applications is the fastica algorithm developed by hyv rinen and oja which uses the negentropy as cost function 
other examples are rather related to blind source separation where more general approach is used 
for example one can drop the independence assumption and separate mutually correlated signals thus statistically dependent signals 
sepp hochreiter and rgen schmidhuber showed how to obtain non linear ica or source separation as by product of regularization 
their method does not require priori knowledge about the number of independent sources 
applications ica can be extended to analyze non physical signals 
for instance ica has been applied to discover discussion topics on bag of news list archives 
some ica applications are listed below optical imaging of neurons neuronal spike sorting face recognition modelling receptive fields of primary visual neurons predicting stock market prices mobile phone communications colour based detection of the ripeness of tomatoes removing artifacts such as eye blinks from eeg data 
predicting decision making using eeg analysis of changes in gene expression over time in single cell rna sequencing experiments 
studies of the resting state network of the brain 
astronomy and cosmology finance availability ica can be applied through the following software sas proc ica scikit learn python implementation sklearn decomposition fastica see also notes references comon pierre independent component analysis new concept 
signal processing the original paper describing the concept of ica hyv rinen karhunen oja independent component analysis new york wiley isbn introductory chapter hyv rinen oja independent component analysis algorithms and application neural networks 
technical but pedagogical introduction 
comon jutten handbook of blind source separation independent component analysis and applications 
academic press oxford uk 
isbn lee independent component analysis theory and applications boston mass kluwer academic publishers isbn acharyya ranjan new approach for blind source separation of convolutive sources wavelet based separation using shrinkage function isbn isbn this book focuses on unsupervised learning with blind source separation external links what is independent component analysis 
by aapo hyv rinen independent component analysis tutorial by aapo hyv rinen tutorial on independent component analysis fastica as package for matlab in language icalab toolboxes for matlab developed at riken high performance signal analysis toolkit provides implementations of fastica and infomax ica toolbox matlab tools for ica with bell sejnowski molgedey schuster and mean field ica 
demonstration of the cocktail party problem eeglab toolbox ica of eeg for matlab developed at ucsd 
fmrlab toolbox ica of fmri for matlab developed at ucsd melodic part of the fmrib software library 
discussion of ica used in biomedical shape representation context fastica cubica jade and tdsep algorithm for python and more group ica toolbox and fusion ica toolbox tutorial using ica for cleaning eeg signals
componential analysis feature analysis or contrast analysis is the analysis of words through structured sets of semantic features which are given as present absent or indifferent with reference to feature 
the method thus departs from the principle of compositionality 
componential analysis is method typical of structural semantics which analyzes the components of word meaning 
thus it reveals the culturally important features by which speakers of the language distinguish different words in semantic field or domain ottenheimer 
examples man male mature or woman male mature or boy male mature or girl male mature or child male mature 
in other words the word girl can have three basic factors or semantic properties human young and female 
another example being edible is an important factor by which plants may be distinguished from one another ottenheimer 
to summarize one word can have basic underlying meanings that are well established depending on the cultural context 
it is crucial to understand these underlying meanings in order to fully understand any language and culture 
historical background structural semantics and the componential analysis were patterned on the phonological methods of the prague school which described sounds by determining the absence and presence of features 
on one hand componential analysis gave birth to various models in generative semantics lexical field theory and transformational grammar 
on the other hand its shortcoming were also visible the discovery procedures for semantic features are not clearly objectifiable 
only part of the vocabulary can be described through more or less structured sets of features 
metalinguistic features are expressed through language again 
features used may not have clear definitions 
limited in focus and mechanical in style as consequence entirely different ways to describe meaning were developed such as prototype semantics 
see also ethnoscience structural linguistics word sense disambiguation references bussmann hadumod routledge dictionary of language and linguistics london routledge 
the anthropology of language 
belmont ca thomson wadsworth
in statistics linear regression is linear approach for modelling the relationship between scalar response and one or more explanatory variables also known as dependent and independent variables 
the case of one explanatory variable is called simple linear regression for more than one the process is called multiple linear regression 
this term is distinct from multivariate linear regression where multiple correlated dependent variables are predicted rather than single scalar variable in linear regression the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data 
such models are called linear models 
most commonly the conditional mean of the response given the values of the explanatory variables or predictors is assumed to be an affine function of those values less commonly the conditional median or some other quantile is used 
like all forms of regression analysis linear regression focuses on the conditional probability distribution of the response given the values of the predictors rather than on the joint probability distribution of all of these variables which is the domain of multivariate analysis 
linear regression was the first type of regression analysis to be studied rigorously and to be used extensively in practical applications 
this is because models which depend linearly on their unknown parameters are easier to fit than models which are non linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine 
linear regression has many practical uses 
most applications fall into one of the following two broad categories if the goal is error reduction in prediction or forecasting linear regression can be used to fit predictive model to an observed data set of values of the response and explanatory variables 
after developing such model if additional values of the explanatory variables are collected without an accompanying response value the fitted model can be used to make prediction of the response 
if the goal is to explain variation in the response variable that can be attributed to variation in the explanatory variables linear regression analysis can be applied to quantify the strength of the relationship between the response and the explanatory variables and in particular to determine whether some explanatory variables may have no linear relationship with the response at all or to identify which subsets of explanatory variables may contain redundant information about the response linear regression models are often fitted using the least squares approach but they may also be fitted in other ways such as by minimizing the lack of fit in some other norm as with least absolute deviations regression or by minimizing penalized version of the least squares cost function as in ridge regression norm penalty and lasso norm penalty 
conversely the least squares approach can be used to fit models that are not linear models 
thus although the terms least squares and linear model are closely linked they are not synonymous 
formulation given data set of statistical units linear regression model assumes that the relationship between the dependent variable and the vector of regressors is linear 
this relationship is modeled through disturbance term or error variable an unobserved random variable that adds noise to the linear relationship between the dependent variable and regressors 
thus the model takes the form where denotes the transpose so that xit is the inner product between vectors xi and often these equations are stacked together and written in matrix notation as where 
notation and terminology is vector of observed values of the variable called the regressand endogenous variable response variable measured variable criterion variable or dependent variable 
this variable is also sometimes known as the predicted variable but this should not be confused with predicted values which are denoted 
the decision as to which variable in data set is modeled as the dependent variable and which are modeled as the independent variables may be based on presumption that the value of one of the variables is caused by or directly influenced by the other variables 
alternatively there may be an operational reason to model one of the variables in terms of the others in which case there need be no presumption of causality 
may be seen as matrix of row vectors or of dimensional column vectors which are known as regressors exogenous variables explanatory variables covariates input variables predictor variables or independent variables not to be confused with the concept of independent random variables 
the matrix is sometimes called the design matrix 
usually constant is included as one of the regressors 
in particular for the corresponding element of is called the intercept 
many statistical inference procedures for linear models require an intercept to be present so it is often included even if theoretical considerations suggest that its value should be zero 
sometimes one of the regressors can be non linear function of another regressor or of the data as in polynomial regression and segmented regression 
the model remains linear as long as it is linear in the parameter vector the values xij may be viewed as either observed values of random variables xj or as fixed values chosen prior to observing the dependent variable 
both interpretations may be appropriate in different cases and they generally lead to the same estimation procedures however different approaches to asymptotic analysis are used in these two situations 
is dimensional parameter vector where is the intercept term if one is included in the model otherwise is dimensional 
its elements are known as effects or regression coefficients although the latter term is sometimes reserved for the estimated effects 
in simple linear regression and the coefficient is known as regression slope 
statistical estimation and inference in linear regression focuses on the elements of this parameter vector are interpreted as the partial derivatives of the dependent variable with respect to the various independent variables 
is vector of values this part of the model is called the error term disturbance term or sometimes noise in contrast with the signal provided by the rest of the model 
this variable captures all other factors which influence the dependent variable other than the regressors the relationship between the error term and the regressors for example their correlation is crucial consideration in formulating linear regression model as it will determine the appropriate estimation method fitting linear model to given data set usually requires estimating the regression coefficients such that the error term is minimized 
for example it is common to use the sum of squared errors as measure of for minimization 
example consider situation where small ball is being tossed up in the air and then we measure its heights of ascent hi at various moments in time ti 
physics tells us that ignoring the drag the relationship can be modeled as where determines the initial velocity of the ball is proportional to the standard gravity and is due to measurement errors 
linear regression can be used to estimate the values of and from the measured data 
this model is non linear in the time variable but it is linear in the parameters and if we take regressors xi xi xi ti ti the model takes on the standard form 
assumptions standard linear regression models with standard estimation techniques make number of assumptions about the predictor variables the response variables and their relationship 
numerous extensions have been developed that allow each of these assumptions to be relaxed 
reduced to weaker form and in some cases eliminated entirely 
generally these extensions make the estimation procedure more complex and time consuming and may also require more data in order to produce an equally precise model 
the following are the major assumptions made by standard linear regression models with standard estimation techniques 
ordinary least squares weak exogeneity 
this essentially means that the predictor variables can be treated as fixed values rather than random variables 
this means for example that the predictor variables are assumed to be error free that is not contaminated with measurement errors 
although this assumption is not realistic in many settings dropping it leads to significantly more difficult errors in variables models 
this means that the mean of the response variable is linear combination of the parameters regression coefficients and the predictor variables 
note that this assumption is much less restrictive than it may at first seem 
because the predictor variables are treated as fixed values see above linearity is really only restriction on the parameters 
the predictor variables themselves can be arbitrarily transformed and in fact multiple copies of the same underlying predictor variable can be added each one transformed differently 
this technique is used for example in polynomial regression which uses linear regression to fit the response variable as an arbitrary polynomial function up to given degree of predictor variable 
with this much flexibility models such as polynomial regression often have too much power in that they tend to overfit the data 
as result some kind of regularization must typically be used to prevent unreasonable solutions coming out of the estimation process 
common examples are ridge regression and lasso regression 
bayesian linear regression can also be used which by its nature is more or less immune to the problem of overfitting 
in fact ridge regression and lasso regression can both be viewed as special cases of bayesian linear regression with particular types of prior distributions placed on the regression coefficients 
this means that the variance of the errors does not depend on the values of the predictor variables 
thus the variability of the responses for given fixed values of the predictors is the same regardless of how large or small the responses are 
this is often not the case as variable whose mean is large will typically have greater variance than one whose mean is small 
for example person whose income is predicted to be may easily have an actual income of or standard deviation of around while another person with predicted income of is unlikely to have the same standard deviation since that would imply their actual income could vary anywhere between and 
in fact as this shows in many cases often the same cases where the assumption of normally distributed errors fails the variance or standard deviation should be predicted to be proportional to the mean rather than constant 
the absence of homoscedasticity is called heteroscedasticity 
in order to check this assumption plot of residuals versus predicted values or the values of each individual predictor can be examined for fanning effect increasing or decreasing vertical spread as one moves left to right on the plot 
plot of the absolute or squared residuals versus the predicted values or each predictor can also be examined for trend or curvature 
formal tests can also be used see heteroscedasticity 
the presence of heteroscedasticity will result in an overall average estimate of variance being used instead of one that takes into account the true variance structure 
this leads to less precise but in the case of ordinary least squares not biased parameter estimates and biased standard errors resulting in misleading tests and interval estimates 
the mean squared error for the model will also be wrong 
various estimation techniques including weighted least squares and the use of heteroscedasticity consistent standard errors can handle heteroscedasticity in quite general way 
bayesian linear regression techniques can also be used when the variance is assumed to be function of the mean 
it is also possible in some cases to fix the problem by applying transformation to the response variable fitting the logarithm of the response variable using linear regression model which implies that the response variable itself has log normal distribution rather than normal distribution 
this assumes that the errors of the response variables are uncorrelated with each other 
actual statistical independence is stronger condition than mere lack of correlation and is often not needed although it can be exploited if it is known to hold 
some methods such as generalized least squares are capable of handling correlated errors although they typically require significantly more data unless some sort of regularization is used to bias the model towards assuming uncorrelated errors 
bayesian linear regression is general way of handling this issue 
lack of perfect multicollinearity in the predictors 
for standard least squares estimation methods the design matrix must have full column rank otherwise perfect multicollinearity exists in the predictor variables meaning linear relationship exists between two or more predictor variables 
this can be caused by accidentally duplicating variable in the data using linear transformation of variable along with the original the same temperature measurements expressed in fahrenheit and celsius or including linear combination of multiple variables in the model such as their mean 
it can also happen if there is too little data available compared to the number of parameters to be estimated fewer data points than regression coefficients 
near violations of this assumption where predictors are highly but not perfectly correlated can reduce the precision of parameter estimates see variance inflation factor 
in the case of perfect multicollinearity the parameter vector will be non identifiable it has no unique solution 
in such case only some of the parameters can be identified their values can only be estimated within some linear subspace of the full parameter space rp 
see partial least squares regression 
methods for fitting linear models with multicollinearity have been developed some of which require additional assumptions such as effect sparsity that large fraction of the effects are exactly zero 
note that the more computationally expensive iterated algorithms for parameter estimation such as those used in generalized linear models do not suffer from this problem beyond these assumptions several other statistical properties of the data strongly influence the performance of different estimation methods the statistical relationship between the error terms and the regressors plays an important role in determining whether an estimation procedure has desirable sampling properties such as being unbiased and consistent 
the arrangement or probability distribution of the predictor variables has major influence on the precision of estimates of sampling and design of experiments are highly developed subfields of statistics that provide guidance for collecting data in such way to achieve precise estimate of 
interpretation fitted linear regression model can be used to identify the relationship between single predictor variable xj and the response variable when all the other predictor variables in the model are held fixed 
specifically the interpretation of is the expected change in for one unit change in xj when the other covariates are held fixed that is the expected value of the partial derivative of with respect to xj 
this is sometimes called the unique effect of xj on in contrast the marginal effect of xj on can be assessed using correlation coefficient or simple linear regression model relating only xj to this effect is the total derivative of with respect to xj 
care must be taken when interpreting regression results as some of the regressors may not allow for marginal changes such as dummy variables or the intercept term while others cannot be held fixed recall the example from the introduction it would be impossible to hold ti fixed and at the same time change the value of ti 
it is possible that the unique effect can be nearly zero even when the marginal effect is large 
this may imply that some other covariate captures all the information in xj so that once that variable is in the model there is no contribution of xj to the variation in conversely the unique effect of xj can be large while its marginal effect is nearly zero 
this would happen if the other covariates explained great deal of the variation of but they mainly explain variation in way that is complementary to what is captured by xj 
in this case including the other variables in the model reduces the part of the variability of that is unrelated to xj thereby strengthening the apparent relationship with xj 
the meaning of the expression held fixed may depend on how the values of the predictor variables arise 
if the experimenter directly sets the values of the predictor variables according to study design the comparisons of interest may literally correspond to comparisons among units whose predictor variables have been held fixed by the experimenter 
alternatively the expression held fixed can refer to selection that takes place in the context of data analysis 
in this case we hold variable fixed by restricting our attention to the subsets of the data that happen to have common value for the given predictor variable 
this is the only interpretation of held fixed that can be used in an observational study 
the notion of unique effect is appealing when studying complex system where multiple interrelated components influence the response variable 
in some cases it can literally be interpreted as the causal effect of an intervention that is linked to the value of predictor variable 
however it has been argued that in many cases multiple regression analysis fails to clarify the relationships between the predictor variables and the response variable when the predictors are correlated with each other and are not assigned following study design 
group effects in multiple linear regression model parameter of predictor variable represents the individual effect of it has an interpretation as the expected change in the response variable when increases by one unit with other predictor variables held constant 
when is strongly correlated with other predictor variables it is improbable that can increase by one unit with other variables held constant 
in this case the interpretation of becomes problematic as it is based on an improbable condition and the effect of cannot be evaluated in isolation 
for group of predictor variables say group effect is defined as linear combination of their parameters where is weight vector satisfying because of the constraint on is also referred to as normalized group effect 
group effect has an interpretation as the expected change in when variables in the group change by the amount respectively at the same time with variables not in the group held constant 
it generalizes the individual effect of variable to group of variables in that if then the group effect reduces to an individual effect and if and for then the group effect also reduces to an individual effect 
group effect is said to be meaningful if the underlying simultaneous changes of the variables is probable 
group effects provide means to study the collective impact of strongly correlated predictor variables in linear regression models 
individual effects of such variables are not well defined as their parameters do not have good interpretations 
furthermore when the sample size is not large none of their parameters can be accurately estimated by the least squares regression due to the multicollinearity problem 
nevertheless there are meaningful group effects that have good interpretations and can be accurately estimated by the least squares regression 
simple way to identify these meaningful group effects is to use an all positive correlations apc arrangement of the strongly correlated variables under which pairwise correlations among these variables are all positive and standardize all predictor variables in the model so that they all have mean zero and length one 
to illustrate this suppose that is group of strongly correlated variables in an apc arrangement and that they are not strongly correlated with predictor variables outside the group 
let be the centred and be the standardized then the standardized linear regression model is parameters in the original model including are simple functions of in the standardized model 
the standardization of variables does not change their correlations so is group of strongly correlated variables in an apc arrangement and they are not strongly correlated with other predictor variables in the standardized model 
group effect of is and its minimum variance unbiased linear estimator is where is the least squares estimator of 
in particular the average group effect of the standardized variables is which has an interpretation as the expected change in when all in the strongly correlated group increase by th of unit at the same time with variables outside the group held constant 
with strong positive correlations and in standardized units variables in the group are approximately equal so they are likely to increase at the same time and in similar amount 
thus the average group effect is meaningful effect 
it can be accurately estimated by its minimum variance unbiased linear estimator even when individually none of the can be accurately estimated by 
not all group effects are meaningful or can be accurately estimated 
for example is special group effect with weights and for but it cannot be accurately estimated by 
it is also not meaningful effect 
in general for group of strongly correlated predictor variables in an apc arrangement in the standardized model group effects whose weight vectors are at or near the centre of the simplex are meaningful and can be accurately estimated by their minimum variance unbiased linear estimators 
effects with weight vectors far away from the centre are not meaningful as such weight vectors represent simultaneous changes of the variables that violate the strong positive correlations of the standardized variables in an apc arrangement 
as such they are not probable 
these effects also cannot be accurately estimated 
applications of the group effects include estimation and inference for meaningful group effects on the response variable testing for group significance of the variables via testing versus and characterizing the region of the predictor variable space over which predictions by the least squares estimated model are accurate 
group effect of the original variables can be expressed as constant times group effect of the standardized variables 
the former is meaningful when the latter is 
thus meaningful group effects of the original variables can be found through meaningful group effects of the standardized variables 
extensions numerous extensions of linear regression have been developed which allow some or all of the assumptions underlying the basic model to be relaxed 
simple and multiple linear regression the very simplest case of single scalar predictor variable and single scalar response variable is known as simple linear regression 
the extension to multiple and or vector valued predictor variables denoted with capital is known as multiple linear regression also known as multivariable linear regression not to be confused with multivariate linear regression 
multiple linear regression is generalization of simple linear regression to the case of more than one independent variable and special case of general linear models restricted to one dependent variable 
the basic model for multiple linear regression is for each observation in the formula above we consider observations of one dependent variable and independent variables 
thus yi is the ith observation of the dependent variable xij is ith observation of the jth independent variable the values represent parameters to be estimated and is the ith independent identically distributed normal error 
in the more general multivariate linear regression there is one equation of the above form for each of dependent variables that share the same set of explanatory variables and hence are estimated simultaneously with each other for all observations indexed as and for all dependent variables indexed as nearly all real world regression models involve multiple predictors and basic descriptions of linear regression are often phrased in terms of the multiple regression model 
note however that in these cases the response variable is still scalar 
another term multivariate linear regression refers to cases where is vector the same as general linear regression 
general linear models the general linear model considers the situation when the response variable is not scalar for each observation but vector yi 
conditional linearity of is still assumed with matrix replacing the vector of the classical linear regression model 
multivariate analogues of ordinary least squares ols and generalized least squares gls have been developed 
general linear models are also called multivariate linear models 
these are not the same as multivariable linear models also called multiple linear models 
heteroscedastic models various models have been created that allow for heteroscedasticity 
the errors for different response variables may have different variances 
for example weighted least squares is method for estimating linear regression models when the response variables may have different error variances possibly with correlated errors 
see also weighted linear least squares and generalized least squares 
heteroscedasticity consistent standard errors is an improved method for use with uncorrelated but potentially heteroscedastic errors 
generalized linear models generalized linear models glms are framework for modeling response variables that are bounded or discrete 
this is used for example when modeling positive quantities 
prices or populations that vary over large scale which are better described using skewed distribution such as the log normal distribution or poisson distribution although glms are not used for log normal data instead the response variable is simply transformed using the logarithm function when modeling categorical data such as the choice of given candidate in an election which is better described using bernoulli distribution binomial distribution for binary choices or categorical distribution multinomial distribution for multi way choices where there are fixed number of choices that cannot be meaningfully ordered when modeling ordinal data 
ratings on scale from to where the different outcomes can be ordered but where the quantity itself may not have any absolute meaning 
rating of may not be twice as good in any objective sense as rating of but simply indicates that it is better than or but not as good as generalized linear models allow for an arbitrary link function that relates the mean of the response variable to the predictors 
the link function is often related to the distribution of the response and in particular it typically has the effect of transforming between the range of the linear predictor and the range of the response variable 
some common examples of glms are poisson regression for count data 
logistic regression and probit regression for binary data 
multinomial logistic regression and multinomial probit regression for categorical data 
ordered logit and ordered probit regression for ordinal data single index models allow some degree of nonlinearity in the relationship between and while preserving the central role of the linear predictor as in the classical linear regression model 
under certain conditions simply applying ols to data from single index model will consistently estimate up to proportionality constant 
hierarchical linear models hierarchical linear models or multilevel regression organizes the data into hierarchy of regressions for example where is regressed on and is regressed on it is often used where the variables of interest have natural hierarchical structure such as in educational statistics where students are nested in classrooms classrooms are nested in schools and schools are nested in some administrative grouping such as school district 
the response variable might be measure of student achievement such as test score and different covariates would be collected at the classroom school and school district levels 
errors in variables errors in variables models or measurement error models extend the traditional linear regression model to allow the predictor variables to be observed with error 
this error causes standard estimators of to become biased 
generally the form of bias is an attenuation meaning that the effects are biased toward zero 
others in dempster shafer theory or linear belief function in particular linear regression model may be represented as partially swept matrix which can be combined with similar matrices representing observations and other assumed normal distributions and state equations 
the combination of swept or unswept matrices provides an alternative method for estimating linear regression models 
estimation methods large number of procedures have been developed for parameter estimation and inference in linear regression 
these methods differ in computational simplicity of algorithms presence of closed form solution robustness with respect to heavy tailed distributions and theoretical assumptions needed to validate desirable statistical properties such as consistency and asymptotic efficiency 
some of the more common estimation techniques for linear regression are summarized below 
least squares estimation and related techniques assuming that the independent variable is and the model parameters are then the model prediction would be if is extended to then would become dot product of the parameter and the independent variable 
in the least squares setting the optimum parameter is defined as such that minimizes the sum of mean squared loss arg min arg min now putting the independent and dependent variables in matrices and respectively the loss function can be rewritten as as the loss is convex the optimum solution lies at gradient zero 
the gradient of the loss function is using denominator layout convention setting the gradient to zero produces the optimum parameter note to prove that the obtained is indeed the local minimum one needs to differentiate once more to obtain the hessian matrix and show that it is positive definite 
this is provided by the gauss markov theorem 
linear least squares methods include mainly ordinary least squares weighted least squares generalized least squares maximum likelihood estimation and related techniques maximum likelihood estimation can be performed when the distribution of the error terms is known to belong to certain parametric family of probability distributions 
when is normal distribution with zero mean and variance the resulting estimate is identical to the ols estimate 
gls estimates are maximum likelihood estimates when follows multivariate normal distribution with known covariance matrix 
ridge regression and other forms of penalized estimation such as lasso regression deliberately introduce bias into the estimation of in order to reduce the variability of the estimate 
the resulting estimates generally have lower mean squared error than the ols estimates particularly when multicollinearity is present or when overfitting is problem 
they are generally used when the goal is to predict the value of the response variable for values of the predictors that have not yet been observed 
these methods are not as commonly used when the goal is inference since it is difficult to account for the bias 
least absolute deviation lad regression is robust estimation technique in that it is less sensitive to the presence of outliers than ols but is less efficient than ols when no outliers are present 
it is equivalent to maximum likelihood estimation under laplace distribution model for adaptive estimation 
if we assume that error terms are independent of the regressors then the optimal estimator is the step mle where the first step is used to non parametrically estimate the distribution of the error term 
other estimation techniques bayesian linear regression applies the framework of bayesian statistics to linear regression 
see also bayesian multivariate linear regression 
in particular the regression coefficients are assumed to be random variables with specified prior distribution 
the prior distribution can bias the solutions for the regression coefficients in way similar to but more general than ridge regression or lasso regression 
in addition the bayesian estimation process produces not single point estimate for the best values of the regression coefficients but an entire posterior distribution completely describing the uncertainty surrounding the quantity 
this can be used to estimate the best coefficients using the mean mode median any quantile see quantile regression or any other function of the posterior distribution 
quantile regression focuses on the conditional quantiles of given rather than the conditional mean of given linear quantile regression models particular conditional quantile for example the conditional median as linear function tx of the predictors 
mixed models are widely used to analyze linear regression relationships involving dependent data when the dependencies have known structure 
common applications of mixed models include analysis of data involving repeated measurements such as longitudinal data or data obtained from cluster sampling 
they are generally fit as parametric models using maximum likelihood or bayesian estimation 
in the case where the errors are modeled as normal random variables there is close connection between mixed models and generalized least squares 
fixed effects estimation is an alternative approach to analyzing this type of data 
principal component regression pcr is used when the number of predictor variables is large or when strong correlations exist among the predictor variables 
this two stage procedure first reduces the predictor variables using principal component analysis and then uses the reduced variables in an ols regression fit 
while it often works well in practice there is no general theoretical reason that the most informative linear function of the predictor variables should lie among the dominant principal components of the multivariate distribution of the predictor variables 
the partial least squares regression is the extension of the pcr method which does not suffer from the mentioned deficiency 
least angle regression is an estimation procedure for linear regression models that was developed to handle high dimensional covariate vectors potentially with more covariates than observations 
the theil sen estimator is simple robust estimation technique that chooses the slope of the fit line to be the median of the slopes of the lines through pairs of sample points 
it has similar statistical efficiency properties to simple linear regression but is much less sensitive to outliers 
other robust estimation techniques including the trimmed mean approach and and estimators have been introduced 
applications linear regression is widely used in biological behavioral and social sciences to describe possible relationships between variables 
it ranks as one of the most important tools used in these disciplines 
trend line trend line represents trend the long term movement in time series data after other components have been accounted for 
it tells whether particular data set say gdp oil prices or stock prices have increased or decreased over the period of time 
trend line could simply be drawn by eye through set of data points but more properly their position and slope is calculated using statistical techniques like linear regression 
trend lines typically are straight lines although some variations use higher degree polynomials depending on the degree of curvature desired in the line 
trend lines are sometimes used in business analytics to show changes in data over time 
this has the advantage of being simple 
trend lines are often used to argue that particular action or event such as training or an advertising campaign caused observed changes at point in time 
this is simple technique and does not require control group experimental design or sophisticated analysis technique 
however it suffers from lack of scientific validity in cases where other potential changes can affect the data 
epidemiology early evidence relating tobacco smoking to mortality and morbidity came from observational studies employing regression analysis 
in order to reduce spurious correlations when analyzing observational data researchers usually include several variables in their regression models in addition to the variable of primary interest 
for example in regression model in which cigarette smoking is the independent variable of primary interest and the dependent variable is lifespan measured in years researchers might include education and income as additional independent variables to ensure that any observed effect of smoking on lifespan is not due to those other socio economic factors 
however it is never possible to include all possible confounding variables in an empirical analysis 
for example hypothetical gene might increase mortality and also cause people to smoke more 
for this reason randomized controlled trials are often able to generate more compelling evidence of causal relationships than can be obtained using regression analyses of observational data 
when controlled experiments are not feasible variants of regression analysis such as instrumental variables regression may be used to attempt to estimate causal relationships from observational data 
finance the capital asset pricing model uses linear regression as well as the concept of beta for analyzing and quantifying the systematic risk of an investment 
this comes directly from the beta coefficient of the linear regression model that relates the return on the investment to the return on all risky assets 
economics linear regression is the predominant empirical tool in economics 
for example it is used to predict consumption spending fixed investment spending inventory investment purchases of country exports spending on imports the demand to hold liquid assets labor demand and labor supply 
environmental science linear regression finds application in wide range of environmental science applications 
in canada the environmental effects monitoring program uses statistical analyses on fish and benthic surveys to measure the effects of pulp mill or metal mine effluent on the aquatic ecosystem 
machine learning linear regression plays an important role in the subfield of artificial intelligence known as machine learning 
the linear regression algorithm is one of the fundamental supervised machine learning algorithms due to its relative simplicity and well known properties 
history least squares linear regression as means of finding good rough linear fit to set of points was performed by legendre and gauss for the prediction of planetary movement 
quetelet was responsible for making the procedure well known and for using it extensively in the social sciences 
see also references citations sources further reading pedhazur elazar 
multiple regression in behavioral research explanation and prediction nd ed 
new york holt rinehart and winston 
isbn mathieu rouaud probability statistics and estimation chapter linear regression linear regression with error bars and nonlinear regression 
chapter linear equations and matrices direct methods 
notes on applied science 
her majesty stationery office 
external links least squares regression phet interactive simulations university of colorado at boulder diy linear fit
in statistics mediation model seeks to identify and explain the mechanism or process that underlies an observed relationship between an independent variable and dependent variable via the inclusion of third hypothetical variable known as mediator variable also mediating variable intermediary variable or intervening variable 
rather than direct causal relationship between the independent variable and the dependent variable mediation model proposes that the independent variable influences the mediator variable which in turn influences the dependent variable 
thus the mediator variable serves to clarify the nature of the relationship between the independent and dependent variables mediation analyses are employed to understand known relationship by exploring the underlying mechanism or process by which one variable influences another variable through mediator variable 
in particular mediation analysis can contribute to better understanding the relationship between an independent variable and dependent variable when these variables do not have an obvious direct connection 
baron and kenny steps for mediation analysis baron and kenny laid out several requirements that must be met to form true mediation relationship 
they are outlined below using real world example 
see the diagram above for visual representation of the overall mediating relationship to be explained 
note hayes critiqued baron and kenny mediation steps approach and as of david kenny on his website stated that mediation can exist in the absence of significant total effect and therefore step below may not be needed 
this situation is sometimes referred to as inconsistent mediation 
later publications by hayes also questioned the concepts of full or partial mediation and advocated for these terms along with the classical mediation steps approach outlined below to be abandoned 
step regress the dependent variable on the independent variable to confirm that the independent variable is significant predictor of the dependent variable independent variable dependent variable is significant step regress the mediator on the independent variable to confirm that the independent variable is significant predictor of the mediator 
if the mediator is not associated with the independent variable then it couldn possibly mediate anything independent variable mediator is significant step regress the dependent variable on both the mediator and independent variable to confirm that the mediator is significant predictor of the dependent variable and the strength of the coefficient of the previously significant independent variable in step is now greatly reduced if not rendered nonsignificant 
is significant should be smaller in absolute value than the original effect for the independent variable above example the following example drawn from howell explains each step of baron and kenny requirements to understand further how mediation effect is characterized 
step and step use simple regression analysis whereas step uses multiple regression analysis 
how you were parented independent variable predicts how confident you feel about parenting your own children dependent variable 
how you were parented independent variable predicts your feelings of competence and self esteem mediator 
your feelings of competence and self esteem mediator predict how confident you feel about parenting your own children dependent variable while controlling for how you were parented independent variable such findings would lead to the conclusion implying that your feelings of competence and self esteem mediate the relationship between how you were parented and how confident you feel about parenting your own children 
if step does not yield significant result one may still have grounds to move to step sometimes there is actually significant relationship between independent and dependent variables but because of small sample sizes or other extraneous factors there could not be enough power to predict the effect that actually exists 
direct versus indirect effects in the diagram shown above the indirect effect is the product of path coefficients and 
the direct effect is the coefficient 
the direct effect measures the extent to which the dependent variable changes when the independent variable increases by one unit and the mediator variable remains unaltered 
in contrast the indirect effect measures the extent to which the dependent variable changes when the independent variable is held constant and the mediator variable changes by the amount it would have changed had the independent variable increased by one unit 
in linear systems the total effect is equal to the sum of the direct and indirect ab in the model above 
in nonlinear models the total effect is not generally equal to the sum of the direct and indirect effects but to modified combination of the two 
full versus partial mediation mediator variable can either account for all or some of the observed relationship between two variables 
full mediation maximum evidence for mediation also called full mediation would occur if inclusion of the mediation variable drops the relationship between the independent variable and dependent variable see pathway in diagram above to zero 
partial mediation partial mediation maintains that the mediating variable accounts for some but not all of the relationship between the independent variable and dependent variable 
partial mediation implies that there is not only significant relationship between the mediator and the dependent variable but also some direct relationship between the independent and dependent variable 
in order for either full or partial mediation to be established the reduction in variance explained by the independent variable must be significant as determined by one of several tests such as the sobel test 
the effect of an independent variable on the dependent variable can become nonsignificant when the mediator is introduced simply because trivial amount of variance is explained not true mediation 
thus it is imperative to show significant reduction in variance explained by the independent variable before asserting either full or partial mediation 
it is possible to have statistically significant indirect effects in the absence of total effect 
this can be explained by the presence of several mediating paths that cancel each other out and become noticeable when one of the cancelling mediators is controlled for 
this implies that the terms partial and full mediation should always be interpreted relative to the set of variables that are present in the model 
in all cases the operation of fixing variable must be distinguished from that of controlling for variable which has been inappropriately used in the literature 
the former stands for physically fixing while the latter stands for conditioning on adjusting for or adding to the regression model 
the two notions coincide only when all error terms not shown in the diagram are statistically uncorrelated 
when errors are correlated adjustments must be made to neutralize those correlations before embarking on mediation analysis see bayesian networks 
sobel test sobel test is performed to determine if the relationship between the independent variable and dependent variable has been significantly reduced after inclusion of the mediator variable 
in other words this test assesses whether mediation effect is significant 
it examines the relationship between the independent variable and the dependent variable compared to the relationship between the independent variable and dependent variable including the mediation factor 
the sobel test is more accurate than the baron and kenny steps explained above however it does have low statistical power 
as such large sample sizes are required in order to have sufficient power to detect significant effects 
this is because the key assumption of sobel test is the assumption of normality 
because sobel test evaluates given sample on the normal distribution small sample sizes and skewness of the sampling distribution can be problematic see normal distribution for more details 
thus the rule of thumb as suggested by mackinnon et al is that sample size of is required to detect small effect sample size of is sufficient in detecting medium effect and sample size of is required to detect large effect 
the equation for sobel is preacher hayes bootstrap method the bootstrapping method provides some advantages to the sobel test primarily an increase in power 
the preacher and hayes bootstrapping method is non parametric test 
as such the bootstrap method does not violate assumptions of normality and is therefore recommended for small sample sizes 
bootstrapping involves repeatedly randomly sampling observations with replacement from the data set to compute the desired statistic in each resample 
computing over hundreds or thousands of bootstrap resamples provide an approximation of the sampling distribution of the statistic of interest 
the preacher hayes method provides point estimates and confidence intervals by which one can assess the significance or nonsignificance of mediation effect 
point estimates reveal the mean over the number of bootstrapped samples and if zero does not fall between the resulting confidence intervals of the bootstrapping method one can confidently conclude that there is significant mediation effect to report 
significance of mediation as outlined above there are few different options one can choose from to evaluate mediation model 
bootstrapping is becoming the most popular method of testing mediation because it does not require the normality assumption to be met and because it can be effectively utilized with smaller sample sizes 
however mediation continues to be most frequently determined using the logic of baron and kenny or the sobel test 
it is becoming increasingly more difficult to publish tests of mediation based purely on the baron and kenny method or tests that make distributional assumptions such as the sobel test 
thus it is important to consider your options when choosing which test to conduct 
approaches to mediation while the concept of mediation as defined within psychology is theoretically appealing the methods used to study mediation empirically have been challenged by statisticians and epidemiologists and interpreted formally 
experimental causal chain design an experimental causal chain design is used when the proposed mediator is experimentally manipulated 
such design implies that one manipulates some controlled third variable that they have reason to believe could be the underlying mechanism of given relationship 
measurement of mediation design measurement of mediation design can be conceptualized as statistical approach 
such design implies that one measures the proposed intervening variable and then uses statistical analyses to establish mediation 
this approach does not involve manipulation of the hypothesized mediating variable but only involves measurement 
criticisms of mediation measurement experimental approaches to mediation must be carried out with caution 
first it is important to have strong theoretical support for the exploratory investigation of potential mediating variable 
criticism of mediation approach rests on the ability to manipulate and measure mediating variable 
thus one must be able to manipulate the proposed mediator in an acceptable and ethical fashion 
as such one must be able to measure the intervening process without interfering with the outcome 
the mediator must also be able to establish construct validity of manipulation 
one of the most common criticisms of the measurement of mediation approach is that it is ultimately correlational design 
consequently it is possible that some other third variable independent from the proposed mediator could be responsible for the proposed effect 
however researchers have worked hard to provide counter evidence to this disparagement 
specifically the following counter arguments have been put forward temporal precedence for example if the independent variable precedes the dependent variable in time this would provide evidence suggesting directional and potentially causal link from the independent variable to the dependent variable 
nonspuriousness and or no confounds for example should one identify other third variables and prove that they do not alter the relationship between the independent variable and the dependent variable he she would have stronger argument for their mediation effect 
see other rd variables below mediation can be an extremely useful and powerful statistical test however it must be used properly 
it is important that the measures used to assess the mediator and the dependent variable are theoretically distinct and that the independent variable and mediator cannot interact 
should there be an interaction between the independent variable and the mediator one would have grounds to investigate moderation 
other third variables confounding another model that is often tested is one in which competing variables in the model are alternative potential mediators or an unmeasured cause of the dependent variable 
an additional variable in causal model may obscure or confound the relationship between the independent and dependent variables 
potential confounders are variables that may have causal impact on both the independent variable and dependent variable 
they include common sources of measurement error as discussed above as well as other influences shared by both the independent and dependent variables 
in experimental studies there is special concern about aspects of the experimental manipulation or setting that may account for study effects rather than the motivating theoretical factor 
any of these problems may produce spurious relationships between the independent and dependent variables as measured 
ignoring confounding variable may bias empirical estimates of the causal effect of the independent variable 
suppression suppressor variable increases the predictive validity of another variable when included in regression equation 
suppression can occur when single causal variable is related to an outcome variable through two separate mediator variables and when one of those mediated effects is positive and one is negative 
in such case each mediator variable suppresses or conceals the effect that is carried through the other mediator variable 
for example higher intelligence scores causal variable may cause an increase in error detection mediator variable which in turn may cause decrease in errors made at work on an assembly line an outcome variable at the same time intelligence could also cause an increase in boredom which in turn may cause an increase in errors 
thus in one causal path intelligence decreases errors and in the other it increases them 
when neither mediator is included in the analysis intelligence appears to have no effect or weak effect on errors 
however when boredom is controlled intelligence will appear to decrease errors and when error detection is controlled intelligence will appear to increase errors 
if intelligence could be increased while only boredom was held constant errors would decrease if intelligence could be increased while holding only error detection constant errors would increase 
in general the omission of suppressors or confounders will lead to either an underestimation or an overestimation of the effect of on thereby either reducing or artificially inflating the magnitude of relationship between two variables 
moderators other important third variables are moderators 
moderators are variables that can make the relationship between two variables either stronger or weaker 
such variables further characterize interactions in regression by affecting the direction and or strength of the relationship between and moderating relationship can be thought of as an interaction 
it occurs when the relationship between variables and depends on the level of see moderation for further discussion 
moderated mediation mediation and moderation can co occur in statistical models 
it is possible to mediate moderation and moderate mediation 
moderated mediation is when the effect of the treatment on the mediator and or the partial effect on the dependent variable depend in turn on levels of another variable moderator 
essentially in moderated mediation mediation is first established and then one investigates if the mediation effect that describes the relationship between the independent variable and dependent variable is moderated by different levels of another variable moderator 
this definition has been outlined by muller judd and yzerbyt and preacher rucker and hayes 
models of moderated mediation there are five possible models of moderated mediation as illustrated in the diagrams below 
in the first model the independent variable also moderates the relationship between the mediator and the dependent variable 
the second possible model of moderated mediation involves new variable which moderates the relationship between the independent variable and the mediator the path 
the third model of moderated mediation involves new moderator variable which moderates the relationship between the mediator and the dependent variable the path 
moderated mediation can also occur when one moderating variable affects both the relationship between the independent variable and the mediator the path and the relationship between the mediator and the dependent variable the path 
the fifth and final possible model of moderated mediation involves two new moderator variables one moderating the path and the other moderating the path 
in addition to the models mentioned above new variable can also exist which moderates the relationship between the independent variable and mediator the path while at the same time have the new variable moderate the relationship between the independent variable and dependent variable the path 
mediated moderation mediated moderation is variant of both moderation and mediation 
this is where there is initially overall moderation and the direct effect of the moderator variable on the outcome is mediated 
the main difference between mediated moderation and moderated mediation is that for the former there is initial overall moderation and this effect is mediated and for the latter there is no moderation but the effect of either the treatment on the mediator path is moderated or the effect of the mediator on the outcome path is moderated in order to establish mediated moderation one must first establish moderation meaning that the direction and or the strength of the relationship between the independent and dependent variables path differs depending on the level of third variable the moderator variable 
researchers next look for the presence of mediated moderation when they have theoretical reason to believe that there is fourth variable that acts as the mechanism or process that causes the relationship between the independent variable and the moderator path or between the moderator and the dependent variable path 
example the following is published example of mediated moderation in psychological research 
participants were presented with an initial stimulus prime that made them think of morality or made them think of might 
they then participated in the prisoner dilemma game pdg in which participants pretend that they and their partner in crime have been arrested and they must decide whether to remain loyal to their partner or to compete with their partner and cooperate with the authorities 
the researchers found that prosocial individuals were affected by the morality and might primes whereas proself individuals were not 
thus social value orientation proself vs prosocial moderated the relationship between the prime independent variable morality vs might and the behaviour chosen in the pdg dependent variable competitive vs cooperative 
the researchers next looked for the presence of mediated moderation effect 
regression analyses revealed that the type of prime morality vs might mediated the moderating relationship of participants social value orientation on pdg behaviour 
prosocial participants who experienced the morality prime expected their partner to cooperate with them so they chose to cooperate themselves 
prosocial participants who experienced the might prime expected their partner to compete with them which made them more likely to compete with their partner and cooperate with the authorities 
in contrast participants with pro self social value orientation always acted competitively 
regression equations for moderated mediation and mediated moderation muller judd and yzerbyt outline three fundamental models that underlie both moderated mediation and mediated moderation 
mo represents the moderator variable me represents the mediator variable and represents the measurement error of each regression equation 
step moderation of the relationship between the independent variable and the dependent variable also called the overall treatment effect path in the diagram 
to establish overall moderation the regression weight must be significant first step for establishing mediated moderation 
establishing moderated mediation requires that there be no moderation effect so the regression weight must not be significant 
step moderation of the relationship between the independent variable and the mediator path 
if the regression weight is significant the moderator affects the relationship between the independent variable and the mediator 
step moderation of both the relationship between the independent and dependent variables path and the relationship between the mediator and the dependent variable path 
if both in step and in step are significant the moderator affects the relationship between the independent variable and the mediator path 
if both in step and in step are significant the moderator affects the relationship between the mediator and the dependent variable path 
either or both of the conditions above may be true 
causal mediation analysis fixing versus conditioning mediation analysis quantifies the extent to which variable participates in the transmittance of change from cause to its effect 
it is inherently causal notion hence it cannot be defined in statistical terms 
traditionally however the bulk of mediation analysis has been conducted within the confines of linear regression with statistical terminology masking the causal character of the relationships involved 
this led to difficulties biases and limitations that have been alleviated by modern methods of causal analysis based on causal diagrams and counterfactual logic 
the source of these difficulties lies in defining mediation in terms of changes induced by adding third variables into regression equation 
such statistical changes are epiphenomena which sometimes accompany mediation but in general fail to capture the causal relationships that mediation analysis aims to quantify 
the basic premise of the causal approach is that it is not always appropriate to control for the mediator when we seek to estimate the direct effect of on see the figure above 
the classical rationale for controlling for is that if we succeed in preventing from changing then whatever changes we measure in are attributable solely to variations in and we are justified then in proclaiming the effect observed as direct effect of on 
unfortunately controlling for does not physically prevent from changing it merely narrows the analyst attention to cases of equal values 
moreover the language of probability theory does not possess the notation to express the idea of preventing from changing or physically holding constant 
the only operator probability provides is conditioning which is what we do when we control for or add as regressor in the equation for the result is that instead of physically holding constant say at and comparing for units under to those under we allow to vary but ignore all units except those in which achieves the value these two operations are fundamentally different and yield different results except in the case of no omitted variables 
to illustrate assume that the error terms of and are correlated 
under such conditions the structural coefficient and between and and between and can no longer be estimated by regressing on and in fact the regression slopes may both be nonzero even when is zero 
this has two consequences 
first new strategies must be devised for estimating the structural coefficients and second the basic definitions of direct and indirect effects must go beyond regression analysis and should invoke an operation that mimics fixing rather than conditioning on definitions such an operator denoted do was defined in pearl and it operates by removing the equation of and replacing it by constant for example if the basic mediation model consists of the equations then after applying the operator do the model becomes and after applying the operator do the model becomes where the functions and as well as the distributions of the error terms and remain unaltered 
if we further rename the variables and resulting from do as and respectively we obtain what came to be known as potential outcomes or structural counterfactuals 
these new variables provide convenient notation for defining direct and indirect effects 
in particular four types of effects have been defined for the transition from to total effect controlled direct effect natural direct effect natural indirect effect where stands for expectation taken over the error terms 
these effects have the following interpretations te measures the expected increase in the outcome as changes from to while the mediator is allowed to track the change in as dictated by the function 
cde measures the expected increase in the outcome as changes from to while the mediator is fixed at pre specified level uniformly over the entire population nde measures the expected increase in as changes from to while setting the mediator variable to whatever value it would have obtained under before the change 
nie measures the expected increase in when the is held constant at and changes to whatever value it would have attained for each individual under the difference te nde measures the extent to which mediation is necessary for explaining the effect while the nie measures the extent to which mediation is sufficient for sustaining it controlled version of the indirect effect does not exist because there is no way of disabling the direct effect by fixing variable to constant 
according to these definitions the total effect can be decomposed as sum where nier stands for the reverse transition from to it becomes additive in linear systems where reversal of transitions entails sign reversal 
the power of these definitions lies in their generality they are applicable to models with arbitrary nonlinear interactions arbitrary dependencies among the disturbances and both continuous and categorical variables 
the mediation formula in linear analysis all effects are determined by sums of products of structural coefficients giving independent of therefore all effects are estimable whenever the model is identified 
in non linear systems more stringent conditions are needed for estimating the direct and indirect effects 
for example if no confounding exists and are mutually independent the following formulas can be derived 
the last two equations are called mediation formulas and have become the target of estimation in many studies of mediation 
they give distribution free expressions for direct and indirect effects and demonstrate that despite the arbitrary nature of the error distributions and the functions and mediated effects can nevertheless be estimated from data using regression 
the analyses of moderated mediation and mediating moderators fall as special cases of the causal mediation analysis and the mediation formulas identify how various interactions coefficients contribute to the necessary and sufficient components of mediation 
example assume the model takes the form where the parameter quantifies the degree to which modifies the effect of on even when all parameters are estimated from data it is still not obvious what combinations of parameters measure the direct and indirect effect of on or more practically how to assess the fraction of the total effect that is explained by mediation and the fraction of that is owed to mediation 
in linear analysis the former fraction is captured by the product the latter by the difference and the two quantities coincide 
in the presence of interaction however each fraction demands separate analysis as dictated by the mediation formula which yields thus the fraction of output response for which mediation would be sufficient is while the fraction for which mediation would be necessary is 
these fractions involve non obvious combinations of the model parameters and can be constructed mechanically with the help of the mediation formula 
significantly due to interaction direct effect can be sustained even when the parameter vanishes and moreover total effect can be sustained even when both the direct and indirect effects vanish 
this illustrates that estimating parameters in isolation tells us little about the effect of mediation and more generally mediation and moderation are intertwined and cannot be assessed separately 
references as of june this article is derived in whole or in part from causal analysis in theory and practice 
the copyright holder has licensed the content in manner that permits reuse under cc by sa and gfdl 
all relevant terms must be followed 
notes bibliographypreacher kristopher hayes andrew 
spss and sas procedures for estimating indirect effects in simple mediation models 
behavior research methods instruments and computers 
pmid preacher kristopher hayes andrew 
asymptotic and resampling strategies for assessing and comparing indirect effects in multiple mediator models 
pmid preacher zyphur zhang 
general multilevel sem framework for assessing multilevel mediation 
pmid baron and kenny 
the moderator mediator variable distinction in social psychological research conceptual strategic and statistical considerations journal of personality and social psychology vol 
statistical power analysis for the behavioral sciences nd ed 
new york ny academic press 
beyond baron and kenny statistical mediation analysis in the new millennium 
statistical methods for psychology th ed 
belmot ca cengage learning 
advances in statistical methods for substance abuse prevention research 
doi pmc pmid preacher kelley 
effect sizes measures for mediation models quantitative strategies for communicating indirect effects 
pmid rucker preacher tormala 
mediation analysis in social psychology current practices and new recommendations 
social and personality psychology compass 
asymptotic confidence intervals for indirect effects in structural equation models 
jstor spencer zanna fong 
establishing causal chain why experiments are often more effective than mediational analyses in examining psychological processes 
journal of personality and social psychology 
the mediation formula guide to the assessment of causal pathways in nonlinear models 
in berzuini dawid bernardinelli 
causality statistical perspectives and applications 
chichester uk john wiley and sons ltd pp 
research methods in psychology th ed pp 
new york mcgraw hill 
the determiners of behavior at choice point 
degrees of hunger reward and nonreward and maze learning in rats 
university of california publications in psychology 
explanation in causal inference 
external links summary of mediation methods at psychwiki example of causal mediation using propensity scores the methodology center penn state university book on moderation and mediation analysis including an introduction to the process macro for spss and sas andrew hayes ohio state university online text of the determiner of behavior at choice point kenneth maccorquodale and paul meehl on distinction between hypothetical constructs and intervening variables classics in the history of psychology retr
the weighted arithmetic mean is similar to an ordinary arithmetic mean the most common type of average except that instead of each of the data points contributing equally to the final average some data points contribute more than others 
the notion of weighted mean plays role in descriptive statistics and also occurs in more general form in several other areas of mathematics 
if all the weights are equal then the weighted mean is the same as the arithmetic mean 
while weighted means generally behave in similar fashion to arithmetic means they do have few counterintuitive properties as captured for instance in simpson paradox 
examples basic example given two school classes one with students one with students and test grades in each class as follows morning class afternoon class the mean for the morning class is and the mean of the afternoon class is the unweighted mean of the two means is however this does not account for the difference in number of students in each class versus hence the value of does not reflect the average student grade independent of class 
the average student grade can be obtained by averaging all the grades without regard to classes add all the grades up and divide by the total number of students or this can be accomplished by weighting the class means by the number of students in each class 
the larger class is given more weight thus the weighted mean makes it possible to find the mean average student grade without knowing each student score 
only the class means and the number of students in each class are needed 
convex combination example since only the relative weights are relevant any weighted mean can be expressed using coefficients that sum to one 
such linear combination is called convex combination 
using the previous example we would get the following weights then apply the weights like this 
mathematical definition formally the weighted mean of non empty finite tuple of data with corresponding non negative weights is which expands to therefore data elements with high weight contribute more to the weighted mean than do elements with low weight 
the weights cannot be negative 
some may be zero but not all of them since division by zero is not allowed 
the formulas are simplified when the weights are normalized such that they sum up to for such normalized weights the weighted mean is equivalently note that one can always normalize the weights by making the following transformation on the original weights the ordinary mean is special case of the weighted mean where all data have equal weights 
if the data elements are independent and identically distributed random variables with variance the standard error of the weighted mean can be shown via uncertainty propagation to be variance defined weights for the weighted mean of list of data for which each element potentially comes from different probability distribution with known variance all having the same mean one possible choice for the weights is given by the reciprocal of variance the weighted mean in this case is and the standard error of the weighted mean with inverse variance weights is note this reduces to when all it is special case of the general formula in previous section the equations above can be combined to obtain the significance of this choice is that this weighted mean is the maximum likelihood estimator of the mean of the probability distributions under the assumption that they are independent and normally distributed with the same mean 
statistical properties expectancy the weighted sample mean is itself random variable 
its expected value and standard deviation are related to the expected values and standard deviations of the observations as follows 
for simplicity we assume normalized weights weights summing to one 
if the observations have expected values then the weighted sample mean has expectation in particular if the means are equal then the expectation of the weighted sample mean will be that value variance simple 
case when treating the weights as constants and having sample of observations from uncorrelated random variables all with the same variance and expectation as is the case for random variables then the variance of the weighted mean can be estimated as the multiplication of the variance by kish design effect see proof var with and however this estimation is rather limited due to the strong assumption about the observations 
this has led to the development of alternative more general estimators 
survey sampling perspective from model based perspective we are interested in estimating the variance of the weighted mean when the different are not random variables 
an alternative perspective for this problem is that of some arbitrary sampling design of the data in which units are selected with unequal probabilities with replacement 
in survey methodology the population mean of some quantity of interest is calculated by taking an estimation of the total of over all elements in the population or sometimes and dividing it by the population size either known or estimated 
in this context each value of is considered constant and the variability comes from the selection procedure 
this in contrast to model based approaches in which the randomness is often described in the values 
the survey sampling procedure yields series of bernoulli indicator values that get if some observation is in the sample and if it was not selected 
this can occur with fixed sample size or varied sample size sampling 
the probability of some element to be chosen given sample is denoted as some sample of size and the one draw probability of selection is one sample draw if is very large and each is very small 
for the following derivation we ll assume that the probability of selecting each element is fully represented by these probabilities 
selecting some element will not influence the probability of drawing another element this doesn apply for things such as cluster sampling design 
since each element is fixed and the randomness comes from it being included in the sample or not we often talk about the multiplication of the two which is random variable 
to avoid confusion in the following section let call this term with the following expectancy and variance 
when each element of the sample is inflated by the inverse of its selection probability it is termed the expanded values 
related quantity is expanded values as above we can add tick mark if multiplying by the indicator function 
in this design based perspective the weights used in the numerator of the weighted mean are obtained from taking the inverse of the selection probability 
variance of the weighted sum pwr estimator for totals if the population size is known we can estimate the population mean using known if the sampling design is one that results in fixed sample size such as in pps sampling then the variance of this estimator is var known an alternative term for when the sampling has random sample size as in poisson sampling is presented in sarndal et al 
as with also where is the probability of selecting both and and and for if the selection probability are uncorrelated 
and when assuming the probability of each element is very small then var pwr known variance of the weighted mean estimator for ratio mean the previous section dealt with estimating the population mean as ratio of an estimated population total with known population size and the variance was estimated in that context 
another common case is that the population size itself is unknown and is estimated using the sample 
the estimation of can be described as the sum of weights 
so when we get 
when using notation from previous sections the ratio we care about is the sum of and 
we can estimate it using our sample with as we moved from using to using we actually know that all the indicator variables get so we could simply write this will be the estimand for specific values of and but the statistical properties comes when including the indicator variable 
this is called ratio estimator and it is approximately unbiased for in this case the variability of the ratio depends on the variability of the random variables both in the numerator and the denominator as well as their correlation 
since there is no closed analytical form to compute this variance various methods are used for approximate estimation 
primarily taylor series first order linearization asymptotics and bootstrap jackknife 
the taylor linearization method could lead to under estimation of the variance for small sample sizes in general but that depends on the complexity of the statistic 
for the weighted mean the approximate variance is supposed to be relatively accurate even for medium sample sizes 
for when the sampling has random sample size as in poisson sampling it is as follows we note that if then either using or would give the same estimator since multiplying by some factor would lead to the same estimator 
it also means that if we scale the sum of weights to be equal to known from before population size the variance calculation would look the same 
when all weights are equal to one another this formula is reduced to the standard unbiased variance estimator 
we have at least two versions of variance for the weighted mean one with known and one with unknown population size estimation 
there is no uniformly better approach but the literature presents several arguments to prefer using the population estimation version even when the population size is known 
for example if all values are constant the estimator with unknown population size will give the correct result while the one with known population size will have some variability 
also when the sample size itself is random 
in poisson sampling the version with unknown population mean is considered more stable 
lastly if the proportion of sampling is negatively correlated with the values 
smaller chance to sample an observation that is large then the un known population size version slightly compensates for that 
bootstrapping validation it has been shown by gatz et al 
that in comparison to bootstrapping methods the following variance estimation of ratio mean using taylor series linearization is reasonable estimation for the square of the standard error of the mean when used in the context of measuring chemical constituents where further simplification leads to gatz et al 
mention that the above formulation was published by endlich et al 
when treating the weighted mean as combination of weighted total estimator divided by an estimator of the population size based on the formulation published by cochran as an approximation to the ratio mean 
however endlich et al 
didn seem to publish this derivation in their paper even though they mention they used it and cochran book includes slightly different formulation 
still it almost identical to the formulations described in previous sections 
replication based estimators because there is no closed analytical form for the variance of the weighted mean it was proposed in the literature to rely on replication methods such as the jackknife and bootstrapping 
other notes for uncorrelated observations with variances the variance of the weighted sample mean is whose square root can be called the standard error of the weighted mean general case 
consequently if all the observations have equal variance the weighted sample mean will have variance where the variance attains its maximum value when all weights except one are zero 
its minimum value is found when all weights are equal unweighted mean in which case we have it degenerates into the standard error of the mean squared 
note that because one can always transform non normalized weights to normalized weights all formula in this section can be adapted to non normalized weights by replacing all 
related concepts weighted sample variance typically when mean is calculated it is important to know the variance and standard deviation about that mean 
when weighted mean is used the variance of the weighted sample is different from the variance of the unweighted sample 
the biased weighted sample variance is defined similarly to the normal biased sample variance where for normalized weights 
if the weights are frequency weights and thus are random variables it can be shown that is the maximum likelihood estimator of for iid gaussian observations 
for small samples it is customary to use an unbiased estimator for the population variance 
in normal unweighted samples the in the denominator corresponding to the sample size is changed to see bessel correction 
in the weighted setting there are actually two different unbiased estimators one for the case of frequency weights and another for the case of reliability weights 
frequency weights if the weights are frequency weights where weight equals the number of occurrences then the unbiased estimator is this effectively applies bessel correction for frequency weights 
for example if values are drawn from the same distribution then we can treat this set as an unweighted sample or we can treat it as the weighted sample with corresponding weights and we get the same result either way 
if the frequency weights are normalized to then the correct expression after bessel correction becomes where the total number of samples is not 
in any case the information on total number of samples is necessary in order to obtain an unbiased correction even if has different meaning other than frequency weight 
note that the estimator can be unbiased only if the weights are not standardized nor normalized these processes changing the data mean and variance and thus leading to loss of the base rate the population count which is requirement for bessel correction 
reliability weights if the weights are instead non random reliability weights we can determine correction factor to yield an unbiased estimator 
assuming each random variable is sampled from the same distribution with mean and actual variance actual taking expectations we have actual actual where and therefore the bias in our estimator is analogous to the bias in the unweighted estimator also notice that is the effective sample size 
this means that to unbias our estimator we need to pre divide by ensuring that the expected value of the estimated variance equals the actual variance of the sampling distribution 
the final unbiased estimate of sample variance is where actual the degrees of freedom of the weighted unbiased sample variance vary accordingly from down to the standard deviation is simply the square root of the variance above 
as side note other approaches have been described to compute the weighted sample variance 
weighted sample covariance in weighted sample each row vector each set of single observations on each of the random variables is assigned weight then the weighted mean vector is given by and the weighted covariance matrix is given by similarly to weighted sample variance there are two different unbiased estimators depending on the type of the weights 
frequency weights if the weights are frequency weights the unbiased weighted estimate of the covariance matrix with bessel correction is given by note that this estimator can be unbiased only if the weights are not standardized nor normalized these processes changing the data mean and variance and thus leading to loss of the base rate the population count which is requirement for bessel correction 
reliability weights in the case of reliability weights the weights are normalized 
if they are not divide the weights by their sum to normalize prior to calculating then the weighted mean vector can be simplified to and the unbiased weighted estimate of the covariance matrix is 
the reasoning here is the same as in the previous section 
since we are assuming the weights are normalized then and this reduces to if all weights are the same 
then the weighted mean and covariance reduce to the unweighted sample mean and covariance above 
vector valued estimates the above generalizes easily to the case of taking the mean of vector valued estimates 
for example estimates of position on plane may have less certainty in one direction than another 
as in the scalar case the weighted mean of multiple estimates can provide maximum likelihood estimate 
we simply replace the variance by the covariance matrix and the arithmetic inverse by the matrix inverse both denoted in the same way via superscripts the weight matrix then reads the weighted mean in this case is where the order of the matrix vector product is not commutative in terms of the covariance of the weighted mean for example consider the weighted mean of the point with high variance in the second component and with high variance in the first component 
then then the weighted mean is which makes sense the estimate is compliant in the second component and the estimate is compliant in the first component so the weighted mean is nearly 
accounting for correlations in the general case suppose that is the covariance matrix relating the quantities is the common mean to be estimated and is design matrix equal to vector of ones of length 
the gauss markov theorem states that the estimate of the mean having minimum variance is given by and where 
decreasing strength of interactions consider the time series of an independent variable and dependent variable with observations sampled at discrete times in many common situations the value of at time depends not only on but also on its past values 
commonly the strength of this dependence decreases as the separation of observations in time increases 
to model this situation one may replace the independent variable by its sliding mean for window size 
exponentially decreasing weights in the scenario described in the previous section most frequently the decrease in interaction strength obeys negative exponential law 
if the observations are sampled at equidistant times then exponential decrease is equivalent to decrease by constant fraction at each time step 
setting we can define normalized weights by where is the sum of the unnormalized weights 
in this case is simply approaching for large values of the damping constant must correspond to the actual decrease of interaction strength 
if this cannot be determined from theoretical considerations then the following properties of exponentially decreasing weights are useful in making suitable choice at step the weight approximately equals the tail area the value the head area the tail area at step is 
where primarily the closest observations matter and the effect of the remaining observations can be ignored safely then choose such that the tail area is sufficiently small 
weighted averages of functions the concept of weighted average can be extended to functions 
weighted averages of functions play an important role in the systems of weighted differential and integral calculus 
correcting for over or under dispersion weighted means are typically used to find the weighted mean of historical data rather than theoretically generated data 
in this case there will be some error in the variance of each data point 
typically experimental errors may be underestimated due to the experimenter not taking into account all sources of error in calculating the variance of each data point 
in this event the variance in the weighted mean must be corrected to account for the fact that is too large 
the correction that must be made is where is the reduced chi squared the square root can be called the standard error of the weighted mean variance weights scale corrected 
when all data variances are equal they cancel out in the weighted mean variance which again reduces to the standard error of the mean squared formulated in terms of the sample standard deviation squared 
see also references further reading bevington philip 
data reduction and error analysis for the physical sciences 
new york mcgraw hill 
data fitting and uncertainty practical introduction to weighted least squares and beyond 
external links david terr 
tool to calculate weighted average
in probability theory and statistics cross covariance matrix is matrix whose element in the position is the covariance between the th element of random vector and th element of another random vector 
random vector is random variable with multiple dimensions 
each element of the vector is scalar random variable 
each element has either finite number of observed empirical values or finite or infinite number of potential values 
the potential values are specified by theoretical joint probability distribution 
intuitively the cross covariance matrix generalizes the notion of covariance to multiple dimensions 
the cross covariance matrix of two random vectors and is typically denoted by or 
definition for random vectors and each containing random elements whose expected value and variance exist the cross covariance matrix of and is defined by where and are vectors containing the expected values of and the vectors and need not have the same dimension and either might be scalar value 
the cross covariance matrix is the matrix whose entry is the covariance cov between the th element of and the th element of this gives the following component wise definition of the cross covariance matrix 
example for example if and are random vectors then cov is matrix whose th entry is cov 
properties for the cross covariance matrix the following basic properties apply cov cov cov cov cov cov cov cov if and are independent or somewhat less restrictedly if every random variable in is uncorrelated with every random variable in then cov where and are random vectors is random vector is vector is vector and are matrices of constants and is matrix of zeroes 
definition for complex random vectors if and are complex random vectors the definition of the cross covariance matrix is slightly changed 
transposition is replaced by hermitian transposition cov for complex random vectors another matrix called the pseudo cross covariance matrix is defined as follows cov uncorrelatedness two random vectors and are called uncorrelated if their cross covariance matrix matrix is zero matrix 
complex random vectors and are called uncorrelated if their covariance matrix and pseudo covariance matrix is zero
aids is caused by human immunodeficiency virus hiv which originated in non human primates in central and west africa 
while various sub groups of the virus acquired human infectivity at different times the present pandemic had its origins in the emergence of one specific strain hiv subgroup in opoldville in the belgian congo now kinshasa in the democratic republic of the congo in the there are two types of hiv hiv and hiv 
hiv is more virulent easily transmitted and is the cause of the vast majority of hiv infections globally 
the pandemic strain of hiv is closely related to virus found in chimpanzees of the subspecies pan troglodytes troglodytes which live in the forests of the central african nations of cameroon equatorial guinea gabon the republic of the congo and the central african republic 
hiv is less transmittable and is largely confined to west africa along with its closest relative virus of the sooty mangabey cercocebus atys atys an old world monkey inhabiting southern senegal guinea bissau guinea sierra leone liberia and western ivory coast 
transmission from non humans to humans research in this area is conducted using molecular phylogenetics comparing viral genomic sequences to determine relatedness 
hiv from chimpanzees and gorillas to humans scientists generally accept that the known strains or groups of hiv are most closely related to the simian immunodeficiency viruses sivs endemic in wild ape populations of west central african forests 
in particular each of the known hiv strains is either closely related to the siv that infects the chimpanzee subspecies pan troglodytes troglodytes sivcpz or closely related to the siv that infects western lowland gorillas gorilla gorilla gorilla called sivgor 
the pandemic hiv strain group or main and rare strain found only in few cameroonian people group are clearly derived from sivcpz strains endemic in pan troglodytes troglodytes chimpanzee populations living in cameroon 
another very rare hiv strain group is clearly derived from sivgor strains of cameroon 
finally the primate ancestor of hiv group strain infecting people mostly from cameroon but also from neighbouring countries was confirmed in to be sivgor 
the pandemic hiv group is most closely related to the sivcpz collected from the southeastern rain forests of cameroon modern east province near the sangha river 
thus this region is presumably where the virus was first transmitted from chimpanzees to humans 
however reviews of the epidemiological evidence of early hiv infection in stored blood samples and of old cases of aids in central africa have led many scientists to believe that hiv group early human centre was probably not in cameroon but rather further south in the democratic republic of the congo then the belgian congo more probably in its capital city kinshasa formerly opoldville using hiv sequences preserved in human biological samples along with estimates of viral mutation rates scientists calculate that the jump from chimpanzee to human probably happened during the late th or early th century time of rapid urbanisation and colonisation in equatorial africa 
exactly when the zoonosis occurred is not known 
some molecular dating studies suggest that hiv group had its most recent common ancestor mrca that is started to spread in the human population in the early th century probably between and study published in analyzing viral sequences recovered from biopsy made in kinshasa in along with previously known sequences suggested common ancestor between and with central estimates varying between and 
genetic recombination had earlier been thought to seriously confound such phylogenetic analysis but later work has suggested that recombination is not likely to systematically bias results although recombination is expected to increase variance 
the results of phylogenetics study support the later work and indicate that hiv evolves fairly reliably 
further research was hindered due to the primates being critically endangered 
sample analyses resulted in little data due to the rarity of experimental material 
the researchers however were able to hypothesize phylogeny from the gathered data 
they were also able to use the molecular clock of specific strain of hiv to determine the initial date of transmission which is estimated to be around 
hiv from sooty mangabeys to humans similar research has been undertaken with siv strains collected from several wild sooty mangabey cercocebus atys atys sivsmm populations of the west african nations of sierra leone liberia and ivory coast 
the resulting phylogenetic analyses show that the viruses most closely related to the two strains of hiv that spread considerably in humans hiv groups and are the sivsmm found in the sooty mangabeys of the tai forest in western ivory coast there are six additional known hiv groups each having been found in just one person 
they all seem to derive from independent transmissions from sooty mangabeys to humans 
groups and have been found in two people from liberia groups and have been discovered in two people from sierra leone and groups and have been detected in two people from the ivory coast 
these hiv strains are probably dead end infections and each of them is most closely related to sivsmm strains from sooty mangabeys living in the same country where the human infection was found molecular dating studies suggest that both the epidemic groups and started to spread among humans between and with the central estimates varying between and 
bushmeat practice according to the natural transfer theory also called hunter theory or bushmeat theory in the simplest and most plausible explanation for the cross species transmission of siv or hiv post mutation the virus was transmitted from an ape or monkey to human when hunter or bushmeat vendor handler was bitten or cut while hunting or butchering the animal 
the resulting exposure to blood or other bodily fluids of the animal can result in siv infection 
prior to wwii some sub saharan africans were forced out of the rural areas because of the european demand for resources 
since rural africans were not keen to pursue agricultural practices in the jungle they turned to non domesticated animals as their primary source of meat 
this over exposure to bushmeat and malpractice of butchery increased blood to blood contact which then increased the probability of transmission 
recent serological survey showed that human infections by siv are not rare in central africa the percentage of people showing seroreactivity to antigens evidence of current or past siv infection was among the general population of cameroon in villages where bushmeat is hunted or used and in the most exposed people of these villages 
how the siv virus would have transformed into hiv after infection of the hunter or bushmeat handler from the ape monkey is still matter of debate although natural selection would favour any viruses capable of adjusting so that they could infect and reproduce in the cells of human host 
emergence unresolved questions about hiv origins and emergence the discovery of the main hiv siv phylogenetic relationships permits explaining broad hiv biogeography the early centres of the hiv groups were in central africa where the primate reservoirs of the related sivcpz and sivgor viruses chimpanzees and gorillas exist similarly the hiv groups had their centres in west africa where sooty mangabeys which harbour the related sivsmm virus exist 
however these relationships do not explain more detailed patterns of biogeography such as why epidemic hiv groups and only evolved in the ivory coast which is one of only six countries harbouring the sooty mangabey 
it is also unclear why the sivcpz endemic in the chimpanzee subspecies pan troglodytes schweinfurthii inhabiting the democratic republic of congo central african republic rwanda burundi uganda and tanzania did not spawn an epidemic hiv strain to humans while the democratic republic of congo was the main centre of hiv group virus descended from sivcpz strains of subspecies pan troglodytes troglodytes that does not exist in this country 
it is clear that the several hiv and hiv strains descend from sivcpz sivgor and sivsmm viruses and that bushmeat practice provides the most plausible cause of cross species transfer to humans 
however some loose ends remain 
it is not yet explained why only four hiv groups hiv groups and and hiv groups and spread considerably in human populations despite bushmeat practices being widespread in central and west africa and the resulting human siv infections being common it also remains unexplained why all epidemic hiv groups emerged in humans nearly simultaneously and only in the th century despite very old human exposure to siv phylogenetic study demonstrated that siv is at least tens of thousands of years old 
origin and epidemic emergence several of the theories of hiv origin accept the established knowledge of the hiv siv phylogenetic relationships and also accept that bushmeat practice was the most likely cause of the initial transfer to humans 
all of them propose that the simultaneous epidemic emergences of four hiv groups in the late th early th century and the lack of previous known emergences are explained by new factor that appeared in the relevant african regions in that timeframe 
these new factor would have acted either to increase human exposures to siv to help it to adapt to the human organism by mutation thus enhancing its between humans transmissibility or to cause an initial burst of transmissions crossing an epidemiological threshold and therefore increasing the probability of continued spread 
genetic studies of the virus suggested in that the most recent common ancestor of the hiv group dates back to the belgian congo city of opoldville modern kinshasa circa proponents of this dating link the hiv epidemic with the emergence of colonialism and growth of large colonial african cities leading to social changes including higher degree of non monogamous sexual activity the spread of prostitution and the concomitant high frequency of genital ulcer diseases such as syphilis in nascent colonial cities in study conducted by scientists from the university of oxford and the university of leuven in belgium revealed that because approximately one million people every year would flow through the prominent city of kinshasa which served as the origin of the first known hiv cases in the passengers riding on the region belgian railway trains were able to spread the virus to larger areas 
the study also identified roaring sex trade rapid population growth and unsterilised needles used in health clinics as other factors which contributed to the emergence of the africa hiv epidemic 
social changes and urbanization beatrice hahn paul sharp and their colleagues proposed that the epidemic emergence of hiv most likely reflects changes in population structure and behaviour in africa during the th century and perhaps medical interventions that provided the opportunity for rapid human to human spread of the virus 
after the scramble for africa started in the european colonial powers established cities towns and other colonial stations 
largely masculine labor force was hastily recruited to work in fluvial and sea ports railways other infrastructures and in plantations 
this disrupted traditional tribal values and favored casual sexual activity with an increased number of partners 
in the nascent cities women felt relatively liberated from rural tribal rules and many remained unmarried or divorced during long periods this being rare in african traditional societies 
this was accompanied by unprecedented increase in people movements 
michael worobey and colleagues observed that the growth of cities probably played role in the epidemic emergence of hiv since the phylogenetic dating of the two older strains of hiv groups and suggest that these viruses started to spread soon after the main central african colonial cities were founded 
colonialism in africa amit chitnis diana rawls and jim moore proposed that hiv may have emerged epidemically as result of harsh conditions forced labor displacement and unsafe injection and vaccination practices associated with colonialism particularly in french equatorial africa 
the workers in plantations construction projects and other colonial enterprises were supplied with bushmeat which would have contributed to an increase in hunting and it follows higher incidence of human exposure to siv 
several historical sources support the view that bushmeat hunting indeed increased both because of the necessity to supply workers and because firearms became more widely available the colonial authorities also gave many vaccinations against smallpox and injections of which many would be made without sterilising the equipment between uses 
proposed that both these parenteral risks and the prostitution associated with forced labor camps could have caused serial transmission or serial passage of siv between humans see discussion of this in the next section 
in addition they proposed that the conditions of extreme stress associated with forced labor could depress the immune system of workers therefore prolonging the primary acute infection period of someone newly infected by siv thus increasing the odds of both adaptation of the virus to humans and of further transmissions the authors proposed that hiv originated in the area of french equatorial africa in the early th century when the colonial abuses and forced labor were at their peak 
later research established that these theories were mostly correct hiv groups and started to spread in humans in late th early th century 
in addition all groups of hiv descend from either sivcpz or sivgor from apes living to the west of the ubangi river either in countries that belonged to the french equatorial africa federation of colonies in equatorial guinea then spanish colony or in cameroon which was german colony between and and then fell to allied forces in world war and had most of its area administered by france in close association with french equatorial africa 
this theory was later dubbed heart of darkness by jim moore alluding to the book of the same title written by joseph conrad the main focus of which is colonial abuses in equatorial africa 
unsterile injections in several articles published since preston marx philip alcabes and ernest drucker proposed that hiv emerged because of rapid serial human to human transmission of siv after bushmeat hunter or handler became siv infected through unsafe or unsterile injections 
although both chitnis et al 
and sharp et al 
also suggested that this may have been one of the major risk factors at play in hiv emergence see above marx et al 
enunciated the underlying mechanisms in greater detail and wrote the first review of the injection campaigns made in colonial africa central to the marx et al 
argument is the concept of adaptation by serial passage or serial transmission an adventitious virus or other pathogen can increase its biological adaptation to new host species if it is rapidly transmitted between hosts while each host is still in the acute infection period 
this process favors the accumulation of adaptive mutations more rapidly therefore increasing the odds that better adapted viral variant will appear in the host before the immune system suppresses the virus 
such better adapted variants could then survive in the human host for longer than the short acute infection period in high numbers high viral load which would grant it more possibilities of epidemic spread 
reported experiments of cross species transfer of siv in captive monkeys some of which made by themselves in which the use of serial passage helped to adapt siv to the new monkey species after passage by three or four animals in agreement with this model is also the fact that while both hiv and hiv attain substantial viral loads in the human organism adventitious siv infecting humans seldom does so people with siv antibodies often have very low or even undetectable siv viral load 
this suggests that both hiv and hiv are adapted to humans and serial passage could have been the process responsible for it 
proposed that unsterile injections that is injections where the needle or syringe is reused without sterilization or cleaning between uses which were likely very prevalent in africa during both the colonial period and afterwards provided the mechanism of serial passage that permitted hiv to adapt to humans therefore explaining why it emerged epidemically only in the th century 
massive injections of the antibiotic era marx et al 
emphasize the massive number of injections administered in africa after antibiotics were introduced around as being the most likely implicated in the origin of hiv because by these times roughly in the period to injection intensity in africa was maximal 
they argued that serial passage chain of or transmissions between humans is an unlikely event the probability of transmission after needle reuse is something between and and only few people have an acute siv infection at any time and so hiv emergence may have required the very high frequency of injections of the antibiotic era the molecular dating studies place the initial spread of the epidemic hiv groups before that time see above 
according to marx et al these studies could have overestimated the age of the hiv groups because they depend on molecular clock assumption may not have accounted for the effects of natural selection in the viruses and the serial passage process alone would be associated with strong natural selection 
injection campaigns against sleeping sickness david gisselquist proposed that the mass injection campaigns to treat trypanosomiasis sleeping sickness in central africa were responsible for the emergence of hiv 
unlike marx et al gisselquist argued that the millions of unsafe injections administered during these campaigns were sufficient to spread rare hiv infections into an epidemic and that evolution of hiv through serial passage was not essential to the emergence of the hiv epidemic in the th century this theory focuses on injection campaigns that peaked in the period that is around the time the hiv groups started to spread 
it also focuses on the fact that many of the injections in these campaigns were intravenous which are more likely to transmit siv hiv than subcutaneous or intramuscular injections and many of the patients received many often more than injections per year therefore increasing the odds of siv serial passage 
other early injection campaigns jacques pin and annie claude labb reviewed the colonial health reports of cameroon and french equatorial africa for the period calculating the incidences of the diseases requiring intravenous injections 
they concluded that trypanosomiasis leprosy yaws and syphilis were responsible for most intravenous injections 
schistosomiasis tuberculosis and vaccinations against smallpox represented lower parenteral risks schistosomiasis cases were relatively few tuberculosis patients only became numerous after mid century and there were few smallpox vaccinations in the lifetime of each person the authors suggested that the very high prevalence of the hepatitis virus in southern cameroon and forested areas of french equatorial africa around can be better explained by the unsterile injections used to treat yaws because this disease was much more prevalent than syphilis trypanosomiasis and leprosy in these areas 
they suggested that all these parenteral risks caused not only the massive spread of hepatitis but also the spread of other pathogens and the emergence of hiv the same procedures could have exponentially amplified hiv from single hunter cook occupationally infected with sivcpz to several thousand patients treated with arsenicals or other drugs threshold beyond which sexual transmission could prosper 
they do not suggest specifically serial passage as the mechanism of adaptation 
according to pin book the origins of aids the virus can be traced to central african bush hunter in with colonial medical campaigns using improperly sterilized syringe and needles playing key role in enabling future epidemic 
pin concludes that aids spread silently in africa for decades fueled by urbanization and prostitution since the initial cross species infection 
pin also claims that the virus was brought to the americas by haitian teacher returning home from zaire in the 
sex tourism and contaminated blood transfusion centers ultimately propelled aids to public consciousness in the and worldwide pandemic 
genital ulcer diseases and evolution of sexual activity jo dinis de sousa viktor ller philippe lemey and anne mieke vandamme proposed that hiv became epidemic through sexual serial transmission in nascent colonial cities helped by high frequency of genital ulcers caused by genital ulcer diseases gud 
gud are simply sexually transmitted diseases that cause genital ulcers examples are syphilis chancroid lymphogranuloma venereum and genital herpes 
these diseases increase the probability of hiv transmission dramatically from around to per heterosexual act because the genital ulcers provide portal of viral entry and contain many activated cells expressing the ccr co receptor the main cell targets of hiv 
probable time interval of cross species transfer sousa et al 
use molecular dating techniques to estimate the time when each hiv group split from its closest siv lineage 
each hiv group necessarily crossed to humans between this time and the time when it started to spread the time of the mrca because after the mrca certainly all lineages were already in humans and before the split with the closest simian strain the lineage was in simian 
hiv groups and split from their closest sivs around and respectively 
this information together with the datations of the hiv groups mrcas mean that all hiv groups likely crossed to humans in the early th century 
strong genital ulcer disease incidence in nascent colonial cities the authors reviewed colonial medical articles and archived medical reports of the countries at or near the ranges of chimpanzees gorillas and sooty mangabeys and found that genital ulcer diseases guds peaked in the colonial cities during their early growth period up to 
the colonial authorities recruited men to work in railways fluvial and sea ports and other infrastructure projects and most of these men did not bring their wives with them 
then the highly male biased sex ratio favoured prostitution which in its turn caused an explosion of gud especially syphilis and chancroid 
after the mid people movements were more tightly controlled and mass surveys and treatments of arsenicals and other drugs were organized and so the gud incidences started to decline 
they declined even further after world war ii because of the heavy use of antibiotics so that by the late opoldville which is the probable center of hiv group had very low gud incidence 
similar processes happened in the cities of cameroon and ivory coast where hiv group and hiv respectively evolved therefore the peak gud incidences in cities have good temporal coincidence with the period when all main hiv groups crossed to humans and started to spread 
in addition the authors gathered evidence that syphilis and the other guds were like injections absent from the densely forested areas of central and west africa before organized colonialism socially disrupted these areas starting in the 
thus this theory also potentially explains why hiv emerged only after the late th century 
female genital mutilation uli linke has argued that the practice of female genital mutilation either or both of clitoridectomy and infibulation is responsible for the high incidence of aids in africa since intercourse with female who has undergone clitoridectomy is conducive to exchange of blood 
male circumcision distribution and hiv origins male circumcision may reduce the probability of hiv acquisition by men 
leaving aside blood transfusions the highest hiv transmissibility ever measured was from female prostitutes with prevalence of hiv to uncircumcised men with gud cumulative seroconverted to hiv after single sexual exposure 
there was no seroconversion in the absence of male gud 
reasoned that the adaptation and epidemic emergence of each hiv group may have required such extreme conditions and thus reviewed the existing ethnographic literature for patterns of male circumcision and hunting of apes and monkeys for bushmeat focusing on the period and on most of the ethnic groups living in central and west africa 
they also collected censuses and other literature showing the ethnic composition of colonial cities in this period 
then they estimated the circumcision frequencies of the central african cities over time 
charts reveal that male circumcision frequencies were much lower in several cities of western and central africa in the early th century than they are currently 
the reason is that many ethnic groups not performing circumcision by that time gradually adopted it to imitate other ethnic groups and enhance the social acceptance of their boys colonialism produced massive intermixing between african ethnic groups 
about of men in opoldville and douala in the early th century should be uncircumcised and these cities were the probable centers of hiv groups and respectively the authors studied early circumcision frequencies in cities of central and west africa to test if this variable correlated with hiv emergence 
this correlation was strong for hiv among west african cities that could have received immigrants infected with sivsmm the two cities from the ivory coast studied abidjan and bouak had much higher frequency of uncircumcised men than the others and epidemic hiv groups emerged initially in this country only 
this correlation was less clear for hiv in central africa 
computer simulations of hiv emergence sousa et al 
then built computer simulations to test if an ill adapted siv meaning simian immunodeficiency virus already infecting human but incapable of transmission beyond the short acute infection period could spread in colonial cities 
the simulations used parameters of sexual transmission obtained from the current hiv literature 
they modelled people sexual links with different levels of sexual partner change among different categories of people prostitutes single women with several partners year married women and men according to data obtained from modern studies of sexual activity in african cities 
the simulations let the parameters city size proportion of people married gud frequency male circumcision frequency and transmission parameters vary and explored several scenarios 
each scenario was run times to test the probability of siv generating long chains of sexual transmission 
the authors postulated that such long chains of sexual transmission were necessary for the siv strain to adapt better to humans becoming an hiv capable of further epidemic emergence 
the main result was that genital ulcer frequency was by far the most decisive factor 
for the gud levels prevailing in opoldville in the early th century long chains of siv transmission had high probability 
for the lower gud levels existing in the same city in the late see above they were much less likely 
and without gud situation typical of villages in forested equatorial africa before colonialism siv could not spread at all 
city size was not an important factor 
the authors propose that these findings explain the temporal patterns of hiv emergence no hiv emerging in tens of thousands of years of human slaughtering of apes and monkeys several hiv groups emerging in the nascent gud riddled colonial cities and no epidemically successful hiv group emerging in mid th century when gud was more controlled and cities were much bigger 
male circumcision had little to moderate effect in their simulations but given the geographical correlation found the authors propose that it could have had an indirect role either by increasing genital ulcer disease itself it is known that syphilis chancroid and several other guds have higher incidences in uncircumcised men or by permitting further spread of the hiv strain after the first chains of sexual transmission permitted adaptation to the human organism 
one of the main advantages of this theory is stressed by the authors it the theory also offers conceptual simplicity because it proposes as causal factors for siv adaptation to humans and initial spread the very same factors that most promote the continued spread of hiv nowadays promiscuous sic sex particularly involving sex workers gud and possibly lack of circumcision 
iatrogenic and other theories iatrogenic theories propose that medical interventions were responsible for hiv origins 
by proposing factors that only appeared in central and west africa after the late th century they seek to explain why all hiv groups also started after that 
the theories centred on the role of parenteral risks such as unsterile injections transfusions or smallpox vaccinations are accepted as plausible by most scientists of the field 
discredited hiv aids origins theories include several iatrogenic theories such as the polio vaccine hypothesis which argues that the early oral polio vaccines were contaminated with chimpanzee virus leading to the central african outbreak 
pathogenicity of siv in non human primates in most non human primate species natural siv infection does not cause fatal disease but see below 
comparison of the gene sequence of siv with hiv should therefore provide information about the factors necessary to cause disease in humans 
the factors that determine the virulence of hiv as compared to most sivs are only now being elucidated 
non human sivs contain nef gene that down regulates cd cd and mhc class expression most non human sivs therefore do not induce immunodeficiency the hiv nef gene however has lost its ability to down regulate cd which results in the immune activation and apoptosis that is characteristic of chronic hiv infection in addition long term survey of chimpanzees naturally infected with sivcpz in gombe national park tanzania found that contrary to the previous paradigm chimpanzees with sivcpz infection do experience an increased mortality and also suffer from human aids like illness 
siv pathogenicity in wild animals could exist in other chimpanzee subspecies and other primate species as well and stay unrecognized by lack of relevant long term studies 
history of spread david carr david carr was an apprentice printer usually mistakenly referred to as sailor carr had served in the navy between and from manchester england who died on august and was for some time mistakenly reported to have died from aids defining opportunistic infections adois 
following the failure of his immune system he succumbed to pneumonia 
doctors baffled by what he had died from preserved of his tissue samples for inspection 
in the tissues were found to be hiv positive 
however in second test by aids researcher david ho found that the strain of hiv present in the tissues was similar to those found in rather than an earlier strain which would have mutated considerably over the course of years 
he concluded that the dna samples provided actually came from patient with aids in the 
upon retesting david carr tissues he found no sign of the virus 
congolese man one of the earliest documented hiv infections was discovered in preserved blood sample taken in from man from opoldville in the belgian congo 
however it is unknown whether this anonymous person ever developed aids and died of its complications 
congolese woman second early documented hiv infection was discovered in preserved lymph node biopsy sample taken in from woman from opoldville belgian congo 
congolese man strain with large amount of the genetic material present was dated to from sample from year old man 
robert rayford in may year old african american robert rayford died at the st louis city hospital from kaposi sarcoma 
in researchers at tulane university school of medicine detected virus closely related or identical to hiv in his preserved blood and tissues 
the doctors who worked on his case at the time suspected he was prostitute or the victim of sexual abuse though the patient did not discuss his sexual history with them in detail 
ugandan children from to researchers drew blood from children in uganda to serve as controls for study of burkitt lymphoma 
in retroactive testing of the frozen blood serum indicated that antibodies to virus related to hiv were present in of the children 
arvid noe in and norwegian sailor with the alias name arvid noe his wife and his seven year old daughter died of aids 
the sailor had first presented symptoms in eight years after he first spent time in ports along the west african coastline 
gonorrhea infection during his first african voyage shows he was sexually active at this time 
tissue samples from the sailor and his wife were tested in and found to contain hiv group 
grethe rask grethe rask was danish surgeon who traveled to za re in then again in to aid the sick 
she was likely directly exposed to blood from many congolese patients one of whom infected her 
she became unwell from then returned to denmark in with her colleagues baffled by her symptoms 
she died of pneumocystis pneumonia in december her tissues were examined and tested by her colleagues and found positive in 
spread to the western hemisphere hiv strains were once thought to have arrived in new york city from haiti around it spread from new york city to san francisco around hiv is believed to have arrived in haiti from central africa possibly from the democratic republic of the congo around the current consensus is that hiv was introduced to haiti by an unknown individual or individuals who contracted it while working in the democratic republic of the congo circa mini epidemic followed and circa yet another unknown individual took hiv from haiti to the united states 
the vast majority of cases of aids outside sub saharan africa can be traced back to that single patient 
later numerous unrelated incidents of aids among haitian immigrants to the were recorded in the early 
also as evidenced by the case of robert rayford isolated occurrences of this infection may have been emerging as early as the virus eventually entered gay male communities in large united states cities where combination of casual multi partner sexual activity with individuals reportedly averaging over unprotected sexual partners per year and relatively high transmission rates associated with anal intercourse allowed it to spread explosively enough to finally be noticed because of the long incubation period of hiv up to decade or longer before symptoms of aids appear and because of the initially low incidence hiv was not noticed at first 
by the time the first reported cases of aids were found in large united states cities the prevalence of hiv infection in some communities had passed 
worldwide hiv infection has spread from urban to rural areas and has appeared in regions such as china and india 
canadian flight attendant theory canadian airline steward named ga tan dugas was referred to as case and later patient with the alphabet letter standing for outside southern california in an early aids study by dr william darrow of the centers for disease control 
because of this many people had considered dugas to be responsible for taking hiv to north america 
however hiv reached new york city around while dugas did not start work at air canada until in randy shilts book and the band played on and the movie based on it dugas is referred to as aids patient zero instead of patient but neither the book nor the movie states that he had been the first to bring the virus to north america 
he was incorrectly called patient zero because at least of the people known to be infected by hiv in had had sex with him or with person who had sexual intercourse with dugas 
homeless people and intravenous drug users in new york volunteer social worker called betty williams quaker who worked with the homeless in new york from the seventies and early eighties onwards has talked about people at that time whose death would be labelled as junkie flu or the dwindles 
in an interview for the act up oral history project in she said of course the horror stories came mainly concerning women who were injection drug users who had pcp pneumonia pneumocystis pneumonia and were told that they just had bronchitis 
she continues actually believe that aids kind of existed among this group of people first because if you look back there was something called junkie pneumonia there was something called the dwindles that addicts got and think this was another early aids population way too helpless to ever do anything for themselves on their own behalf 
julia epstein writes in her book altered conditions disease medicine and storytelling that as we uncover more of the early history of hiv infection it becomes clear that by at least the the virus was already making major inroads into the immune systems of number of diverse populations in the united states the retrospectively diagnosed epidemic of junkie pneumonia in new york city in the late for example and had for some time been causing devastation in several countries in africa 
anecdotal evidence suggests that so called junkie pneumonia first began to afflict heroin addicts in new york in in her book engendering aids deconstructing sex text and epidemic tamsin wilton writes people had been sickening and dying of mysterious conditions since the early conditions that we can retrospectively diagnose as aids related 
there was for example phenomenon known as junkie pneumonia which spread among some populations of injecting street drug users in the and which is now believed to have been caused by hiv infection 
melinda cooper writes in her book family values between neoliberalism and the new social conservatism it is plausible that these cases of aids did not come to light in the for the same reason that junkie pneumonia was not recognized as the sign of an emerging infectious disease the people in question had such precarious access to health care that news of their death was never communicated to public health authorities 
an article by pattrice maurer in the newspaper agenda from april explores some of the issues surrounding junkie pneumonia 
it starts in the late while the epidemic known as disco fever swept through the an epidemic known as junkie pneumonia raged among injection drug users in new york city 
it continues few people were aware that large numbers of injections drug users were inexplicably dying of pneumonia 
those few who did notice these deaths did not feel compelled to investigate the public health puzzle they posed 
the author opinion is that if anyone had bothered to investigate these deaths they would have found an immune system disorder that is now called aids steven thrasher writes in the guardian indeed those of us who study aids have long known that long before common symptoms such as kaposi sarcoma and pneumonia were showing up among hemophiliacs and gay men they were likely affecting homeless people who lived off society radar people who used iv intravenous drugs and those who avoided medical treatment out of fear 
chapter in the proceedings of the world conference of therapeutic communities th san francisco california september gives details about serum samples that were tested for signs of hiv then called htlv iii lav antibodies 
quoting we have also conducted historical studies of the epidemic in new york city using serum samples that were originally collected for other purposes 
we have sera from iv drug users that go back to the middle 
the first indication of htlv iii lav antibody presence is in one of eleven samples from of samples in of samples from and of samples from the htlv iii lav virus appears to have been introduced among iv drug users in the late in new york city 
anna thompson writes on the website thebody com in an article dated autumn many women were dying in the late of pneumonia cervical cancer and other illnesses complicated by mysteriously suppressed immune systems 
yet it was not until that case of aids in woman was first reported by the centers for disease control cdc 
she continues the cdc refusal to address women issues led to the overall perception that women do not get aids 
in an article published in aids cultural analysis cultural activism author douglas crimp draws attention to anecdotal evidence about junkie pneumonia 
quoting even these statistics are based on cdc epidemiology that continues to see the beginning of the epidemic as in spite of widespread anecdotal reporting of high rate of deaths throughout the from what was known as junkie pneumonia and was likely pneumocystis pneumonia 
the statistics crimp writes about were taken from new york times article from october about nyc department of health study that showed that of aids sufferers were people who injected drugs more than percent higher than previously reported 
quoting city health officials estimated that half of the city intravenous drug users were infected with the virus that causes aids the study hiv infection among intravenous drug users in manhattan new york city from through published in february seeks to understand long term trends in the spread of hiv among intravenous drug users idus 
aids surveillance data and studies which detail the number of persons who tested hiv positive in manhattan are used to compile information deemed critical to realising the extent of the aids epidemic 
it starts by stating that up to september idu was the risk behaviour in or of the first cases of aids in the us 
cases among idus in new york city in the same period numbered approximately third of national idu cases 
the study continues to outline the methodology used in the compilation of data 
it says that while truly representative samples of idus within community are probably impossible to obtain samples of idus entering treatment provide good source for monitoring trends 
in the results section it states quoting the first evidence for hiv infection among iv drug users in new york is from three cases of aids in children born in these cases were later reported to the new york city department of health aids surveillance unit 
these children did not receive any known transfusions prior to developing aids and were born to mothers known to be iv drug users 
it continues to outline that the earliest known case of aids in an adult idu occurred in mixed risk and that known cases among idus increased rapidly from the cases in mixed risk to cases in to cases in and to cases in statistics on the incidence of positive tests for hiv mainly using archived samples are out of in out of in out of in out of between out of and out of in out of in and out of in in the comments section it states the three cases in of apparent perinatal transmission mother to child from iv drug using women strongly suggest that the introduction of hiv into the iv drug use group occurred around or or perhaps even earlier 
it says that without extensive samples from this period it is not possible to be certain about the spread of hiv among idus but the samples from idus with chronic liver disease suggest that the rates of infection were below for the first or years after its introduction hiv is thought to have entered the population of people using intravenous drugs in new york city in approximately in spring the government of new york city underwent fiscal crisis which led to the closing of many social services with people who used intravenous drugs living in hostile sociopolitical and legal environment 
this fiscal crisis led to many agencies with health responsibilities being particularly hard hit which in turn might have led to an increase in hiv aids and tuberculosis tb 
quoting from american journal of public health study between and the department of health doh budget in ny was cut by and by the department had lost staff members of its workforce 
to achieve these reductions the department closed of district health centers cut million from its methadone program terminated the employment of of health educators and closed of child health stations and of chest clinics the units responsible for tb screening and diagnosis 
study published in the journal of the american medical association in linked tb and hiv aids severe and unusual presentation of overwhelming tuberculosis in appropriate clinical circumstances may be considered an infection predictive of the presence of aids 
further study from stated there was link between the rise in tb aids and drug users within the united states aids thus compounds the risk of acquiring tuberculosis and in the united states most patients with aids and tuberculosis have been drug users 
newsletter from spring by the national coalition of gay std services featured an article titled tuberculosis and aids connecticut that suggested an association between tb and aids within that state 
from grid to aids the aids epidemic officially began on june when the centers for disease control and prevention in its morbidity and mortality weekly report newsletter reported unusual clusters of pneumocystis pneumonia pcp caused by form of pneumocystis carinii now recognized as distinct species pneumocystis jirovecii in five homosexual men in los angeles 
over the next months more pcp clusters were discovered among otherwise healthy men in cities throughout the country along with other opportunistic diseases such as kaposi sarcoma and persistent generalized lymphadenopathy common in immunosuppressed patients 
in june report of group of cases amongst gay men in southern california suggested that sexually transmitted infectious agent might be the etiological agent 
the syndrome was initially termed grid or gay related immune deficiency other less common gay specific terms included gay compromise syndrome gay lymph node syndrome gay cancer gay plague homosexual syndrome community acquired immunodeficiency caid and acquired community immunodeficiency syndrome acids 
health authorities soon realized however that nearly half of the people identified with the syndrome were not homosexual men 
the same opportunistic infections were also reported among hemophiliacs users of intravenous drugs such as heroin and haitian immigrants leading some researchers to call it the disease 
by august the disease was being referred to by its new cdc coined name acquired immune deficiency syndrome aids 
activism by aids patients and families in new york city nathan fain larry kramer larry mass paul popham paul rapoport and edmund white officially established the gay men health crisis gmhc in also in michael callen and richard berkowitz published how to have sex in an epidemic one approach 
in this short work they described ways gay men could be sexual and affectionate while dramatically reducing the risk of contracting or spreading hiv 
both authors were themselves gay men living with aids 
this booklet was one of the first times men were advised to use condoms when having sexual relations with other men at the beginning of the aids epidemic in the there was very little information about the disease 
because aids affected stigmatized groups such as lgbtq people people of low socioeconomic status sex workers and addicts there was also initially little mass media coverage when the epidemic started 
however with the rise of activist groups composed of people suffering from aids either directly or through loved one more public attention was brought to the epidemic 
identification of the virus may lav in may team of doctors at the pasteur institute in france including fran oise barr sinoussi and luc montagnier reported that they had isolated new retrovirus from lymphoid ganglions that they believed was the cause of aids 
the virus was later named lymphadenopathy associated virus lav and sample was sent to the centers for disease control which was later passed to the national cancer institute nci 
may htlv iii in may team led by robert gallo of the united states confirmed the discovery of the virus but they renamed it human lymphotropic virus type iii htlv iii 
august arv dr jay levy group at the university of california san francisco also played role in the discovery of hiv 
he independently isolated the aids virus in and named it the aids associated retrovirus arv publishing his findings in the journal science in 
january both found to be the same in january number of more detailed reports were published concerning lav and htlv iii and by march it was clear that the viruses were the same indeed it was later determined that the virus isolated by the gallo lab was from the lymph nodes of the patient studied in the original report by montagnier and was the etiological agent of aids 
may the name hiv in may the international committee on taxonomy of viruses ruled that both names should be dropped and new name hiv human immunodeficiency virus be used 
nobel whether barr sinoussi and montagnier deserve more credit than gallo for the discovery of the virus that causes aids has been matter of considerable controversy 
barr sinoussi and montagnier were awarded the nobel prize in physiology or medicine for their discovery of human immunodeficiency virus and harald zur hausen also shared the prize for his discovery that human papilloma virus leads to cervical cancer but gallo was left out 
gallo said that it was disappointment that he was not named co recipient 
montagnier said he was surprised gallo was not recognized by the nobel committee it was important to prove that hiv was the cause of aids and gallo had very important role in that 
very sorry for robert gallo 
dr levy contribution to the discovery of hiv was also cited in the nobel prize ceremony 
case definition for epidemiological surveillance since june many definitions have been developed for epidemiological surveillance such as the bangui definition and the expanded world health organization aids case definition 
genetic studies according to study published in the proceedings of the national academy of sciences in team led by robert shafer at stanford university school of medicine discovered that the gray mouse lemur has an endogenous lentivirus the genus to which hiv belongs in its genetic makeup 
this suggests that lentiviruses have existed for at least million years much longer than the currently known existence of hiv 
in addition the time frame falls in the period when madagascar was still connected to what is now the african continent the said lemurs later developed immunity to the virus strain and survived an era when the lentivirus was widespread among other mammals 
the study was hailed as crucial as it fills the blanks in the origin of the virus as well as in its evolution and could be important in the development of new antiviral drugs in researchers reported that siv had infected monkeys in bioko for at least years 
previous to this time it was thought that siv infection in monkeys had happened over the past few hundred years 
scientists estimated that it would take similar amount of time before humans adapted naturally to hiv infection in the way monkeys in africa have adapted to siv and not suffer any harm from the infection czech study of the genome of malayan flying lemurs an order of mammals parallel to primates and sharing an immediate common ancestor with them found endogenous lentiviruses that emerged an estimated million years ago based on rates of viral mutation versus modern lentiviruses 
debunked hiv aids conspiracy theories aids denialism aids denialists argue that aids does not exist or that aids is not caused by hiv some of its proponents believe that aids is caused by lifestyle including sexuality or drug use and not by hiv 
both forms of aids denialism contradict scientific consensus 
the evidence that hiv causes aids is generally considered conclusive among pathologists 
most arguments for denialism are based on misrepresentations of outdated data 
the belief that hiv was created by the us government as bioweapon an idea invented by soviet propaganda operation is held by disproportionately high number of africans and african americans 
influence on bolsonaro conspiracy theorists influence reached peak in with brazilian president jair bolsonaro claiming that covid vaccines can lead to aids 
the supreme federal court of brazil ordered an investigation into bolsonaro for falsely claiming that covid vaccines could increase the risk of contracting aids 
see also timeline of hiv aids references further reading shilts randy 
and the band played on politics people and the aids epidemic 
new york st martin press 
isbn oclc brier jennier 
infectious ideas political responses to the aids crisis 
chapel hill university of north carolina press
in music theory the circle of fifths is way of organizing the chromatic pitches as sequence of perfect fifths 
this is strictly true in the standard tone equal temperament system using different system requires one interval of diminished sixth to be treated as fifth 
if is chosen as starting point the sequence is continuing the pattern from returns the sequence to its starting point of this order places the most closely related key signatures adjacent to one another 
it is usually illustrated in the form of circle 
definition the circle of fifths organizes pitches in sequence of perfect fifths generally shown as circle with the pitches and their corresponding keys in clockwise progression 
musicians and composers often use the circle of fifths to describe the musical relationships between pitches 
its design is helpful in composing and harmonizing melodies building chords and modulating to different keys within composition using the system of just intonation perfect fifth consists of two pitches with frequency ratio of but generating twelve successive perfect fifths in this way does not result in return to the pitch class of the starting note 
to adjust for this instruments are generally tuned with the equal temperament system 
twelve equal temperament fifths lead to note exactly seven octaves above the initial tone this results in perfect fifth that is equivalent to seven equal temperament semitones 
the top of the circle shows the key of major with no sharps or flats 
proceeding clockwise the pitches ascend by fifths 
the key signatures associated with those pitches also change the key of has one sharp the key of has sharps and so on 
similarly proceeding counterclockwise from the top of the circle the notes change by descending fifths and the key signatures change accordingly the key of has one flat the key of has flats and so on 
some keys at the bottom of the circle can be notated either in sharps or in flats 
starting at any pitch and ascending by fifth generates all twelve tones before returning to the beginning pitch class pitch class consists of all of the notes indicated by given letter regardless of octave all for example belong to the same pitch class 
moving counterclockwise the pitches descend by fifth but ascending by perfect fourth will lead to the same note an octave higher therefore in the same pitch class 
moving counter clockwise from could be thought of as descending by fifth to or ascending by fourth to structure and use diatonic key signatures each of the twelve pitches can serve as the tonic of major or minor key and each of these keys will have diatonic scale associated with it 
the circle diagram shows the number of sharps or flats in each key signature with the major key indicated by capital letter and the minor key indicated by lower case letter 
major and minor keys that have the same key signature are referred to as relative major and relative minor of one another 
modulation and chord progression tonal music often modulates to new tonal center whose key signature differs from the original by only one flat or sharp 
these closely related keys are fifth apart from each other and are therefore adjacent in the circle of fifths 
chord progressions also often move between chords whose roots are related by perfect fifth making the circle of fifths useful in illustrating the harmonic distance between chords 
the circle of fifths is used to organize and describe the harmonic function of chords 
chords can progress in pattern of ascending perfect fourths alternately viewed as descending perfect fifths in functional succession 
this can be shown by the circle of fifths in which therefore scale degree ii is closer to the dominant than scale degree iv 
in this view the tonic is considered the end point of chord progression derived from the circle of fifths 
according to richard franko goldman harmony in western music the iv chord is in the simplest mechanisms of diatonic relationships at the greatest distance from in terms of the descending circle of fifths it leads away from rather than toward it 
he states that the progression ii an authentic cadence would feel more final or resolved than iv plagal cadence 
goldman concurs with nattiez who argues that the chord on the fourth degree appears long before the chord on ii and the subsequent final in the progression iv viio iii vi ii and is farther from the tonic there as well 
in this and related articles upper case roman numerals indicate major triads while lower case roman numerals indicate minor triads 
circle closure in non equal tuning systems using the exact ratio of frequencies to define perfect fifth just intonation does not quite result in return to the pitch class of the starting note after going around the circle of fifths 
equal temperament tuning produces fifths that return to tone exactly seven octaves above the initial tone and makes the frequency ratio of each half step the same 
an equal tempered fifth has frequency ratio of or about approximately two cents narrower than justly tuned fifth at ratio of ascending by justly tuned fifths fails to close the circle by an excess of approximately cents roughly quarter of semitone an interval known as the pythagorean comma 
in pythagorean tuning this problem is solved by markedly shortening the width of one of the twelve fifths which makes it severely dissonant 
this anomalous fifth is called the wolf fifth humorous reference to wolf howling an off pitch note 
the quarter comma meantone tuning system uses eleven fifths slightly narrower than the equally tempered fifth and requires much wider and even more dissonant wolf fifth to close the circle 
more complex tuning systems based on just intonation such as limit tuning use at most eight justly tuned fifths and at least three non just fifths some slightly narrower and some slightly wider than the just fifth to close the circle 
other tuning systems use up to tones the original tones and more between them in order to close the circle of fifths 
history the circle of fifths developed in the late and early to theorize the modulation of the baroque era see baroque era 
the first circle of fifths diagram appears in the grammatika of the composer and theorist nikolay diletsky who intended to present music theory as tool for composition 
it was the first of its kind aimed at teaching russian audience how to write western style polyphonic compositions 
circle of fifths diagram was independently created by german composer and theorist johann david heinichen in his neu erfundene und gr ndliche anweisung which he called the musical circle german musicalischer circul 
this was also published in his der general bass in der composition 
heinichen placed the relative minor key next to the major key which did not reflect the actual proximity of keys 
johann mattheson and others attempted to improve this david kellner proposed having the major keys on one circle and the relative minor keys on second inner circle 
this was later developed into chordal space incorporating the parallel minor as well some sources imply that the circle of fifths was known in antiquity by pythagoras 
this is misunderstanding and an anachronism 
tuning by fifths so called pythagorean tuning dates to ancient mesopotamia see music of mesopotamia music theory though they did not extend this to twelve note scale stopping at seven 
the pythagorean comma was calculated by euclid and by chinese mathematicians in the huainanzi see pythagorean comma history 
thus it was known in antiquity that cycle of twelve fifths was almost exactly seven octaves more practically alternating ascending fifths and descending fourths was almost exactly an octave 
however this was theoretical knowledge and was not used to construct repeating twelve tone scale nor to modulate 
this was done later in meantone temperament and twelve tone equal temperament which allowed modulation while still being in tune but did not develop in europe until about 
use in musical pieces from the baroque music era and the classical era of music and in western popular music traditional music and folk music when pieces or songs modulate to new key these modulations are often associated with the circle of fifths 
in practice compositions rarely make use of the entire circle of fifths 
more commonly composers make use of the compositional idea of the cycle of ths when music moves consistently through smaller or larger segment of the tonal structural resources which the circle abstractly represents 
the usual practice is to derive the circle of fifths progression from the seven tones of the diatonic scale rather from the full range of twelve tones present in the chromatic scale 
in this diatonic version of the circle one of the fifths is not true fifth it is tritone or diminished fifth 
between and in the natural diatonic scale 
without sharps or flats 
here is how the circle of fifths derives through permutation from the diatonic major scale and from the natural minor scale the following is the basic sequence of chords that can be built over the major bass line and over the minor adding sevenths to the chords creates greater sense of forward momentum to the harmony baroque era according to richard taruskin arcangelo corelli was the most influential composer to establish the pattern as standard harmonic trope it was precisely in corelli time the late seventeenth century that the circle of fifths was being theorized as the main propellor of harmonic motion and it was corelli more than any one composer who put that new idea into telling practice 
the circle of fifths progression occurs frequently in the music of bach 
in the following from jauchzet gott in allen landen bwv even when the solo bass line implies rather than states the chords involved handel uses circle of fifths progression as the basis for the passacaglia movement from his harpsichord suite no 
baroque composers learnt to enhance the propulsive force of the harmony engendered by the circle of fifths by adding sevenths to most of the constituent chords 
these sevenths being dissonances create the need for resolution thus turning each progression of the circle into simultaneous reliever and re stimulator of harmonic tension hence harnessed for expressive purposes 
striking passages that illustrate the use of sevenths occur in the aria pena tiranna in handel opera amadigi di gaula and in bach keyboard arrangement of alessandro marcello concerto for oboe and strings 
nineteenth century during the nineteenth century composers made use of the circle of fifths to enhance the expressive character of their music 
franz schubert poignant impromptu in flat major contains such passage as does the intermezzo movement from mendelssohn string quartet no robert schumann evocative child falling asleep from his kinderszenen springs surprise at the end of the progression the piece ends on an minor chord instead of the expected tonic minor 
in wagner opera tterd mmerung cycle of fifths progression occurs in the music which transitions from the end of the prologue into the first scene of act set in the imposing hall of the wealthy gibichungs 
status and reputation are written all over the motifs assigned to gunther chief of the gibichung clan jazz and popular music the enduring popularity of the circle of fifths as both form building device and as an expressive musical trope is evident in the number of standard popular songs composed during the twentieth century 
it is also favored as vehicle for improvisation by jazz musicians 
bart howard fly me to the moon the song opens with pattern of descending phrases in essence the hook of the song presented with soothing predictability almost as if the future direction of the melody is dictated by the opening five notes 
the harmonic progression for its part rarely departs from the circle of fifths 
jerome kern all the things you are ray noble cherokee 
many jazz musicians have found this particularly challenging as the middle eight progresses so rapidly through the circle creating series of ii progressions that temporarily pass through several tonalities 
kosmo prevert and mercer autumn leaves the beatles you never give me your money mike oldfield incantations carlos santana europa earth cry heaven smile gloria gaynor will survive pet shop boys it sin donna summer love to love you baby related concepts diatonic circle of fifths the diatonic circle of fifths is the circle of fifths encompassing only members of the diatonic scale 
therefore it contains diminished fifth in major between and see structure implies multiplicity 
the circle progression is commonly circle of fifths through the diatonic chords including one diminished chord 
circle progression in major with chords iv viio iii vi ii is shown below 
chromatic circle the circle of fifths is closely related to the chromatic circle which also arranges the twelve equal tempered pitch classes in circular ordering 
key difference between the two circles is that the chromatic circle can be understood as continuous space where every point on the circle corresponds to conceivable pitch class and every conceivable pitch class corresponds to point on the circle 
by contrast the circle of fifths is fundamentally discrete structure and there is no obvious way to assign pitch classes to each of its points 
in this sense the two circles are mathematically quite different 
however the twelve equal tempered pitch classes can be represented by the cyclic group of order twelve or equivalently the residue classes modulo twelve the group has four generators which can be identified with the ascending and descending semitones and the ascending and descending perfect fifths 
the semitonal generator gives rise to the chromatic circle while the perfect fifth gives rise to the circle of fifths 
relation with chromatic scale the circle of fifths or fourths may be mapped from the chromatic scale by multiplication and vice versa 
to map between the circle of fifths and the chromatic scale in integer notation multiply by and for the circle of fourths multiply by 
here is demonstration of this procedure 
start off with an ordered tuple tone row of integers representing the notes of the chromatic scale 
now multiply the entire tuple by and then apply modulo reduction to each of the numbers subtract from each number as many times as necessary until the number becomes smaller than which is equivalent to which is the circle of fifths 
note that this is enharmonically equivalent to 
enharmonic equivalents theoretical keys and the spiral of fifths equal temperament tuning does not use the exact ratio of frequencies that defines perfect fifth wheras the system of just intonation uses this exact ratio 
ascending by fifths in equal temperament leads to return to the starting pitch class starting with and ascending by fifths leads to another after twelve iterations 
this does not occur if an exact ratio is used just intonation 
the adjustment made in equal temperament tuning is called the pythagorean comma 
because of this difference pitches that are enharmonically equivalent in equal temperament tuning and are not equivalent when using just intonation 
in just intonation the sequence of fifths can therefore be visualized as spiral not circle sequence of twelve fifths results in comma pump by the pythagorean comma visualized as going up level in the spiral 
see also circle closure in non equal tuning systems 
without enharmonic equivalence continuing sequence of fifths results in notes with double accidentals double sharps or double flats 
when using equal temperament these can be replaced by an enharmonically equivalent note 
keys with double sharps or flats in the key signatures are called theoretical keys their use is extremely rare 
notation in these cases is not standardized 
the default behaviour of lilypond pictured above writes single sharps or flats in the circle of fifths order before proceeding to double sharps or flats 
this is the format used in john foulds world requiem op 
which ends with the key signature of major as displayed above 
the sharps in the key signature of major here proceed single sharps or flats in the key signature are sometimes repeated as courtesy 
max reger supplement to the theory of modulation which contains minor key signatures on pp 
these have at the start and also at the end with double flat symbol going the convention of lilypond and foulds would suppress the initial 
sometimes the double signs are written at the beginning of the key signature followed by the single signs 
for example the key signature is notated as 
this convention is used by victor ewald by the program finale software and by some theoretical works 
see also approach chord sonata form well temperament circle of fifths text table pitch constellation multiplicative group of integers modulo notes references barnett gregory 
tonal organization in seventeenth century music theory 
in thomas christensen ed 
the cambridge history of western music theory 
cambridge cambridge university press 
the jazz standards guide to the repertoire 
isbn goldman richard franko 
harmony in western music 
theoretical work of late seventeenth century muscovy nikolai diletskii grammatika and the earliest circle of fifths 
journal of the american musicological society 
between modes and keys german theory 
prelude to musical geometry 
the college mathematics journal 
jstor archived from the original on retrieved nattiez jean jacques 
music and discourse toward semiology of music translated by carolyn abbate 
princeton new jersey princeton university press 
originally published in french as musicologie rale et miologie 
the oxford history of western music music in the seventeenth and eighteenth centuries 
further reading indy vincent 
cours de composition musicale 
paris durand et fils 
between modes and keys german theory 
the complete idiot guide to music theory nd ed 
indianapolis in alpha isbn purwins hendrik 
profiles of pitch classes circularity of relative pitch and key experiments models computational music analysis and perspectives 
berlin technische universit berlin 
purwins hendrik benjamin blankertz and klaus obermayer 
toroidal models in tonal theory and pitch class analysis 
in computing in musicology tonal theory for the digital age 
external links decoding the circle of vths interactive circle of fifths interactive circle of fifths for guitarists
in statistics generalized least squares gls is technique for estimating the unknown parameters in linear regression model when there is certain degree of correlation between the residuals in regression model 
in these cases ordinary least squares and weighted least squares can be statistically inefficient or even give misleading inferences 
gls was first described by alexander aitken in 
method outline in standard linear regression models we observe data on statistical units 
the response values are placed in vector and the predictor values are placed in the design matrix where is vector of the predictor variables including constant for the ith unit 
the model forces the conditional mean of given to be linear function of and assumes the conditional variance of the error term given is known nonsingular covariance matrix this is usually written as cov here is vector of unknown constants known as regression coefficients that must be estimated from the data 
suppose is candidate estimate for then the residual vector for will be the generalized least squares method estimates by minimizing the squared mahalanobis length of this residual vector argmin argmin where the last two terms evaluate to scalars resulting in argmin this objective is quadratic form in taking the gradient of this quadratic form with respect to and equating it to zero when gives therefore the minimum of the objective function can be computed yielding the explicit formula the quantity is known as the precision matrix or dispersion matrix generalization of the diagonal weight matrix 
properties the gls estimator is unbiased consistent efficient and asymptotically normal with and cov gls is equivalent to applying ordinary least squares to linearly transformed version of the data 
to see this factor for instance using the cholesky decomposition 
then if we pre multiply both sides of the equation by we get an equivalent linear model where and in this model var where is the identity matrix 
thus we can efficiently estimate by applying ordinary least squares ols to the transformed data which requires minimizing 
this has the effect of standardizing the scale of the errors and de correlating them 
since ols is applied to data with homoscedastic errors the gauss markov theorem applies and therefore the gls estimate is the best linear unbiased estimator for 
weighted least squares special case of gls called weighted least squares wls occurs when all the off diagonal entries of are this situation arises when the variances of the observed values are unequal 
heteroscedasticity is present but where no correlations exist among the observed variances 
the weight for unit is proportional to the reciprocal of the variance of the response for unit 
feasible generalized least squares if the covariance of the errors is unknown one can get consistent estimate of say using an implementable version of gls known as the feasible generalized least squares fgls estimator 
in fgls modeling proceeds in two stages the model is estimated by ols or another consistent but inefficient estimator and the residuals are used to build consistent estimator of the errors covariance matrix to do so one often needs to examine the model adding additional constraints for example if the errors follow time series process statistician generally needs some theoretical assumptions on this process to ensure that consistent estimator is available and using the consistent estimator of the covariance matrix of the errors one can implement gls ideas 
whereas gls is more efficient than ols under heteroscedasticity also spelled heteroskedasticity or autocorrelation this is not true for fgls 
the feasible estimator is provided the errors covariance matrix is consistently estimated asymptotically more efficient but for small or medium size sample it can be actually less efficient than ols 
this is why some authors prefer to use ols and reformulate their inferences by simply considering an alternative estimator for the variance of the estimator robust to heteroscedasticity or serial autocorrelation 
but for large samples fgls is preferred over ols under heteroskedasticity or serial correlation 
cautionary note is that the fgls estimator is not always consistent 
one case in which fgls might be inconsistent is if there are individual specific fixed effects in general this estimator has different properties than gls 
for large samples asymptotically all properties are under appropriate conditions common with respect to gls but for finite samples the properties of fgls estimators are unknown they vary dramatically with each particular model and as general rule their exact distributions cannot be derived analytically 
for finite samples fgls may be even less efficient than ols in some cases 
thus while gls can be made feasible it is not always wise to apply this method when the sample is small 
method sometimes used to improve the accuracy of the estimators in finite samples is to iterate 
taking the residuals from fgls to update the errors covariance estimator and then updating the fgls estimation applying the same idea iteratively until the estimators vary less than some tolerance 
but this method does not necessarily improve the efficiency of the estimator very much if the original sample was small 
reasonable option when samples are not too large is to apply ols but throwing away the classical variance estimator which is inconsistent in this framework and using hac heteroskedasticity and autocorrelation consistent estimator 
for example in autocorrelation context we can use the bartlett estimator often known as newey west estimator estimator since these authors popularized the use of this estimator among econometricians in their econometrica article and in heteroskedastic context we can use the eicker white estimator 
this approach is much safer and it is the appropriate path to take unless the sample is large and large is sometimes slippery issue 
if the errors distribution is asymmetric the required sample would be much larger 
the ordinary least squares ols estimator is calculated as usual by ols and estimates of the residuals ols are constructed 
for simplicity consider the model for heteroscedastic and not autocorrelated errors 
assume that the variance covariance matrix of the error vector is diagonal or equivalently that errors from distinct observations are uncorrelated 
then each diagonal entry may be estimated by the fitted residuals so may be constructed by ols diag 
it is important to notice that the squared residuals cannot be used in the previous expression we need an estimator of the errors variances 
to do so we can use parametric heteroskedasticity model or nonparametric estimator 
once this step is fulfilled we can proceed estimate using ols using weighted least squares ols ols the procedure can be iterated 
the first iteration is given by diag this estimation of can be iterated to convergence 
under regularity conditions any of the fgls estimator or that of any of its iterations if we iterate finite number of times is asymptotically distributed as 
where is the sample size and here lim means limit in probability see also confidence region effective degrees of freedom prais winsten estimation references further reading amemiya takeshi 
generalized least squares theory 
econometric methods second ed 
new york mcgraw hill 
generalized linear regression model and its applications 
elements of econometrics second ed 
isbn beck nathaniel katz jonathan september 
what to do and not to do with time series cross section data 
american political science review
in probability theory and statistics collection of random variables is independent and identically distributed if each random variable has the same probability distribution as the others and all are mutually independent 
this property is usually abbreviated as iid or iid 
iid was first defined in statistics and finds application in different fields such as data mining and signal processing 
introduction in statistics we commonly deal with random samples 
random sample can be thought of as set of objects that are chosen randomly 
or more formally it sequence of independent identically distributed iid random variables 
in other words the terms random sample and iid are basically one and the same 
in statistics we usually say random sample but in probability it more common to say iid identically distributed means that there are no overall trends the distribution doesn fluctuate and all items in the sample are taken from the same probability distribution 
independent means that the sample items are all independent events 
in other words they aren connected to each other in any way 
in other words knowledge of the value of one variable gives no information about the value of the other and vice versa 
application independent and identically distributed random variables are often used as an assumption which tends to simplify the underlying mathematics 
in practical applications of statistical modeling however the assumption may or may not be realistic the 
assumption is also used in central limit theorem which states that the probability distribution of the sum or average of 
variables with finite variance approaches normal distribution often the 
assumption arises in the context of sequences of random variables 
then independent and identically distributed implies that an element in the sequence is independent of the random variables that came before it 
in this way an 
sequence is different from markov sequence where the probability distribution for the nth random variable is function of the previous random variable in the sequence for first order markov sequence 
sequence does not imply the probabilities for all elements of the sample space or event space must be the same 
for example repeated throws of loaded dice will produce sequence that is despite the outcomes being biased 
definition definition for two random variables suppose that the random variables and are defined to assume values in let and be the cumulative distribution functions of and respectively and denote their joint cumulative distribution function by 
two random variables and are identically distributed if and only if two random variables and are independent if and only if 
see further independence probability theory two random variables 
two random variables and are 
if they are independent and identically distributed 
if and only if definition for more than two random variables the definition extends naturally to more than two random variables 
we say that random variables are 
if they are independent see further independence probability theory more than two random variables and identically distributed 
if and only if where denotes the joint cumulative distribution function of definition for independence in probability theory two events are called independent if and only if and 
in the following ab is short for and 
suppose there are the two events of the experiment if there is possibility 
generally the occurrence of has an effect on the probability of which is called conditional probability and only when the occurrence of has no effect on the occurrence of there is 
note if then are mutually independent which cannot be established with mutually incompatible at the same time that is independence must be compatible and mutual exclusion must be related 
suppose are three events 
if ab bc ac abc are satisfied then the events are independent of each other 
more general definition is there are events an 
if the probabilities of the product events for any events are equal to the product of the probabilities of each event then the events an are independent of each other 
examples example sequence of outcomes of spins of fair or unfair roulette wheel is 
one implication of this is that if the roulette ball lands on red for example times in row the next spin is no more or less likely to be black than on any other spin see the gambler fallacy 
sequence of fair or loaded dice rolls is 
sequence of fair or unfair coin flips is 
in signal processing and image processing the notion of transformation to 
implies two specifications the 
the signal level must be balanced on the time axis 
the signal spectrum must be flattened 
transformed by filtering such as deconvolution to white noise signal 
signal where all frequencies are equally present 
example toss coin times and record how many times does the coin lands on head 
independent each outcome of landing will not affect the other outcome which means the results are independent from each other 
identically distributed if the coin is homogeneous material each time the probability for head is which means the probability is identical for each time 
example roll dice times and record how many time the result is independent each outcome of the dice will not affect the next one which means the results are independent from each other 
identically distributed if the dice is homogeneous material each time the probability for the number is which means the probability is identical for each time 
example choose card from standard deck of cards containing cards then place the card back in the deck 
repeat it for times 
record the number of king appears independent each outcome of the card will not affect the next one which means the results are independent from each other 
identically distributed after drawing one card from it each time the probability for king is which means the probability is identical for each time 
generalizations many results that were first proven under the assumption that the random variables are 
have been shown to be true even under weaker distributional assumption 
exchangeable random variables the most general notion which shares the main properties of 
variables are exchangeable random variables introduced by bruno de finetti 
exchangeability means that while variables may not be independent future ones behave like past ones formally any value of finite sequence is as likely as any permutation of those values the joint probability distribution is invariant under the symmetric group 
this provides useful generalization for example sampling without replacement is not independent but is exchangeable 
vy process in stochastic calculus 
variables are thought of as discrete time vy process each variable gives how much one changes from one time to another 
for example sequence of bernoulli trials is interpreted as the bernoulli process 
one may generalize this to include continuous time vy processes and many vy processes can be seen as limits of 
variables for instance the wiener process is the limit of the bernoulli process 
in machine learning why assume the data in machine learning are independent and identically distributed 
machine learning uses currently acquired massive quantities of data to deliver faster more accurate results 
therefore we need to use historical data with overall representativeness 
if the data obtained is not representative of the overall situation then the rules will be summarized badly or wrongly 
hypothesis the number of individual cases in the training sample can be greatly reduced 
this assumption makes maximization very easy to calculate mathematically 
observing the assumption of independent and identical distribution in mathematics simplifies the calculation of the likelihood function in optimization problems 
because of the assumption of independence the likelihood function can be written like this 
in order to maximize the probability of the observed event take the log function and maximize the parameter that is to say to compute log where log log log log 
log the computer is very efficient to calculate multiple additions but it is not efficient to calculate the multiplication 
this simplification is the core reason for the increase in computational efficiency 
and this log transformation is also in the process of maximizing turning many exponential functions into linear functions 
for two reasons this hypothesis is easy to use the central limit theorem in practical applications 
even if the sample comes from more complex non gaussian distribution it can also approximate well 
because it can be simplified from the central limit theorem to gaussian distribution 
for large number of observable samples the sum of many random variables will have an approximately normal distribution 
the second reason is that the accuracy of the model depends on the simplicity and representative power of the model unit as well as the data quality 
because the simplicity of the unit makes it easy to interpret and scale and the representative power scale out of the unit improves the model accuracy 
like in deep neural network each neuron is very simple but has strong representative power layer by layer to represent more complex features to improve model accuracy 
see also de finetti theorem pairwise independent variables central limit theorem references further reading
linear least squares lls is the least squares approximation of linear functions to data 
it is set of formulations for solving statistical problems involved in linear regression including variants for ordinary unweighted weighted and generalized correlated residuals 
numerical methods for linear least squares include inverting the matrix of the normal equations and orthogonal decomposition methods 
main formulations the three main linear least squares formulations are ordinary least squares ols is the most common estimator 
ols estimates are commonly used to analyze both experimental and observational data 
the ols method minimizes the sum of squared residuals and leads to closed form expression for the estimated value of the unknown parameter vector where is vector whose ith element is the ith observation of the dependent variable and is matrix whose ij element is the ith observation of the jth independent variable 
the estimator is unbiased and consistent if the errors have finite variance and are uncorrelated with the regressors where is the transpose of row of the matrix it is also efficient under the assumption that the errors have finite variance and are homoscedastic meaning that xi does not depend on the condition that the errors are uncorrelated with the regressors will generally be satisfied in an experiment but in the case of observational data it is difficult to exclude the possibility of an omitted covariate that is related to both the observed covariates and the response variable 
the existence of such covariate will generally lead to correlation between the regressors and the response variable and hence to an inconsistent estimator of the condition of homoscedasticity can fail with either experimental or observational data 
if the goal is either inference or predictive modeling the performance of ols estimates can be poor if multicollinearity is present unless the sample size is large 
weighted least squares wls are used when heteroscedasticity is present in the error terms of the model 
generalized least squares gls is an extension of the ols method that allows efficient estimation of when either heteroscedasticity or correlations or both are present among the error terms of the model as long as the form of heteroscedasticity and correlation is known independently of the data 
to handle heteroscedasticity when the error terms are uncorrelated with each other gls minimizes weighted analogue to the sum of squared residuals from ols regression where the weight for the ith case is inversely proportional to var 
this special case of gls is called weighted least squares 
the gls solution to an estimation problem is where is the covariance matrix of the errors 
gls can be viewed as applying linear transformation to the data so that the assumptions of ols are met for the transformed data 
for gls to be applied the covariance structure of the errors must be known up to multiplicative constant 
alternative formulations other formulations include iteratively reweighted least squares irls is used when heteroscedasticity or correlations or both are present among the error terms of the model but where little is known about the covariance structure of the errors independently of the data 
in the first iteration ols or gls with provisional covariance structure is carried out and the residuals are obtained from the fit 
based on the residuals an improved estimate of the covariance structure of the errors can usually be obtained 
subsequent gls iteration is then performed using this estimate of the error structure to define the weights 
the process can be iterated to convergence but in many cases only one iteration is sufficient to achieve an efficient estimate of instrumental variables regression iv can be performed when the regressors are correlated with the errors 
in this case we need the existence of some auxiliary instrumental variables zi such that zi if is the matrix of instruments then the estimator can be given in closed form as optimal instruments regression is an extension of classical iv regression to the situation where zi total least squares tls is an approach to least squares estimation of the linear regression model that treats the covariates and response variable in more geometrically symmetric manner than ols 
it is one approach to handling the errors in variables problem and is also sometimes used even when the covariates are assumed to be error free percentage least squares focuses on reducing percentage errors which is useful in the field of forecasting or time series analysis 
it is also useful in situations where the dependent variable has wide range without constant variance as here the larger residuals at the upper end of the range would dominate if ols were used 
when the percentage or relative error is normally distributed least squares percentage regression provides maximum likelihood estimates 
percentage regression is linked to multiplicative error model whereas ols is linked to models containing an additive error term constrained least squares indicates linear least squares problem with additional constraints on the solution 
objective function in ols assuming unweighted observations the optimal value of the objective function is found by substituting the optimal expression for the coefficient vector where the latter equality holding since is symmetric and idempotent 
it can be shown from this that under an appropriate assignment of weights the expected value of is if instead unit weights are assumed the expected value of is where is the variance of each observation 
if it is assumed that the residuals belong to normal distribution the objective function being sum of weighted squared residuals will belong to chi squared distribution with degrees of freedom 
some illustrative percentile values of are given in the following table 
these values can be used for statistical criterion as to the goodness of fit 
when unit weights are used the numbers should be divided by the variance of an observation 
for wls the ordinary objective function above is replaced for weighted average of residuals 
discussion in statistics and mathematics linear least squares is an approach to fitting mathematical or statistical model to data in cases where the idealized value provided by the model for any data point is expressed linearly in terms of the unknown parameters of the model 
the resulting fitted model can be used to summarize the data to predict unobserved values from the same system and to understand the mechanisms that may underlie the system 
mathematically linear least squares is the problem of approximately solving an overdetermined system of linear equations where is not an element of the column space of the matrix the approximate solution is realized as an exact solution to where is the projection of onto the column space of the best approximation is then that which minimizes the sum of squared differences between the data values and their corresponding modeled values 
the approach is called linear least squares since the assumed function is linear in the parameters to be estimated 
linear least squares problems are convex and have closed form solution that is unique provided that the number of data points used for fitting equals or exceeds the number of unknown parameters except in special degenerate situations 
in contrast non linear least squares problems generally must be solved by an iterative procedure and the problems can be non convex with multiple optima for the objective function 
if prior distributions are available then even an underdetermined system can be solved using the bayesian mmse estimator 
in statistics linear least squares problems correspond to particularly important type of statistical model called linear regression which arises as particular form of regression analysis 
one basic form of such model is an ordinary least squares model 
the present article concentrates on the mathematical aspects of linear least squares problems with discussion of the formulation and interpretation of statistical regression models and statistical inferences related to these being dealt with in the articles just mentioned 
see outline of regression analysis for an outline of the topic 
properties if the experimental errors are uncorrelated have mean of zero and constant variance the gauss markov theorem states that the least squares estimator has the minimum variance of all estimators that are linear combinations of the observations 
in this sense it is the best or optimal estimator of the parameters 
note particularly that this property is independent of the statistical distribution function of the errors 
in other words the distribution function of the errors need not be normal distribution 
however for some probability distributions there is no guarantee that the least squares solution is even possible given the observations still in such cases it is the best estimator that is both linear and unbiased 
for example it is easy to show that the arithmetic mean of set of measurements of quantity is the least squares estimator of the value of that quantity 
if the conditions of the gauss markov theorem apply the arithmetic mean is optimal whatever the distribution of errors of the measurements might be 
however in the case that the experimental errors do belong to normal distribution the least squares estimator is also maximum likelihood estimator these properties underpin the use of the method of least squares for all types of data fitting even when the assumptions are not strictly valid 
limitations an assumption underlying the treatment given above is that the independent variable is free of error 
in practice the errors on the measurements of the independent variable are usually much smaller than the errors on the dependent variable and can therefore be ignored 
when this is not the case total least squares or more generally errors in variables models or rigorous least squares should be used 
this can be done by adjusting the weighting scheme to take into account errors on both the dependent and independent variables and then following the standard procedure in some cases the weighted normal equations matrix xtx is ill conditioned 
when fitting polynomials the normal equations matrix is vandermonde matrix 
vandermonde matrices become increasingly ill conditioned as the order of the matrix increases 
in these cases the least squares estimate amplifies the measurement noise and may be grossly inaccurate 
various regularization techniques can be applied in such cases the most common of which is called ridge regression 
if further information about the parameters is known for example range of possible values of then various techniques can be used to increase the stability of the solution 
for example see constrained least squares 
another drawback of the least squares estimator is the fact that the norm of the residuals is minimized whereas in some cases one is truly interested in obtaining small error in the parameter small value of 
however since the true parameter is necessarily unknown this quantity cannot be directly minimized 
if prior probability on is known then bayes estimator can be used to minimize the mean squared error 
the least squares method is often applied when no prior is known 
surprisingly when several parameters are being estimated jointly better estimators can be constructed an effect known as stein phenomenon 
for example if the measurement error is gaussian several estimators are known which dominate or outperform the least squares technique the best known of these is the james stein estimator 
this is an example of more general shrinkage estimators that have been applied to regression problems 
applications polynomial fitting models are polynomials in an independent variable straight line quadratic cubic quartic and higher polynomials 
for regression with high order polynomials the use of orthogonal polynomials is recommended 
numerical smoothing and differentiation this is an application of polynomial fitting 
multinomials in more than one independent variable including surface fitting curve fitting with splines chemometrics calibration curve standard addition gran plot analysis of mixtures uses in data fitting the primary application of linear least squares is in data fitting 
given set of data points consisting of experimentally measured values taken at values of an independent variable may be scalar or vector quantities and given model function with it is desired to find the parameters such that the model function best fits the data 
in linear least squares linearity is meant to be with respect to parameters so here the functions may be nonlinear with respect to the variable ideally the model function fits the data exactly so for all this is usually not possible in practice as there are more data points than there are parameters to be determined 
the approach chosen then is to find the minimal possible value of the sum of squares of the residuals so to minimize the function after substituting for and then for this minimization problem becomes the quadratic minimization problem above with and the best fit can be found by solving the normal equations 
example as result of an experiment four data points were obtained and shown in red in the diagram on the right 
we hope to find line that best fits these four points 
in other words we would like to find the numbers and that approximately solve the overdetermined linear system of four equations in two unknowns in some best sense 
represents the residual at each point between the curve fit and the data the least squares approach to solving this problem is to try to make the sum of the squares of these residuals as small as possible that is to find the minimum of the function the minimum is determined by calculating the partial derivatives of with respect to and and setting them to zero this results in system of two equations in two unknowns called the normal equations which when solved give and the equation is the line of best fit 
the residuals that is the differences between the values from the observations and the predicated variables by using the line of best fit are then found to be and see the diagram on the right 
the minimum value of the sum of squares of the residuals is more generally one can have regressors and linear model using quadratic model importantly in linear least squares we are not restricted to using line as the model as in the above example 
for instance we could have chosen the restricted quadratic model this model is still linear in the parameter so we can still perform the same analysis constructing system of equations from the data points the partial derivatives with respect to the parameters this time there is only one are again computed and set to and solved leading to the resulting best fit model 
see also line line intersection nearest point to non intersecting lines an application line fitting nonlinear least squares regularized least squares simple linear regression partial least squares regression linear function references further reading bevington philip robinson keith 
data reduction and error analysis for the physical sciences 
external links least squares fitting from mathworld least squares fitting polynomial from mathworld
in the field of multivariate statistics kernel principal component analysis kernel pca is an extension of principal component analysis pca using techniques of kernel methods 
using kernel the originally linear operations of pca are performed in reproducing kernel hilbert space 
background linear pca recall that conventional pca operates on zero centered data that is where is one of the multivariate observations 
it operates by diagonalizing the covariance matrix in other words it gives an eigendecomposition of the covariance matrix which can be rewritten as for see also covariance matrix as linear operator introduction of the kernel to pca to understand the utility of kernel pca particularly for clustering observe that while points cannot in general be linearly separated in dimensions they can almost always be linearly separated in dimensions 
that is given points if we map them to an dimensional space with where it is easy to construct hyperplane that divides the points into arbitrary clusters 
of course this creates linearly independent vectors so there is no covariance on which to perform eigendecomposition explicitly as we would in linear pca 
instead in kernel pca non trivial arbitrary function is chosen that is never calculated explicitly allowing the possibility to use very high dimensional if we never have to actually evaluate the data in that space 
since we generally try to avoid working in the space which we will call the feature space we can create the by kernel which represents the inner product space see gramian matrix of the otherwise intractable feature space 
the dual form that arises in the creation of kernel allows us to mathematically formulate version of pca in which we never actually solve the eigenvectors and eigenvalues of the covariance matrix in the space see kernel trick 
the elements in each column of represent the dot product of one point of the transformed data with respect to all the transformed points points 
some well known kernels are shown in the example below 
because we are never working directly in the feature space the kernel formulation of pca is restricted in that it computes not the principal components themselves but the projections of our data onto those components 
to evaluate the projection from point in the feature space onto the kth principal component where superscript means the component not powers of we note that denotes dot product which is simply the elements of the kernel it seems all that left is to calculate and normalize the which can be done by solving the eigenvector equation where is the number of data points in the set and and are the eigenvalues and eigenvectors of then to normalize the eigenvectors we require that care must be taken regarding the fact that whether or not has zero mean in its original space it is not guaranteed to be centered in the feature space which we never compute explicitly 
since centered data is required to perform an effective principal component analysis we centralize to become where denotes by matrix for which each element takes value we use to perform the kernel pca algorithm described above 
one caveat of kernel pca should be illustrated here 
in linear pca we can use the eigenvalues to rank the eigenvectors based on how much of the variation of the data is captured by each principal component 
this is useful for data dimensionality reduction and it could also be applied to kpca 
however in practice there are cases that all variations of the data are same 
this is typically caused by wrong choice of kernel scale 
large datasets in practice large data set leads to large and storing may become problem 
one way to deal with this is to perform clustering on the dataset and populate the kernel with the means of those clusters 
since even this method may yield relatively large it is common to compute only the top eigenvalues and eigenvectors of the eigenvalues are calculated in this way 
example consider three concentric clouds of points shown we wish to use kernel pca to identify these groups 
the color of the points does not represent information involved in the algorithm but only shows how the transformation relocates the data points 
first consider the kernel applying this to kernel pca yields the next image 
now consider gaussian kernel that is this kernel is measure of closeness equal to when the points coincide and equal to at infinity 
note in particular that the first principal component is enough to distinguish the three different groups which is impossible using only linear pca because linear pca operates only in the given in this case two dimensional space in which these concentric point clouds are not linearly separable 
applications kernel pca has been demonstrated to be useful for novelty detection and image de noising 
see also cluster analysis nonlinear dimensionality reduction spectral clustering references
the chromatic circle is clock diagram for displaying relationships among the equal tempered pitch classes making up the familiar chromatic scale on circle 
explanation if one starts on any equal tempered pitch and repeatedly ascends by the musical interval of semitone one will eventually land on pitch with the same pitch class as the initial one having passed through all the other equal tempered chromatic pitch classes in between 
since the space is circular it is also possible to descend by semitone 
the chromatic circle is useful because it represents melodic distance which is often correlated with physical distance on musical instruments 
for instance to move from any on piano keyboard to the nearest one must move up four semitones corresponding to four clockwise steps on the chromatic circle 
one can also move down by eight semitones corresponding to eight counterclockwise steps on the pitch class circle 
larger motions on the piano or in pitch space can be represented in pitch class space by paths that wrap around the chromatic circle one or more times 
one can represent the twelve equal tempered pitch classes by the cyclic group of order twelve or equivalently the residue classes modulo twelve 
the group has four generators which can be identified with the ascending and descending semitones and the ascending and descending perfect fifths 
the semitonal generator gives rise to the chromatic circle while the perfect fifth gives rise to the circle of fifths 
comparison with circle of fifths key difference between the chromatic circle and the circle of fifths is that the former is truly continuous space every point on the circle corresponds to conceivable pitch class and every conceivable pitch class corresponds to point on the circle 
by contrast the circle of fifths is fundamentally discrete structure and there is no obvious way to assign pitch classes to each of its points 
pitch constellation pitch constellation is graphical representation of pitches used to describe musical scales modes chords or other groupings of pitches within an octave range 
it consists of circle with markings along the circumference or lines from the center which indicate pitches 
most pitch constellations use of subset of pitches chosen from the twelve pitch chromatic scale 
in this case the points on the circle are spaced like the twelve hour markings on an analog clock where each tick mark represents semitone 
scales and modes the pitch constellation provides an easy way to identify certain patterns and similarities between harmonic structures 
major scale consists of circle with markings at or and clock 
minor scale consists of circle with markings at or and clock 
the diagrams above show the two scales marked with scale degrees 
it can be observed that the tonic second fourth and fifth are shared while the minor scale flattens the third sixth and seventh notes relative to the major scale 
another observation is that the minor scale constellation is the same as the major scale but rotated degrees 
in the following drawing all of the major minor scales are drawn 
note that the constellation for all the major scales or all the minor scales are identical 
the different scales are generated by rotating the note overlay 
the notes that need to be sharpened flattened can be easily identified 
moreover if we draw all seven diatonic modes we can see them all as rotations of the ionian mode 
note also the significance of the clock point 
this corresponds to tritone 
the modes including pitches tritone from the tonic locrian and lydian are least used 
the clock and clock pitches are also important points corresponding to perfect fourth and perfect fifth respectively 
the most used scales modes major ionian mode minor aeolian mode and mixolydian include these pitches 
symmetric scales have simple representations in this scheme 
more exotic scales such as the pentatonic blues and octatonic can also be drawn and related to the common scales 
more complete list of musical scales and modes other overlays in previous sections we saw how various overlays scale degrees semi tone numbering notes can be used to notate the circumference of the constellation 
various other overlays can be laid around the constellation 
pitch ratios ratios of pitch frequencies 
note that once pitch constellation has been determined any number of overlays notes solf ge intervals etc 
may be placed on top for analysis comparison 
often generating one harmonic relationship from another is simply matter of rotating the overlay or constellation or shifting one or two pitch locations 
chords similarities between chords can also be observed as well as the significance of augmented diminished notes for triads we have the following and for seventh chords circle of fifths beginning with pitch constellation of chromatic scale the notes of circle of fifths can be easily generated 
starting at and moving across the circle and then one tick clockwise line is drawn with an arrow indicating the direction moved 
continuing from that point across the circle and one tick clockwise all points are connected 
moving through this pattern the notes of the circle of fifths can be determined 
one can also depict non tempered intervals on chromatic circle which allows one to depict commas small intervals particularly comma pumps 
for example using sequence of twelve just fifths ratio does not quite return to the starting point the size of the gap is the pythagorean comma resulting in broken circle of fifths 
technical note the ratio of the frequencies between two pitches in the constellation can be determined as follows 
take the length of the arc measured clockwise between the two points and divide by the circumference of the circle 
the frequency ratio is two raised to this power 
for example for fifth which is located at clock relative to the tonic the frequency ratio is references further reading brower candace cognitive theory of musical meaning journal of music theory duke university press doi jstor ku inskas darius symmetry in creative work of mikalojus konstantinas iurlionis pdf menotyra olson harry music physics and engineering dover publications isbn external links notenscheibe web application pitch constellations of scales triads intervals and the circle of fifths with basic audio on line app illustrating pitch constellations scaletapper iphone app which utilizes pitch constellations 
pdf of musical scales
inferno italian rno italian for hell is the first part of italian writer dante alighieri th century epic poem divine comedy 
it is followed by purgatorio and paradiso 
the inferno describes dante journey through hell guided by the ancient roman poet virgil 
in the poem hell is depicted as nine concentric circles of torment located within the earth it is the realm of those who have rejected spiritual values by yielding to bestial appetites or violence or by perverting their human intellect to fraud or malice against their fellowmen as an allegory the divine comedy represents the journey of the soul toward god with the inferno describing the recognition and rejection of sin 
prelude to hell canto the poem begins on the night of maundy thursday on march or april shortly before the dawn of good friday 
the narrator dante himself is thirty five years old and thus midway in the journey of our life nel mezzo del cammin di nostra vita half of the biblical lifespan of seventy psalm vulgate psalm kjv 
the poet finds himself lost in dark wood selva oscura astray from the straight way diritta via also translatable as right way of salvation 
he sets out to climb directly up small mountain but his way is blocked by three beasts he cannot evade lonza usually rendered as leopard or leopon leone lion and lupa she wolf 
the three beasts taken from jeremiah are thought to symbolize the three kinds of sin that bring the unrepentant soul into one of the three major divisions of hell 
according to john ciardi these are incontinence the she wolf violence and bestiality the lion and fraud and malice the leopard dorothy sayers assigns the leopard to incontinence and the she wolf to fraud malice 
it is now dawn of good friday april with the sun rising in aries 
the beasts drive him back despairing into the darkness of error lower place basso loco where the sun is silent sol tace 
however dante is rescued by figure who announces that he was born sub iulio in the time of julius caesar and lived under augustus it is the shade of the roman poet virgil author of the aeneid latin epic 
canto ii on the evening of good friday dante hesitates as he follows virgil virgil explains that he has been sent by beatrice the symbol of divine love 
beatrice had been moved to aid dante by the virgin mary symbolic of compassion and saint lucia symbolic of illuminating grace 
rachel symbolic of the contemplative life also appears in the heavenly scene recounted by virgil 
the two of them then begin their journey to the underworld 
canto iii vestibule of hell dante passes through the gate of hell which bears an inscription ending with the phrase lasciate ogne speranza voi ch intrate most frequently translated as abandon all hope ye who enter here 
dante and his guide hear the anguished screams of the uncommitted 
these are the souls of people who in life took no sides the opportunists who were for neither good nor evil but instead were merely concerned with themselves 
among these dante recognizes figure who made the great refusal implied to be pope celestine whose cowardice in selfish terror for his own welfare served as the door through which so much evil entered the church 
mixed with them are outcasts who took no side in the rebellion of angels 
these souls are forever unclassified they are neither in hell nor out of it but reside on the shores of the acheron 
naked and futile they race around through the mist in eternal pursuit of an elusive wavering banner symbolic of their pursuit of ever shifting self interest while relentlessly chased by swarms of wasps and hornets who continually sting them 
loathsome maggots and worms at the sinners feet drink the putrid mixture of blood pus and tears that flows down their bodies 
this symbolizes the sting of their guilty conscience and the repugnance of sin 
this may also be seen as reflection of the spiritual stagnation in which they lived 
after passing through the vestibule dante and virgil reach the ferry that will take them across the river acheron and to hell proper 
the ferry is piloted by charon who does not want to let dante enter for he is living being 
virgil forces charon to take him by declaring vuolsi cos col dove si puote ci che si vuole it is so willed there where is power to do that which is willed referring to the fact that dante is on his journey on divine grounds 
the wailing and blasphemy of the damned souls entering charon boat contrast with the joyful singing of the blessed souls arriving by ferry in the purgatorio 
the passage across the acheron however is undescribed since dante faints and does not awaken until they reach the other side 
nine circles of hell overview virgil proceeds to guide dante through the nine circles of hell 
the circles are concentric representing gradual increase in wickedness and culminating at the centre of the earth where satan is held in bondage 
the sinners of each circle are punished for eternity in fashion fitting their crimes each punishment is contrapasso symbolic instance of poetic justice 
for example later in the poem dante and virgil encounter fortune tellers who must walk forward with their heads on backward unable to see what is ahead because they tried to see the future through forbidden means 
such contrapasso functions not merely as form of divine revenge but rather as the fulfilment of destiny freely chosen by each soul during his or her life 
people who sinned but prayed for forgiveness before their deaths are found not in hell but in purgatory where they labour to become free of their sins 
those in hell are people who tried to justify their sins and are unrepentant 
dante hell is structurally based on the ideas of aristotle but with certain christian symbolisms exceptions and misconstructions of aristotle text and further supplement from cicero de officiis 
virgil reminds dante the character of those pages where the ethics tells of three conditions contrary to heaven will and rule incontinence vice and brute bestiality 
cicero for his part had divided sins between violence and fraud 
by conflating cicero violence with aristotle bestiality and his fraud with malice or vice dante the poet obtained three major categories of sin as symbolized by the three beasts that dante encounters in canto these are incontinence violence bestiality and fraud malice 
sinners punished for incontinence also known as wantonness the lustful the gluttonous the hoarders and wasters and the wrathful and sullen all demonstrated weakness in controlling their appetites desires and natural urges according to aristotle ethics incontinence is less condemnable than malice or bestiality and therefore these sinners are located in four circles of upper hell circles 
these sinners endure lesser torments than do those consigned to lower hell located within the walls of the city of dis for committing acts of violence and fraud the latter of which involves as dorothy sayers writes abuse of the specifically human faculty of reason 
the deeper levels are organized into one circle for violence circle and two circles for fraud circles and 
as christian dante adds circle limbo to upper hell and circle heresy to lower hell making circles in total incorporating the vestibule of the futile this leads to hell containing main divisions 
this structure is also found within the purgatorio and paradiso 
lower hell is further subdivided circle violence is divided into three rings circle fraud is divided into ten bolge and circle treachery is divided into four regions 
thus hell contains in total divisions 
first circle limbo canto iv dante wakes up to find that he has crossed the acheron and virgil leads him to the first circle of the abyss limbo where virgil himself resides 
the first circle contains the unbaptized and the virtuous pagans who although not sinful enough to warrant damnation did not accept christ 
dorothy sayers writes after those who refused choice come those without opportunity of choice 
they could not that is choose christ they could and did choose human virtue and for that they have their reward 
limbo shares many characteristics with the asphodel meadows and thus the guiltless damned are punished by living in deficient form of heaven 
without baptism the portal of the faith that you embrace they lacked the hope for something greater than rational minds can conceive 
when dante asked if anyone has ever left limbo virgil states that he saw jesus mighty one descend into limbo and take adam abel noah moses abraham david rachel and others see limbo of the patriarchs into his all forgiving arms and transport them to heaven as the first human souls to be saved 
the event known as the harrowing of hell supposedly occurred around ad or dante encounters the poets homer horace ovid and lucan who include him in their number and make him sixth in that high company 
they reach the base of great castle the dwelling place of the wisest men of antiquity surrounded by seven gates and flowing brook 
after passing through the seven gates the group comes to an exquisite green meadow and dante encounters the inhabitants of the citadel 
these include figures associated with the trojans and their descendants the romans electra mother of troy founder dardanus hector aeneas julius caesar in his role as roman general in his armor falcon eyed camilla penthesilea queen of the amazons king latinus and his daughter lavinia lucius junius brutus who overthrew tarquin to found the roman republic lucretia julia marcia and cornelia africana 
dante also sees saladin muslim military leader known for his battle against the crusaders as well as his generous chivalrous and merciful conduct 
dante next encounters group of philosophers including aristotle with socrates and plato at his side as well as democritus diogenes either diogenes the cynic or diogenes of apollonia anaxagoras thales empedocles heraclitus and zeno either zeno of elea or zeno of citium 
he sees the scientist dioscorides the mythical greek poets orpheus and linus and roman statesmen marcus tullius cicero and seneca 
dante sees the alexandrian geometer euclid and ptolemy the alexandrian astronomer and geographer as well as the physicians hippocrates and galen 
he also encounters avicenna persian polymath and averroes medieval andalusian polymath known for his commentaries on aristotle works 
dante and virgil depart from the four other poets and continue their journey 
although dante implies that all virtuous non christians find themselves here he later encounters two cato of utica and statius in purgatory and two trajan and ripheus in heaven 
xxii virgil names several additional inhabitants of limbo who were not mentioned in the inferno 
second circle lust canto dante and virgil leave limbo and enter the second circle the first of the circles of incontinence where the punishments of hell proper begin 
it is described as part where no thing gleams 
they find their way hindered by the serpentine minos who judges all of those condemned for active deliberately willed sin to one of the lower circles 
at this point in inferno every soul is required to confess all of their sins to minos after which minos sentences each soul to its torment by wrapping his tail around himself number of times corresponding to the circle of hell to which the soul must go 
the role of minos here is combination of his classical role as condemner and unjust judge of the underworld and the role of classical rhadamanthus interrogator and confessor of the underworld 
this mandatory confession makes it so every soul verbalizes and sanctions their own ranking amongst the condemned since these confessions are the sole grounds for their placement in hell 
dante is not forced to make this confession instead virgil rebukes minos and he and dante continue on 
in the second circle of hell are those overcome by lust 
these carnal malefactors are condemned for allowing their appetites to sway their reason 
these souls are buffeted back and forth by the terrible winds of violent storm without rest 
this symbolizes the power of lust to blow needlessly and aimlessly as the lovers drifted into self indulgence and were carried away by their passions so now they drift for ever 
the bright voluptuous sin is now seen as it is howling darkness of helpless discomfort 
since lust involves mutual indulgence and is not therefore completely self centered dante deems it the least heinous of the sins and its punishment is the most benign within hell proper 
the ruined slope in this circle is thought to be reference to the earthquake that occurred after the death of christ 
in this circle dante sees semiramis dido cleopatra helen of troy paris achilles tristan and many others who were overcome by sexual love during their life 
due to the presence of so many rulers among the lustful the fifth canto of inferno has been called the canto of the queens 
dante comes across francesca da rimini who married the deformed giovanni malatesta also known as gianciotto for political purposes but fell in love with his younger brother paolo malatesta the two began to carry on an adulterous affair 
sometime between and giovanni surprised them together in francesca bedroom and violently stabbed them both to death 
francesca explains francesca further reports that she and paolo yielded to their love when reading the story of the adultery between lancelot and guinevere in the old french romance lancelot du lac 
francesca says galeotto fu libro chi lo scrisse 
the word galeotto means pander but is also the italian term for gallehaut who acted as an intermediary between lancelot and guinevere encouraging them on to love 
john ciardi renders line as that book and he who wrote it was pander 
inspired by dante author giovanni boccaccio invoked the name prencipe galeotto in the alternative title to the decameron th century collection of novellas 
ultimately francesca never makes full confession to dante 
rather than admit to her and paolo sins the very reasons they reside in this circle of hell she consistently takes an erroneously passive role in the adulterous affair 
the english poet john keats in his sonnet on dream imagines what dante does not give us the point of view of paolo but to that second circle of sad hell where mid the gust the whirlwind and the flaw of rain and hail stones lovers need not tell their sorrows 
pale were the sweet lips saw pale were the lips kiss and fair the form floated with about that melancholy storm 
as he did at the end of canto iii dante overcome by pity and anguish describes his swoon fainted as if had met my death 
and then fell as dead body falls 
third circle gluttony canto vi in the third circle the gluttonous wallow in vile putrid slush produced by ceaseless foul icy rain great storm of putrefaction as punishment for subjecting their reason to voracious appetite 
cerberus described as il gran vermo literally the great worm line the monstrous three headed beast of hell ravenously guards the gluttons lying in the freezing mire mauling and flaying them with his claws as they howl like dogs 
virgil obtains safe passage past the monster by filling its three mouths with mud 
dorothy sayers writes that the surrender to sin which began with mutual indulgence leads by an imperceptible degradation to solitary self indulgence 
the gluttons grovel in the mud by themselves sightless and heedless of their neighbors symbolizing the cold selfish and empty sensuality of their lives 
just as lust has revealed its true nature in the winds of the previous circle here the slush reveals the true nature of sensuality which includes not only overindulgence in food and drink but also other kinds of addiction in this circle dante converses with florentine contemporary identified as ciacco which means hog 
character with the same nickname later appears in the decameron of giovanni boccaccio where his gluttonous behaviour is clearly portrayed 
ciacco speaks to dante regarding strife in florence between the white and black guelphs which developed after the guelph ghibelline strife ended with the complete defeat of the ghibellines 
in the first of several political prophecies in the inferno ciacco predicts the expulsion of the white guelphs dante party from florence by the black guelphs aided by pope boniface viii which marked the start of dante long exile from the city 
these events occurred in prior to when the poem was written but in the future at easter time of the time in which the poem is set 
fourth circle greed canto vii the fourth circle is guarded by figure dante names as pluto this is plutus the deity of wealth in classical mythology 
although the two are often conflated he is distinct figure from pluto dis the classical ruler of the underworld 
at the start of canto vii he menaces virgil and dante with the cryptic phrase pape sat pape sat aleppe but virgil protects dante from him 
those whose attitude toward material goods deviated from the appropriate mean are punished in the fourth circle 
they include the avaricious or miserly including many clergymen and popes and cardinals who hoarded possessions and the prodigal who squandered them 
the hoarders and spendthrifts joust using great weights as weapons that they push with their chests relating this sin of incontinence to the two that preceded it lust and gluttony dorothy sayers writes mutual indulgence has already declined into selfish appetite now that appetite becomes aware of the incompatible and equally selfish appetites of other people 
indifference becomes mutual antagonism imaged here by the antagonism between hoarding and squandering 
the contrast between these two groups leads virgil to discourse on the nature of fortune who raises nations to greatness and later plunges them into poverty as she shifts those empty goods from nation unto nation clan to clan 
this speech fills what would otherwise be gap in the poem since both groups are so absorbed in their activity that virgil tells dante that it would be pointless to try to speak to them indeed they have lost their individuality and been rendered unrecognizable 
fifth circle wrath in the swampy stinking waters of the river styx the fifth circle the actively wrathful fight each other viciously on the surface of the slime while the sullen the passively wrathful lie beneath the water withdrawn into black sulkiness which can find no joy in god or man or the universe 
at the surface of the foul stygian marsh dorothy sayers writes the active hatreds rend and snarl at one another at the bottom the sullen hatreds lie gurgling unable even to express themselves for the rage that chokes them 
as the last circle of incontinence the savage self frustration of the fifth circle marks the end of that which had its tender and romantic beginnings in the dalliance of indulged passion canto viii phlegyas reluctantly transports dante and virgil across the styx in his skiff 
on the way they are accosted by filippo argenti black guelph from the prominent adimari family 
little is known about argenti although giovanni boccaccio describes an incident in which he lost his temper early commentators state that argenti brother seized some of dante property after his exile from florence 
just as argenti enabled the seizing of dante property he himself is seized by all the other wrathful souls 
when dante responds in weeping and in grieving accursed spirit may you long remain virgil blesses him with words used to describe christ himself luke 
literally this reflects the fact that souls in hell are eternally fixed in the state they have chosen but allegorically it reflects dante beginning awareness of his own sin 
entrance to dis in the distance dante perceives high towers that resemble fiery red mosques 
virgil informs him that they are approaching the city of dis 
dis itself surrounded by the stygian marsh contains lower hell within its walls 
dis is one of the names of pluto the classical king of the underworld in addition to being the name of the realm 
the walls of dis are guarded by fallen angels 
virgil is unable to convince them to let dante and him enter 
canto ix dante is threatened by the furies consisting of alecto megaera and tisiphone and medusa 
an angel sent from heaven secures entry for the poets opening the gate by touching it with wand and rebukes those who opposed dante 
allegorically this reveals the fact that the poem is beginning to deal with sins that philosophy and humanism cannot fully understand 
virgil also mentions to dante how erichtho sent him down to the lowest circle of hell to bring back spirit from there 
sixth circle heresy canto in the sixth circle heretics such as epicurus and his followers who say the soul dies with the body are trapped in flaming tombs 
dante holds discourse with pair of epicurian florentines in one of the tombs farinata degli uberti famous ghibelline leader following the battle of montaperti in september farinata strongly protested the proposed destruction of florence at the meeting of the victorious ghibellines he died in and was posthumously condemned for heresy in and cavalcante de cavalcanti guelph who was the father of dante friend and fellow poet guido cavalcanti 
the political affiliation of these two men allows for further discussion of florentine politics 
in response to question from dante about the prophecy he has received farinata explains that what the souls in hell know of life on earth comes from seeing the future not from any observation of the present 
consequently when the portal of the future has been shut it will no longer be possible for them to know anything 
farinata explains that also crammed within the tomb are emperor frederick ii commonly reputed to be an epicurean and ottaviano degli ubaldini whom dante refers to as il cardinale 
canto xi dante reads an inscription on one of the tombs indicating it belongs to pope anastasius ii although some modern scholars hold that dante erred in the verse mentioning anastasius anastasio papa guardo lo qual trasse fotin de la via dritta lines confusing the pope with the byzantine emperor of the time anastasius pausing for moment before the steep descent to the foul smelling seventh circle virgil explains the geography and rationale of lower hell in which the sins of violence or bestiality and fraud or malice are punished 
in his explanation virgil refers to the nicomachean ethics and the physics of aristotle with medieval interpretations 
virgil asserts that there are only two legitimate sources of wealth natural resources nature and human labor and activity art 
usury to be punished in the next circle is therefore an offence against both it is kind of blasphemy since it is an act of violence against art which is the child of nature and nature derives from god virgil then indicates the time through his unexplained awareness of the stars positions 
the wain the great bear now lies in the northwest over caurus the northwest wind 
the constellation pisces the fish is just appearing over the horizon it is the zodiacal sign preceding aries the ram 
canto notes that the sun is in aries and since the twelve zodiac signs rise at two hour intervals it must now be about two hours prior to sunrise am on holy saturday april 
seventh circle violence canto xii the seventh circle divided into three rings houses the violent 
dante and virgil descend jumble of rocks that had once formed cliff to reach the seventh circle from the sixth circle having first to evade the minotaur infamia di creti the infamy of crete line at the sight of them the minotaur gnaws his flesh 
virgil assures the monster that dante is not its hated enemy theseus 
this causes the minotaur to charge them as dante and virgil swiftly enter the seventh circle 
virgil explains the presence of shattered stones around them they resulted from the great earthquake that shook the earth at the moment of christ death matt 
at the time of the harrowing of hell 
ruins resulting from the same shock were previously seen at the beginning of upper hell the entrance of the second circle canto 
ring against neighbors in the first round of the seventh circle the murderers war makers plunderers and tyrants are immersed in phlegethon river of boiling blood and fire 
ciardi writes as they wallowed in blood during their lives so they are immersed in the boiling blood forever each according to the degree of his guilt 
the centaurs commanded by chiron and pholus patrol the ring shooting arrows into any sinners who emerge higher out of the boiling blood than each is allowed 
the centaur nessus guides the poets along phlegethon and points out alexander the great disputed dionysius either dionysius or dionysius ii or both they were bloodthirsty unpopular tyrants of sicily ezzelino iii da romano the cruelest of the ghibelline tyrants obizzo este and guy de montfort 
the river grows shallower until it reaches ford after which it comes full circle back to the deeper part where dante and virgil first approached it immersed here are tyrants including attila king of the huns flagello in terra scourge on earth line pyrrhus either the bloodthirsty son of achilles or king pyrrhus of epirus sextus rinier da corneto and rinier pazzo 
after bringing dante and virgil to the shallow ford nessus leaves them to return to his post 
this passage may have been influenced by the early medieval visio karoli grossi 
canto xiii ring against self the second round of the seventh circle is the wood of the suicides in which the souls of the people who attempted or died by suicide are transformed into gnarled thorny trees and then fed upon by harpies hideous clawed birds with the faces of women the trees are only permitted to speak when broken and bleeding 
dante breaks twig off one of the trees and from the bleeding trunk hears the tale of pietro della vigna powerful minister of emperor frederick ii until he fell out of favor and was imprisoned and blinded 
he subsequently committed suicide his presence here rather than in the ninth circle indicates that dante believes that the accusations made against him were false 
the harpies and the characteristics of the bleeding bushes are based on book of the aeneid 
according to dorothy sayers dante presents the sin of suicide as an insult to the body so here the shades are deprived of even the semblance of the human form 
as they refused life they remain fixed in dead and withered sterility 
they are the image of the self hatred which dries up the very sap of energy and makes all life infertile 
the trees can also be interpreted as metaphor for the state of mind in which suicide is committed 
dante learns that these suicides unique among the dead will not be corporally resurrected after the final judgement since they threw their bodies away instead they will maintain their bushy form with their own corpses hanging from the thorny limbs 
after pietro della vigna finishes his story dante notices two shades lano da siena and jacopo sant andrea race through the wood chased and savagely mauled by ferocious bitches this is the punishment of the violently profligate who possessed by depraved passion dissipated their goods for the sheer wanton lust of wreckage and disorder 
the destruction wrought upon the wood by the profligates flight and punishment as they crash through the undergrowth causes further suffering to the suicides who cannot move out of the way 
canto xiv ring against god art and nature the third round of the seventh circle is great plain of burning sand scorched by great flakes of flame falling slowly down from the sky an image derived from the fate of sodom and gomorrah gen 
the blasphemers the violent against god are stretched supine upon the burning sand the sodomites the violent against nature run in circles while the usurers the violent against art which is the grandchild of god as explained in canto xi crouch huddled and weeping 
ciardi writes blasphemy sodomy and usury are all unnatural and sterile actions thus the unbearing desert is the eternity of these sinners and thus the rain which in nature should be fertile and cool descends as fire 
dante finds capaneus stretched out on the sands for blasphemy against jove he was struck down with thunderbolt during the war of the seven against thebes he is still scorning jove in the afterlife 
the overflow of phlegethon the river of blood from the first ring flows boiling through the wood of the suicides the second ring and crosses the burning plain 
virgil explains the origin of the rivers of hell which includes references to the old man of crete 
canto xv protected by the powers of the boiling rivulet dante and virgil progress across the burning plain 
they pass roving group of sodomites and dante to his surprise recognizes brunetto latini 
dante addresses brunetto with deep and sorrowful affection paying him the highest tribute offered to any sinner in the inferno thus refuting suggestions that dante only placed his enemies in hell 
dante has great respect for brunetto and feels spiritual indebtedness to him and his works you taught me how man makes himself eternal and while live my gratitude for that must always be apparent in my words brunetto prophesies dante bad treatment by the florentines 
he also identifies other sodomites including priscian francesco accorso and bishop andrea de mozzi 
canto xvi the poets begin to hear the waterfall that plunges over the great cliff into the eighth circle when three shades break from their company and greet them 
they are iacopo rusticucci guido guerra and tegghiaio aldobrandi all florentines much admired by dante 
rusticucci blames his savage wife for his torments 
the sinners ask for news of florence and dante laments the current state of the city 
at the top of the falls at virgil order dante removes cord from about his waist and virgil drops it over the edge as if in answer large distorted shape swims up through the filthy air of the abyss 
canto xvii the creature is geryon the monster of fraud virgil announces that they must fly down from the cliff on the monster back 
dante goes alone to examine the usurers he does not recognize them but each has heraldic device emblazoned on leather purse around his neck on these their streaming eyes appeared to feast 
the coats of arms indicate that they came from prominent florentine families they indicate the presence of catello di rosso gianfigliazzi ciappo ubriachi the paduan reginaldo degli scrovegni who predicts that his fellow paduan vitaliano di iacopo vitaliani will join him here and giovanni di buiamonte 
dante then rejoins virgil and both mounted atop geryon back the two begin their descent from the great cliff in the eighth circle the hell of the fraudulent and malicious 
geryon the winged monster who allows dante and virgil to descend vast cliff to reach the eighth circle was traditionally represented as giant with three heads and three conjoined bodies 
dante geryon meanwhile is an image of fraud combining human bestial and reptilian elements geryon is monster with the general shape of wyvern but with the tail of scorpion hairy arms gaudily marked reptilian body and the face of just and honest man 
the pleasant human face on this grotesque body evokes the insincere fraudster whose intentions behind the face are all monstrous cold blooded and stinging with poison 
eighth circle fraud canto xviii dante now finds himself in the eighth circle called malebolge evil ditches the upper half of the hell of the fraudulent and malicious 
the eighth circle is large funnel of stone shaped like an amphitheatre around which run series of ten deep narrow concentric ditches or trenches called bolge singular bolgia 
within these ditches are punished those guilty of simple fraud 
from the foot of the great cliff to the well which forms the neck of the funnel are large spurs of rock like umbrella ribs or spokes which serve as bridges over the ten ditches 
dorothy sayers writes that the malebolge is the image of the city in corruption the progressive disintegration of every social relationship personal and public 
sexuality ecclesiastical and civil office language ownership counsel authority psychic influence and material interdependence all the media of the community interchange are perverted and falsified 
bolgia panderers and seducers these sinners make two files one along either bank of the ditch and march quickly in opposite directions while being whipped by horned demons for eternity 
they deliberately exploited the passions of others and so drove them to serve their own interests are themselves driven and scourged 
dante makes reference to recent traffic rule developed for the jubilee year of in rome 
in the group of panderers the poets notice venedico caccianemico bolognese guelph who sold his own sister ghisola to the marchese este 
in the group of seducers virgil points out jason the greek hero who led the argonauts to fetch the golden fleece from ae tes king of colchis 
he gained the help of the king daughter medea by seducing and marrying her only to later desert her for creusa 
jason had previously seduced hypsipyle when the argonauts landed at lemnos on their way to colchis but abandoned her alone and pregnant 
bolgia flatterers these also exploited other people this time abusing and corrupting language to play upon others desires and fears 
they are steeped in excrement representative of the false flatteries they told on earth as they howl and fight amongst themselves 
alessio interminei of lucca and tha are seen here canto xix bolgia simoniacs dante now forcefully expresses his condemnation of those who committed simony or the sale of ecclesiastic favors and offices and therefore made money for themselves out of what belongs to god rapacious ones who take the things of god that ought to be the brides of righteousness and make them fornicate for gold and silver 
the time has come to let the trumpet sound for you 
the sinners are placed head downwards in round tube like holes within the rock debased mockeries of baptismal fonts with flames burning the soles of their feet 
the heat of the fire is proportioned to their guilt 
the simile of baptismal fonts gives dante an incidental opportunity to clear his name of an accusation of malicious damage to the font at the baptistery of san giovanni 
simon magus who offered gold in exchange for holy power to saint peter and after whom the sin is named is mentioned here although dante does not encounter him 
one of the sinners pope nicholas iii must serve in the hellish baptism by fire from his death in until the arrival in hell of pope boniface viii who will take his predecessor place in the stone tube until when he will in turn be replaced by pope clement puppet of king philip iv of france who moved the papal see to avignon ushering in the avignon papacy 
dante delivers denunciation of simoniacal corruption of the church 
canto xx bolgia sorcerers in the middle of the bridge of the fourth bolgia dante looks down at the souls of fortune tellers diviners astrologers and other false prophets 
the punishment of those who attempted to usurp god prerogative by prying into the future is to have their heads twisted around on their bodies in this horrible contortion of the human form these sinners are compelled to walk backwards for eternity blinded by their own tears 
john ciardi writes thus those who sought to penetrate the future cannot even see in front of themselves they attempted to move themselves forward in time so must they go backwards through all eternity and as the arts of sorcery are distortion of god law so are their bodies distorted in hell 
while referring primarily to attempts to see into the future by forbidden means this also symbolises the twisted nature of magic in general 
dante weeps in pity and virgil rebukes him saying here pity only lives when it is dead for who can be more impious than he who links god judgment to passivity 
virgil gives lengthy explanation of the founding of his native city of mantua 
among the sinners in this circle are king amphiaraus one of the seven against thebes foreseeing his death in the war he sought to avert it by hiding from battle but died in an earthquake trying to flee and two theban soothsayers tiresias in ovid metamorphoses iii tiresias was transformed into woman upon striking two coupling serpents with his rod seven years later he was changed back to man in an identical encounter and his daughter manto 
also in this bolgia are aruns an etruscan soothsayer who predicted the caesar victory in the roman civil war in lucan pharsalia the greek augur eurypylus astrologers michael scot served at frederick ii court at palermo and guido bonatti served the court of guido da montefeltro and asdente shoemaker and soothsayer from parma 
virgil implies that the moon is now setting over the pillars of hercules in the west the time is just after am the dawn of holy saturday 
canto xxi bolgia barrators corrupt politicians who made money by trafficking in public offices the political analogue of the simoniacs are immersed in lake of boiling pitch which represents the sticky fingers and dark secrets of their corrupt deals 
they are guarded by demons called the malebranche evil claws who tear them to pieces with claws and grappling hooks if they catch them above the surface of the pitch 
the poets observe demon arrive with grafting senator of lucca and throw him into the pitch where the demons set upon him 
virgil secures safe conduct from the leader of the malebranche named malacoda evil tail 
he informs them that the bridge across the sixth bolgia has collapsed as result of the earthquake that shook hell at the death of christ in ad but that there is another path further on 
he sends squad of demons led by barbariccia to escort them safely 
based on details in this canto and if christ death is taken to have occurred at exactly noon the time is now am of holy saturday 
the demons provide some satirical black comedy in the last line of canto xxi the sign for their march is provided by fart and he had made trumpet of his ass canto xxii one of the grafters an unidentified navarrese identified by early commentators as ciampolo is seized by the demons and virgil questions him 
the sinner speaks of his fellow grafters friar gomita corrupt friar in gallura eventually hanged by nino visconti see purg 
viii for accepting bribes to let prisoners escape and michel zanche corrupt vicar of logodoro under king enzo of sardinia 
he offers to lure some of his fellow sufferers into the hands of the demons and when his plan is accepted he escapes back into the pitch 
alichino and calcabrina start brawl in mid air and fall into the pitch themselves and barbariccia organizes rescue party 
dante and virgil take advantage of the confusion to slip away 
canto xxiii bolgia hypocrites the poets escape the pursuing malebranche by sliding down the sloping bank of the next pit 
here they find the hypocrites listlessly walking around narrow track for eternity weighted down by leaden robes 
the robes are brilliantly gilded on the outside and are shaped like monk habit the hypocrite outward appearance shines brightly and passes for holiness but under that show lies the terrible weight of his deceit falsity that weighs them down and makes spiritual progress impossible for them 
dante speaks with catalano dei malavolti and loderingo degli andal two bolognese brothers of the jovial friars an order that had acquired reputation for not living up to its vows and was eventually disbanded by papal decree 
friar catalano points out caiaphas the high priest of israel under pontius pilate who counseled the pharisees to crucify jesus for the public good john 
he himself is crucified to the floor of hell by three large stakes and in such position that every passing sinner must walk upon him he must suffer upon his body the weight of all the world hypocrisy 
the jovial friars explain to virgil how he may climb from the pit virgil discovers that malacoda lied to him about the bridges over the sixth bolgia 
canto xxiv bolgia thieves dante and virgil leave the bolgia of the hypocrites by climbing the ruined rocks of bridge destroyed by the great earthquake after which they cross the bridge of the seventh bolgia to the far side to observe the next chasm 
the pit is filled with monstrous reptiles the shades of thieves are pursued and bitten by snakes and lizards who curl themselves about the sinners and bind their hands behind their backs 
the full horror of the thieves punishment is revealed gradually just as they stole other people substance in life their very identity becomes subject to theft here 
one sinner who reluctantly identifies himself as vanni fucci is bitten by serpent at the jugular vein bursts into flames and is re formed from the ashes like phoenix 
vanni tells dark prophecy against dante canto xxv vanni hurls an obscenity at god and the serpents swarm over him 
the centaur cacus arrives to punish him he has fire breathing dragon on his shoulders and snakes covering his equine back 
in roman mythology cacus the monstrous fire breathing son of vulcan was killed by hercules for raiding the hero cattle in aeneid viii virgil did not describe him as centaur 
dante then meets five noble thieves of florence and observes their various transformations 
agnello brunelleschi in human form is merged with the six legged serpent that is cianfa donati 
figure named buoso perhaps either buoso degli abati or buoso donati the latter of whom is mentioned in inf 
xxx first appears as man but exchanges forms with francesco de cavalcanti who bites buoso in the form of four footed serpent 
puccio sciancato remains unchanged for the time being 
canto xxvi bolgia counsellors of fraud dante addresses passionate lament to florence before turning to the next bolgia 
here fraudulent advisers or evil counsellors move about hidden from view inside individual flames 
these are not people who gave false advice but people who used their position to advise others to engage in fraud 
ulysses and diomedes are punished together within great double headed flame they are condemned for the stratagem of the trojan horse resulting in the fall of troy for persuading achilles to sail for troy causing deidamia to die of grief and for the theft of the sacred statue of pallas the palladium upon which it was believed the fate of troy depended 
ulysses the figure in the larger horn of the flame narrates the tale of his last voyage and death creation of dante that illustrates the extent of his own pride despite his condemnation of this principal vice throughout the divine comedy 
ulysses tells how after his detainment by circe his love for neither his son his father nor his wife could overpower his desire to set out on the open sea to gain experience of the world and of the vices and the worth of men 
as they approach the pillars of hercules ulysses urges his crew consider well the seed that gave you birth you were not made to live your lives as brutes but to be followers of worth and knowledge 
this passage exemplifies the danger of utilizing rhetoric without proper wisdom failing condemned by several of dante most prominent philosophical influences 
although ulysses successfully convinces his crew to venture into the unknown he lacks the wisdom to understand the danger this entails leading to their death in shipwreck after sighting mount purgatory in the southern hemisphere canto xxvii dante is approached by guido da montefeltro head of the ghibellines of romagna asking for news of his country 
dante replies with tragic summary of the current state of the cities of romagna 
guido then recounts his life he advised pope boniface viii to offer false amnesty to the colonna family who in had walled themselves inside the castle of palestrina in the lateran 
when the colonna accepted the terms and left the castle the pope razed it to the ground and left them without refuge 
guido describes how st francis founder of the franciscan order came to take his soul to heaven only to have demon assert prior claim 
although boniface had absolved guido in advance for his evil advice the demon points out the invalidity absolution requires contrition and man cannot be contrite for sin at the same time that he is intending to commit it canto xxviii bolgia sowers of discord in the ninth bolgia the sowers of discord are hacked and mutilated for all eternity by large demon wielding bloody sword their bodies are divided as in life their sin was to tear apart what god had intended to be united these are the sinners who are ready to rip up the whole fabric of society to gratify sectional egotism 
the souls must drag their ruined bodies around the ditch their wounds healing in the course of the circuit only to have the demon tear them apart anew 
these are divided into three categories religious schism and discord ii civil strife and political discord and iii family disunion or discord between kinsmen 
chief among the first category is muhammad the founder of islam his body is ripped from groin to chin with his entrails hanging out 
dante apparently saw muhammad as causing schism within christianity when he and his followers splintered off 
dante also condemns muhammad son in law ali for schism between sunni and shiite his face is cleft from top to bottom 
muhammad tells dante to warn the schismatic and heretic fra dolcino 
in the second category are pier da medicina his throat slit nose slashed off as far as the eyebrows wound where one of his ears had been the roman tribune gaius scribonius curio who advised caesar to cross the rubicon and thus begin the civil war his tongue is cut off and mosca dei lamberti who incited the amidei family to kill buondelmonte dei buondelmonti resulting in conflict between guelphs and ghibellines his arms are hacked off 
finally in the third category of sinner dante sees bertran de born 
the knight carries his severed head by its own hair swinging it like lantern 
bertran is said to have caused quarrel between henry ii of england and his son prince henry the young king his punishment in hell is decapitation since dividing father and son is like severing the head from the body canto xxix bolgia falsifiers the final bolgia of the eighth circle is home to various sorts of falsifiers 
disease on society they are themselves afflicted with different types of afflictions horrible diseases stench thirst filth darkness and screaming 
some lie prostrate while others run hungering through the pit tearing others to pieces 
shortly before their arrival in this pit virgil indicates that it is approximately noon of holy saturday and he and dante discuss one of dante kinsmen geri de bello among the sowers of discord in the previous ditch 
the first category of falsifiers dante encounters are the alchemists falsifiers of things 
he speaks with two spirits viciously scrubbing and clawing at their leprous scabs griffolino arezzo an alchemist who extracted money from the foolish alberto da siena on the promise of teaching him to fly alberto reputed father the bishop of siena had griffolino burned at the stake and capocchio burned at the stake at siena in for practicing alchemy 
canto xxx suddenly two spirits gianni schicchi de cavalcanti and myrrha both punished as imposters falsifiers of persons run rabid through the pit 
schicchi sinks his teeth into the neck of an alchemist capocchio and drags him away like prey 
griffolino explains how myrrha disguised herself to commit incest with her father king cinyras while schicchi impersonated the dead buoso donati to dictate will giving himself several profitable bequests 
dante then encounters master adam of brescia one of the counterfeiters falsifiers of money for manufacturing florentine florins of twenty one rather than twenty four carat gold he was burned at the stake in he is punished by loathsome dropsy like disease which gives him bloated stomach prevents him from moving and an eternal unbearable thirst 
master adam points out two sinners of the fourth class the perjurers falsifiers of words 
these are potiphar wife punished for her false accusation of joseph gen and sinon the achaean spy who lied to the trojans to convince them to take the trojan horse into their city aeneid ii sinon is here rather than in bolgia because his advice was false as well as evil 
both suffer from burning fever 
master adam and sinon exchange abuse which dante watches until he is rebuked by virgil 
as result of his shame and repentance dante is forgiven by his guide 
sayers remarks that the descent through malebolge began with the sale of the sexual relationship and went on to the sale of church and state now the very money is itself corrupted every affirmation has become perjury and every identity lie so that every aspect of social interaction has been progressively destroyed 
central well of malebolge canto xxxi dante and virgil approach the central well at the bottom of which lies the ninth and final circle of hell 
the classical and biblical giants who perhaps symbolize pride and other spiritual flaws lying behind acts of treachery stand perpetual guard inside the well pit their legs embedded in the banks of the ninth circle while their upper halves rise above the rim and can be visible from the malebolge 
dante initially mistakes them for great towers of city 
among the giants virgil identifies nimrod who tried to build the tower of babel he shouts out the unintelligible raph mai am cche zab almi ephialtes who with his brother otus tried to storm olympus during the gigantomachy he has his arms chained up and briareus who dante claimed had challenged the gods and tityos and typhon who insulted jupiter 
also here is antaeus who did not join in the rebellion against the olympian gods and therefore is not chained 
at virgil persuasion antaeus takes the poets in his large palm and lowers them gently to the final level of hell 
ninth circle treachery canto xxxii at the base of the well dante finds himself within large frozen lake cocytus the ninth circle of hell 
trapped in the ice each according to his guilt are punished sinners guilty of treachery against those with whom they had special relationships 
the lake of ice is divided into four concentric rings or rounds of traitors corresponding in order of seriousness to betrayal of family ties betrayal of community ties betrayal of guests and betrayal of lords 
this is in contrast to the popular image of hell as fiery as ciardi writes the treacheries of these souls were denials of love which is god and of all human warmth 
only the remorseless dead center of the ice will serve to express their natures 
as they denied god love so are they furthest removed from the light and warmth of his sun 
as they denied all human ties so are they bound only by the unyielding ice 
this final deepest level of hell is reserved for traitors betrayers and oathbreakers its most famous inmate is judas iscariot 
round caina this round is named after cain who killed his own brother in the first act of murder gen 
this round houses the traitors to their kindred they have their necks and heads out of the ice and are allowed to bow their heads allowing some protection from the freezing wind 
here dante sees the brothers alessandro and napoleone degli alberti who killed each other over their inheritance and their politics some time between and camiscion de pazzi ghibelline who murdered his kinsman ubertino identifies several other sinners mordred traitorous son of king arthur vanni de cancellieri nicknamed focaccia white guelph of pistoia who killed his cousin detto de cancellieri and sassol mascheroni of the noble toschi family of florence murdered relative 
camiscion is aware that in july his relative carlino de pazzi would accept bribe to surrender the castle of piantravigne to the blacks betraying the whites 
as traitor to his party carlino belongs in antenora the next circle down his greater sin will make camiscion look virtuous by comparison 
round antenora the second round is named after antenor trojan soldier who betrayed his city to the greeks 
here lie the traitors to their country those who committed treason against political entities parties cities or countries have their heads above the ice but they cannot bend their necks 
dante accidentally kicks the head of bocca degli abati traitorous guelph of florence and then proceeds to treat him more savagely than any other soul he has thus far met 
also punished in this level are buoso da duera ghibelline leader bribed by the french to betray manfred king of naples tesauro dei beccheria ghibelline of pavia beheaded by the florentine guelphs for treason in gianni de soldanieri noble florentine ghibelline who joined with the guelphs after manfred death in ganelon betrayed the rear guard of charlemagne to the muslims at roncesvalles according to the french epic poem the song of roland and tebaldello de zambrasi of faenza ghibelline who turned his city over to the bolognese guelphs on nov 
the poets then see two heads frozen in one hole one gnawing the nape of the other neck canto xxxiii the gnawing sinner tells his story he is count ugolino and the head he gnaws belongs to archbishop ruggieri 
in the most pathetic and dramatic passage of the inferno ugolino describes how he conspired with ruggieri in to oust his nephew nino visconti and take control over the guelphs of pisa 
however as soon as nino was gone the archbishop sensing the guelphs weakened position turned on ugolino and imprisoned him with his sons and grandsons in the torre dei gualandi 
in march the archbishop condemned the prisoners to death by starvation in the tower 
round ptolomaea the third region of cocytus is named after ptolemy who invited his father in law simon maccabaeus and his sons to banquet and then killed them maccabees 
traitors to their guests lie supine in the ice while their tears freeze in their eye sockets sealing them with small visors of crystal even the comfort of weeping is denied to them 
dante encounters fra alberigo one of the jovial friars and native of faenza who asks dante to remove the visor of ice from his eyes 
in alberigo invited his opponents manfred his brother and alberghetto manfred son to banquet at which his men murdered the dinner guests 
he explains that often living person soul falls to ptolomea before he dies before dark atropos has cut their thread 
then on earth demon inhabits the body until the body natural death 
fra alberigo sin is identical in kind to that of branca oria genoese ghibelline who in invited his father in law michel zanche seen in the eighth circle bolgia and had him cut to pieces 
branca that is his earthly body did not die until but his soul together with that of his nephew who assisted in his treachery fell to ptolomaea before michel zanche soul arrived at the bolgia of the barrators 
dante leaves without keeping his promise to clear fra alberigo eyes of ice and yet did not open them for him and it was courtesy to show him rudeness canto xxxiv round judecca the fourth division of cocytus named for judas iscariot contains the traitors to their lords and benefactors 
upon entry into this round virgil says vexilla regis prodeunt inferni the banners of the king of hell draw closer 
judecca is completely silent all of the sinners are fully encapsulated in ice distorted and twisted in every conceivable position 
the sinners present an image of utter immobility it is impossible to talk with any of them so dante and virgil quickly move on to the centre of hell 
centre of hell in the very centre of hell condemned for committing the ultimate sin personal treachery against god is the devil referred to by virgil as dis the roman god of the underworld the name dis was often used for pluto in antiquity such as in virgil aeneid 
the arch traitor lucifer was once held by god to be fairest of the angels before his pride led him to rebel against god resulting in his expulsion from heaven 
lucifer is giant terrifying beast trapped waist deep in the ice fixed and suffering 
he has three faces each different color one red the middle one pale yellow the right and one black the left dorothy sayers notes that satan three faces are thought by some to suggest his control over the three human races red for the europeans from japheth yellow for the asiatic from shem and black for the african the race of ham 
all interpretations recognize that the three faces represent fundamental perversion of the trinity satan is impotent ignorant and full of hate in contrast to the all powerful all knowing and all loving nature of god 
lucifer retains his six wings he originally belonged to the angelic order of seraphim described in isaiah but these are now dark bat like and futile the icy wind that emanates from the beating of lucifer wings only further ensures his own imprisonment in the frozen lake 
he weeps from his six eyes and his tears mix with bloody froth and pus as they pour down his three chins 
each face has mouth that chews eternally on prominent traitor 
marcus junius brutus and gaius cassius longinus dangle with their feet in the left and right mouths respectively for their involvement in the assassination of julius caesar march bc an act which to dante represented the destruction of unified italy and the killing of the man who was divinely appointed to govern the world 
in the central most vicious mouth is judas iscariot the apostle who betrayed christ 
judas is receiving the most horrifying torture of the three traitors his head is gnawed inside lucifer mouth while his back is forever flayed and shredded by lucifer claws 
according to dorothy sayers just as judas figures treason against god so brutus and cassius figure treason against man in society or we may say that we have here the images of treason against the divine and the secular government of the world at about on saturday evening virgil and dante begin their escape from hell by clambering down satan ragged fur feet first 
when they reach satan genitalia the poets pass through the center of the universe and of gravity from the northern hemisphere of land to the southern hemisphere of water 
when virgil changes direction and begins to climb upward towards the surface of the earth at the antipodes dante in his confusion initially believes they are returning to hell 
virgil indicates that the time is halfway between the canonical hours of prime and terce that is of the same holy saturday which was just about to end 
dante is confused as to how after about an hour and half of climbing it is now apparently morning 
virgil explains that it is as result of passing through the earth center into the southern hemisphere which is twelve hours ahead of jerusalem the central city of the northern hemisphere where therefore it is currently 
virgil goes on to explain how the southern hemisphere was once covered with dry land but the land recoiled in horror to the north when lucifer fell from heaven and was replaced by the ocean 
meanwhile the inner rock lucifer displaced as he plunged into the center of the earth rushed upwards to the surface of the southern hemisphere to avoid contact with him forming the mountain of purgatory 
this mountain the only land mass in the waters of the southern hemisphere rises above the surface at point directly opposite jerusalem 
the poets then ascend narrow chasm of rock through the space contained between the floor formed by the convex side of cocytus and the underside of the earth above moving in opposition to lethe the river of oblivion which flows down from the summit of mount purgatory 
the poets finally emerge just before dawn on the morning of easter sunday april beneath sky studded with stars 
illustrations see also notes references external links texts dante dartmouth project full text of more than italian latin and english commentaries on the commedia ranging in date from iacopo alighieri to the robert hollander world of dante multimedia website that offers italian text of divine comedy allen mandelbaum translation gallery interactive maps timeline musical recordings and searchable database for students and teachers by deborah parker and iath institute for advanced technologies in the humanities of the university of virginia dante divine comedy full text paraphrased in modern english verse by scottish author and artist alasdair gray audiobooks public domain recordings from librivox in italian longfellow translation some additional recordings secondary materials piece art collection featured in dante hell animated and inferno by dante films 
on line concordance to the divine comedy wikisummaries summary and analysis of inferno danteworlds multimedia presentation of the divine comedy for students by guy raffa of the university of texas dante places map still prototype of the places named by dante in the commedia created with googlemaps 
explanatory pdf is available for download see more dante inferno images by selecting the heaven hell subject at the persuasive cartography the pj mode collection cornell university library mapping dante inferno one circle of hell at time article by anika burgess atlas obscura july dante inferno on in our time at the bbc
robust principal component analysis rpca is modification of the widely used statistical procedure of principal component analysis pca which works well with respect to grossly corrupted observations 
number of different approaches exist for robust pca including an idealized version of robust pca which aims to recover low rank matrix from highly corrupted measurements 
this decomposition in low rank and sparse matrices can be achieved by techniques such as principal component pursuit method pcp stable pcp quantized pcp block based pcp and local pcp 
then optimization methods are used such as the augmented lagrange multiplier method alm alternating direction method adm fast alternating minimization fam iteratively reweighted least squares irls or alternating projections ap 
algorithms non convex method the guaranteed algorithm for the robust pca problem with the input matrix being is an alternating minimization type algorithm 
the computational complexity is log where the input is the superposition of low rank of rank and sparse matrix of dimension and is the desired accuracy of the recovered solution where is the true low rank component and is the estimated or recovered low rank component 
intuitively this algorithm performs projections of the residual on to the set of low rank matrices via the svd operation and sparse matrices via entry wise hard thresholding in an alternating manner that is low rank projection of the difference the input matrix and the sparse matrix obtained at given iteration followed by sparse projection of the difference of the input matrix and the low rank matrix obtained in the previous step and iterating the two steps until convergence 
this alternating projections algorithm is later improved by an accelerated version coined accaltproj 
the acceleration is achieved by applying tangent space projection before project the residue onto the set of low rank matrices 
this trick improves the computational complexity to log with much smaller constant in front while it maintains the theoretically guaranteed linear convergence 
another fast version of accelerated alternating projections algorithm is ircur 
it uses the structure of cur decomposition in alternating projections framework to dramatically reduces the compuational complexity of rpca to max log log log convex relaxation this method consists of relaxing the rank constraint in the optimization problem to the nuclear norm and the sparsity constraint to norm the resulting program can be solved using methods such as the method of augmented lagrange multipliers 
deep learning augmented method some recent works propose rpca algorithms with learnable training parameters 
such learnable trainable algorithm can be unfolded as deep neural network whose parameters can be learned via machine learning techniques from given dataset or problem distribution 
the learned algorithm will have superior performance on the corresponding problem distribution 
applications rpca has many real life important applications particularly when the data under study can naturally be modeled as low rank plus sparse contribution 
following examples are inspired by contemporary challenges in computer science and depending on the applications either the low rank component or the sparse component could be the object of interest video surveillance given sequence of surveillance video frames it is often required to identify the activities that stand out from the background 
if we stack the video frames as columns of matrix then the low rank component naturally corresponds to the stationary background and the sparse component captures the moving objects in the foreground 
face recognition images of convex lambertian surface under varying illuminations span low dimensional subspace 
this is one of the reasons for effectiveness of low dimensional models for imagery data 
in particular it is easy to approximate images of human face by low dimensional subspace 
to be able to correctly retrieve this subspace is crucial in many applications such as face recognition and alignment 
it turns out that rpca can be applied successfully to this problem to exactly recover the face 
surveys robust pca dynamic rpca decomposition into low rank plus additive matrices low rank models books journals and workshops books bouwmans aybat and zahzah 
handbook on robust low rank and sparse matrix decomposition applications in image and video processing crc press taylor and francis group may 
more information http www crcpress com product isbn lin zhang low rank models in visual analysis theories algorithms and applications academic press elsevier june 
more information https www elsevier com books low rank models in visual analysis lin journals vaswani chi bouwmans special issue on rethinking pca for modern datasets theory algorithms and applications proceedings of the ieee bouwmans vaswani rodriguez vidal lin special issue on robust subspace learning and tracking theory algorithms and applications ieee journal of selected topics in signal processing december 
workshops rsl cv workshop on robust subspace learning and computer vision in conjunction with iccv for more information http rsl cv univ lr fr workshop rsl cv workshop on robust subspace learning and computer vision in conjunction with iccv for more information http rsl cv univ lr fr rsl cv workshop on robust subspace learning and computer vision in conjunction with iccv for more information https rsl cv univ lr fr sessions special session on online algorithms for static and dynamic robust pca and compressive sensing in conjunction with ssp 
more information https ssp org resources and libraries websites background subtraction website dlam website documentation from the university of illinois archive link libraries the lrs library developed by andrews sobral provides collection of low rank and sparse decomposition algorithms in matlab 
the library was designed for moving object detection in videos but it can be also used for other computer vision machine learning tasks 
currently the lrslibrary offers more than algorithms based on matrix and tensor methods 
references external links lrslibrary
in probability theory and statistics variance is the expectation of the squared deviation of random variable from its population mean or sample mean 
variance is measure of dispersion meaning it is measure of how far set of numbers is spread out from their average value 
variance has central role in statistics where some ideas that use it include descriptive statistics statistical inference hypothesis testing goodness of fit and monte carlo sampling 
variance is an important tool in the sciences where statistical analysis of data is common 
the variance is the square of the standard deviation the second central moment of distribution and the covariance of the random variable with itself and it is often represented by var or an advantage of variance as measure of dispersion is that it is more amenable to algebraic manipulation than other measures of dispersion such as the expected absolute deviation for example the variance of sum of uncorrelated random variables is equal to the sum of their variances 
disadvantage of the variance for practical applications is that unlike the standard deviation its units differ from the random variable which is why the standard deviation is more commonly reported as measure of dispersion once the calculation is finished 
there are two distinct concepts that are both called variance 
one as discussed above is part of theoretical probability distribution and is defined by an equation 
the other variance is characteristic of set of observations 
when variance is calculated from observations those observations are typically measured from real world system 
if all possible observations of the system are present then the calculated variance is called the population variance 
normally however only subset is available and the variance calculated from this is called the sample variance 
the variance calculated from sample is considered an estimate of the full population variance 
there are multiple ways to calculate an estimate of the population variance as discussed in the section below 
the two kinds of variance are closely related 
to see how consider that theoretical probability distribution can be used as generator of hypothetical observations 
if an infinite number of observations are generated using distribution then the sample variance calculated from that infinite set will match the value calculated using the distribution equation for variance 
etymology the term variance was first introduced by ronald fisher in his paper the correlation between relatives on the supposition of mendelian inheritance the great body of available statistics show us that the deviations of human measurement from its mean follow very closely the normal law of errors and therefore that the variability may be uniformly measured by the standard deviation corresponding to the square root of the mean square error 
when there are two independent causes of variability capable of producing in an otherwise uniform population distributions with standard deviations and it is found that the distribution when both causes act together has standard deviation it is therefore desirable in analysing the causes of variability to deal with the square of the standard deviation as the measure of variability 
we shall term this quantity the variance definition the variance of random variable is the expected value of the squared deviation from the mean of var 
this definition encompasses random variables that are generated by processes that are discrete continuous neither or mixed 
the variance can also be thought of as the covariance of random variable with itself var cov 
the variance is also equivalent to the second cumulant of probability distribution that generates the variance is typically designated as var or sometimes as or or symbolically as or simply pronounced sigma squared 
the expression for the variance can be expanded as follows var in other words the variance of is equal to the mean of the square of minus the square of the mean of this equation should not be used for computations using floating point arithmetic because it suffers from catastrophic cancellation if the two components of the equation are similar in magnitude 
for other numerically stable alternatives see algorithms for calculating variance 
discrete random variable if the generator of random variable is discrete with probability mass function then var where is the expected value 
when such discrete weighted variance is specified by weights whose sum is not then one divides by the sum of the weights 
the variance of collection of equally likely values can be written as var where is the average value 
that is the variance of set of equally likely values can be equivalently expressed without directly referring to the mean in terms of squared deviations of all pairwise squared distances of points from each other var 
absolutely continuous random variable if the random variable has probability density function and is the corresponding cumulative distribution function then var or equivalently var where is the expected value of given by 
in these formulas the integrals with respect to and are lebesgue and lebesgue stieltjes integrals respectively 
if the function is riemann integrable on every finite interval then var where the integral is an improper riemann integral 
examples exponential distribution the exponential distribution with parameter is continuous distribution whose probability density function is given by on the interval 
its mean can be shown to be using integration by parts and making use of the expected value already calculated we have thus the variance of is given by var 
fair die fair six sided die can be modeled as discrete random variable with outcomes through each with equal probability 
the expected value of is therefore the variance of is var the general formula for the variance of the outcome of an sided die is var 
commonly used probability distributions the following table lists the variance for some commonly used probability distributions 
properties basic properties variance is non negative because the squares are positive or zero var the variance of constant is zero 
var conversely if the variance of random variable is then it is almost surely constant 
that is it always has the same value var 
issues of finiteness if distribution does not have finite expected value as is the case for the cauchy distribution then the variance cannot be finite either 
however some distributions may not have finite variance despite their expected value being finite 
an example is pareto distribution whose index satisfies 
decomposition the general formula for variance decomposition or the law of total variance is if and are two random variables and the variance of exists then var var var 
the conditional expectation of given and the conditional variance var may be understood as follows 
given any particular value of the random variable there is conditional expectation given the event this quantity depends on the particular value it is function 
that same function evaluated at the random variable is the conditional expectation 
in particular if is discrete random variable assuming possible values with corresponding probabilities then in the formula for total variance the first term on the right hand side becomes var where var 
similarly the second term on the right hand side becomes var where and thus the total variance is given by var 
similar formula is applied in analysis of variance where the corresponding formula is total between within here refers to the mean of the squares 
in linear regression analysis the corresponding formula is total regression residual 
this can also be derived from the additivity of variances since the total observed score is the sum of the predicted score and the error score where the latter two are uncorrelated 
similar decompositions are possible for the sum of squared deviations sum of squares total between within total regression residual 
calculation from the cdf the population variance for non negative random variable can be expressed in terms of the cumulative distribution function using this expression can be used to calculate the variance in situations where the cdf but not the density can be conveniently expressed 
characteristic property the second moment of random variable attains the minimum value when taken around the first moment mean of the random variable 
conversely if continuous function satisfies for all random variables then it is necessarily of the form where this also holds in the multidimensional case 
units of measurement unlike the expected absolute deviation the variance of variable has units that are the square of the units of the variable itself 
for example variable measured in meters will have variance measured in meters squared 
for this reason describing data sets via their standard deviation or root mean square deviation is often preferred over using the variance 
in the dice example the standard deviation is slightly larger than the expected absolute deviation of the standard deviation and the expected absolute deviation can both be used as an indicator of the spread of distribution 
the standard deviation is more amenable to algebraic manipulation than the expected absolute deviation and together with variance and its generalization covariance is used frequently in theoretical statistics however the expected absolute deviation tends to be more robust as it is less sensitive to outliers arising from measurement anomalies or an unduly heavy tailed distribution 
propagation addition and multiplication by constant variance is invariant with respect to changes in location parameter 
that is if constant is added to all values of the variable the variance is unchanged var var 
if all values are scaled by constant the variance is scaled by the square of that constant var var 
the variance of sum of two random variables is given by var var var cov var var var cov where cov is the covariance 
linear combinations in general for the sum of random variables the variance becomes var cov var cov see also general bienaym identity 
these results lead to the variance of linear combination as var cov var cov var cov 
if the random variables are such that cov then they are said to be uncorrelated 
it follows immediately from the expression given earlier that if the random variables are uncorrelated then the variance of their sum is equal to the sum of their variances or expressed symbolically var var 
since independent random variables are always uncorrelated see covariance uncorrelatedness and independence the equation above holds in particular when the random variables are independent 
thus independence is sufficient but not necessary for the variance of the sum to equal the sum of the variances 
matrix notation for the variance of linear combination define as column vector of random variables and as column vector of scalars therefore is linear combination of these random variables where denotes the transpose of also let be the covariance matrix of the variance of is then given by var this implies that the variance of the mean can be written as with column vector of ones var var 
sum of variables sum of uncorrelated variables one reason for the use of the variance in preference to other measures of dispersion is that the variance of the sum or the difference of uncorrelated random variables is the sum of their variances var var 
this statement is called the bienaym formula and was discovered in it is often made with the stronger condition that the variables are independent but being uncorrelated suffices 
so if all the variables have the same variance then since division by is linear transformation this formula immediately implies that the variance of their mean is var var var that is the variance of the mean decreases when increases 
this formula for the variance of the mean is used in the definition of the standard error of the sample mean which is used in the central limit theorem 
to prove the initial statement it suffices to show that var var var 
the general result then follows by induction 
starting with the definition var using the linearity of the expectation operator and the assumption of independence or uncorrelatedness of and this further simplifies as follows var var var 
sum of correlated variables sum of correlated variables with fixed sample size in general the variance of the sum of variables is the sum of their covariances var cov var cov 
note the second equality comes from the fact that cov xi xi var xi 
here cov is the covariance which is zero for independent random variables if it exists 
the formula states that the variance of sum is equal to the sum of all elements in the covariance matrix of the components 
the next expression states equivalently that the variance of the sum is the sum of the diagonal of covariance matrix plus two times the sum of its upper triangular elements or its lower triangular elements this emphasizes that the covariance matrix is symmetric 
this formula is used in the theory of cronbach alpha in classical test theory 
so if the variables have equal variance and the average correlation of distinct variables is then the variance of their mean is var this implies that the variance of the mean increases with the average of the correlations 
in other words additional correlated observations are not as effective as additional independent observations at reducing the uncertainty of the mean 
moreover if the variables have unit variance for example if they are standardized then this simplifies to var this formula is used in the spearman brown prediction formula of classical test theory 
this converges to if goes to infinity provided that the average correlation remains constant or converges too 
so for the variance of the mean of standardized variables with equal correlations or converging average correlation we have lim var therefore the variance of the mean of large number of standardized variables is approximately equal to their average correlation 
this makes clear that the sample mean of correlated variables does not generally converge to the population mean even though the law of large numbers states that the sample mean will converge for independent variables 
sum of uncorrelated variables with random sample size there are cases when sample is taken without knowing in advance how many observations will be acceptable according to some criterion 
in such cases the sample size is random variable whose variation adds to the variation of such that var var var which follows from the law of total variance 
if has poisson distribution then var with estimator so the estimator of var becomes giving se weighted sum of variables the scaling property and the bienaym formula along with the property of the covariance cov ax by ab cov jointly imply that var var var cov 
this implies that in weighted sum of variables the variable with the largest weight will have disproportionally large weight in the variance of the total 
for example if and are uncorrelated and the weight of is two times the weight of then the weight of the variance of will be four times the weight of the variance of the expression above can be extended to weighted sum of multiple variables var var cov product of variables product of independent variables if two variables and are independent the variance of their product is given by var var var var var 
equivalently using the basic properties of expectation it is given by var 
product of statistically dependent variables in general if two variables are statistically dependent then the variance of their product is given by var cov cov var var cov arbitrary functions the delta method uses second order taylor expansions to approximate the variance of function of one or more random variables see taylor expansions for the moments of functions of random variables 
for example the approximate variance of function of one variable is given by var var provided that is twice differentiable and that the mean and variance of are finite 
population variance and sample variance real world observations such as the measurements of yesterday rain throughout the day typically cannot be complete sets of all possible observations that could be made 
as such the variance calculated from the finite set will in general not match the variance that would have been calculated from the full population of possible observations 
this means that one estimates the mean and variance from limited set of observations by using an estimator equation 
the estimator is function of the sample of observations drawn without observational bias from the whole population of potential observations 
in this example that sample would be the set of actual measurements of yesterday rainfall from available rain gauges within the geography of interest 
the simplest estimators for population mean and population variance are simply the mean and variance of the sample the sample mean and uncorrected sample variance these are consistent estimators they converge to the correct value as the number of samples increases but can be improved 
estimating the population variance by taking the sample variance is close to optimal in general but can be improved in two ways 
most simply the sample variance is computed as an average of squared deviations about the sample mean by dividing by however using values other than improves the estimator in various ways 
four common values for the denominator are and is the simplest population variance of the sample eliminates bias minimizes mean squared error for the normal distribution and mostly eliminates bias in unbiased estimation of standard deviation for the normal distribution 
firstly if the true population mean is unknown then the sample variance which uses the sample mean in place of the true mean is biased estimator it underestimates the variance by factor of correcting by this factor dividing by instead of is called bessel correction 
the resulting estimator is unbiased and is called the corrected sample variance or unbiased sample variance 
for example when the variance of single observation about the sample mean itself is obviously zero regardless of the population variance 
if the mean is determined in some other way than from the same samples used to estimate the variance then this bias does not arise and the variance can safely be estimated as that of the samples about the independently known mean 
secondly the sample variance does not generally minimize mean squared error between sample variance and population variance 
correcting for bias often makes this worse one can always choose scale factor that performs better than the corrected sample variance though the optimal scale factor depends on the excess kurtosis of the population see mean squared error variance and introduces bias 
this always consists of scaling down the unbiased estimator dividing by number larger than and is simple example of shrinkage estimator one shrinks the unbiased estimator towards zero 
for the normal distribution dividing by instead of or minimizes mean squared error 
the resulting estimator is biased however and is known as the biased sample variation 
population variance in general the population variance of finite population of size with values xi is given by where the population mean is the population variance can also be computed using this is true because the population variance matches the variance of the generating probability distribution 
in this sense the concept of population can be extended to continuous random variables with infinite populations 
sample variance biased sample variance in many practical situations the true variance of population is not known priori and must be computed somehow 
when dealing with extremely large populations it is not possible to count every object in the population so the computation must be performed on sample of the population 
sample variance can also be applied to the estimation of the variance of continuous distribution from sample of that distribution 
we take sample with replacement of values yn from the population where and estimate the variance on the basis of this sample 
directly taking the variance of the sample data gives the average of the squared deviations here denotes the sample mean since the yi are selected randomly both and are random variables 
their expected values can be evaluated by averaging over the ensemble of all possible samples of size from the population 
for this gives hence gives an estimate of the population variance that is biased by factor of for this reason is referred to as the biased sample variance 
unbiased sample variance correcting for this bias yields the unbiased sample variance denoted either estimator may be simply referred to as the sample variance when the version can be determined by context 
the same proof is also applicable for samples taken from continuous probability distribution 
the use of the term is called bessel correction and it is also used in sample covariance and the sample standard deviation the square root of variance 
the square root is concave function and thus introduces negative bias by jensen inequality which depends on the distribution and thus the corrected sample standard deviation using bessel correction is biased 
the unbiased estimation of standard deviation is technically involved problem though for the normal distribution using the term yields an almost unbiased estimator 
the unbiased sample variance is statistic for the function meaning that it is obtained by averaging sample statistic over element subsets of the population 
distribution of the sample variance being function of random variables the sample variance is itself random variable and it is natural to study its distribution 
in the case that yi are independent observations from normal distribution cochran theorem shows that follows scaled chi squared distribution see also asymptotic properties as direct consequence it follows that and var var var if the yi are independent and identically distributed but not necessarily normally distributed then var where is the kurtosis of the distribution and is the fourth central moment 
if the conditions of the law of large numbers hold for the squared observations is consistent estimator of 
one can see indeed that the variance of the estimator tends asymptotically to zero 
an asymptotically equivalent formula was given in kenney and keeping rose and smith and weisstein 
samuelson inequality samuelson inequality is result that states bounds on the values that individual observations in sample can take given that the sample mean and biased variance have been calculated 
values must lie within the limits 
relations with the harmonic and arithmetic means it has been shown that for sample of positive real numbers max where ymax is the maximum of the sample is the arithmetic mean is the harmonic mean of the sample and is the biased variance of the sample 
this bound has been improved and it is known that variance is bounded by max max max min min min where ymin is the minimum of the sample 
tests of equality of variances the test of equality of variances and the chi square tests are adequate when the sample is normally distributed 
non normality makes testing for the equality of two or more variances more difficult 
several non parametric tests have been proposed these include the barton david ansari freund siegel tukey test the capon test mood test the klotz test and the sukhatme test 
the sukhatme test applies to two variances and requires that both medians be known and equal to zero 
the mood klotz capon and barton david ansari freund siegel tukey tests also apply to two variances 
they allow the median to be unknown but do require that the two medians are equal 
the lehmann test is parametric test of two variances 
of this test there are several variants known 
other tests of the equality of variances include the box test the box anderson test and the moses test 
resampling methods which include the bootstrap and the jackknife may be used to test the equality of variances 
moment of inertia the variance of probability distribution is analogous to the moment of inertia in classical mechanics of corresponding mass distribution along line with respect to rotation about its center of mass 
it is because of this analogy that such things as the variance are called moments of probability distributions 
the covariance matrix is related to the moment of inertia tensor for multivariate distributions 
the moment of inertia of cloud of points with covariance matrix of is given by tr 
this difference between moment of inertia in physics and in statistics is clear for points that are gathered along line 
suppose many points are close to the axis and distributed along it 
the covariance matrix might look like 
that is there is the most variance in the direction 
physicists would consider this to have low moment about the axis so the moment of inertia tensor is 
semivariance the semivariance is calculated in the same manner as the variance but only those observations that fall below the mean are included in the calculation it is also described as specific measure in different fields of application 
for skewed distributions the semivariance can provide additional information that variance does not for inequalities associated with the semivariance see chebyshev inequality semivariances 
generalizations for complex variables if is scalar complex valued random variable with values in then its variance is where is the complex conjugate of this variance is real scalar 
for vector valued random variables as matrix if is vector valued random variable with values in and thought of as column vector then natural generalization of variance is where and is the transpose of and so is row vector 
the result is positive semi definite square matrix commonly referred to as the variance covariance matrix or simply as the covariance matrix 
if is vector and complex valued random variable with values in then the covariance matrix is where is the conjugate transpose of this matrix is also positive semi definite and square 
as scalar another generalization of variance for vector valued random variables which results in scalar value rather than in matrix is the generalized variance det the determinant of the covariance matrix 
the generalized variance can be shown to be related to the multidimensional scatter of points around their mean different generalization is obtained by considering the euclidean distance between the random variable and its mean 
this results in tr which is the trace of the covariance matrix 
see also bhatia davis inequality coefficient of variation homoscedasticity least squares spectral analysis for computing frequency spectrum with spectral magnitudes in of variance or in db popoviciu inequality on variances measures for statistical dispersion variance stabilizing transformation types of variance correlation distance variance explained variance pooled variance pseudo variance references
the axolotl from classical nahuatl tl lo listen ambystoma mexicanum is paedomorphic salamander closely related to the tiger salamander 
axolotls are unusual among amphibians in that they reach adulthood without undergoing metamorphosis 
instead of taking to the land adults remain aquatic and gilled 
the species was originally found in several lakes underlying what is now mexico city such as lake xochimilco and lake chalco 
these lakes were drained by spanish settlers after the conquest of the aztec empire leading to the destruction of much of the axolotl natural habitat 
axolotls should not be confused with the larval stage of the closely related tiger salamander tigrinum which are widespread in much of north america and occasionally become paedomorphic 
neither should they be confused with mudpuppies necturus spp 
fully aquatic salamanders from different family that are not closely related to the axolotl but bear superficial resemblance as of wild axolotls were near extinction due to urbanization in mexico city and consequent water pollution as well as the introduction of invasive species such as tilapia and perch 
they are listed as critically endangered in the wild with decreasing population of around to adult individuals by the international union for conservation of nature and natural resources iucn and are listed under appendix ii of the convention on international trade in endangered species cites 
axolotls are used extensively in scientific research due to their ability to regenerate limbs gills and parts of their eyes and brains 
axolotls were also sold as food in mexican markets and were staple in the aztec diet 
description sexually mature adult axolotl at age months ranges in length from to cm to in although size close to cm in is most common and greater than cm in is rare 
axolotls possess features typical of salamander larvae including external gills and caudal fin extending from behind the head to the vent 
external gills are usually lost when salamander species mature into adulthood although the axolotl maintains this feature 
this is due to their neoteny evolution where axolotls are much more aquatic than other salamander species their heads are wide and their eyes are lidless 
their limbs are underdeveloped and possess long thin digits 
males are identified by their swollen cloacae lined with papillae while females are noticeable for their wider bodies full of eggs 
three pairs of external gill stalks rami originate behind their heads and are used to move oxygenated water 
the external gill rami are lined with filaments fimbriae to increase surface area for gas exchange 
four gill slits lined with gill rakers are hidden underneath the external gills which prevent food from entering and allow particles to filter through 
axolotls have barely visible vestigial teeth which develop during metamorphosis 
the primary method of feeding is by suction during which their rakers interlock to close the gill slits 
external gills are used for respiration although buccal pumping gulping air from the surface may also be used to provide oxygen to their lungs 
buccal pumping can occur in two stroke manner that pumps air from the mouth to the lungs and with four stroke that reverses this pathway with compression forces 
axolotls have four pigmentation genes when mutated they create different color variants 
the normal wild type animal is brown tan with gold speckles and an olive undertone 
the five more common mutant colors are leucistic pale pink with black eyes golden albino golden with gold eyes xanthic grey with black eyes albino pale pink white with red eyes which is more common in axolotls than some other creatures and melanoid all black dark blue with no gold speckling or olive tone 
in addition there is wide individual variability in the size frequency and intensity of the gold speckling and at least one variant that develops black and white piebald appearance on reaching maturity 
because pet breeders frequently cross the variant colors double homozygous mutants are common in the pet trade especially white pink animals with pink eyes that are double homozygous mutants for both the albino and leucistic trait 
axolotls also have some limited ability to alter their color to provide better camouflage by changing the relative size and thickness of their melanophores 
habitat and ecology the axolotl is native only to the freshwater of lake xochimilco and lake chalco in the valley of mexico 
lake chalco no longer exists having been drained as flood control measure and lake xochimilco remains remnant of its former self existing mainly as canals 
the water temperature in xochimilco rarely rises above although it may fall to in the winter and perhaps lower surveys in and found and axolotls per square kilometer in its lake xochimilco habitat respectively 
four month long search in however turned up no surviving individuals in the wild 
just month later two wild ones were spotted in network of canals leading from xochimilco the wild population has been put under heavy pressure by the growth of mexico city 
the axolotl is currently on the international union for conservation of nature annual red list of threatened species 
non native fish such as african tilapia and asian carp have also recently been introduced to the waters 
these new fish have been eating the axolotls young as well as their primary source of food axolotls are members of the tiger salamander or ambystoma tigrinum species complex along with all other mexican species of ambystoma 
their habitat is like that of most neotenic species high altitude body of water surrounded by risky terrestrial environment 
these conditions are thought to favor neoteny 
however terrestrial population of mexican tiger salamanders occupies and breeds in the axolotl habitat the axolotl is carnivorous consuming small prey such as mollusks worms insects other arthropods and small fish in the wild 
axolotls locate food by smell and will snap at any potential meal sucking the food into their stomachs with vacuum force 
use as model organism today the axolotl is still used in research as model organism and large numbers are bred in captivity 
they are especially easy to breed compared to other salamanders in their family which are rarely captive bred due to the demands of terrestrial life 
one attractive feature for research is the large and easily manipulated embryo which allows viewing of the full development of vertebrate 
axolotls are used in heart defect studies due to the presence of mutant gene that causes heart failure in embryos 
since the embryos survive almost to hatching with no heart function the defect is very observable 
the axolotl is also considered an ideal animal model for the study of neural tube closure due to the similarities between human and axolotl neural plate and tube formation the axolotl neural tube unlike the frog is not hidden under layer of superficial epithelium 
there are also mutations affecting other organ systems some of which are not well characterized and others that are 
the genetics of the color variants of the axolotl have also been widely studied 
regeneration the feature of the axolotl that attracts most attention is its healing ability the axolotl does not heal by scarring and is capable of the regeneration of entire lost appendages in period of months and in certain cases more vital structures such as tail limb central nervous system and tissues of the eye and heart 
they can even restore less vital parts of their brains 
they can also readily accept transplants from other individuals including eyes and parts of the brain restoring these alien organs to full functionality 
in some cases axolotls have been known to repair damaged limb as well as regenerating an additional one ending up with an extra appendage that makes them attractive to pet owners as novelty 
in metamorphosed individuals however the ability to regenerate is greatly diminished 
the axolotl is therefore used as model for the development of limbs in vertebrates 
there are three basic requirements for regeneration of the limb the wound epithelium nerve signaling and the presence of cells from the different limb axes 
wound epidermis is quickly formed by the cells to cover up the site of the wound 
in the following days the cells of the wound epidermis divide and grow quickly forming blastema which means the wound is ready to heal and undergo patterning to form the new limb 
it is believed that during limb generation axolotls have different system to regulate their internal macrophage level and suppress inflammation as scarring prevents proper healing and regeneration 
however this belief has been questioned by other studies 
axolotl regenerative properties leave the species as the perfect model to study the process of stem cells and its own neoteny feature 
current research can record specific examples of these regenerative properties through tracking cell fates and behaviors lineage tracing skin triploid cell grafts pigmentation imaging electroporation tissue clearing and lineage tracing from dye labeling 
the newer technologies of germline modification and transgenesis are better suited for live imaging the regenerative processes that occur for axolotls 
genome the billion base pair long sequence of the axolotl genome was published in and was the largest animal genome completed at the time 
it revealed species specific genetic pathways that may be responsible for limb regeneration 
although the axolotl genome is about times as large as the human genome it encodes similar number of proteins namely the human genome encodes about proteins 
the size difference is mostly explained by large fraction of repetitive sequences but such repeated elements also contribute to increased median intron sizes bp which are and times that observed in human bp mouse bp and tibetan frog bp respectively 
neoteny when most amphibians are young they live in water and they use gills that can breathe in the water 
when they become adults they go through process called metamorphosis in which they lose their gills and start living on land 
however the axolotl is unusual in that it has lack of thyroid stimulating hormone which is needed for the thyroid to produce thyroxine in order for the axolotl to go through metamorphosis therefore it keeps its gills and lives in water all its life even after it becomes an adult and is able to reproduce 
its body has the capacity to go through metamorphosis if given the necessary hormone but axolotls do not produce it and must be exposed to it from an external source after which an axolotl undergoes an artificially induced metamorphosis and begins living on land 
one method of artificial metamorphosis induction is through an injection of iodine which is used in the production of thyroid hormones 
an axolotl undergoing metamorphosis experiences number of physiological changes that help them adapt to life on land 
these include increased muscle tone in limbs the absorption of gills and fins into the body the development of eyelids and reduction in the skin permeability to water allowing the axolotl to stay more easily hydrated when on land 
the lungs of an axolotl though present alongside gills after reaching non metamorphosed adulthood develop further during metamorphosis an axolotl that has gone through metamorphosis resembles an adult plateau tiger salamander though the axolotl differs in its longer toes 
the process of artificially inducing metamorphosis can often result in death during or even following successful attempt and so casual hobbyists are generally discouraged from attempting to induce metamorphosis in pet axolotls neoteny is the term for reaching sexual maturity without undergoing metamorphosis 
many other species within the axolotl genus are also either entirely neotenic or have neotenic populations 
sirens and necturus are other neotenic salamanders although unlike axolotls they cannot be induced to metamorphose by an injection of iodine or thyroxine hormone 
the genes responsible for neoteny in laboratory animals may have been identified however they are not linked in wild populations suggesting artificial selection is the cause of complete neoteny in laboratory and pet axolotls six adult axolotls including leucistic specimen were shipped from mexico city to the jardin des plantes in paris in unaware of their neoteny auguste dum ril was surprised when instead of the axolotl he found in the vivarium new species similar to the salamander 
this discovery was the starting point of research about neoteny 
it is not certain that ambystoma velasci specimens were not included in the original shipment 
vilem laufberger in prague used thyroid hormone injections to induce an axolotl to grow into terrestrial adult salamander 
the experiment was repeated by englishman julian huxley who was unaware the experiment had already been done using ground thyroids 
since then experiments have been done often with injections of iodine or various thyroid hormones used to induce metamorphosis neoteny has been observed in all salamander families in which it seems to be survival mechanism in aquatic environments only of mountain and hill with little food and in particular with little iodine 
in this way salamanders can reproduce and survive in the form of smaller larval stage which is aquatic and requires lower quality and quantity of food compared to the big adult which is terrestrial 
if the salamander larvae ingest sufficient amount of iodine directly or indirectly through cannibalism they quickly begin metamorphosis and transform into bigger terrestrial adults with higher dietary requirements 
in fact in some high mountain lakes there live dwarf forms of salmonids that are caused by deficiencies in food and in particular iodine which causes cretinism and dwarfism due to hypothyroidism as it does in humans 
captive care the axolotl is popular exotic pet like its relative the tiger salamander ambystoma tigrinum 
as for all poikilothermic organisms lower temperatures result in slower metabolism and very unhealthily reduced appetite 
temperatures at approximately to are suggested for captive axolotls to ensure sufficient food intake stress resulting from more than day exposure to lower temperatures may quickly lead to disease and death and temperatures higher than may lead to metabolic rate increase also causing stress and eventually death 
chlorine commonly added to tapwater is harmful to axolotls 
single axolotl typically requires litre us gallon tank 
axolotls spend the majority of the time at the bottom of the tank 
salts such as holtfreter solution are often added to the water to prevent infection in captivity axolotls eat variety of readily available foods including trout and salmon pellets frozen or live bloodworms earthworms and waxworms 
axolotls can also eat feeder fish but care should be taken as fish may contain parasites substrates are another important consideration for captive axolotls as axolotls like other amphibians and reptiles tend to ingest bedding material together with food and are commonly prone to gastrointestinal obstruction and foreign body ingestion 
some common substrates used for animal enclosures can be harmful for amphibians and reptiles 
gravel common in aquarium use should not be used and is recommended that any sand consists of smooth particles with grain size of under mm 
one guide to axolotl care for laboratories notes that bowel obstructions are common cause of death and recommends that no items with diameter below cm or approximately the size of the animal head should be available to the animal there is some evidence that axolotls might seek out appropriately sized gravel for use as gastroliths based on experiments conducted at the university of manitoba axolotl colony but these studies are outdated and not conclusive 
as there is no conclusive evidence pointing to gastrolith use gravel should be avoided due to the high risk of impaction 
cultural significance the species is named after the aztec deity xolotl who transformed himself into an axolotl 
they continue to play an outsized cultural role in mexico and have appeared in cartoons and murals in it was announced that the axolotl will be featured on the new design for mexico peso banknote along with images of maize and chinampas 
see also mudpuppies olm texas salamander texas blind salamander lake patzcuaro salamander barred tiger salamander amphibious fish handfish regenerative biomedicine references external links ambystomatidae at curlie follow the eggs hatchlings and juveniles mating dance and laying eggs follow the eggs and hatchlings nd batch indiana axolotl colony university of ky axolotl colony mystical amphibian venerated by aztecs nears extinction the animal that everywhere and nowhere axolotl 
the tao of axolotl thetolteciching com on folklore
in statistics the logistic model or logit model is statistical model that models the probability of an event taking place by having the log odds for the event be linear combination of one or more independent variables 
in regression analysis logistic regression or logit regression is estimating the parameters of logistic model the coefficients in the linear combination 
formally in binary logistic regression there is single binary dependent variable coded by an indicator variable where the two values are labeled and while the independent variables can each be binary variable two classes coded by an indicator variable or continuous variable any real value 
the corresponding probability of the value labeled can vary between certainly the value and certainly the value hence the labeling the function that converts log odds to probability is the logistic function hence the name 
the unit of measurement for the log odds scale is called logit from logistic unit hence the alternative names 
see background and definition for formal mathematics and example for worked example 
binary variables are widely used in statistics to model the probability of certain class or event taking place such as the probability of team winning of patient being healthy etc 
see applications and the logistic model has been the most commonly used model for binary regression since about binary variables can be generalized to categorical variables when there are more than two possible values 
whether an image is of cat dog lion etc 
and the binary logistic regression generalized to multinomial logistic regression 
if the multiple categories are ordered one can use the ordinal logistic regression for example the proportional odds ordinal logistic model 
see extensions for further extensions 
the logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification it is not classifier though it can be used to make classifier for instance by choosing cutoff value and classifying inputs with probability greater than the cutoff as one class below the cutoff as the other this is common way to make binary classifier 
analogous linear models for binary variables with different sigmoid function instead of the logistic function to convert the linear combination to probability can also be used most notably the probit model see alternatives 
the defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at constant rate with each independent variable having its own parameter for binary dependent variable this generalizes the odds ratio 
more abstractly the logistic function is the natural parameter for the bernoulli distribution and in this sense is the simplest way to convert real number to probability 
in particular it maximizes entropy minimizes added information and in this sense makes the fewest assumptions of the data being modeled see maximum entropy 
the parameters of logistic regression are most commonly estimated by maximum likelihood estimation mle 
this does not have closed form expression unlike linear least squares see model fitting 
logistic regression by mle plays similarly basic role for binary or categorical responses as linear regression by ordinary least squares ols plays for scalar responses it is simple well analyzed baseline model see comparison with linear regression for discussion 
the logistic regression as general statistical model was originally developed and popularized primarily by joseph berkson beginning in berkson where he coined logit see history 
applications logistic regression is used in various fields including machine learning most medical fields and social sciences 
for example the trauma and injury severity score triss which is widely used to predict mortality in injured patients was originally developed by boyd et al 
many other medical scales used to assess severity of patient have been developed using logistic regression 
logistic regression may be used to predict the risk of developing given disease 
diabetes coronary heart disease based on observed characteristics of the patient age sex body mass index results of various blood tests etc 
another example might be to predict whether nepalese voter will vote nepali congress or communist party of nepal or any other party based on age income sex race state of residence votes in previous elections etc 
the technique can also be used in engineering especially for predicting the probability of failure of given process system or product 
it is also used in marketing applications such as prediction of customer propensity to purchase product or halt subscription etc 
in economics it can be used to predict the likelihood of person ending up in the labor force and business application would be to predict the likelihood of homeowner defaulting on mortgage 
conditional random fields an extension of logistic regression to sequential data are used in natural language processing 
example problem as simple example we can use logistic regression with one explanatory variable and two categories to answer the following question group of students spends between and hours studying for an exam 
how does the number of hours spent studying affect the probability of the student passing the exam 
the reason for using logistic regression for this problem is that the values of the dependent variable pass and fail while represented by and are not cardinal numbers 
if the problem was changed so that pass fail was replaced with the grade cardinal numbers then simple regression analysis could be used 
the table shows the number of hours each student spent studying and whether they passed or failed 
we wish to fit logistic function to the data consisting of the hours studied xk and the outcome of the test yk for pass for fail 
the data points are indexed by the subscript which runs from to the variable is called the explanatory variable and the variable is called the categorical variable consisting of two categories pass or fail corresponding to the categorical values and respectively 
model the logistic function is of the form where is location parameter the midpoint of the curve where and is scale parameter 
this expression may be rewritten as where and is known as the intercept it is the vertical intercept or intercept of the line and inverse scale parameter or rate parameter these are the intercept and slope of the log odds as function of conversely and 
fit the usual measure of goodness of fit for logistic regression uses logistic loss or log loss the negative log likelihood 
for given xk and yk write 
the are the probabilities that the corresponding will be unity and are the probabilities that they will be zero see bernoulli distribution 
we wish to find the values of and which give the best fit to the data 
in the case of linear regression the sum of the squared deviations of the fit from the data points yk the squared error loss is taken as measure of the goodness of fit and the best fit is obtained when that function is minimized 
the log loss for the th point is ln if ln if the log loss can be interpreted as the surprisal of the actual outcome relative to the prediction and is measure of information content 
note that log loss is always greater than or equal to equals only in case of perfect prediction when and or and and approaches infinity as the prediction gets worse when and or and meaning the actual outcome is more surprising 
since the value of the logistic function is always strictly between zero and one the log loss is always greater than zero and less than infinity 
note that unlike in linear regression where the model can have zero loss at point by passing through data point and zero loss overall if all points are on line in logistic regression it is not possible to have zero loss at any points since is either or but these can be combined into single expression ln ln 
this expression is more formally known as the cross entropy of the predicted distribution from the actual distribution as probability distributions on the two element space of pass fail 
the sum of these the total loss is the overall negative log likelihood and the best fit is obtained for those choices of and for which is minimized 
alternatively instead of minimizing the loss one can maximize its inverse the positive log likelihood ln ln ln ln or equivalently maximize the likelihood function itself which is the probability that the given data set is produced by particular logistic function this method is known as maximum likelihood estimation 
parameter estimation since is nonlinear in and determining their optimum values will require numerical methods 
note that one method of maximizing is to require the derivatives of with respect to and to be zero and the maximization procedure can be accomplished by solving the above two equations for and which again will generally require the use of numerical methods 
the values of and which maximize and using the above data are found to be which yields value for and of predictions the and coefficients may be entered into the logistic regression equation to estimate the probability of passing the exam 
for example for student who studies hours entering the value into the equation gives the estimated probability of passing the exam of probability of passing exam similarly for student who studies hours the estimated probability of passing the exam is probability of passing exam this table shows the estimated probability of passing the exam for several values of hours studying 
model evaluation the logistic regression analysis gives the following output 
by the wald test the output indicates that hours studying is significantly associated with the probability of passing the exam 
rather than the wald method the recommended method to calculate the value for logistic regression is the likelihood ratio test lrt which for this data gives see error and significance of the fit below 
generalizations this simple model is an example of binary logistic regression and has one explanatory variable and binary categorical variable which can assume one of two categorical values 
multinomial logistic regression is the generalization of binary logistic regression to include any number of explanatory variables and any number of categories 
background definition of the logistic function an explanation of logistic regression can begin with an explanation of the standard logistic function 
the logistic function is sigmoid function which takes any real input and outputs value between zero and one 
for the logit this is interpreted as taking input log odds and having output probability 
the standard logistic function is defined as follows graph of the logistic function on the interval is shown in figure let us assume that is linear function of single explanatory variable the case where is linear combination of multiple explanatory variables is treated similarly 
we can then express as follows and the general logistic function can now be written as in the logistic model is interpreted as the probability of the dependent variable equaling success case rather than failure non case 
it clear that the response variables are not identically distributed differs from one data point to another though they are independent given design matrix and shared parameters 
definition of the inverse of the logistic function we can now define the logit log odds function as the inverse of the standard logistic function 
it is easy to see that it satisfies logit ln and equivalently after exponentiating both sides we have the odds 
interpretation of these terms in the above equations the terms are as follows is the logit function 
the equation for illustrates that the logit log odds or natural logarithm of the odds is equivalent to the linear regression expression 
ln denotes the natural logarithm 
is the probability that the dependent variable equals case given some linear combination of the predictors 
the formula for illustrates that the probability of the dependent variable equaling case is equal to the value of the logistic function of the linear regression expression 
this is important in that it shows that the value of the linear regression expression can vary from negative to positive infinity and yet after transformation the resulting expression for the probability ranges between and is the intercept from the linear regression equation the value of the criterion when the predictor is equal to zero 
is the regression coefficient multiplied by some value of the predictor 
base denotes the exponential function 
definition of the odds the odds of the dependent variable equaling case given some linear combination of the predictors is equivalent to the exponential function of the linear regression expression 
this illustrates how the logit serves as link function between the probability and the linear regression expression 
given that the logit ranges between negative and positive infinity it provides an adequate criterion upon which to conduct linear regression and the logit is easily converted back into the odds so we define odds of the dependent variable equaling case given some linear combination of the predictors as follows odds 
the odds ratio for continuous independent variable the odds ratio can be defined as odds odds this exponential relationship provides an interpretation for the odds multiply by for every unit increase in for binary independent variable the odds ratio is defined as where and are cells in contingency table 
multiple explanatory variables if there are multiple explanatory variables the above expression can be revised to then when this is used in the equation relating the log odds of success to the values of the predictors the linear regression will be multiple regression with explanators the parameters for all are all estimated 
again the more traditional equations are log and where usually definition the basic setup of logistic regression is as follows 
we are given dataset containing points 
each point consists of set of input variables xm also called independent variables explanatory variables predictor variables features or attributes and binary outcome variable yi also known as dependent variable response variable output variable or class 
it can assume only the two possible values often meaning no or failure or often meaning yes or success 
the goal of logistic regression is to use the dataset to create predictive model of the outcome variable 
as in linear regression the outcome variables yi are assumed to depend on the explanatory variables xm 
explanatory variablesthe explanatory variables may be of any type real valued binary categorical etc 
the main distinction is between continuous variables and discrete variables 
discrete variables referring to more than two possible choices are typically coded using dummy variables or indicator variables that is separate explanatory variables taking the value or are created for each possible value of the discrete variable with meaning variable does have the given value and meaning variable does not have that value 
outcome variablesformally the outcomes yi are described as being bernoulli distributed data where each outcome is determined by an unobserved probability pi that is specific to the outcome at hand but related to the explanatory variables 
this can be expressed in any of the following equivalent forms bernoulli pr if if pr the meanings of these four lines are the first line expresses the probability distribution of each yi conditioned on the explanatory variables it follows bernoulli distribution with parameters pi the probability of the outcome of for trial as noted above each separate trial has its own probability of success just as each trial has its own explanatory variables 
the probability of success pi is not observed only the outcome of an individual bernoulli trial using that probability 
the second line expresses the fact that the expected value of each yi is equal to the probability of success pi which is general property of the bernoulli distribution 
in other words if we run large number of bernoulli trials using the same probability of success pi then take the average of all the and outcomes then the result would be close to pi 
this is because doing an average this way simply computes the proportion of successes seen which we expect to converge to the underlying probability of success 
the third line writes out the probability mass function of the bernoulli distribution specifying the probability of seeing each of the two possible outcomes 
the fourth line is another way of writing the probability mass function which avoids having to write separate cases and is more convenient for certain types of calculations 
this relies on the fact that yi can take only the value or in each case one of the exponents will be choosing the value under it while the other is canceling out the value under it 
hence the outcome is either pi or pi as in the previous line linear predictor functionthe basic idea of logistic regression is to use the mechanism already developed for linear regression by modeling the probability pi using linear predictor function 
linear combination of the explanatory variables and set of regression coefficients that are specific to the model at hand but the same for all trials 
the linear predictor function for particular data point is written as where are regression coefficients indicating the relative effect of particular explanatory variable on the outcome 
the model is usually put into more compact form as follows the regression coefficients are grouped into single vector of size for each data point an additional explanatory pseudo variable is added with fixed value of corresponding to the intercept coefficient 
the resulting explanatory variables xm are then grouped into single vector xi of size this makes it possible to write the linear predictor function as follows using the notation for dot product between two vectors 
many explanatory variables two categories the above example of binary logistic regression on one explanatory variable can be generalized to binary logistic regression on any number of explanatory variables and any number of categorical values 
to begin with we may consider logistic model with explanatory variables xm and as in the example above two categorical values and 
for the simple binary logistic regression model we assumed linear relationship between the predictor variable and the log odds also called logit of the event that this linear relationship may be extended to the case of explanatory variables log where is the log odds and are parameters of the model 
an additional generalization has been introduced in which the base of the model is not restricted to the euler number in most applications the base of the logarithm is usually taken to be however in some cases it can be easier to communicate results by working in base or base for more compact notation we will specify the explanatory variables and the coefficients as dimensional vectors with an added explanatory variable 
the logit may now be written as solving for the probability that yields where is the sigmoid function with base the above formula shows that once the are fixed we can easily compute either the log odds that for given observation or the probability that for given observation 
the main use case of logistic model is to be given an observation and estimate the probability that the optimum beta coefficients may again be found by maximizing the log likelihood 
for measurements defining xk as the explanatory vector of the th measurement and yk as the categorical outcome of that measurement the log likelihood may be written in form very similar to the simple case above log log as in the simple example above finding the optimum parameters will require numerical methods 
one useful technique is to equate the derivatives of the log likelihood with respect to each of the parameters to zero yielding set of equations which will hold at the maximum of the log likelihood where xmk is the value of the xm explanatory variable from the th measurement 
consider an example with explanatory variables and coefficients and which have been determined by the above method 
to be concrete the model is log where is the probability of the event that this can be interpreted as follows is the intercept 
it is the log odds of the event that when the predictors by exponentiating we can see that when the odds of the event that are to or similarly the probability of the event that when can be computed as means that increasing by increases the log odds by so if increases by the odds that increase by factor of note that the probability of has also increased but it has not increased by as much as the odds have increased 
means that increasing by increases the log odds by so if increases by the odds that increase by factor of note how the effect of on the log odds is twice as great as the effect of but the effect on the odds is times greater 
but the effect on the probability of is not as much as times greater it only the effect on the odds that is times greater 
multinomial logistic regression many explanatory variables and many categories in the above cases of two categories binomial logistic regression the categories were indexed by and and we had two probability distributions the probability that the outcome was in category was given by and the probability that the outcome was in category was given by 
the sum of both probabilities is equal to unity as they must be 
in general if we have explanatory variables including and categories we will need separate probability distributions one for each category indexed by which describe the probability that the categorical outcome for explanatory vector will be in category 
it will also be required that the sum of these probabilities over all categories be equal to unity 
using the mathematically convenient base these probabilities are for each of the probabilities except will have their own set of regression coefficients it can be seen that as required the sum of the over all categories is unity 
note that the selection of to be defined in terms of the other probabilities is artificial 
any of the probabilities could have been selected to be so defined 
this special value of is termed the pivot index and the log odds tn are expressed in terms of the pivot probability and are again expressed as linear combination of the explanatory variables ln note also that for the simple case of the two category case is recovered with and 
the log likelihood that particular set of measurements or data points will be generated by the above probabilities can now be calculated 
indexing each measurement by let the th set of measured explanatory variables be denoted by and their categorical outcomes be denoted by which can be equal to any integer in 
the log likelihood is then ln where is an indicator function which is equal to unity if yk and zero otherwise 
in the case of two explanatory variables this indicator function was defined as yk when and yk when this was convenient but not necessary 
again the optimum beta coefficients may be found by maximizing the log likelihood function generally using numerical methods 
possible method of solution is to set the derivatives of the log likelihood with respect to each beta coefficient equal to zero and solve for the beta coefficients where is the th coefficient of the vector and is the th explanatory variable of the th measurement 
once the beta coefficients have been estimated from the data we will be able to estimate the probability that any subsequent set of explanatory variables will result in any of the possible outcome categories 
interpretations there are various equivalent specifications and interpretations of logistic regression which fit into different types of more general models and allow different generalizations 
as generalized linear model the particular model used by logistic regression which distinguishes it from standard linear regression and from other types of regression analysis used for binary valued outcomes is the way the probability of particular outcome is linked to the linear predictor function logit logit ln written using the more compact notation described above this is logit logit ln this formulation expresses logistic regression as type of generalized linear model which predicts variables with various types of probability distributions by fitting linear predictor function of the above form to some sort of arbitrary transformation of the expected value of the variable 
the intuition for transforming using the logit function the natural log of the odds was explained above 
it also has the practical effect of converting the probability which is bounded to be between and to variable that ranges over thereby matching the potential range of the linear prediction function on the right side of the equation 
note that both the probabilities pi and the regression coefficients are unobserved and the means of determining them is not part of the model itself 
they are typically determined by some sort of optimization procedure 
maximum likelihood estimation that finds values that best fit the observed data 
that give the most accurate predictions for the data already observed usually subject to regularization conditions that seek to exclude unlikely values 
extremely large values for any of the regression coefficients 
the use of regularization condition is equivalent to doing maximum posteriori map estimation an extension of maximum likelihood 
regularization is most commonly done using squared regularizing function which is equivalent to placing zero mean gaussian prior distribution on the coefficients but other regularizers are also possible 
whether or not regularization is used it is usually not possible to find closed form solution instead an iterative numerical method must be used such as iteratively reweighted least squares irls or more commonly these days quasi newton method such as the bfgs method the interpretation of the parameter estimates is as the additive effect on the log of the odds for unit change in the the explanatory variable 
in the case of dichotomous explanatory variable for instance gender is the estimate of the odds of having the outcome for say males compared with females 
an equivalent formula uses the inverse of the logit function which is the logistic function 
logit the formula can also be written as probability distribution specifically using probability mass function pr as latent variable model the logistic model has an equivalent formulation as latent variable model 
this formulation is common in the theory of discrete choice models and makes it easier to extend to certain more complicated models with multiple correlated choices as well as to compare logistic regression to the closely related probit model 
imagine that for each trial there is continuous latent variable yi 
an unobserved random variable that is distributed as follows where logistic 
the latent variable can be written directly in terms of the linear predictor function and an additive random error variable that is distributed according to standard logistic distribution 
then yi can be viewed as an indicator for whether this latent variable is positive if 
the choice of modeling the error variable specifically with standard logistic distribution rather than general logistic distribution with the location and scale set to arbitrary values seems restrictive but in fact it is not 
it must be kept in mind that we can choose the regression coefficients ourselves and very often can use them to offset changes in the parameters of the error variable distribution 
for example logistic error variable distribution with non zero location parameter which sets the mean is equivalent to distribution with zero location parameter where has been added to the intercept coefficient 
both situations produce the same value for yi regardless of settings of explanatory variables 
similarly an arbitrary scale parameter is equivalent to setting the scale parameter to and then dividing all regression coefficients by in the latter case the resulting value of yi will be smaller by factor of than in the former case for all sets of explanatory variables but critically it will always remain on the same side of and hence lead to the same yi choice 
note that this predicts that the irrelevancy of the scale parameter may not carry over into more complex models where more than two choices are available 
it turns out that this formulation is exactly equivalent to the preceding one phrased in terms of the generalized linear model and without any latent variables 
this can be shown as follows using the fact that the cumulative distribution function cdf of the standard logistic distribution is the logistic function which is the inverse of the logit function 
pr logit then pr pr pr pr pr because the logistic distribution is symmetric logit see above this formulation which is standard in discrete choice models makes clear the relationship between logistic regression the logit model and the probit model which uses an error variable distributed according to standard normal distribution instead of standard logistic distribution 
both the logistic and normal distributions are symmetric with basic unimodal bell curve shape 
the only difference is that the logistic distribution has somewhat heavier tails which means that it is less sensitive to outlying data and hence somewhat more robust to model mis specifications or erroneous data 
two way latent variable model yet another formulation uses two separate latent variables where ev ev where ev is standard type extreme value distribution 
pr pr then if otherwise 
this model has separate latent variable and separate set of regression coefficients for each possible outcome of the dependent variable 
the reason for this separation is that it makes it easy to extend logistic regression to multi outcome categorical variables as in the multinomial logit model 
in such model it is natural to model each possible outcome using different set of regression coefficients 
it is also possible to motivate each of the separate latent variables as the theoretical utility associated with making the associated choice and thus motivate logistic regression in terms of utility theory 
in terms of utility theory rational actor always chooses the choice with the greatest associated utility 
this is the approach taken by economists when formulating discrete choice models because it both provides theoretically strong foundation and facilitates intuitions about the model which in turn makes it easy to consider various sorts of extensions 
see the example below 
the choice of the type extreme value distribution seems fairly arbitrary but it makes the mathematics work out and it may be possible to justify its use through rational choice theory 
it turns out that this model is equivalent to the previous model although this seems non obvious since there are now two sets of regression coefficients and error variables and the error variables have different distribution 
in fact this model reduces directly to the previous one with the following substitutions an intuition for this comes from the fact that since we choose based on the maximum of two values only their difference matters not the exact values and this effectively removes one degree of freedom 
another critical fact is that the difference of two type extreme value distributed variables is logistic distribution 
we can demonstrate the equivalent as follows pr pr pr pr pr pr pr substitute as above pr substitute as above pr now same as above model pr logit example as an example consider province level election where the choice is between right of center party left of center party and secessionist party 
the parti qu cois which wants quebec to secede from canada 
we would then use three latent variables one for each choice 
then in accordance with utility theory we can then interpret the latent variables as expressing the utility that results from making each of the choices 
we can also interpret the regression coefficients as indicating the strength that the associated factor 
explanatory variable has in contributing to the utility or more correctly the amount by which unit change in an explanatory variable changes the utility of given choice 
voter might expect that the right of center party would lower taxes especially on rich people 
this would give low income people no benefit 
no change in utility since they usually don pay taxes would cause moderate benefit 
somewhat more money or moderate utility increase for middle incoming people would cause significant benefits for high income people 
on the other hand the left of center party might be expected to raise taxes and offset it with increased welfare and other assistance for the lower and middle classes 
this would cause significant positive benefit to low income people perhaps weak benefit to middle income people and significant negative benefit to high income people 
finally the secessionist party would take no direct actions on the economy but simply secede 
low income or middle income voter might expect basically no clear utility gain or loss from this but high income voter might expect negative utility since he she is likely to own companies which will have harder time doing business in such an environment and probably lose money 
these intuitions can be expressed as follows this clearly shows that separate sets of regression coefficients need to exist for each choice 
when phrased in terms of utility this can be seen very easily 
different choices have different effects on net utility furthermore the effects vary in complex ways that depend on the characteristics of each individual so there need to be separate sets of coefficients for each characteristic not simply single extra per choice characteristic 
even though income is continuous variable its effect on utility is too complex for it to be treated as single variable 
either it needs to be directly split up into ranges or higher powers of income need to be added so that polynomial regression on income is effectively done 
as log linear model yet another formulation combines the two way latent variable formulation above with the original formulation higher up without latent variables and in the process provides link to one of the standard formulations of the multinomial logit 
here instead of writing the logit of the probabilities pi as linear predictor we separate the linear predictor into two one for each of the two outcomes ln pr ln ln pr ln two separate sets of regression coefficients have been introduced just as in the two way latent variable model and the two equations appear form that writes the logarithm of the associated probability as linear predictor with an extra term ln at the end 
this term as it turns out serves as the normalizing factor ensuring that the result is distribution 
this can be seen by exponentiating both sides pr pr in this form it is clear that the purpose of is to ensure that the resulting distribution over yi is in fact probability distribution 
it sums to this means that is simply the sum of all un normalized probabilities and by dividing each probability by the probabilities become normalized 
that is and the resulting equations are pr pr or generally pr this shows clearly how to generalize this formulation to more than two outcomes as in multinomial logit 
note that this general formulation is exactly the softmax function as in pr softmax 
in order to prove that this is equivalent to the previous model note that the above model is overspecified in that pr and pr cannot be independently specified rather pr pr so knowing one automatically determines the other 
as result the model is nonidentifiable in that multiple combinations of and will produce the same probabilities for all possible explanatory variables 
in fact it can be seen that adding any constant vector to both of them will produce the same probabilities pr as result we can simplify matters and restore identifiability by picking an arbitrary value for one of the two vectors 
we choose to set then and so pr which shows that this formulation is indeed equivalent to the previous formulation 
as in the two way latent variable formulation any settings where will produce equivalent results 
note that most treatments of the multinomial logit model start out either by extending the log linear formulation presented here or the two way latent variable formulation presented above since both clearly show the way that the model could be extended to multi way outcomes 
in general the presentation with latent variables is more common in econometrics and political science where discrete choice models and utility theory reign while the log linear formulation here is more common in computer science 
machine learning and natural language processing 
as single layer perceptron the model has an equivalent formulation 
this functional form is commonly called single layer perceptron or single layer artificial neural network 
single layer neural network computes continuous output instead of step function 
the derivative of pi with respect to xk is computed from the general form where is an analytic function in with this choice the single layer neural network is identical to the logistic regression model 
this function has continuous derivative which allows it to be used in backpropagation 
this function is also preferred because its derivative is easily calculated 
in terms of binomial data closely related model assumes that each is associated not with single bernoulli trial but with ni independent identically distributed trials where the observation yi is the number of successes observed the sum of the individual bernoulli distributed random variables and hence follows binomial distribution bin for an example of this distribution is the fraction of seeds pi that germinate after ni are planted 
in terms of expected values this model is expressed as follows so that logit logit ln or equivalently pr this model can be fit using the same sorts of methods as the above more basic model 
model fitting maximum likelihood estimation mle the regression coefficients are usually estimated using maximum likelihood estimation 
unlike linear regression with normally distributed residuals it is not possible to find closed form expression for the coefficient values that maximize the likelihood function so that an iterative process must be used instead for example newton method 
this process begins with tentative solution revises it slightly to see if it can be improved and repeats this revision until no more improvement is made at which point the process is said to have converged in some instances the model may not reach convergence 
non convergence of model indicates that the coefficients are not meaningful because the iterative process was unable to find appropriate solutions 
failure to converge may occur for number of reasons having large ratio of predictors to cases multicollinearity sparseness or complete separation 
having large ratio of variables to cases results in an overly conservative wald statistic discussed below and can lead to non convergence 
regularized logistic regression is specifically intended to be used in this situation 
multicollinearity refers to unacceptably high correlations between predictors 
as multicollinearity increases coefficients remain unbiased but standard errors increase and the likelihood of model convergence decreases 
to detect multicollinearity amongst the predictors one can conduct linear regression analysis with the predictors of interest for the sole purpose of examining the tolerance statistic used to assess whether multicollinearity is unacceptably high 
sparseness in the data refers to having large proportion of empty cells cells with zero counts 
zero cell counts are particularly problematic with categorical predictors 
with continuous predictors the model can infer values for the zero cell counts but this is not the case with categorical predictors 
the model will not converge with zero cell counts for categorical predictors because the natural logarithm of zero is an undefined value so that the final solution to the model cannot be reached 
to remedy this problem researchers may collapse categories in theoretically meaningful way or add constant to all cells 
another numerical problem that may lead to lack of convergence is complete separation which refers to the instance in which the predictors perfectly predict the criterion all cases are accurately classified and the likelihood maximized with infinite coefficients 
in such instances one should re examine the data as there may be some kind of error 
one can also take semi parametric or non parametric approaches via local likelihood or nonparametric quasi likelihood methods which avoid assumptions of parametric form for the index function and is robust to the choice of the link function probit or logit 
iteratively reweighted least squares irls binary logistic regression or can for example be calculated using iteratively reweighted least squares irls which is equivalent to maximizing the log likelihood of bernoulli distributed process using newton method 
if the problem is written in vector matrix form with parameters explanatory variables and expected value of the bernoulli distribution the parameters can be found using the following iterative algorithm where diag is diagonal weighting matrix the vector of expected values the regressor matrix and the vector of response variables 
more details can be found in the literature 
bayesian in bayesian statistics context prior distributions are normally placed on the regression coefficients for example in the form of gaussian distributions 
there is no conjugate prior of the likelihood function in logistic regression 
when bayesian inference was performed analytically this made the posterior distribution difficult to calculate except in very low dimensions 
now though automatic software such as openbugs jags pymc stan or turing jl allows these posteriors to be computed using simulation so lack of conjugacy is not concern 
however when the sample size or the number of parameters is large full bayesian simulation can be slow and people often use approximate methods such as variational bayesian methods and expectation propagation 
rule of ten widely used rule of thumb the one in ten rule states that logistic regression models give stable values for the explanatory variables if based on minimum of about events per explanatory variable epv where event denotes the cases belonging to the less frequent category in the dependent variable 
thus study designed to use explanatory variables for an event 
myocardial infarction expected to occur in proportion of participants in the study will require total of participants 
however there is considerable debate about the reliability of this rule which is based on simulation studies and lacks secure theoretical underpinning 
according to some authors the rule is overly conservative in some circumstances with the authors stating if we somewhat subjectively regard confidence interval coverage less than percent type error greater than percent or relative bias greater than percent as problematic our results indicate that problems are fairly frequent with epv uncommon with epv and still observed with epv 
the worst instances of each problem were not severe with epv and usually comparable to those with epv others have found results that are not consistent with the above using different criteria 
useful criterion is whether the fitted model will be expected to achieve the same predictive discrimination in new sample as it appeared to achieve in the model development sample 
for that criterion events per candidate variable may be required 
also one can argue that observations are needed only to estimate the model intercept precisely enough that the margin of error in predicted probabilities is with confidence level 
error and significance of fit deviance and likelihood ratio test simple case in any fitting procedure the addition of another fitting parameter to model 
the beta parameters in logistic regression model will almost always improve the ability of the model to predict the measured outcomes 
this will be true even if the additional term has no predictive value since the model will simply be overfitting to the noise in the data 
the question arises as to whether the improvement gained by the addition of another fitting parameter is significant enough to recommend the inclusion of the additional term or whether the improvement is simply that which may be expected from overfitting 
in short for logistic regression statistic known as the deviance is defined which is measure of the error between the logistic model fit and the outcome data 
in the limit of large number of data points the deviance is chi squared distributed which allows chi squared test to be implemented in order to determine the significance of the explanatory variables 
linear regression and logistic regression have many similarities 
for example in simple linear regression set of data points xk yk are fitted to proposed model function of the form the fit is obtained by choosing the parameters which minimize the sum of the squares of the residuals the squared error term for each data point the minimum value which constitutes the fit will be denoted by the idea of null model may be introduced in which it is assumed that the variable is of no use in predicting the yk outcomes the data points are fitted to null model function of the form with squared error term the fitting process consists of choosing value of which minimizes of the fit to the null model denoted by where the subscript denotes the null model 
it is seen that the null model is optimized by where is the mean of the yk values and the optimized is which is proportional to the square of the uncorrected sample standard deviation of the yk data points 
we can imagine case where the yk data points are randomly assigned to the various xk and then fitted using the proposed model 
specifically we can consider the fits of the proposed model to every permutation of the yk outcomes 
it can be shown that the optimized error of any of these fits will never be less than the optimum error of the null model and that the difference between these minimum error will follow chi squared distribution distribution with degrees of freedom equal those of the proposed model minus those of the null model which in this case will be 
using the chi squared test we may then estimate how many of these permuted sets of yk will yield an minimum error less than or equal to the minimum error using the original yk and so we can estimate how significant an improvement is given by the inclusion of the variable in the proposed model 
for logistic regression the measure of goodness of fit is the likelihood function or its logarithm the log likelihood the likelihood function is analogous to the in the linear regression case except that the likelihood is maximized rather than minimized 
denote the maximized log likelihood of the proposed model by 
in the case of simple binary logistic regression the set of data points are fitted in probabilistic sense to function of the form where is the probability that the log odds are given by and the log likelihood is ln ln for the null model the probability that is given by the log odds for the null model are given by and the log likelihood is ln ln since we have at the maximum of the maximum log likelihood for the null model is ln ln the optimum is ln where is again the mean of the yk values 
again we can conceptually consider the fit of the proposed model to every permutation of the yk and it can be shown that the maximum log likelihood of these permutation fits will never be smaller than that of the null model also as an analog to the error of the linear regression case we may define the deviance of logistic regression fit as ln which will always be positive or zero 
the reason for this choice is that not only is the deviance good measure of the goodness of fit it is also approximately chi squared distributed with the approximation improving as the number of data points increases becoming exactly chi square distributed in the limit of an infinite number of data points 
as in the case of linear regression we may use this fact to estimate the probability that random set of data points will give better fit than the fit obtained by the proposed model and so have an estimate how significantly the model is improved by including the xk data points in the proposed model 
for the simple model of student test scores described above the maximum value of the log likelihood of the null model is the maximum value of the log likelihood for the simple model is so that the deviance is using the chi squared test of significance the integral of the chi squared distribution with one degree of freedom from to infinity is equal to this effectively means that about out of fits to random yk can be expected to have better fit smaller deviance than the given yk and so we can conclude that the inclusion of the variable and data in the proposed model is very significant improvement over the null model 
in other words we reject the null hypothesis with confidence 
goodness of fit summary goodness of fit in linear regression models is generally measured using 
since this has no direct analog in logistic regression various methods ch including the following can be used instead 
deviance and likelihood ratio tests in linear regression analysis one is concerned with partitioning variance via the sum of squares calculations variance in the criterion is essentially divided into variance accounted for by the predictors and residual variance 
in logistic regression analysis deviance is used in lieu of sum of squares calculations 
deviance is analogous to the sum of squares calculations in linear regression and is measure of the lack of fit to the data in logistic regression model 
when saturated model is available model with theoretically perfect fit deviance is calculated by comparing given model with the saturated model 
this computation gives the likelihood ratio test ln likelihood of the fitted model likelihood of the saturated model 
in the above equation represents the deviance and ln represents the natural logarithm 
the log of this likelihood ratio the ratio of the fitted model to the saturated model will produce negative value hence the need for negative sign 
can be shown to follow an approximate chi squared distribution 
smaller values indicate better fit as the fitted model deviates less from the saturated model 
when assessed upon chi square distribution nonsignificant chi square values indicate very little unexplained variance and thus good model fit 
conversely significant chi square value indicates that significant amount of the variance is unexplained 
when the saturated model is not available common case deviance is calculated simply as log likelihood of the fitted model and the reference to the saturated model log likelihood can be removed from all that follows without harm 
two measures of deviance are particularly important in logistic regression null deviance and model deviance 
the null deviance represents the difference between model with only the intercept which means no predictors and the saturated model 
the model deviance represents the difference between model with at least one predictor and the saturated model 
in this respect the null model provides baseline upon which to compare predictor models 
given that deviance is measure of the difference between given model and the saturated model smaller values indicate better fit 
thus to assess the contribution of predictor or set of predictors one can subtract the model deviance from the null deviance and assess the difference on chi square distribution with degrees of freedom equal to the difference in the number of parameters estimated 
let null ln likelihood of null model likelihood of the saturated model fitted ln likelihood of fitted model likelihood of the saturated model 
then the difference of both is null fitted ln likelihood of null model likelihood of the saturated model ln likelihood of fitted model likelihood of the saturated model ln likelihood of null model likelihood of the saturated model likelihood of fitted model likelihood of the saturated model ln likelihood of the null model likelihood of fitted model 
if the model deviance is significantly smaller than the null deviance then one can conclude that the predictor or set of predictors significantly improve the model fit 
this is analogous to the test used in linear regression analysis to assess the significance of prediction 
pseudo squared in linear regression the squared multiple correlation is used to assess goodness of fit as it represents the proportion of variance in the criterion that is explained by the predictors 
in logistic regression analysis there is no agreed upon analogous measure but there are several competing measures each with limitations four of the most commonly used indices and one less commonly used one are examined on this page likelihood ratio cox and snell cs nagelkerke mcfadden mcf tjur hosmer lemeshow test the hosmer lemeshow test uses test statistic that asymptotically follows distribution to assess whether or not the observed event rates match expected event rates in subgroups of the model population 
this test is considered to be obsolete by some statisticians because of its dependence on arbitrary binning of predicted probabilities and relative low power 
coefficient significance after fitting the model it is likely that researchers will want to examine the contribution of individual predictors 
to do so they will want to examine the regression coefficients 
in linear regression the regression coefficients represent the change in the criterion for each unit change in the predictor 
in logistic regression however the regression coefficients represent the change in the logit for each unit change in the predictor 
given that the logit is not intuitive researchers are likely to focus on predictor effect on the exponential function of the regression coefficient the odds ratio see definition 
in linear regression the significance of regression coefficient is assessed by computing test 
in logistic regression there are several different tests designed to assess the significance of an individual predictor most notably the likelihood ratio test and the wald statistic 
likelihood ratio test the likelihood ratio test discussed above to assess model fit is also the recommended procedure to assess the contribution of individual predictors to given model 
in the case of single predictor model one simply compares the deviance of the predictor model with that of the null model on chi square distribution with single degree of freedom 
if the predictor model has significantly smaller deviance 
chi square using the difference in degrees of freedom of the two models then one can conclude that there is significant association between the predictor and the outcome 
although some common statistical packages 
spss do provide likelihood ratio test statistics without this computationally intensive test it would be more difficult to assess the contribution of individual predictors in the multiple logistic regression case 
to assess the contribution of individual predictors one can enter the predictors hierarchically comparing each new model with the previous to determine the contribution of each predictor 
there is some debate among statisticians about the appropriateness of so called stepwise procedures 
the fear is that they may not preserve nominal statistical properties and may become misleading 
wald statistic alternatively when assessing the contribution of individual predictors in given model one may examine the significance of the wald statistic 
the wald statistic analogous to the test in linear regression is used to assess the significance of coefficients 
the wald statistic is the ratio of the square of the regression coefficient to the square of the standard error of the coefficient and is asymptotically distributed as chi square distribution 
although several statistical packages spss sas report the wald statistic to assess the contribution of individual predictors the wald statistic has limitations 
when the regression coefficient is large the standard error of the regression coefficient also tends to be larger increasing the probability of type ii error 
the wald statistic also tends to be biased when data are sparse 
case control sampling suppose cases are rare 
then we might wish to sample them more frequently than their prevalence in the population 
for example suppose there is disease that affects person in and to collect our data we need to do complete physical 
it may be too expensive to do thousands of physicals of healthy people in order to obtain data for only few diseased individuals 
thus we may evaluate more diseased individuals perhaps all of the rare outcomes 
this is also retrospective sampling or equivalently it is called unbalanced data 
as rule of thumb sampling controls at rate of five times the number of cases will produce sufficient control data logistic regression is unique in that it may be estimated on unbalanced data rather than randomly sampled data and still yield correct coefficient estimates of the effects of each independent variable on the outcome 
that is to say if we form logistic model from such data if the model is correct in the general population the parameters are all correct except for we can correct if we know the true prevalence as follows log log where is the true prevalence and is the prevalence in the sample 
discussion like other forms of regression analysis logistic regression makes use of one or more predictor variables that may be either continuous or categorical 
unlike ordinary linear regression however logistic regression is used for predicting dependent variables that take membership in one of limited number of categories treating the dependent variable in the binomial case as the outcome of bernoulli trial rather than continuous outcome 
given this difference the assumptions of linear regression are violated 
in particular the residuals cannot be normally distributed 
in addition linear regression may make nonsensical predictions for binary dependent variable 
what is needed is way to convert binary variable into continuous one that can take on any real value negative or positive 
to do that binomial logistic regression first calculates the odds of the event happening for different levels of each independent variable and then takes its logarithm to create continuous criterion as transformed version of the dependent variable 
the logarithm of the odds is the logit of the probability the logit is defined as follows although the dependent variable in logistic regression is bernoulli the logit is on an unrestricted scale 
the logit function is the link function in this kind of generalized linear model 
is the bernoulli distributed response variable and is the predictor variable the values are the linear parameters 
the logit of the probability of success is then fitted to the predictors 
the predicted value of the logit is converted back into predicted odds via the inverse of the natural logarithm the exponential function 
thus although the observed dependent variable in binary logistic regression is or variable the logistic regression estimates the odds as continuous variable that the dependent variable is success 
in some applications the odds are all that is needed 
in others specific yes or no prediction is needed for whether the dependent variable is or is not success this categorical prediction can be based on the computed odds of success with predicted odds above some chosen cutoff value being translated into prediction of success 
maximum entropy of all the functional forms used for estimating the probabilities of particular categorical outcome which optimize the fit by maximizing the likelihood function 
probit regression poisson regression etc 
the logistic regression solution is unique in that it is maximum entropy solution 
this is case of general property an exponential family of distributions maximizes entropy given an expected value 
in the case of the logistic model the logistic function is the natural parameter of the bernoulli distribution it is in canonical form and the logistic function is the canonical link function while other sigmoid functions are non canonical link functions this underlies its mathematical elegance and ease of optimization 
see exponential family maximum entropy derivation for details 
proof in order to show this we use the method of lagrange multipliers 
the lagrangian is equal to the entropy plus the sum of the products of lagrange multipliers times various constraint expressions 
the general multinomial case will be considered since the proof is not made that much simpler by considering simpler cases 
equating the derivative of the lagrangian with respect to the various probabilities to zero yields functional form for those probabilities which corresponds to those used in logistic regression as in the above section on multinomial logistic regression we will consider explanatory variables denoted and which include there will be total of data points indexed by and the data points are given by and the xmk will also be represented as an dimensional vector 
there will be possible values of the categorical variable ranging from to let pn be the probability given explanatory variable vector that the outcome will be define which is the probability that for the th measurement the categorical outcome is the lagrangian will be expressed as function of the probabilities pnk and will minimized by equating the derivatives of the lagrangian with respect to these probabilities to zero 
an important point is that the probabilities are treated equally and the fact that they sum to unity is part of the lagrangian formulation rather than being assumed from the beginning 
the first contribution to the lagrangian is the entropy ln the log likelihood is ln assuming the multinomial logistic function the derivative of the log likelihood with respect the beta coefficients was found to be very important point here is that this expression is remarkably not an explicit function of the beta coefficients 
it is only function of the probabilities pnk and the data 
rather than being specific to the assumed multinomial logistic case it is taken to be general statement of the condition at which the log likelihood is maximized and makes no reference to the functional form of pnk 
there are then fitting constraints and the fitting constraint term in the lagrangian is then where the nm are the appropriate lagrange multipliers 
there are normalization constraints which may be written so that the normalization term in the lagrangian is where the are the appropriate lagrange multipliers 
the lagrangian is then the sum of the above three terms setting the derivative of the lagrangian with respect to one of the probabilities to zero yields ln using the more condensed vector notation and dropping the primes on the and indices and then solving for yields where imposing the normalization constraint we can solve for the zk and write the probabilities as the are not all independent 
we can add any constant dimensional vector to each of the without changing the value of the probabilities so that there are only rather than independent in the multinomial logistic regression section above the was subtracted from each which set the exponential term involving to unity and the beta coefficients were given by 
other approaches in machine learning applications where logistic regression is used for binary classification the mle minimises the cross entropy loss function 
logistic regression is an important machine learning algorithm 
the goal is to model the probability of random variable being or given experimental data consider generalized linear model function parameterized by pr therefore pr and since we see that pr is given by pr 
we now calculate the likelihood function assuming that all the observations in the sample are independently bernoulli distributed pr pr typically the log likelihood is maximized log log pr which is maximized using optimization techniques such as gradient descent 
assuming the pairs are drawn uniformly from the underlying distribution then in the limit of large lim log pr pr log pr pr log pr pr log pr kl where is the conditional entropy and kl is the kullback leibler divergence 
this leads to the intuition that by maximizing the log likelihood of model you are minimizing the kl divergence of your model from the maximal entropy distribution 
intuitively searching for the model that makes the fewest assumptions in its parameters 
comparison with linear regression logistic regression can be seen as special case of the generalized linear model and thus analogous to linear regression 
the model of logistic regression however is based on quite different assumptions about the relationship between the dependent and independent variables from those of linear regression 
in particular the key differences between these two models can be seen in the following two features of logistic regression 
first the conditional distribution is bernoulli distribution rather than gaussian distribution because the dependent variable is binary 
second the predicted values are probabilities and are therefore restricted to through the logistic distribution function because logistic regression predicts the probability of particular outcomes rather than the outcomes themselves 
alternatives common alternative to the logistic model logit model is the probit model as the related names suggest 
from the perspective of generalized linear models these differ in the choice of link function the logistic model uses the logit function inverse logistic function while the probit model uses the probit function inverse error function 
equivalently in the latent variable interpretations of these two methods the first assumes standard logistic distribution of errors and the second standard normal distribution of errors 
other sigmoid functions or error distributions can be used instead 
logistic regression is an alternative to fisher method linear discriminant analysis 
if the assumptions of linear discriminant analysis hold the conditioning can be reversed to produce logistic regression 
the converse is not true however because logistic regression does not require the multivariate normal assumption of discriminant analysis the assumption of linear predictor effects can easily be relaxed using techniques such as spline functions 
history detailed history of the logistic regression is given in cramer 
the logistic function was developed as model of population growth and named logistic by pierre fran ois verhulst in the and under the guidance of adolphe quetelet see logistic function history for details 
in his earliest paper verhulst did not specify how he fit the curves to the data 
in his more detailed paper verhulst determined the three parameters of the model by making the curve pass through three observed points which yielded poor predictions the logistic function was independently developed in chemistry as model of autocatalysis wilhelm ostwald 
an autocatalytic reaction is one in which one of the products is itself catalyst for the same reaction while the supply of one of the reactants is fixed 
this naturally gives rise to the logistic equation for the same reason as population growth the reaction is self reinforcing but constrained 
the logistic function was independently rediscovered as model of population growth in by raymond pearl and lowell reed published as pearl reed which led to its use in modern statistics 
they were initially unaware of verhulst work and presumably learned about it from gustave du pasquier but they gave him little credit and did not adopt his terminology 
verhulst priority was acknowledged and the term logistic revived by udny yule in and has been followed since 
pearl and reed first applied the model to the population of the united states and also initially fitted the curve by making it pass through three points as with verhulst this again yielded poor results in the the probit model was developed and systematized by chester ittner bliss who coined the term probit in bliss and by john gaddum in gaddum and the model fit by maximum likelihood estimation by ronald fisher in fisher as an addendum to bliss work 
the probit model was principally used in bioassay and had been preceded by earlier work dating to see probit model history 
the probit model influenced the subsequent development of the logit model and these models competed with each other the logistic model was likely first used as an alternative to the probit model in bioassay by edwin bidwell wilson and his student jane worcester in wilson worcester 
however the development of the logistic model as general alternative to the probit model was principally due to the work of joseph berkson over many decades beginning in berkson where he coined logit by analogy with probit and continuing through berkson and following years 
the logit model was initially dismissed as inferior to the probit model but gradually achieved an equal footing with the logit particularly between and by the logit model achieved parity with the probit model in use in statistics journals and thereafter surpassed it 
this relative popularity was due to the adoption of the logit outside of bioassay rather than displacing the probit within bioassay and its informal use in practice the logit popularity is credited to the logit model computational simplicity mathematical properties and generality allowing its use in varied fields various refinements occurred during that time notably by david cox as in cox the multinomial logit model was introduced independently in cox and thiel which greatly increased the scope of application and the popularity of the logit model 
in daniel mcfadden linked the multinomial logit to the theory of discrete choice specifically luce choice axiom showing that the multinomial logit followed from the assumption of independence of irrelevant alternatives and interpreting odds of alternatives as relative preferences this gave theoretical foundation for the logistic regression 
extensions there are large numbers of extensions multinomial logistic regression or multinomial logit handles the case of multi way categorical dependent variable with unordered values also called classification 
note that the general case of having dependent variables with more than two values is termed polytomous regression 
ordered logistic regression or ordered logit handles ordinal dependent variables ordered values 
mixed logit is an extension of multinomial logit that allows for correlations among the choices of the dependent variable 
an extension of the logistic model to sets of interdependent variables is the conditional random field 
conditional logistic regression handles matched or stratified data when the strata are small 
it is mostly used in the analysis of observational studies 
software most statistical software can do binary logistic regression 
spss for basic logistic regression 
stata sas proc logistic for basic logistic regression 
proc catmod when all the variables are categorical 
proc glimmix for multilevel model logistic regression 
glm in the stats package using family binomial lrm in the rms package glmnet package for an efficient implementation regularized logistic regression lmer for mixed effects logistic regression rfast package command gm logistic for fast and heavy calculations involving large scale data 
arm package for bayesian logistic regression python logit in the statsmodels module 
logisticregression in the scikit learn module 
logisticregressor in the tensorflow module 
full example of logistic regression in the theano tutorial bayesian logistic regression with ard prior code tutorial variational bayes logistic regression with ard prior code tutorial bayesian logistic regression code tutorial ncss logistic regression in ncss matlab mnrfit in the statistics and machine learning toolbox with incorrect coded as instead of fminunc fmincon fitglm mnrfit fitclinear mle can all do logistic regression 
java jvm liblinear apache flink apache spark sparkml supports logistic regression fpga logistic regresesion ip core in hls for fpga notably microsoft excel statistics extension package does not include it 
see also logistic function discrete choice jarrow turnbull model limited dependent variable multinomial logit model ordered logit hosmer lemeshow test brier score mlpack contains implementation of logistic regression local case control sampling logistic model tree references further reading external links media related to logistic regression at wikimedia commons econometrics lecture topic logit model on youtube by mark thoma logistic regression tutorial mlelr software in for teaching purposes
the circle of fifths text table shows the number of flats or sharps in each of the diatonic musical scales and keys 
both major and minor keys have no flats or sharps 
in the table minor keys are written with lowercase letters for brevity 
however in common guitar tabs notation minor key is designated with lowercase 
for example minor is am and sharp minor is 
the small interval between equivalent notes such as sharp and flat is the pythagorean comma 
minor scales start with major scales start with 
see also circle of fifths key signature musical notation notes
in mathematics variable from latin variabilis changeable is symbol and placeholder for any mathematical object 
in particular variable may represent number vector matrix function the argument of function set or an element of set algebraic computations with variables as if they were explicit numbers solve range of problems in single computation 
for example the quadratic formula solves any quadratic equation by substituting the numeric values of the coefficients of that equation for the variables that represent them in the quadratic formula 
in mathematical logic variable is either symbol representing an unspecified term of the theory meta variable or basic object of the theory that is manipulated without referring to its possible intuitive interpretation 
history in ancient works such as euclid elements single letters refer to geometric points and shapes 
in the th century brahmagupta used different colours to represent the unknowns in algebraic equations in the br hmasphu asiddh nta 
one section of this book is called equations of several colours at the end of the th century fran ois vi te introduced the idea of representing known and unknown numbers by letters nowadays called variables and the idea of computing with them as if they were numbers in order to obtain the result by simple replacement 
vi te convention was to use consonants for known values and vowels for unknowns in ren descartes invented the convention of representing unknowns in equations by and and knowns by and 
contrarily to vi te convention descartes is still commonly in use 
the history of the letter in math was discussed in scientific american article starting in the isaac newton and gottfried wilhelm leibniz independently developed the infinitesimal calculus which essentially consists of studying how an infinitesimal variation of variable quantity induces corresponding variation of another quantity which is function of the first variable 
almost century later leonhard euler fixed the terminology of infinitesimal calculus and introduced the notation for function its variable and its value until the end of the th century the word variable referred almost exclusively to the arguments and the values of functions 
in the second half of the th century it appeared that the foundation of infinitesimal calculus was not formalized enough to deal with apparent paradoxes such as nowhere differentiable continuous function 
to solve this problem karl weierstrass introduced new formalism consisting of replacing the intuitive notion of limit by formal definition 
the older notion of limit was when the variable varies and tends toward then tends toward without any accurate definition of tends 
weierstrass replaced this sentence by the formula in which none of the five variables is considered as varying 
this static formulation led to the modern notion of variable which is simply symbol representing mathematical object that either is unknown or may be replaced by any element of given set the set of real numbers 
notation variables are generally denoted by single letter most often from the latin alphabet and less often from the greek which may be lowercase or capitalized 
the letter may be followed by subscript number as in another variable xi word or abbreviation of word xtotal or mathematical expression 
under the influence of computer science some variable names in pure mathematics consist of several letters and digits 
following ren descartes letters at the beginning of the alphabet such as are commonly used for known values and parameters and letters at the end of the alphabet such as are commonly used for unknowns and variables of functions 
in printed mathematics the norm is to set variables and constants in an italic typeface for example general quadratic function is conventionally written as where and are parameters also called constants because they are constant functions while is the variable of the function 
more explicit way to denote this function is which clarifies the function argument status of and the constant status of and since occurs in term that is constant function of it is called the constant term specific branches and applications of mathematics have specific naming conventions for variables 
variables with similar roles or meanings are often assigned consecutive letters or the same letter with different subscripts 
for example the three axes in coordinate space are conventionally called and in physics the names of variables are largely determined by the physical quantity they describe but various naming conventions exist 
convention often followed in probability and statistics is to use for the names of random variables keeping for variables representing corresponding better defined values 
specific kinds of variables it is common for variables to play different roles in the same mathematical formula and names or qualifiers have been introduced to distinguish them 
for example the general cubic equation is interpreted as having five variables four which are taken to be given numbers and the fifth variable is understood to be an unknown number 
to distinguish them the variable is called an unknown and the other variables are called parameters or coefficients or sometimes constants although this last terminology is incorrect for an equation and should be reserved for the function defined by the left hand side of this equation 
in the context of functions the term variable refers commonly to the arguments of the functions 
this is typically the case in sentences like function of real variable is the variable of the function is function of the variable meaning that the argument of the function is referred to by the variable 
in the same context variables that are independent of define constant functions and are therefore called constant 
for example constant of integration is an arbitrary constant function that is added to particular antiderivative to obtain the other antiderivatives 
because the strong relationship between polynomials and polynomial function the term constant is often used to denote the coefficients of polynomial which are constant functions of the indeterminates 
this use of constant as an abbreviation of constant function must be distinguished from the normal meaning of the word in mathematics 
constant or mathematical constant is well and unambiguously defined number or other mathematical object as for example the numbers and the identity element of group 
since variable may represent any mathematical object letter that represents constant is often called variable 
this is in particular the case of and even when they represents euler number and other specific names for variables are an unknown is variable in an equation which has to be solved for 
an indeterminate is symbol commonly called variable that appears in polynomial or formal power series 
formally speaking an indeterminate is not variable but constant in the polynomial ring or the ring of formal power series 
however because of the strong relationship between polynomials or power series and the functions that they define many authors consider indeterminates as special kind of variables 
parameter is quantity usually number which is part of the input of problem and remains constant during the whole solution of this problem 
for example in mechanics the mass and the size of solid body are parameters for the study of its movement 
in computer science parameter has different meaning and denotes an argument of function 
free variables and bound variables random variable is kind of variable that is used in probability theory and its applications all these denominations of variables are of semantic nature and the way of computing with them syntax is the same for all 
dependent and independent variables in calculus and its application to physics and other sciences it is rather common to consider variable say whose possible values depend on the value of another variable say in mathematical terms the dependent variable represents the value of function of to simplify formulas it is often useful to use the same symbol for the dependent variable and the function mapping onto for example the state of physical system depends on measurable quantities such as the pressure the temperature the spatial position and all these quantities vary when the system evolves that is they are function of the time 
in the formulas describing the system these quantities are represented by variables which are dependent on the time and thus considered implicitly as functions of the time 
therefore in formula dependent variable is variable that is implicitly function of another or several other variables 
an independent variable is variable that is not dependent the property of variable to be dependent or independent depends often of the point of view and is not intrinsic 
for example in the notation the three variables may be all independent and the notation represents function of three variables 
on the other hand if and depend on are dependent variables then the notation represents function of the single independent variable 
examples if one defines function from the real numbers to the real numbers by sin then is variable standing for the argument of the function being defined which can be any real number 
in the identity the variable is summation variable which designates in turn each of the integers it is also called index because its variation is over discrete set of values while is parameter it does not vary within the formula 
in the theory of polynomials polynomial of degree is generally denoted as ax bx where and are called coefficients they are assumed to be fixed parameters of the problem considered while is called variable 
when studying this polynomial for its polynomial function this stands for the function argument 
when studying the polynomial as an object in itself is taken to be an indeterminate and would often be written with capital letter instead to indicate this status 
example the ideal gas law consider the equation describing the ideal gas law this equation would generally be interpreted to have four variables and one constant 
the constant is the boltzmann constant 
one of the variables the number of particles is positive integer and therefore discrete variable while the other three and for pressure volume and temperature are continuous variables 
one could rearrange this equation to obtain as function of the other variables then as function of the other variables is the dependent variable while its arguments and are independent variables 
one could approach this function more formally and think about its domain and range in function notation here is function however in an experiment in order to determine the dependence of pressure on single one of the independent variables it is necessary to fix all but one of the variables say this gives function where now and are also regarded as constants 
mathematically this constitutes partial application of the earlier function this illustrates how independent variables and constants are largely dependent on the point of view taken 
one could even regard as variable to obtain function moduli spaces considering constants and variables can lead to the concept of moduli spaces 
for illustration consider the equation for parabola where and are all considered to be real 
the set of points in the plane satisfying this equation trace out the graph of parabola 
here and are regarded as constants which specify the parabola while and are variables 
then instead regarding and as variables we observe that each set of tuples corresponds to different parabola 
that is they specify coordinates on the space of parabolas this is known as moduli space of parabolas 
conventional variable names sometimes extended to for parameters or coefficients for situations where distinct letters are inconvenient ai or ui for the th term of sequence or the th coefficient of series for euler number for functions as in for the imaginary unit sometimes or for varying integers or indices in an indexed family or unit vectors and for the length and width of figure also for line or in number theory for prime number not equal to with as second choice for fixed integer such as count of objects or the degree of an equation for prime number or probability for prime power or quotient for radius remainder or correlation coefficient for time for the three cartesian coordinates of point in euclidean geometry or the corresponding axes for complex number or in statistics normal random variable for angle measures with as second choice for an arbitrarily small positive number for an eigenvalue capital sigma for sum or lowercase sigma in statistics for the standard deviation for mean see also lambda calculus observable variable physical constant propositional variable references bibliography
in statistics econometrics epidemiology and related disciplines the method of instrumental variables iv is used to estimate causal relationships when controlled experiments are not feasible or when treatment is not successfully delivered to every unit in randomized experiment 
intuitively ivs are used when an explanatory variable of interest is correlated with the error term in which case ordinary least squares and anova give biased results 
valid instrument induces changes in the explanatory variable but has no independent effect on the dependent variable allowing researcher to uncover the causal effect of the explanatory variable on the dependent variable 
instrumental variable methods allow for consistent estimation when the explanatory variables covariates are correlated with the error terms in regression model 
such correlation may occur when changes in the dependent variable change the value of at least one of the covariates reverse causation there are omitted variables that affect both the dependent and independent variables or the covariates are subject to non random measurement error explanatory variables that suffer from one or more of these issues in the context of regression are sometimes referred to as endogenous 
in this situation ordinary least squares produces biased and inconsistent estimates 
however if an instrument is available consistent estimates may still be obtained 
an instrument is variable that does not itself belong in the explanatory equation but is correlated with the endogenous explanatory variables conditionally on the value of other covariates 
in linear models there are two main requirements for using ivs the instrument must be correlated with the endogenous explanatory variables conditionally on the other covariates 
if this correlation is strong then the instrument is said to have strong first stage 
weak correlation may provide misleading inferences about parameter estimates and standard errors 
the instrument cannot be correlated with the error term in the explanatory equation conditionally on the other covariates 
in other words the instrument cannot suffer from the same problem as the original predicting variable 
if this condition is met then the instrument is said to satisfy the exclusion restriction 
history first use of an instrument variable occurred in book by philip wright best known for his excellent description of the production transport and sale of vegetable and animal oils in the early in the united states while in olav reiers applied the same approach in the context of errors in variables models in his dissertation giving the method its name wright attempted to determine the supply and demand for butter using panel data on prices and quantities sold in the united states 
the idea was that regression analysis could produce demand or supply curve because they are formed by the path between prices and quantities demanded or supplied 
the problem was that the observational data did not form demand or supply curve as such but rather cloud of point observations that took different shapes under varying market conditions 
it seemed that making deductions from the data remained elusive 
the problem was that price affected both supply and demand so that function describing only one of the two could not be constructed directly from the observational data 
wright correctly concluded that he needed variable that correlated with either demand or supply but not both that is an instrumental variable 
after much deliberation wright decided to use regional rainfall as his instrumental variable he concluded that rainfall affected grass production and hence milk production and ultimately butter supply but not butter demand 
in this way he was able to construct regression equation with only the instrumental variable of price and supply 
theory while the ideas behind iv extend to broad class of models very common context for iv is in linear regression 
traditionally an instrumental variable is defined as variable that is correlated with the independent variable and uncorrelated with the error term in the linear equation is vector 
is matrix usually with column of ones and perhaps with additional columns for other covariates 
consider how an instrument allows to be recovered 
recall that ols solves for such that cov when we minimize the sum of squared errors min the first order condition is exactly 
if the true model is believed to have cov due to any of the reasons listed above for example if there is an omitted variable which affects both and separately then this ols procedure will not yield the causal impact of on ols will simply pick the parameter that makes the resulting errors appear uncorrelated with consider for simplicity the single variable case 
suppose we are considering regression with one variable and constant perhaps no other covariates are necessary or perhaps we have partialed out any other relevant covariates in this case the coefficient on the regressor of interest is given by cov var 
substituting for gives cov var cov var cov var cov var cov var where is what the estimated coefficient vector would be if were not correlated with in this case it can be shown that is an unbiased estimator of if cov in the underlying model that we believe then ols gives coefficient which does not reflect the underlying causal effect of interest 
iv helps to fix this problem by identifying the parameters not based on whether is uncorrelated with but based on whether another variable is uncorrelated with if theory suggests that is related to the first stage but uncorrelated with the exclusion restriction then iv may identify the causal parameter of interest where ols fails 
because there are multiple specific ways of using and deriving iv estimators even in just the linear case iv sls gmm we save further discussion for the estimation section below 
example informally in attempting to estimate the causal effect of some variable on another an instrument is third variable which affects only through its effect on for example suppose researcher wishes to estimate the causal effect of smoking on general health 
correlation between health and smoking does not imply that smoking causes poor health because other variables such as depression may affect both health and smoking or because health may affect smoking 
it is at best difficult and expensive to conduct controlled experiments on smoking status in the general population 
the researcher may attempt to estimate the causal effect of smoking on health from observational data by using the tax rate for tobacco products as an instrument for smoking 
the tax rate for tobacco products is reasonable choice for an instrument because the researcher assumes that it can only be correlated with health through its effect on smoking 
if the researcher then finds tobacco taxes and state of health to be correlated this may be viewed as evidence that smoking causes changes in health 
angrist and krueger present survey of the history and uses of instrumental variable techniques 
graphical definition of course iv techniques have been developed among much broader class of non linear models 
general definitions of instrumental variables using counterfactual and graphical formalism were given by pearl 
the graphical definition requires that satisfy the following conditions where stands for separation and stands for the graph in which all arrows entering are cut off 
the counterfactual definition requires that satisfies where yx stands for the value that would attain had been and stands for independence 
if there are additional covariates then the above definitions are modified so that qualifies as an instrument if the given criteria hold conditional on the essence of pearl definition is the equations of interest are structural not regression 
the error term stands for all exogenous factors that affect when is held constant 
the instrument should be independent of the instrument should not affect when is held constant exclusion restriction 
the instrument should not be independent of these conditions do not rely on specific functional form of the equations and are applicable therefore to nonlinear equations where can be non additive see non parametric analysis 
they are also applicable to system of multiple equations in which and other factors affect through several intermediate variables 
an instrumental variable need not be cause of proxy of such cause may also be used if it satisfies conditions 
the exclusion restriction condition is redundant it follows from conditions and 
selecting suitable instruments since is unobserved the requirement that be independent of cannot be inferred from data and must instead be determined from the model structure the data generating process 
causal graphs are representation of this structure and the graphical definition given above can be used to quickly determine whether variable qualifies as an instrumental variable given set of covariates to see how consider the following example 
suppose that we wish to estimate the effect of university tutoring program on grade point average gpa 
the relationship between attending the tutoring program and gpa may be confounded by number of factors 
students who attend the tutoring program may care more about their grades or may be struggling with their work 
this confounding is depicted in the figures on the right through the bidirected arc between tutoring program and gpa 
if students are assigned to dormitories at random the proximity of the student dorm to the tutoring program is natural candidate for being an instrumental variable 
however what if the tutoring program is located in the college library 
in that case proximity may also cause students to spend more time at the library which in turn improves their gpa see figure 
using the causal graph depicted in the figure we see that proximity does not qualify as an instrumental variable because it is connected to gpa through the path proximity library hours gpa in 
however if we control for library hours by adding it as covariate then proximity becomes an instrumental variable since proximity is separated from gpa given library hours in 
now suppose that we notice that student natural ability affects his or her number of hours in the library as well as his or her gpa as in figure using the causal graph we see that library hours is collider and conditioning on it opens the path proximity library hours gpa 
as result proximity cannot be used as an instrumental variable 
finally suppose that library hours does not actually affect gpa because students who do not study in the library simply study elsewhere as in figure in this case controlling for library hours still opens spurious path from proximity to gpa 
however if we do not control for library hours and remove it as covariate then proximity can again be used an instrumental variable 
estimation we now revisit and expand upon the mechanics of iv in greater detail 
suppose the data are generated by process of the form where indexes observations is the th value of the dependent variable is vector of the th values of the independent variable and constant is the th value of an unobserved error term representing all causes of other than and is an unobserved parameter vector the parameter vector is the causal effect on of one unit change in each element of holding all other causes of constant 
the econometric goal is to estimate for simplicity sake assume the draws of are uncorrelated and that they are drawn from distributions with the same variance that is that the errors are serially uncorrelated and homoskedastic 
suppose also that regression model of nominally the same form is proposed 
given random sample of observations from this process the ordinary least squares estimator is where and denote column vectors of length this equation is similar to the equation involving cov in the introduction this is the matrix version of that equation 
when and are uncorrelated under certain regularity conditions the second term has an expected value conditional on of zero and converges to zero in the limit so the estimator is unbiased and consistent 
when and the other unmeasured causal variables collapsed into the term are correlated however the ols estimator is generally biased and inconsistent for in this case it is valid to use the estimates to predict values of given values of but the estimate does not recover the causal effect of on to recover the underlying parameter we introduce set of variables that is highly correlated with each endogenous component of but in our underlying model is not correlated with for simplicity one might consider to be matrix composed of column of constants and one endogenous variable and to be consisting of column of constants and one instrumental variable 
however this technique generalizes to being matrix of constant and say endogenous variables with being matrix composed of constant and instruments 
in the discussion that follows we will assume that is matrix and leave this value unspecified 
an estimator in which and are both matrices is referred to as just identified 
suppose that the relationship between each endogenous component xi and the instruments is given by the most common iv specification uses the following estimator this specification approaches the true parameter as the sample gets large so long as in the true model as long as in the underlying process which generates the data the appropriate use of the iv estimator will identify this parameter 
this works because iv solves for the unique parameter that satisfies and therefore hones in on the true underlying parameter as the sample size grows 
now an extension suppose that there are more instruments than there are covariates in the equation of interest so that is matrix with this is often called the over identified case 
in this case the generalized method of moments gmm can be used 
the gmm iv estimator is where refers to the projection matrix this expression collapses to the first when the number of instruments is equal to the number of covariates in the equation of interest 
the over identified iv is therefore generalization of the just identified iv 
there is an equivalent under identified estimator for the case where since the parameters are the solutions to set of linear equations an under identified model using the set of equations does not have unique solution 
interpretation as two stage least squares one computational method which can be used to calculate iv estimates is two stage least squares sls or tsls 
in the first stage each explanatory variable that is an endogenous covariate in the equation of interest is regressed on all of the exogenous variables in the model including both exogenous covariates in the equation of interest and the excluded instruments 
the predicted values from these regressions are obtained stage regress each column of on errors and save the predicted values in the second stage the regression of interest is estimated as usual except that in this stage each endogenous covariate is replaced with the predicted values from the first stage stage regress on the predicted values from the first stage which gives sls this method is only valid in linear models 
for categorical endogenous covariates one might be tempted to use different first stage than ordinary least squares such as probit model for the first stage followed by ols for the second 
this is commonly known in the econometric literature as the forbidden regression because second stage iv parameter estimates are consistent only in special cases 
the resulting estimator of is numerically identical to the expression displayed above 
small correction must be made to the sum of squared residuals in the second stage fitted model in order that the covariance matrix of is calculated correctly 
non parametric analysis when the form of the structural equations is unknown an instrumental variable can still be defined through the equations where and are two arbitrary functions and is independent of unlike linear models however measurements of and do not allow for the identification of the average causal effect of on denoted ace ace pr do 
balke and pearl derived tight bounds on ace and showed that these can provide valuable information on the sign and size of ace in linear analysis there is no test to falsify the assumption the is instrumental relative to the pair 
this is not the case when is discrete 
pearl has shown that for all and the following constraint called instrumental inequality must hold whenever satisfies the two equations above max max pr 
interpretation under treatment effect heterogeneity the exposition above assumes that the causal effect of interest does not vary across observations that is that is constant 
generally different subjects will respond in different ways to changes in the treatment when this possibility is recognized the average effect in the population of change in on may differ from the effect in given subpopulation 
for example the average effect of job training program may substantially differ across the group of people who actually receive the training and the group which chooses not to receive training 
for these reasons iv methods invoke implicit assumptions on behavioral response or more generally assumptions over the correlation between the response to treatment and propensity to receive treatment the standard iv estimator can recover local average treatment effects late rather than average treatment effects ate 
imbens and angrist demonstrate that the linear iv estimate can be interpreted under weak conditions as weighted average of local average treatment effects where the weights depend on the elasticity of the endogenous regressor to changes in the instrumental variables 
roughly that means that the effect of variable is only revealed for the subpopulations affected by the observed changes in the instruments and that subpopulations which respond most to changes in the instruments will have the largest effects on the magnitude of the iv estimate 
for example if researcher uses presence of land grant college as an instrument for college education in an earnings regression she identifies the effect of college on earnings in the subpopulation which would obtain college degree if college is present but which would not obtain degree if college is not present 
this empirical approach does not without further assumptions tell the researcher anything about the effect of college among people who would either always or never get college degree regardless of whether local college exists 
weak instruments problem as bound jaeger and baker note problem is caused by the selection of weak instruments instruments that are poor predictors of the endogenous question predictor in the first stage equation 
in this case the prediction of the question predictor by the instrument will be poor and the predicted values will have very little variation 
consequently they are unlikely to have much success in predicting the ultimate outcome when they are used to replace the question predictor in the second stage equation 
in the context of the smoking and health example discussed above tobacco taxes are weak instruments for smoking if smoking status is largely unresponsive to changes in taxes 
if higher taxes do not induce people to quit smoking or not start smoking then variation in tax rates tells us nothing about the effect of smoking on health 
if taxes affect health through channels other than through their effect on smoking then the instruments are invalid and the instrumental variables approach may yield misleading results 
for example places and times with relatively health conscious populations may both implement high tobacco taxes and exhibit better health even holding smoking rates constant so we would observe correlation between health and tobacco taxes even if it were the case that smoking has no effect on health 
in this case we would be mistaken to infer causal effect of smoking on health from the observed correlation between tobacco taxes and health 
testing for weak instruments the strength of the instruments can be directly assessed because both the endogenous covariates and the instruments are observable 
common rule of thumb for models with one endogenous regressor is the statistic against the null that the excluded instruments are irrelevant in the first stage regression should be larger than 
statistical inference and hypothesis testing when the covariates are exogenous the small sample properties of the ols estimator can be derived in straightforward manner by calculating moments of the estimator conditional on when some of the covariates are endogenous so that instrumental variables estimation is implemented simple expressions for the moments of the estimator cannot be so obtained 
generally instrumental variables estimators only have desirable asymptotic not finite sample properties and inference is based on asymptotic approximations to the sampling distribution of the estimator 
even when the instruments are uncorrelated with the error in the equation of interest and when the instruments are not weak the finite sample properties of the instrumental variables estimator may be poor 
for example exactly identified models produce finite sample estimators with no moments so the estimator can be said to be neither biased nor unbiased the nominal size of test statistics may be substantially distorted and the estimates may commonly be far away from the true value of the parameter 
testing the exclusion restriction the assumption that the instruments are not correlated with the error term in the equation of interest is not testable in exactly identified models 
if the model is overidentified there is information available which may be used to test this assumption 
the most common test of these overidentifying restrictions called the sargan hansen test is based on the observation that the residuals should be uncorrelated with the set of exogenous variables if the instruments are truly exogenous 
the sargan hansen test statistic can be calculated as the number of observations multiplied by the coefficient of determination from the ols regression of the residuals onto the set of exogenous variables 
this statistic will be asymptotically chi squared with degrees of freedom under the null that the error term is uncorrelated with the instruments 
see also control function econometrics optimal instruments references further reading greene william 
econometric analysis sixth ed 
upper saddle river pearson prentice hall 
isbn gujarati damodar porter dawn 
basic econometrics fifth ed 
new york mcgraw hill irwin 
lectures on advanced econometric theory 
introductory econometrics modern approach fifth international ed 
mason oh south western 
quasi likelihood methods for count data handbook of applied econometrics volume ed 
pesaran and schmidt oxford blackwell pp 
terza estimating count models with endogenous switching sample selection and endogenous treatment effects 
journal of econometrics pp 
econometric analysis of cross section and panel data mit press cambridge massachusetts 
external links chapter from daniel mcfadden textbook econometrics lecture topic instrumental variable on youtube by mark thoma 
econometrics lecture topic two stages least square on youtube by mark thoma
on computer keyboards the enter key enter and return key return are two closely related keys with overlapping and distinct functions dependent on operating system and application 
functions the return key has its origins in two typewriter functions carriage return which would reset the carriage to the beginning of the line of text and line feed which would advance the paper one line downward 
these were often combined into single return key convention that continues in modern computer word processing to insert paragraph break 
the enter key is computer innovation which in most cases causes command line window form or dialog box to operate its default function 
this is typically to finish an entry and begin the desired process and is usually an alternative to clicking an ok button 
additionally it can act as the equal to button in calculator programs 
on modern computers both keys generally have all the functions of the other allowing for either key to be used or even for them to be combined into single key as is the case with most laptops 
microsoft windows makes no distinction between them whatsoever and usually both keys are labelled as enter on windows keyboards with the united states layout 
other operating systems such as apple darwin based oss generally treat them equivalently while still maintaining the technical and descriptive distinction allowing applications to treat the two keys differently if necessary 
location the enter key is typically located to the right of the and 
keys on the lower right of the numeric keypad while the return key is situated on the right edge of the main alphanumeric portion of the keyboard 
on iso and jis keyboards return is stepped double height key spanning the second and third rows below backspace and above the right hand shift 
on ansi keyboards it is wider but located on the third row only as the backslash key is located between it and backspace 
some variants of the ansi keyboard layout create double height return key by subsuming the backslash key into it 
this alternate form is most popular in asia particularly russia and korea 
however this requires the relocation of the backslash key and is relatively uncommon on modern keyboards elsewhere 
keyboard symbols the return key symbol is ce return symbol an arrow pointing down and leftward however rendering of the symbol varies greatly by typeface with it appearing hollow in some or with an additional initial rightward bar in others 
for this reason or are sometimes used instead 
on most iso and other keyboards worldwide the return key is labelled solely with the symbol across all platforms 
meanwhile on ansi us keyboards it is labelled as enter by windows oems sometimes even without the return symbol and as return by apple for enter enter symbol exists in unicode for the iso enter key symbol however it is infrequently used one example being the french canadian keyboard 
windows keyboards worldwide tend to simply label the key with the text enter while apple uses the symbol or on iso and jis keyboards and the text enter on ansi us keyboards this is acknowledged by an annotation enter key on in the unicode code chart 
history on ibm and line of terminals the enter key was located to the right of the space bar and was used to send the contents of the terminal buffer to the host computer 
the return key was located in more standard location and was used to generate new line 
apple also took advantage of this situation to create an editable command line environment called worksheet in the macintosh programmer workshop where return was used strictly as formatting key while enter was used to execute shell command or series of commands in direct mode 
this strict dichotomy has since been relaxed so that now there are very few situations within macos where enter and return are not equivalent 
one example of this continued division of use is the type tool in adobe photoshop where the return key produces new line while the enter key ends editing mode 
another is mathematica where the return key creates new line while the enter key or shift return submits the current command for execution 
historically many computer models did not have separate keypad and only had one button to function as enter or return 
for example the commodore manufactured from had only the return key 
most laptop computers continue in this combined tradition 
before computers on electric typewriters the return key was kept comparatively large 
this is due to the frequency of usage which also includes the space bar and therefore is kept large to reduce the likelihood of finger slips 
see also carriage return numeric keypad references
public key cryptography or asymmetric cryptography is the field of cryptographic systems that use pairs of related keys 
each key pair consists of public key and corresponding private key 
key pairs are generated with cryptographic algorithms based on mathematical problems termed one way functions 
security of public key cryptography depends on keeping the private key secret the public key can be openly distributed without compromising security in public key encryption system anyone with public key can encrypt message yielding ciphertext but only those who know the corresponding private key can decrypt the ciphertext to obtain the original message for example journalist can publish the public key of an encryption key pair on web site so that sources can send secret messages to the news organization in ciphertext 
only the journalist who knows the corresponding private key can decrypt the ciphertexts to obtain the sources messages an eavesdropper reading email on its way to the journalist can decrypt the ciphertexts 
however public key encryption doesn conceal metadata like what computer source used to send message when they sent it or how long it is 
public key encryption on its own also doesn tell the recipient anything about who sent message it just conceals the content of message in ciphertext that can only be decrypted with the private key 
in digital signature system sender can use private key together with message to create signature 
anyone with the corresponding public key can verify whether the signature matches the message but forger who doesn know the private key can find any message signature pair that will pass verification with the public key for example software publisher can create signature key pair and include the public key in software installed on computers 
later the publisher can distribute an update to the software signed using the private key and any computer receiving an update can confirm it is genuine by verifying the signature using the public key 
as long as the software publisher keeps the private key secret even if forger can distribute malicious updates to computers they can convince the computers that any malicious updates are genuine 
public key algorithms are fundamental security primitives in modern cryptosystems including applications and protocols which offer assurance of the confidentiality authenticity and non repudiability of electronic communications and data storage 
they underpin numerous internet standards such as transport layer security tls ssh mime and pgp 
some public key algorithms provide key distribution and secrecy diffie hellman key exchange some provide digital signatures digital signature algorithm and some provide both rsa 
compared to symmetric encryption asymmetric encryption is rather slower than good symmetric encryption too slow for many purposes 
today cryptosystems such as tls secure shell use both symmetric encryption and asymmetric encryption often by using asymmetric encryption to securely exchange secret key which is then used for symmetric encryption 
description before the mid all cipher systems used symmetric key algorithms in which the same cryptographic key is used with the underlying algorithm by both the sender and the recipient who must both keep it secret 
of necessity the key in every such system had to be exchanged between the communicating parties in some secure way prior to any use of the system for instance via secure channel 
this requirement is never trivial and very rapidly becomes unmanageable as the number of participants increases or when secure channels aren available or when as is sensible cryptographic practice keys are frequently changed 
in particular if messages are meant to be secure from other users separate key is required for each possible pair of users 
by contrast in public key system the public keys can be disseminated widely and openly and only the corresponding private keys need be kept secret by its owner 
two of the best known uses of public key cryptography are public key encryption in which message is encrypted with the intended recipient public key 
for properly chosen and used algorithms messages cannot in practice be decrypted by anyone who does not possess the matching private key who is thus presumed to be the owner of that key and so the person associated with the public key 
this can be used to ensure confidentiality of message 
digital signatures in which message is signed with the sender private key and can be verified by anyone who has access to the sender public key 
this verification proves that the sender had access to the private key and therefore is very likely to be the person associated with the public key 
it also proves that the signature was prepared for that exact message since verification will fail for any other message one could devise without using the private key one important issue is confidence proof that particular public key is authentic 
that it is correct and belongs to the person or entity claimed and has not been tampered with or replaced by some perhaps malicious third party 
there are several possible approaches including public key infrastructure pki in which one or more third parties known as certificate authorities certify ownership of key pairs 
tls relies upon this 
this implies that the pki system software hardware and management is trust able by all involved 
web of trust which decentralizes authentication by using individual endorsements of links between user and the public key belonging to that user 
pgp uses this approach in addition to lookup in the domain name system dns 
the dkim system for digitally signing emails also uses this approach 
applications the most obvious application of public key encryption system is for encrypting communication to provide confidentiality message that sender encrypts using the recipient public key which can be decrypted only by the recipient paired private key 
another application in public key cryptography is the digital signature 
digital signature schemes can be used for sender authentication 
non repudiation systems use digital signatures to ensure that one party cannot successfully dispute its authorship of document or communication 
further applications built on this foundation include digital cash password authenticated key agreement time stamping services and non repudiation protocols 
hybrid cryptosystems because asymmetric key algorithms are nearly always much more computationally intensive than symmetric ones it is common to use public private asymmetric key exchange algorithm to encrypt and exchange symmetric key which is then used by symmetric key cryptography to transmit data using the now shared symmetric key for symmetric key encryption algorithm 
pgp ssh and the ssl tls family of schemes use this procedure they are thus called hybrid cryptosystems 
the initial asymmetric cryptography based key exchange to share server generated symmetric key from the server to client has the advantage of not requiring that symmetric key be pre shared manually such as on printed paper or discs transported by courier while providing the higher data throughput of symmetric key cryptography over asymmetric key cryptography for the remainder of the shared connection 
weaknesses as with all security related systems it is important to identify potential weaknesses 
aside from poor choice of an asymmetric key algorithm there are few which are widely regarded as satisfactory or too short key length the chief security risk is that the private key of pair becomes known 
all security of messages authentication etc will then be lost 
algorithms all public key schemes are in theory susceptible to brute force key search attack 
however such an attack is impractical if the amount of computation needed to succeed termed the work factor by claude shannon is out of reach of all potential attackers 
in many cases the work factor can be increased by simply choosing longer key 
but other algorithms may inherently have much lower work factors making resistance to brute force attack from longer keys irrelevant 
some special and specific algorithms have been developed to aid in attacking some public key encryption algorithms both rsa and elgamal encryption have known attacks that are much faster than the brute force approach 
none of these are sufficiently improved to be actually practical however 
major weaknesses have been found for several formerly promising asymmetric key algorithms 
the knapsack packing algorithm was found to be insecure after the development of new attack 
as with all cryptographic functions public key implementations may be vulnerable to side channel attacks that exploit information leakage to simplify the search for secret key 
these are often independent of the algorithm being used 
research is underway to both discover and to protect against new attacks 
alteration of public keys another potential security vulnerability in using asymmetric keys is the possibility of man in the middle attack in which the communication of public keys is intercepted by third party the man in the middle and then modified to provide different public keys instead 
encrypted messages and responses must in all instances be intercepted decrypted and re encrypted by the attacker using the correct public keys for the different communication segments so as to avoid suspicion 
communication is said to be insecure where data is transmitted in manner that allows for interception also called sniffing 
these terms refer to reading the sender private data in its entirety 
communication is particularly unsafe when interceptions can be prevented or monitored by the sender man in the middle attack can be difficult to implement due to the complexities of modern security protocols 
however the task becomes simpler when sender is using insecure media such as public networks the internet or wireless communication 
in these cases an attacker can compromise the communications infrastructure rather than the data itself 
hypothetical malicious staff member at an internet service provider isp might find man in the middle attack relatively straightforward 
capturing the public key would only require searching for the key as it gets sent through the isp communications hardware in properly implemented asymmetric key schemes this is not significant risk 
in some advanced man in the middle attacks one side of the communication will see the original data while the other will receive malicious variant 
asymmetric man in the middle attacks can prevent users from realizing their connection is compromised 
this remains so even when one user data is known to be compromised because the data appears fine to the other user 
this can lead to confusing disagreements between users such as it must be on your end 
when neither user is at fault 
hence man in the middle attacks are only fully preventable when the communications infrastructure is physically controlled by one or both parties such as via wired route inside the sender own building 
in summation public keys are easier to alter when the communications hardware used by sender is controlled by an attacker 
public key infrastructure one approach to prevent such attacks involves the use of public key infrastructure pki set of roles policies and procedures needed to create manage distribute use store and revoke digital certificates and manage public key encryption 
however this has potential weaknesses 
for example the certificate authority issuing the certificate must be trusted by all participating parties to have properly checked the identity of the key holder to have ensured the correctness of the public key when it issues certificate to be secure from computer piracy and to have made arrangements with all participants to check all their certificates before protected communications can begin 
web browsers for instance are supplied with long list of self signed identity certificates from pki providers these are used to check the bona fides of the certificate authority and then in second step the certificates of potential communicators 
an attacker who could subvert one of those certificate authorities into issuing certificate for bogus public key could then mount man in the middle attack as easily as if the certificate scheme were not used at all 
in an alternative scenario rarely discussed an attacker who penetrates an authority servers and obtains its store of certificates and keys public and private would be able to spoof masquerade decrypt and forge transactions without limit 
despite its theoretical and potential problems this approach is widely used 
examples include tls and its predecessor ssl which are commonly used to provide security for web browser transactions for example to securely send credit card details to an online store 
aside from the resistance to attack of particular key pair the security of the certification hierarchy must be considered when deploying public key systems 
some certificate authority usually purpose built program running on server computer vouches for the identities assigned to specific private keys by producing digital certificate 
public key digital certificates are typically valid for several years at time so the associated private keys must be held securely over that time 
when private key used for certificate creation higher in the pki server hierarchy is compromised or accidentally disclosed then man in the middle attack is possible making any subordinate certificate wholly insecure 
examples examples of well regarded asymmetric key techniques for varied purposes include diffie hellman key exchange protocol dss digital signature standard which incorporates the digital signature algorithm elgamal elliptic curve cryptography elliptic curve digital signature algorithm ecdsa elliptic curve diffie hellman ecdh ed and ed eddsa and ecdh eddh various password authenticated key agreement techniques paillier cryptosystem rsa encryption algorithm pkcs cramer shoup cryptosystem yak authenticated key agreement protocolexamples of asymmetric key algorithms not yet widely adopted include ntruencrypt cryptosystem kyber mceliece cryptosystemexamples of notable yet insecure asymmetric key algorithms include merkle hellman knapsack cryptosystemexamples of protocols using asymmetric key algorithms include mime gpg an implementation of openpgp and an internet standard emv emv certificate authority ipsec pgp zrtp secure voip protocol transport layer security standardized by ietf and its predecessor secure socket layer silc ssh bitcoin off the record messaging history during the early history of cryptography two parties would rely upon key that they would exchange by means of secure but non cryptographic method such as face to face meeting or trusted courier 
this key which both parties must then keep absolutely secret could then be used to exchange encrypted messages 
number of significant practical difficulties arise with this approach to distributing keys 
anticipation in his book the principles of science william stanley jevons wrote can the reader say what two numbers multiplied together will produce the number 
think it unlikely that anyone but myself will ever know 
here he described the relationship of one way functions to cryptography and went on to discuss specifically the factorization problem used to create trapdoor function 
in july mathematician solomon golomb said jevons anticipated key feature of the rsa algorithm for public key cryptography although he certainly did not invent the concept of public key cryptography 
classified discovery in james ellis british cryptographer at the uk government communications headquarters gchq conceived of the possibility of non secret encryption now called public key cryptography but could see no way to implement it 
in his colleague clifford cocks implemented what has become known as the rsa encryption algorithm giving practical method of non secret encryption and in another gchq mathematician and cryptographer malcolm williamson developed what is now known as diffie hellman key exchange 
the scheme was also passed to the us national security agency 
both organisations had military focus and only limited computing power was available in any case the potential of public key cryptography remained unrealised by either organization judged it most important for military use if you can share your key rapidly and electronically you have major advantage over your opponent 
only at the end of the evolution from berners lee designing an open internet architecture for cern its adaptation and adoption for the arpanet did public key cryptography realise its full potential 
ralph benjamin these discoveries were not publicly acknowledged for years until the research was declassified by the british government in 
public discovery in an asymmetric key cryptosystem was published by whitfield diffie and martin hellman who influenced by ralph merkle work on public key distribution disclosed method of public key agreement 
this method of key exchange which uses exponentiation in finite field came to be known as diffie hellman key exchange 
this was the first published practical method for establishing shared secret key over an authenticated but not confidential communications channel without using prior shared secret 
merkle public key agreement technique became known as merkle puzzles and was invented in and only published in this makes asymmetric encryption rather new field in cryptography although cryptography itself dates back more than years in generalization of cocks scheme was independently invented by ron rivest adi shamir and leonard adleman all then at mit 
the latter authors published their work in in martin gardner scientific american column and the algorithm came to be known as rsa from their initials 
rsa uses exponentiation modulo product of two very large primes to encrypt and decrypt performing both public key encryption and public key digital signatures 
its security is connected to the extreme difficulty of factoring large integers problem for which there is no known efficient general technique though prime factorization may be obtained through brute force attacks this grows much more difficult the larger the prime factors are 
description of the algorithm was published in the mathematical games column in the august issue of scientific american since the large number and variety of encryption digital signature key agreement and other techniques have been developed including the rabin cryptosystem elgamal encryption dsa and elliptic curve cryptography 
see also notes references external links oral history interview with martin hellman charles babbage institute university of minnesota 
leading cryptography scholar martin hellman discusses the circumstances and fundamental insights of his invention of public key cryptography with collaborators whitfield diffie and ralph merkle at stanford university in the mid 
an account of how gchq kept their invention of pke secret until
dependent and independent variables are variables in mathematical modeling statistical modeling and experimental sciences 
dependent variables receive this name because in an experiment their values are studied under the supposition or demand that they depend by some law or rule by mathematical function on the values of other variables 
independent variables in turn are not seen as depending on any other variable in the scope of the experiment in question 
in this sense some common independent variables are time space density mass fluid flow rate and previous values of some observed value of interest 
human population size to predict future values the dependent variable of the two it is always the dependent variable whose variation is being studied by altering inputs also known as regressors in statistical context 
in an experiment any variable that can be attributed value without attributing value to any other variable is called an independent variable 
models and experiments test the effects that the independent variables have on the dependent variables 
sometimes even if their influence is not of direct interest independent variables may be included for other reasons such as to account for their potential confounding effect 
mathematics in mathematics function is rule for taking an input in the simplest case number or set of numbers and providing an output which may also be number 
symbol that stands for an arbitrary input is called an independent variable while symbol that stands for an arbitrary output is called dependent variable 
the most common symbol for the input is and the most common symbol for the output is the function itself is commonly written it is possible to have multiple independent variables or multiple dependent variables 
for instance in multivariable calculus one often encounters functions of the form where is dependent variable and and are independent variables 
functions with multiple outputs are often referred to as vector valued functions 
modeling in mathematical modeling the dependent variable is studied to see if and how much it varies as the independent variables vary 
in the simple stochastic linear model yi bxi ei the term yi is the ith value of the dependent variable and xi is the ith value of the independent variable 
the term ei is known as the error and contains the variability of the dependent variable not explained by the independent variable 
with multiple independent variables the model is yi bxi bxi bxi ei where is the number of independent variables the linear regression model is now discussed 
to use linear regression scatter plot of data is generated with as the independent variable and as the dependent variable 
this is also called bivariate dataset xi yi 
the simple linear regression model takes the form of yi bxi ui for in this case ui un are independent random variables 
this occurs when the measurements do not influence each other 
through propagation of independence the independence of ui implies independence of yi even though each yi has different expectation value 
each ui has an expectation value of and variance of expectation of yi proof the line of best fit for the bivariate dataset takes the form and is called the regression line 
and correspond to the intercept and slope respectively 
simulation in simulation the dependent variable is changed in response to changes in the independent variables 
statistics in an experiment the variable manipulated by an experimenter is something that is proven to work called an independent variable 
the dependent variable is the event expected to change when the independent variable is manipulated in data mining tools for multivariate statistics and machine learning the dependent variable is assigned role as target variable or in some tools as label attribute while an independent variable may be assigned role as regular variable 
known values for the target variable are provided for the training data set and test data set but should be predicted for other data 
the target variable is used in supervised learning algorithms but not in unsupervised learning 
statistics synonyms depending on the context an independent variable is sometimes called predictor variable regressor covariate manipulated variable explanatory variable exposure variable see reliability theory risk factor see medical statistics feature in machine learning and pattern recognition or input variable 
in econometrics the term control variable is usually used instead of covariate 
explanatory variable is preferred by some authors over independent variable when the quantities treated as independent variables may not be statistically independent or independently manipulable by the researcher 
if the independent variable is referred to as an explanatory variable then the term response variable is preferred by some authors for the dependent variable from the economics community the independent variables are also called exogenous 
depending on the context dependent variable is sometimes called response variable regressand criterion predicted variable measured variable explained variable experimental variable responding variable outcome variable output variable target or label 
in economics endogenous variables are usually referencing the target 
explained variable is preferred by some authors over dependent variable when the quantities treated as dependent variables may not be statistically dependent 
if the dependent variable is referred to as an explained variable then the term predictor variable is preferred by some authors for the independent variable variables may also be referred to by their form continuous or categorical which in turn may be binary dichotomous nominal categorical and ordinal categorical among others 
an example is provided by the analysis of trend in sea level by woodworth 
here the dependent variable and variable of most interest was the annual mean sea level at given location for which series of yearly values were available 
the primary independent variable was time 
use was made of covariate consisting of yearly values of annual mean atmospheric pressure at sea level 
the results showed that inclusion of the covariate allowed improved estimates of the trend against time to be obtained compared to analyses which omitted the covariate 
other variables variable may be thought to alter the dependent or independent variables but may not actually be the focus of the experiment 
so that the variable will be kept constant or monitored to try to minimize its effect on the experiment 
such variables may be designated as either controlled variable control variable or fixed variable 
extraneous variables if included in regression analysis as independent variables may aid researcher with accurate response parameter estimation prediction and goodness of fit but are not of substantive interest to the hypothesis under examination 
for example in study examining the effect of post secondary education on lifetime earnings some extraneous variables might be gender ethnicity social class genetics intelligence age and so forth 
variable is extraneous only when it can be assumed or shown to influence the dependent variable 
if included in regression it can improve the fit of the model 
if it is excluded from the regression and if it has non zero covariance with one or more of the independent variables of interest its omission will bias the regression result for the effect of that independent variable of interest 
this effect is called confounding or omitted variable bias in these situations design changes and or controlling for variable statistical control is necessary 
extraneous variables are often classified into three types subject variables which are the characteristics of the individuals being studied that might affect their actions 
these variables include age gender health status mood background etc 
blocking variables or experimental variables are characteristics of the persons conducting the experiment which might influence how person behaves 
gender the presence of racial discrimination language or other factors may qualify as such variables 
situational variables are features of the environment in which the study or research was conducted which have bearing on the outcome of the experiment in negative way 
included are the air temperature level of activity lighting and time of day in modelling variability that is not covered by the independent variable is designated by and is known as the residual side effect error unexplained share residual variable disturbance or tolerance 
examples effect of fertilizer on plant growths in study measuring the influence of different quantities of fertilizer on plant growth the independent variable would be the amount of fertilizer used 
the dependent variable would be the growth in height or mass of the plant 
the controlled variables would be the type of plant the type of fertilizer the amount of sunlight the plant gets the size of the pots etc effect of drug dosage on symptom severity in study of how different doses of drug affect the severity of symptoms researcher could compare the frequency and intensity of symptoms when different doses are administered 
here the independent variable is the dose and the dependent variable is the frequency intensity of symptoms effect of temperature on pigmentation in measuring the amount of color removed from beetroot samples at different temperatures temperature is the independent variable and amount of pigment removed is the dependent variable effect of sugar added in coffee the taste varies with the amount of sugar added in the coffee 
here the sugar is the independent variable while the taste is the dependent variable 
see also abscissa and ordinate blocking statistics latent variable versus observable variable notes references
in cryptography related key attack is any form of cryptanalysis where the attacker can observe the operation of cipher under several different keys whose values are initially unknown but where some mathematical relationship connecting the keys is known to the attacker 
for example the attacker might know that the last bits of the keys are always the same even though they don know at first what the bits are 
this appears at first glance to be an unrealistic model it would certainly be unlikely that an attacker could persuade human cryptographer to encrypt plaintexts under numerous secret keys related in some way 
kasumi kasumi is an eight round bit block cipher with bit key 
it is based upon misty and was designed to form the basis of the confidentiality and integrity algorithms 
mark blunden and adrian escott described differential related key attacks on five and six rounds of kasumi 
differential attacks were introduced by biham and shamir 
related key attacks were first introduced by biham 
differential related key attacks are discussed in kelsey et al 
wep an important example of cryptographic protocol that failed because of related key attack is wired equivalent privacy wep used in wi fi wireless networks 
each client wi fi network adapter and wireless access point in wep protected network shares the same wep key 
encryption uses the rc algorithm stream cipher 
it is essential that the same key never be used twice with stream cipher 
to prevent this from happening wep includes bit initialization vector iv in each message packet 
the rc key for that packet is the iv concatenated with the wep key 
wep keys have to be changed manually and this typically happens infrequently 
an attacker therefore can assume that all the keys used to encrypt packets share single wep key 
this fact opened up wep to series of attacks which proved devastating 
the simplest to understand uses the fact that the bit iv only allows little under million possibilities 
because of the birthday paradox it is likely that for every packets two will share the same iv and hence the same rc key allowing the packets to be attacked 
more devastating attacks take advantage of certain weak keys in rc and eventually allow the wep key itself to be recovered 
in agents from the federal bureau of investigation publicly demonstrated the ability to do this with widely available software tools in about three minutes 
preventing related key attacks one approach to preventing related key attacks is to design protocols and applications so that encryption keys will never have simple relationship with each other 
for example each encryption key can be generated from the underlying key material using key derivation function 
for example replacement for wep wi fi protected access wpa uses three levels of keys master key working key and rc key 
the master wpa key is shared with each client and access point and is used in protocol called temporal key integrity protocol tkip to create new working keys frequently enough to thwart known attack methods 
the working keys are then combined with longer bit iv to form the rc key for each packet 
this design mimics the wep approach enough to allow wpa to be used with first generation wi fi network cards some of which implemented portions of wep in hardware 
however not all first generation access points can run wpa 
another more conservative approach is to employ cipher designed to prevent related key attacks altogether usually by incorporating strong key schedule 
newer version of wi fi protected access wpa uses the aes block cipher instead of rc in part for this reason 
there are related key attacks against aes but unlike those against rc they re far from practical to implement and wpa key generation functions may provide some security against them 
many older network cards cannot run wpa
in statistics ordinary least squares ols is type of linear least squares method for choosing the unknown parameters in linear regression model with fixed level one effects of linear function of set of explanatory variables by the principle of least squares minimizing the sum of the squares of the differences between the observed dependent variable values of the variable being observed in the input dataset and the output of the linear function of the independent variable 
geometrically this is seen as the sum of the squared distances parallel to the axis of the dependent variable between each data point in the set and the corresponding point on the regression surface the smaller the differences the better the model fits the data 
the resulting estimator can be expressed by simple formula especially in the case of simple linear regression in which there is single regressor on the right side of the regression equation 
the ols estimator is consistent for the level one fixed effects when the regressors are exogenous and forms perfect colinearity rank condition consistent for the variance estimate of the residuals when regressors have finite fourth moments and by the gauss markov theorem optimal in the class of linear unbiased estimators when the errors are homoscedastic and serially uncorrelated 
under these conditions the method of ols provides minimum variance mean unbiased estimation when the errors have finite variances 
under the additional assumption that the errors are normally distributed with zero mean ols is the maximum likelihood estimator that outperforms any non linear unbiased estimator 
linear model suppose the data consists of observations each observation includes scalar response and column vector of parameters regressors in linear regression model the response variable is linear function of the regressors or in vector form where as introduced previously is column vector of the th observation of all the explanatory variables is vector of unknown parameters and the scalar represents unobserved random variables errors of the th observation 
accounts for the influences upon the responses from sources other than the explanators this model can also be written in matrix notation as where and are vectors of the response variables and the errors of the observations and is an matrix of regressors also sometimes called the design matrix whose row is and contains the th observations on all the explanatory variables 
as rule the constant term is always included in the set of regressors say by taking for all the coefficient corresponding to this regressor is called the intercept 
regressors do not have to be independent there can be any desired relationship between the regressors so long as it is not linear relationship 
for instance we might suspect the response depends linearly both on value and its square in which case we would include one regressor whose value is just the square of another regressor 
in that case the model would be quadratic in the second regressor but none the less is still considered linear model because the model is still linear in the parameters 
matrix vector formulation consider an overdetermined system of linear equations in unknown coefficients with note for linear model as above not all elements in contains information on the data points 
the first column is populated with ones only the other columns contain actual data 
so here is equal to the number of regressors plus one 
this can be written in matrix form as where 
such system usually has no exact solution so the goal is instead to find the coefficients which fit the equations best in the sense of solving the quadratic minimization problem where the objective function is given by justification for choosing this criterion is given in properties below 
this minimization problem has unique solution provided that the columns of the matrix are linearly independent given by solving the so called normal equations the matrix is known as the normal matrix or gram matrix and the matrix is known as the moment matrix of regressand by regressors 
finally is the coefficient vector of the least squares hyperplane expressed as or 
estimation suppose is candidate value for the parameter vector the quantity yi xitb called the residual for the th observation measures the vertical distance between the data point xi yi and the hyperplane xtb and thus assesses the degree of fit between the actual data and the model 
the sum of squared residuals ssr also called the error sum of squares ess or residual sum of squares rss is measure of the overall model fit where denotes the matrix transpose and the rows of denoting the values of all the independent variables associated with particular value of the dependent variable are xi xit 
the value of which minimizes this sum is called the ols estimator for the function is quadratic in with positive definite hessian and therefore this function possesses unique global minimum at which can be given by the explicit formula proof argmin the product xt is gram matrix and its inverse is the cofactor matrix of closely related to its covariance matrix 
the matrix xt xt xt is called the moore penrose pseudoinverse matrix of this formulation highlights the point that estimation can be carried out if and only if there is no perfect multicollinearity between the explanatory variables which would cause the gram matrix to have no inverse 
after we have estimated the fitted values or predicted values from the regression will be where xtx xt is the projection matrix onto the space spanned by the columns of this matrix is also sometimes called the hat matrix because it puts hat onto the variable another matrix closely related to is the annihilator matrix in this is projection matrix onto the space orthogonal to both matrices and are symmetric and idempotent meaning that and and relate to the data matrix via identities px and mx matrix creates the residuals from the regression using these residuals we can estimate the value of using the reduced chi squared statistic the denominator is the statistical degrees of freedom 
the first quantity is the ols estimate for whereas the second is the mle estimate for 
the two estimators are quite similar in large samples the first estimator is always unbiased while the second estimator is biased but has smaller mean squared error 
in practice is used more often since it is more convenient for the hypothesis testing 
the square root of is called the regression standard error standard error of the regression or standard error of the equation it is common to assess the goodness of fit of the ols regression by comparing how much the initial variation in the sample can be reduced by regressing onto the coefficient of determination is defined as ratio of explained variance to the total variance of the dependent variable in the cases where the regression sum of squares equals the sum of squares of residuals where tss is the total sum of squares for the dependent variable and is an matrix of ones 
is centering matrix which is equivalent to regression on constant it simply subtracts the mean from variable 
in order for to be meaningful the matrix of data on regressors must contain column vector of ones to represent the constant whose coefficient is the regression intercept 
in that case will always be number between and with values close to indicating good degree of fit 
the variance in the prediction of the independent variable as function of the dependent variable is given in the article polynomial least squares 
simple linear regression model if the data matrix contains only two variables constant and scalar regressor xi then this is called the simple regression model 
this case is often considered in the beginner statistics classes as it provides much simpler formulas even suitable for manual calculation 
the parameters are commonly denoted as the least squares estimates in this case are given by simple formulas alternative derivations in the previous section the least squares estimator was obtained as value that minimizes the sum of squared residuals of the model 
however it is also possible to derive the same estimator from other approaches 
in all cases the formula for ols estimator remains the same xtx xty the only difference is in how we interpret this result 
projection for mathematicians ols is an approximate solution to an overdetermined system of linear equations where is the unknown 
assuming the system cannot be solved exactly the number of equations is much larger than the number of unknowns we are looking for solution that could provide the smallest discrepancy between the right and left hand sides 
in other words we are looking for the solution that satisfies min where is the standard norm in the dimensional euclidean space rn 
the predicted quantity is just certain linear combination of the vectors of regressors 
thus the residual vector will have the smallest length when is projected orthogonally onto the linear subspace spanned by the columns of the ols estimator in this case can be interpreted as the coefficients of vector decomposition of py along the basis of in other words the gradient equations at the minimum can be written as geometrical interpretation of these equations is that the vector of residuals is orthogonal to the column space of since the dot product is equal to zero for any conformal vector this means that is the shortest of all possible vectors that is the variance of the residuals is the minimum possible 
this is illustrated at the right 
introducing and matrix with the assumption that matrix is non singular and kt cf 
orthogonal projections the residual vector should satisfy the following equation 
the equation and solution of linear least squares are thus described as follows another way of looking at it is to consider the regression line to be weighted average of the lines passing through the combination of any two points in the dataset 
although this way of calculation is more computationally expensive it provides better intuition on ols 
maximum likelihood the ols estimator is identical to the maximum likelihood estimator mle under the normality assumption for the error terms 
proof this normality assumption has historical importance as it provided the basis for the early work in linear regression analysis by yule and pearson 
from the properties of mle we can infer that the ols estimator is asymptotically efficient in the sense of attaining the cram rao bound for variance if the normality assumption is satisfied 
generalized method of moments in iid case the ols estimator can also be viewed as gmm estimator arising from the moment conditions these moment conditions state that the regressors should be uncorrelated with the errors 
since xi is vector the number of moment conditions is equal to the dimension of the parameter vector and thus the system is exactly identified 
this is the so called classical gmm case when the estimator does not depend on the choice of the weighting matrix 
note that the original strict exogeneity assumption xi implies far richer set of moment conditions than stated above 
in particular this assumption implies that for any vector function the moment condition xi will hold 
however it can be shown using the gauss markov theorem that the optimal choice of function is to take which results in the moment equation posted above 
properties assumptions there are several different frameworks in which the linear regression model can be cast in order to make the ols technique applicable 
each of these settings produces the same formulas and same results 
the only difference is the interpretation and the assumptions which have to be imposed in order for the method to give meaningful results 
the choice of the applicable framework depends mostly on the nature of data in hand and on the inference task which has to be performed 
one of the lines of difference in interpretation is whether to treat the regressors as random variables or as predefined constants 
in the first case random design the regressors xi are random and sampled together with the yi from some population as in an observational study 
this approach allows for more natural study of the asymptotic properties of the estimators 
in the other interpretation fixed design the regressors are treated as known constants set by design and is sampled conditionally on the values of as in an experiment 
for practical purposes this distinction is often unimportant since estimation and inference is carried out while conditioning on all results stated in this article are within the random design framework 
classical linear regression model the classical model focuses on the finite sample estimation and inference meaning that the number of observations is fixed 
this contrasts with the other approaches which study the asymptotic behavior of ols and in which the number of observations is allowed to grow to infinity 
the linear functional form must coincide with the form of the actual data generating process 
the errors in the regression should have conditional mean zero the immediate consequence of the exogeneity assumption is that the errors have mean zero and that the regressors are uncorrelated with the errors xt the exogeneity assumption is critical for the ols theory 
if it holds then the regressor variables are called exogenous 
if it doesn then those regressors that are correlated with the error term are called endogenous and the ols estimator becomes biased 
in such case the method of instrumental variables may be used to carry out inference no linear dependence 
the regressors in must all be linearly independent 
mathematically this means that the matrix must have full column rank almost surely pr rank usually it is also assumed that the regressors have finite moments up to at least the second moment 
then the matrix qxx xtx is finite and positive semi definite 
when this assumption is violated the regressors are called linearly dependent or perfectly multicollinear 
in such case the value of the regression coefficient cannot be learned although prediction of values is still possible for new values of the regressors that lie in the same linearly dependent subspace spherical errors var where in is the identity matrix in dimension and is parameter which determines the variance of each observation 
this is considered nuisance parameter in the model although usually it is also estimated 
if this assumption is violated then the ols estimates are still valid but no longer efficient 
it is customary to split this assumption into two parts homoscedasticity which means that the error term has the same variance in each observation 
when this requirement is violated this is called heteroscedasticity in such case more efficient estimator would be weighted least squares 
if the errors have infinite variance then the ols estimates will also have infinite variance although by the law of large numbers they will nonetheless tend toward the true values so long as the errors have zero mean 
in this case robust estimation techniques are recommended 
no autocorrelation the errors are uncorrelated between observations for this assumption may be violated in the context of time series data panel data cluster samples hierarchical data repeated measures data longitudinal data and other data with dependencies 
in such cases generalized least squares provides better alternative than the ols 
another expression for autocorrelation is serial correlation normality 
it is sometimes additionally assumed that the errors have normal distribution conditional on the regressors 
this assumption is not needed for the validity of the ols method although certain additional finite sample properties can be established in case when it does especially in the area of hypotheses testing 
also when the errors are normal the ols estimator is equivalent to the maximum likelihood estimator mle and therefore it is asymptotically efficient in the class of all regular estimators 
importantly the normality assumption applies only to the error terms contrary to popular misconception the response dependent variable is not required to be normally distributed 
independent and identically distributed iid in some applications especially with cross sectional data an additional assumption is imposed that all observations are independent and identically distributed 
this means that all observations are taken from random sample which makes all the assumptions listed earlier simpler and easier to interpret 
also this framework allows one to state asymptotic results as the sample size which are understood as theoretical possibility of fetching new independent observations from the data generating process 
the list of assumptions in this case is iid observations xi yi is independent from and has the same distribution as xj yj for all no perfect multicollinearity qxx xi xit is positive definite matrix exogeneity xi homoscedasticity var xi 
time series model the stochastic process is co integrating 
the regressors are predetermined xi for all the matrix qxx xi xit is of full rank and hence positive definite is martingale difference sequence with finite matrix of second moments qxx xi xit 
finite sample properties first of all under the strict exogeneity assumption the ols estimators and are unbiased meaning that their expected values coincide with the true values of the parameters proof if the strict exogeneity does not hold as is the case with many time series models where exogeneity is assumed only with respect to the past shocks but not the future ones then these estimators will be biased in finite samples 
the variance covariance matrix or simply covariance matrix of is equal to var in particular the standard error of each coefficient is equal to square root of the th diagonal element of this matrix 
the estimate of this standard error is obtained by replacing the unknown quantity with its estimate 
thus it can also be easily shown that the estimator is uncorrelated with the residuals from the model cov the gauss markov theorem states that under the spherical errors assumption that is the errors should be uncorrelated and homoscedastic the estimator is efficient in the class of linear unbiased estimators 
this is called the best linear unbiased estimator blue 
efficiency should be understood as if we were to find some other estimator which would be linear in and unbiased then var var in the sense that this is nonnegative definite matrix 
this theorem establishes optimality only in the class of linear unbiased estimators which is quite restrictive 
depending on the distribution of the error terms other non linear estimators may provide better results than ols 
assuming normality the properties listed so far are all valid regardless of the underlying distribution of the error terms 
however if you are willing to assume that the normality assumption holds that is that in then additional properties of the ols estimators can be stated 
the estimator is normally distributed with mean and variance as given before 
this estimator reaches the cram rao bound for the model and thus is optimal in the class of all unbiased estimators 
note that unlike the gauss markov theorem this result establishes optimality among both linear and non linear estimators but only in the case of normally distributed error terms 
the estimator will be proportional to the chi squared distribution the variance of this estimator is equal to which does not attain the cram rao bound of 
however it was shown that there are no unbiased estimators of with variance smaller than that of the estimator 
if we are willing to allow biased estimators and consider the class of estimators that are proportional to the sum of squared residuals ssr of the model then the best in the sense of the mean squared error estimator in this class will be ssr which even beats the cram rao bound in case when there is only one regressor moreover the estimators and are independent the fact which comes in useful when constructing the and tests for the regression 
influential observations as was mentioned before the estimator is linear in meaning that it represents linear combination of the dependent variables yi 
the weights in this linear combination are functions of the regressors and generally are unequal 
the observations with high weights are called influential because they have more pronounced effect on the value of the estimator 
to analyze which observations are influential we remove specific th observation and consider how much the estimated quantities are going to change similarly to the jackknife method 
it can be shown that the change in the ols estimator for will be equal to where hj xjt xtx xj is the th diagonal element of the hat matrix and xj is the vector of regressors corresponding to the th observation 
similarly the change in the predicted value for th observation resulting from omitting that observation from the dataset will be equal to from the properties of the hat matrix hj and they sum up to so that on average hj 
these quantities hj are called the leverages and observations with high hj are called leverage points 
usually the observations with high leverage ought to be scrutinized more carefully in case they are erroneous or outliers or in some other way atypical of the rest of the dataset 
partitioned regression sometimes the variables and corresponding parameters in the regression can be logically split into two groups so that the regression takes form where and have dimensions and are and vectors with the frisch waugh lovell theorem states that in this regression the residuals and the ols estimate will be numerically identical to the residuals and the ols estimate for in the following regression where is the annihilator matrix for regressors 
the theorem can be used to establish number of theoretical results 
for example having regression with constant and another regressor is equivalent to subtracting the means from the dependent variable and the regressor and then running the regression for the de meaned variables but without the constant term 
constrained estimation suppose it is known that the coefficients in the regression satisfy system of linear equations where is matrix of full rank and is vector of known constants where in this case least squares estimation is equivalent to minimizing the sum of squared residuals of the model subject to the constraint the constrained least squares cls estimator can be given by an explicit formula 
this expression for the constrained estimator is valid as long as the matrix xtx is invertible 
it was assumed from the beginning of this article that this matrix is of full rank and it was noted that when the rank condition fails will not be identifiable 
however it may happen that adding the restriction makes identifiable in which case one would like to find the formula for the estimator 
the estimator is equal to where is matrix such that the matrix is non singular and rtq such matrix can always be found although generally it is not unique 
the second formula coincides with the first in case when xtx is invertible 
large sample properties the least squares estimators are point estimates of the linear regression model parameters however generally we also want to know how close those estimates might be to the true values of parameters 
in other words we want to construct the interval estimates 
since we haven made any assumption about the distribution of error term it is impossible to infer the distribution of the estimators and nevertheless we can apply the central limit theorem to derive their asymptotic properties as sample size goes to infinity 
while the sample size is necessarily finite it is customary to assume that is large enough so that the true distribution of the ols estimator is close to its asymptotic limit 
we can show that under the model assumptions the least squares estimator for is consistent that is converges in probability to and asymptotically normal proof where 
intervals using this asymptotic distribution approximate two sided confidence intervals for the th component of the vector can be constructed as at the confidence level where denotes the quantile function of standard normal distribution and jj is the th diagonal element of matrix 
similarly the least squares estimator for is also consistent and asymptotically normal provided that the fourth moment of exists with limiting distribution 
these asymptotic distributions can be used for prediction testing hypotheses constructing other estimators etc as an example consider the problem of prediction 
suppose is some point within the domain of distribution of the regressors and one wants to know what the response variable would have been at that point 
the mean response is the quantity whereas the predicted response is 
clearly the predicted response is random variable its distribution can be derived from that of which allows construct confidence intervals for mean response to be constructed at the confidence level 
hypothesis testing two hypothesis tests are particularly widely used 
first one wants to know if the estimated regression equation is any better than simply predicting that all values of the response variable equal its sample mean if not it is said to have no explanatory power 
the null hypothesis of no explanatory value of the estimated regression is tested using an test 
if the calculated value is found to be large enough to exceed its critical value for the pre chosen level of significance the null hypothesis is rejected and the alternative hypothesis that the regression has explanatory power is accepted 
otherwise the null hypothesis of no explanatory power is accepted 
second for each explanatory variable of interest one wants to know whether its estimated coefficient differs significantly from zero that is whether this particular explanatory variable in fact has explanatory power in predicting the response variable 
here the null hypothesis is that the true coefficient is zero 
this hypothesis is tested by computing the coefficient statistic as the ratio of the coefficient estimate to its standard error 
if the statistic is larger than predetermined value the null hypothesis is rejected and the variable is found to have explanatory power with its coefficient significantly different from zero 
otherwise the null hypothesis of zero value of the true coefficient is accepted 
in addition the chow test is used to test whether two subsamples both have the same underlying true coefficient values 
the sum of squared residuals of regressions on each of the subsets and on the combined data set are compared by computing an statistic if this exceeds critical value the null hypothesis of no difference between the two subsets is rejected otherwise it is accepted 
example with real data the following data set gives average heights and weights for american women aged source the world almanac and book of facts 
when only one dependent variable is being modeled scatterplot will suggest the form and strength of the relationship between the dependent variable and regressors 
it might also reveal outliers heteroscedasticity and other aspects of the data that may complicate the interpretation of fitted regression model 
the scatterplot suggests that the relationship is strong and can be approximated as quadratic function 
ols can handle non linear relationships by introducing the regressor height 
the regression model then becomes multiple linear model the output from most popular statistical packages will look similar to this in this table the value column gives the least squares estimates of parameters the std error column shows standard errors of each coefficient estimate the statistic and value columns are testing whether any of the coefficients might be equal to zero 
the statistic is calculated simply as if the errors follow normal distribution follows student distribution 
under weaker conditions is asymptotically normal 
large values of indicate that the null hypothesis can be rejected and that the corresponding coefficient is not zero 
the second column value expresses the results of the hypothesis test as significance level 
conventionally values smaller than are taken as evidence that the population coefficient is nonzero 
squared is the coefficient of determination indicating goodness of fit of the regression 
this statistic will be equal to one if fit is perfect and to zero when regressors have no explanatory power whatsoever 
this is biased estimate of the population squared and will never decrease if additional regressors are added even if they are irrelevant 
adjusted squared is slightly modified version of designed to penalize for the excess number of regressors which do not add to the explanatory power of the regression 
this statistic is always smaller than can decrease as new regressors are added and even be negative for poorly fitting models log likelihood is calculated under the assumption that errors follow normal distribution 
even though the assumption is not very reasonable this statistic may still find its use in conducting lr tests 
durbin watson statistic tests whether there is any evidence of serial correlation between the residuals 
as rule of thumb the value smaller than will be an evidence of positive correlation 
akaike information criterion and schwarz criterion are both used for model selection 
generally when comparing two alternative models smaller values of one of these criteria will indicate better model 
standard error of regression is an estimate of standard error of the error term 
total sum of squares model sum of squared and residual sum of squares tell us how much of the initial variation in the sample were explained by the regression 
statistic tries to test the hypothesis that all coefficients except the intercept are equal to zero 
this statistic has distribution under the null hypothesis and normality assumption and its value indicates probability that the hypothesis is indeed true 
note that when errors are not normal this statistic becomes invalid and other tests such as wald test or lr test should be used 
ordinary least squares analysis often includes the use of diagnostic plots designed to detect departures of the data from the assumed form of the model 
these are some of the common diagnostic plots residuals against the explanatory variables in the model 
non linear relation between these variables suggests that the linearity of the conditional mean function may not hold 
different levels of variability in the residuals for different levels of the explanatory variables suggests possible heteroscedasticity 
residuals against explanatory variables not in the model 
any relation of the residuals to these variables would suggest considering these variables for inclusion in the model 
residuals against the fitted values 
residuals against the preceding residual 
this plot may identify serial correlations in the residuals an important consideration when carrying out statistical inference using regression models is how the data were sampled 
in this example the data are averages rather than measurements on individual women 
the fit of the model is very good but this does not imply that the weight of an individual woman can be predicted with high accuracy based only on her height 
sensitivity to rounding this example also demonstrates that coefficients determined by these calculations are sensitive to how the data is prepared 
the heights were originally given rounded to the nearest inch and have been converted and rounded to the nearest centimetre 
since the conversion factor is one inch to cm this is not an exact conversion 
the original inches can be recovered by round and then re converted to metric without rounding 
if this is done the results become using either of these equations to predict the weight of woman gives similar values kg with rounding vs kg without rounding 
thus seemingly small variation in the data has real effect on the coefficients but small effect on the results of the equation 
while this may look innocuous in the middle of the data range it could become significant at the extremes or in the case where the fitted model is used to project outside the data range extrapolation 
this highlights common error this example is an abuse of ols which inherently requires that the errors in the independent variable in this case height are zero or at least negligible 
the initial rounding to nearest inch plus any actual measurement errors constitute finite and non negligible error 
as result the fitted parameters are not the best estimates they are presumed to be 
though not totally spurious the error in the estimation will depend upon relative size of the and errors 
another example with less real data problem statement we can use the least square mechanism to figure out the equation of two body orbit in polar base co ordinates 
the equation typically used is cos where is the radius of how far the object is from one of the bodies 
in the equation the parameters and are used to determine the path of the orbit 
we have measured the following data 
we need to find the least squares approximation of and for the given data 
solution first we need to represent and in linear form 
so we are going to rewrite the equation as cos 
now we can use this form to represent our observational data as where is and is and is constructed by the first column being the coefficient of and the second column being the coefficient of and is the values for the respective so and 
on solving we get so and see also bayesian least squares fama macbeth regression nonlinear least squares numerical methods for linear least squares nonlinear system identification references further reading dougherty christopher 
introduction to econometrics nd ed 
new york oxford university press 
isbn gujarati damodar porter dawn 
basic econometics fifth ed 
boston mcgraw hill irwin 
isbn heij christiaan boer paul franses philip kloek teun van dijk herman 
econometric methods with applications in business and economics st ed 
oxford oxford university press 
isbn hill carter griffiths william lim guay 
principles of econometrics rd ed 
hoboken nj john wiley sons 
the simple regression model 
introductory econometrics modern approach th ed 
mason oh cengage learning
in statistical modeling regression analysis is set of statistical processes for estimating the relationships between dependent variable often called the outcome or response variable or label in machine learning parlance and one or more independent variables often called predictors covariates explanatory variables or features 
the most common form of regression analysis is linear regression in which one finds the line or more complex linear combination that most closely fits the data according to specific mathematical criterion 
for example the method of ordinary least squares computes the unique line or hyperplane that minimizes the sum of squared differences between the true data and that line or hyperplane 
for specific mathematical reasons see linear regression this allows the researcher to estimate the conditional expectation or population average value of the dependent variable when the independent variables take on given set of values 
less common forms of regression use slightly different procedures to estimate alternative location parameters quantile regression or necessary condition analysis or estimate the conditional expectation across broader collection of non linear models nonparametric regression 
regression analysis is primarily used for two conceptually distinct purposes 
first regression analysis is widely used for prediction and forecasting where its use has substantial overlap with the field of machine learning 
second in some situations regression analysis can be used to infer causal relationships between the independent and dependent variables 
importantly regressions by themselves only reveal relationships between dependent variable and collection of independent variables in fixed dataset 
to use regressions for prediction or to infer causal relationships respectively researcher must carefully justify why existing relationships have predictive power for new context or why relationship between two variables has causal interpretation 
the latter is especially important when researchers hope to estimate causal relationships using observational data 
history the earliest form of regression was the method of least squares which was published by legendre in and by gauss in legendre and gauss both applied the method to the problem of determining from astronomical observations the orbits of bodies about the sun mostly comets but also later the then newly discovered minor planets 
gauss published further development of the theory of least squares in including version of the gauss markov theorem 
the term regression was coined by francis galton in the th century to describe biological phenomenon 
the phenomenon was that the heights of descendants of tall ancestors tend to regress down towards normal average phenomenon also known as regression toward the mean 
for galton regression had only this biological meaning but his work was later extended by udny yule and karl pearson to more general statistical context 
in the work of yule and pearson the joint distribution of the response and explanatory variables is assumed to be gaussian 
this assumption was weakened by fisher in his works of and fisher assumed that the conditional distribution of the response variable is gaussian but the joint distribution need not be 
in this respect fisher assumption is closer to gauss formulation of in the and economists used electromechanical desk calculators to calculate regressions 
before it sometimes took up to hours to receive the result from one regression regression methods continue to be an area of active research 
in recent decades new methods have been developed for robust regression regression involving correlated responses such as time series and growth curves regression in which the predictor independent variable or response variables are curves images graphs or other complex data objects regression methods accommodating various types of missing data nonparametric regression bayesian methods for regression regression in which the predictor variables are measured with error regression with more predictor variables than observations and causal inference with regression 
regression model in practice researchers first select model they would like to estimate and then use their chosen method ordinary least squares to estimate the parameters of that model 
regression models involve the following components the unknown parameters often denoted as scalar or vector the independent variables which are observed in data and are often denoted as vector where denotes row of data 
the dependent variable which are observed in data and often denoted using the scalar the error terms which are not directly observed in data and are often denoted using the scalar in various fields of application different terminologies are used in place of dependent and independent variables 
most regression models propose that is function of and with representing an additive error term that may stand in for un modeled determinants of or random statistical noise the researchers goal is to estimate the function that most closely fits the data 
to carry out regression analysis the form of the function must be specified 
sometimes the form of this function is based on knowledge about the relationship between and that does not rely on the data 
if no such knowledge is available flexible or convenient form for is chosen 
for example simple univariate regression may propose suggesting that the researcher believes to be reasonable approximation for the statistical process generating the data 
once researchers determine their preferred statistical model different forms of regression analysis provide tools to estimate the parameters for example least squares including its most common variant ordinary least squares finds the value of that minimizes the sum of squared errors given regression method will ultimately provide an estimate of usually denoted to distinguish the estimate from the true unknown parameter value that generated the data 
using this estimate the researcher can then use the fitted value for prediction or to assess the accuracy of the model in explaining the data 
whether the researcher is intrinsically interested in the estimate or the predicted value will depend on context and their goals 
as described in ordinary least squares least squares is widely used because the estimated function approximates the conditional expectation 
however alternative variants least absolute deviations or quantile regression are useful when researchers want to model other functions 
it is important to note that there must be sufficient data to estimate regression model 
for example suppose that researcher has access to rows of data with one dependent and two independent variables 
suppose further that the researcher wants to estimate bivariate linear model via least squares if the researcher only has access to data points then they could find infinitely many combinations that explain the data equally well any combination can be chosen that satisfies all of which lead to and are therefore valid solutions that minimize the sum of squared residuals 
to understand why there are infinitely many options note that the system of equations is to be solved for unknowns which makes the system underdetermined 
alternatively one can visualize infinitely many dimensional planes that go through fixed points 
more generally to estimate least squares model with distinct parameters one must have distinct data points 
if then there does not generally exist set of parameters that will perfectly fit the data 
the quantity appears often in regression analysis and is referred to as the degrees of freedom in the model 
moreover to estimate least squares model the independent variables 
must be linearly independent one must not be able to reconstruct any of the independent variables by adding and multiplying the remaining independent variables 
as discussed in ordinary least squares this condition ensures that is an invertible matrix and therefore that unique solution exists 
underlying assumptions by itself regression is simply calculation using the data 
in order to interpret the output of regression as meaningful statistical quantity that measures real world relationships researchers often rely on number of classical assumptions 
these assumptions often include the sample is representative of the population at large 
the independent variables are measured with no error 
deviations from the model have an expected value of zero conditional on covariates the variance of the residuals is constant across observations homoscedasticity 
the residuals are uncorrelated with one another 
mathematically the variance covariance matrix of the errors is diagonal handful of conditions are sufficient for the least squares estimator to possess desirable properties in particular the gauss markov assumptions imply that the parameter estimates will be unbiased consistent and efficient in the class of linear unbiased estimators 
practitioners have developed variety of methods to maintain some or all of these desirable properties in real world settings because these classical assumptions are unlikely to hold exactly 
for example modeling errors in variables can lead to reasonable estimates independent variables are measured with errors 
heteroscedasticity consistent standard errors allow the variance of to change across values of correlated errors that exist within subsets of the data or follow specific patterns can be handled using clustered standard errors geographic weighted regression or newey west standard errors among other techniques 
when rows of data correspond to locations in space the choice of how to model within geographic units can have important consequences 
the subfield of econometrics is largely focused on developing techniques that allow researchers to make reasonable real world conclusions in real world settings where classical assumptions do not hold exactly 
linear regression in linear regression the model specification is that the dependent variable is linear combination of the parameters but need not be linear in the independent variables 
for example in simple linear regression for modeling data points there is one independent variable and two parameters and straight line in multiple linear regression there are several independent variables or functions of independent variables 
adding term in to the preceding regression gives parabola this is still linear regression although the expression on the right hand side is quadratic in the independent variable it is linear in the parameters and in both cases is an error term and the subscript indexes particular observation 
returning our attention to the straight line case given random sample from the population we estimate the population parameters and obtain the sample linear regression model the residual is the difference between the value of the dependent variable predicted by the model and the true value of the dependent variable one method of estimation is ordinary least squares 
this method obtains parameter estimates that minimize the sum of squared residuals ssr minimization of this function results in set of normal equations set of simultaneous linear equations in the parameters which are solved to yield the parameter estimators in the case of simple regression the formulas for the least squares estimates are where is the mean average of the values and is the mean of the values 
under the assumption that the population error term has constant variance the estimate of that variance is given by this is called the mean square error mse of the regression 
the denominator is the sample size reduced by the number of model parameters estimated from the same data for regressors or if an intercept is used 
in this case so the denominator is the standard errors of the parameter estimates are given by under the further assumption that the population error term is normally distributed the researcher can use these estimated standard errors to create confidence intervals and conduct hypothesis tests about the population parameters 
general linear model in the more general multiple regression model there are independent variables where is the th observation on the th independent variable 
if the first independent variable takes the value for all then is called the regression intercept 
the least squares parameter estimates are obtained from normal equations 
the residual can be written as the normal equations are in matrix notation the normal equations are written as where the element of is the element of the column vector is and the element of is thus is is and is the solution is 
diagnostics once regression model has been constructed it may be important to confirm the goodness of fit of the model and the statistical significance of the estimated parameters 
commonly used checks of goodness of fit include the squared analyses of the pattern of residuals and hypothesis testing 
statistical significance can be checked by an test of the overall fit followed by tests of individual parameters 
interpretations of these diagnostic tests rest heavily on the model assumptions 
although examination of the residuals can be used to invalidate model the results of test or test are sometimes more difficult to interpret if the model assumptions are violated 
for example if the error term does not have normal distribution in small samples the estimated parameters will not follow normal distributions and complicate inference 
with relatively large samples however central limit theorem can be invoked such that hypothesis testing may proceed using asymptotic approximations 
limited dependent variables limited dependent variables which are response variables that are categorical variables or are variables constrained to fall only in certain range often arise in econometrics 
the response variable may be non continuous limited to lie on some subset of the real line 
for binary zero or one variables if analysis proceeds with least squares linear regression the model is called the linear probability model 
nonlinear models for binary dependent variables include the probit and logit model 
the multivariate probit model is standard method of estimating joint relationship between several binary dependent variables and some independent variables 
for categorical variables with more than two values there is the multinomial logit 
for ordinal variables with more than two values there are the ordered logit and ordered probit models 
censored regression models may be used when the dependent variable is only sometimes observed and heckman correction type models may be used when the sample is not randomly selected from the population of interest 
an alternative to such procedures is linear regression based on polychoric correlation or polyserial correlations between the categorical variables 
such procedures differ in the assumptions made about the distribution of the variables in the population 
if the variable is positive with low values and represents the repetition of the occurrence of an event then count models like the poisson regression or the negative binomial model may be used 
nonlinear regression when the model function is not linear in the parameters the sum of squares must be minimized by an iterative procedure 
this introduces many complications which are summarized in differences between linear and non linear least squares 
interpolation and extrapolation regression models predict value of the variable given known values of the variables 
prediction within the range of values in the dataset used for model fitting is known informally as interpolation 
prediction outside this range of the data is known as extrapolation 
performing extrapolation relies strongly on the regression assumptions 
the further the extrapolation goes outside the data the more room there is for the model to fail due to differences between the assumptions and the sample data or the true values 
it is generally advised that when performing extrapolation one should accompany the estimated value of the dependent variable with prediction interval that represents the uncertainty 
such intervals tend to expand rapidly as the values of the independent variable moved outside the range covered by the observed data 
for such reasons and others some tend to say that it might be unwise to undertake extrapolation however this does not cover the full set of modeling errors that may be made in particular the assumption of particular form for the relation between and properly conducted regression analysis will include an assessment of how well the assumed form is matched by the observed data but it can only do so within the range of values of the independent variables actually available 
this means that any extrapolation is particularly reliant on the assumptions being made about the structural form of the regression relationship 
best practice advice here is that linear in variables and linear in parameters relationship should not be chosen simply for computational convenience but that all available knowledge should be deployed in constructing regression model 
if this knowledge includes the fact that the dependent variable cannot go outside certain range of values this can be made use of in selecting the model even if the observed dataset has no values particularly near such bounds 
the implications of this step of choosing an appropriate functional form for the regression can be great when extrapolation is considered 
at minimum it can ensure that any extrapolation arising from fitted model is realistic or in accord with what is known 
power and sample size calculations there are no generally agreed methods for relating the number of observations versus the number of independent variables in the model 
one method conjectured by good and hardin is where is the sample size is the number of independent variables and is the number of observations needed to reach the desired precision if the model had only one independent variable 
for example researcher is building linear regression model using dataset that contains patients 
if the researcher decides that five observations are needed to precisely define straight line then the maximum number of independent variables the model can support is because log log 
other methods although the parameters of regression model are usually estimated using the method of least squares other methods which have been used include bayesian methods 
bayesian linear regression percentage regression for situations where reducing percentage errors is deemed more appropriate 
least absolute deviations which is more robust in the presence of outliers leading to quantile regression nonparametric regression requires large number of observations and is computationally intensive scenario optimization leading to interval predictor models distance metric learning which is learned by the search of meaningful distance metric in given input space 
software all major statistical software packages perform least squares regression analysis and inference 
simple linear regression and multiple regression using least squares can be done in some spreadsheet applications and on some calculators 
while many statistical software packages can perform various types of nonparametric and robust regression these methods are less standardized 
different software packages implement different methods and method with given name may be implemented differently in different packages 
specialized regression software has been developed for use in fields such as survey analysis and neuroimaging 
see also references further reading william kruskal and judith tanur ed 
linear hypotheses international encyclopedia of statistics 
free press evan williams regression pp 
analysis of variance pp 
regression and correlation analysis new palgrave dictionary of economics pp 
birkes david and dodge alternative methods of regression 
isbn chatfield calculating interval forecasts journal of business and economic statistics pp 
applied regression analysis rd ed 
applied regression analysis linear models and related methods 
sage hardle applied nonparametric regression isbn meade nigel islam towhidul 
prediction intervals for growth curve forecasts 
sen srivastava regression analysis theory methods and applications springer verlag berlin th printing 
strutz data fitting and uncertainty practical introduction to weighted least squares and beyond 
vieweg teubner isbn stulp freek and olivier sigaud 
many regression algorithms one unified model review 
https doi org neunet 
operations and production systems with multiple objectives 
external links regression analysis encyclopedia of mathematics ems press earliest uses regression basic history and references what is multiple regression used for 
multiple regression regression of weakly correlated data how linear regression mistakes can appear when range is much smaller than range
in statistics principal component regression pcr is regression analysis technique that is based on principal component analysis pca 
more specifically pcr is used for estimating the unknown regression coefficients in standard linear regression model 
in pcr instead of regressing the dependent variable on the explanatory variables directly the principal components of the explanatory variables are used as regressors 
one typically uses only subset of all the principal components for regression making pcr kind of regularized procedure and also type of shrinkage estimator 
often the principal components with higher variances the ones based on eigenvectors corresponding to the higher eigenvalues of the sample variance covariance matrix of the explanatory variables are selected as regressors 
however for the purpose of predicting the outcome the principal components with low variances may also be important in some cases even more important one major use of pcr lies in overcoming the multicollinearity problem which arises when two or more of the explanatory variables are close to being collinear 
pcr can aptly deal with such situations by excluding some of the low variance principal components in the regression step 
in addition by usually regressing on only subset of all the principal components pcr can result in dimension reduction through substantially lowering the effective number of parameters characterizing the underlying model 
this can be particularly useful in settings with high dimensional covariates 
also through appropriate selection of the principal components to be used for regression pcr can lead to efficient prediction of the outcome based on the assumed model 
the principle the pcr method may be broadly divided into three major steps perform pca on the observed data matrix for the explanatory variables to obtain the principal components and then usually select subset based on some appropriate criteria of the principal components so obtained for further use 
now regress the observed vector of outcomes on the selected principal components as covariates using ordinary least squares regression linear regression to get vector of estimated regression coefficients with dimension equal to the number of selected principal components now transform this vector back to the scale of the actual covariates using the selected pca loadings the eigenvectors corresponding to the selected principal components to get the final pcr estimator with dimension equal to the total number of covariates for estimating the regression coefficients characterizing the original model 
details of the method data representation let denote the vector of observed outcomes and denote the corresponding data matrix of observed covariates where and denote the size of the observed sample and the number of covariates respectively with each of the rows of denotes one set of observations for the dimensional covariate and the respective entry of denotes the corresponding observed outcome 
data pre processing assume that and each of the columns of have already been centered so that all of them have zero empirical means 
this centering step is crucial at least for the columns of since pcr involves the use of pca on and pca is sensitive to centering of the data 
underlying model following centering the standard gauss markov linear regression model for on can be represented as where denotes the unknown parameter vector of regression coefficients and denotes the vector of random errors with and var for some unknown variance parameter objective the primary goal is to obtain an efficient estimator for the parameter based on the data 
one frequently used approach for this is ordinary least squares regression which assuming is full column rank gives the unbiased estimator of pcr is another technique that may be used for the same purpose of estimating pca step pcr starts by performing pca on the centered data matrix for this let denote the singular value decomposition of where diag with denoting the non negative singular values of while the columns of and are both orthonormal sets of vectors denoting the left and right singular vectors of respectively 
the principal components gives spectral decomposition of where diag diag with denoting the non negative eigenvalues also known as the principal values of while the columns of denote the corresponding orthonormal set of eigenvectors 
then and respectively denote the principal component and the principal component direction or pca loading corresponding to the th largest principal value for each 
derived covariates for any let denote the matrix with orthonormal columns consisting of the first columns of let denote the matrix having the first principal components as its columns 
may be viewed as the data matrix obtained by using the transformed covariates instead of using the original covariates the pcr estimator let denote the vector of estimated regression coefficients obtained by ordinary least squares regression of the response vector on the data matrix then for any the final pcr estimator of based on using the first principal components is given by fundamental characteristics and applications of the pcr estimator two basic properties the fitting process for obtaining the pcr estimator involves regressing the response vector on the derived data matrix which has orthogonal columns for any since the principal components are mutually orthogonal to each other 
thus in the regression step performing multiple linear regression jointly on the selected principal components as covariates is equivalent to carrying out independent simple linear regressions or univariate regressions separately on each of the selected principal components as covariate 
when all the principal components are selected for regression so that then the pcr estimator is equivalent to the ordinary least squares estimator 
thus this is easily seen from the fact that and also observing that is an orthogonal matrix 
variance reduction for any the variance of is given by var diag in particular var var hence for all we have var var thus for all we have var var where indicates that square symmetric matrix is non negative definite 
consequently any given linear form of the pcr estimator has lower variance compared to that of the same linear form of the ordinary least squares estimator 
addressing multicollinearity under multicollinearity two or more of the covariates are highly correlated so that one can be linearly predicted from the others with non trivial degree of accuracy 
consequently the columns of the data matrix that correspond to the observations for these covariates tend to become linearly dependent and therefore tends to become rank deficient losing its full column rank structure 
more quantitatively one or more of the smaller eigenvalues of get very close or become exactly equal to under such situations 
the variance expressions above indicate that these small eigenvalues have the maximum inflation effect on the variance of the least squares estimator thereby destabilizing the estimator significantly when they are close to this issue can be effectively addressed through using pcr estimator obtained by excluding the principal components corresponding to these small eigenvalues 
dimension reduction pcr may also be used for performing dimension reduction 
to see this let denote any matrix having orthonormal columns for any 
suppose now that we want to approximate each of the covariate observations through the rank linear transformation for some 
then it can be shown that is minimized at the matrix with the first principal component directions as columns and the corresponding dimensional derived covariates 
thus the dimensional principal components provide the best linear approximation of rank to the observed data matrix the corresponding reconstruction error is given by thus any potential dimension reduction may be achieved by choosing the number of principal components to be used through appropriate thresholding on the cumulative sum of the eigenvalues of since the smaller eigenvalues do not contribute significantly to the cumulative sum the corresponding principal components may be continued to be dropped as long as the desired threshold limit is not exceeded 
the same criteria may also be used for addressing the multicollinearity issue whereby the principal components corresponding to the smaller eigenvalues may be ignored as long as the threshold limit is maintained 
regularization effect since the pcr estimator typically uses only subset of all the principal components for regression it can be viewed as some sort of regularized procedure 
more specifically for any the pcr estimator denotes the regularized solution to the following constrained minimization problem min subject to 
the constraint may be equivalently written as where 
thus when only proper subset of all the principal components are selected for regression the pcr estimator so obtained is based on hard form of regularization that constrains the resulting solution to the column space of the selected principal component directions and consequently restricts it to be orthogonal to the excluded directions 
optimality of pcr among class of regularized estimators given the constrained minimization problem as defined above consider the following generalized version of it min subject to where denotes any full column rank matrix of order with let denote the corresponding solution 
thus arg min subject to then the optimal choice of the restriction matrix for which the corresponding estimator achieves the minimum prediction error is given by where diag 
quite clearly the resulting optimal estimator is then simply given by the pcr estimator based on the first principal components 
efficiency since the ordinary least squares estimator is unbiased for we have var mse where mse denotes the mean squared error 
now if for some we additionally have then the corresponding is also unbiased for and therefore var mse 
we have already seen that var var which then implies mse mse for that particular thus in that case the corresponding would be more efficient estimator of compared to based on using the mean squared error as the performance criteria 
in addition any given linear form of the corresponding would also have lower mean squared error compared to that of the same linear form of now suppose that for given then the corresponding is biased for however since var var it is still possible that mse mse especially if is such that the excluded principal components correspond to the smaller eigenvalues thereby resulting in lower bias 
in order to ensure efficient estimation and prediction performance of pcr as an estimator of park proposes the following guideline for selecting the principal components to be used for regression drop the principal component if and only if practical implementation of this guideline of course requires estimates for the unknown model parameters and in general they may be estimated using the unrestricted least squares estimates obtained from the original full model 
park however provides slightly modified set of estimates that may be better suited for this purpose unlike the criteria based on the cumulative sum of the eigenvalues of which is probably more suited for addressing the multicollinearity problem and for performing dimension reduction the above criteria actually attempts to improve the prediction and estimation efficiency of the pcr estimator by involving both the outcome as well as the covariates in the process of selecting the principal components to be used in the regression step 
alternative approaches with similar goals include selection of the principal components based on cross validation or the mallow cp criteria 
often the principal components are also selected based on their degree of association with the outcome 
shrinkage effect of pcr in general pcr is essentially shrinkage estimator that usually retains the high variance principal components corresponding to the higher eigenvalues of as covariates in the model and discards the remaining low variance components corresponding to the lower eigenvalues of 
thus it exerts discrete shrinkage effect on the low variance components nullifying their contribution completely in the original model 
in contrast the ridge regression estimator exerts smooth shrinkage effect through the regularization parameter or the tuning parameter inherently involved in its construction 
while it does not completely discard any of the components it exerts shrinkage effect over all of them in continuous manner so that the extent of shrinkage is higher for the low variance components and lower for the high variance components 
frank and friedman conclude that for the purpose of prediction itself the ridge estimator owing to its smooth shrinkage effect is perhaps better choice compared to the pcr estimator having discrete shrinkage effect 
in addition the principal components are obtained from the eigen decomposition of that involves the observations for the explanatory variables only 
therefore the resulting pcr estimator obtained from using these principal components as covariates need not necessarily have satisfactory predictive performance for the outcome 
somewhat similar estimator that tries to address this issue through its very construction is the partial least squares pls estimator 
similar to pcr pls also uses derived covariates of lower dimensions 
however unlike pcr the derived covariates for pls are obtained based on using both the outcome as well as the covariates 
while pcr seeks the high variance directions in the space of the covariates pls seeks the directions in the covariate space that are most useful for the prediction of the outcome 
variant of the classical pcr known as the supervised pcr was proposed 
in spirit similar to that of pls it attempts at obtaining derived covariates of lower dimensions based on criterion that involves both the outcome as well as the covariates 
the method starts by performing set of simple linear regressions or univariate regressions wherein the outcome vector is regressed separately on each of the covariates taken one at time 
then for some the first covariates that turn out to be the most correlated with the outcome based on the degree of significance of the corresponding estimated regression coefficients are selected for further use 
conventional pcr as described earlier is then performed but now it is based on only the data matrix corresponding to the observations for the selected covariates 
the number of covariates used and the subsequent number of principal components used are usually selected by cross validation 
generalization to kernel settings the classical pcr method as described above is based on classical pca and considers linear regression model for predicting the outcome based on the covariates 
however it can be easily generalized to kernel machine setting whereby the regression function need not necessarily be linear in the covariates but instead it can belong to the reproducing kernel hilbert space associated with any arbitrary possibly non linear symmetric positive definite kernel 
the linear regression model turns out to be special case of this setting when the kernel function is chosen to be the linear kernel 
in general under the kernel machine setting the vector of covariates is first mapped into high dimensional potentially infinite dimensional feature space characterized by the kernel function chosen 
the mapping so obtained is known as the feature map and each of its coordinates also known as the feature elements corresponds to one feature may be linear or non linear of the covariates 
the regression function is then assumed to be linear combination of these feature elements 
thus the underlying regression model in the kernel machine setting is essentially linear regression model with the understanding that instead of the original set of covariates the predictors are now given by the vector potentially infinite dimensional of feature elements obtained by transforming the actual covariates using the feature map 
however the kernel trick actually enables us to operate in the feature space without ever explicitly computing the feature map 
it turns out that it is only sufficient to compute the pairwise inner products among the feature maps for the observed covariate vectors and these inner products are simply given by the values of the kernel function evaluated at the corresponding pairs of covariate vectors 
the pairwise inner products so obtained may therefore be represented in the form of symmetric non negative definite matrix also known as the kernel matrix 
pcr in the kernel machine setting can now be implemented by first appropriately centering this kernel matrix say with respect to the feature space and then performing kernel pca on the centered kernel matrix say whereby an eigendecomposition of is obtained 
kernel pcr then proceeds by usually selecting subset of all the eigenvectors so obtained and then performing standard linear regression of the outcome vector on these selected eigenvectors 
the eigenvectors to be used for regression are usually selected using cross validation 
the estimated regression coefficients having the same dimension as the number of selected eigenvectors along with the corresponding selected eigenvectors are then used for predicting the outcome for future observation 
in machine learning this technique is also known as spectral regression 
clearly kernel pcr has discrete shrinkage effect on the eigenvectors of quite similar to the discrete shrinkage effect of classical pcr on the principal components as discussed earlier 
however the feature map associated with the chosen kernel could potentially be infinite dimensional and hence the corresponding principal components and principal component directions could be infinite dimensional as well 
therefore these quantities are often practically intractable under the kernel machine setting 
kernel pcr essentially works around this problem by considering an equivalent dual formulation based on using the spectral decomposition of the associated kernel matrix 
under the linear regression model which corresponds to choosing the kernel function as the linear kernel this amounts to considering spectral decomposition of the corresponding kernel matrix and then regressing the outcome vector on selected subset of the eigenvectors of so obtained 
it can be easily shown that this is the same as regressing the outcome vector on the corresponding principal components which are finite dimensional in this case as defined in the context of the classical pcr 
thus for the linear kernel the kernel pcr based on dual formulation is exactly equivalent to the classical pcr based on primal formulation 
however for arbitrary and possibly non linear kernels this primal formulation may become intractable owing to the infinite dimensionality of the associated feature map 
thus classical pcr becomes practically infeasible in that case but kernel pcr based on the dual formulation still remains valid and computationally scalable 
see also principal component analysis partial least squares regression ridge regression canonical correlation deming regression total sum of squares references further reading amemiya takeshi
weighted least squares wls also known as weighted linear regression is generalization of ordinary least squares and linear regression in which knowledge of the variance of observations is incorporated into the regression 
wls is also specialization of generalized least squares 
introduction special case of generalized least squares called weighted least squares can be used when all the off diagonal entries of the covariance matrix of the residuals are null the variances of the observations along the covariance matrix diagonal may still be unequal heteroscedasticity 
the fit of model to data point is measured by its residual defined as the difference between measured value of the dependent variable and the value predicted by the model 
if the errors are uncorrelated and have equal variance then the function is minimised at such that the gauss markov theorem shows that when this is so is best linear unbiased estimator blue 
if however the measurements are uncorrelated but have different uncertainties modified approach might be adopted 
aitken showed that when weighted sum of squared residuals is minimized is the blue if each weight is equal to the reciprocal of the variance of the measurement the gradient equations for this sum of squares are which in linear least squares system give the modified normal equations when the observational errors are uncorrelated and the weight matrix is diagonal these may be written as if the errors are correlated the resulting estimator is the blue if the weight matrix is equal to the inverse of the variance covariance matrix of the observations 
when the errors are uncorrelated it is convenient to simplify the calculations to factor the weight matrix as the normal equations can then be written in the same form as ordinary least squares where we define the following scaled matrix and vector diag diag this is type of whitening transformation the last expression involves an entrywise division 
for non linear least squares systems similar argument shows that the normal equations should be modified as follows 
note that for empirical tests the appropriate is not known for sure and must be estimated 
for this feasible generalized least squares fgls techniques may be used in this case it is specialized for diagonal covariance matrix thus yielding feasible weighted least squares solution 
if the uncertainty of the observations is not known from external sources then the weights could be estimated from the given observations 
this can be useful for example to identify outliers 
after the outliers have been removed from the data set the weights should be reset to one 
motivation in some cases the observations may be weighted for example they may not be equally reliable 
in this case one can minimize the weighted sum of squares where wi is the weight of the ith observation and is the diagonal matrix of such weights 
the weights should ideally be equal to the reciprocal of the variance of the measurement 
this implies that the observations are uncorrelated 
if the observations are correlated the expression applies 
in this case the weight matrix should ideally be equal to the inverse of the variance covariance matrix of the observations 
the normal equations are then this method is used in iteratively reweighted least squares 
parameter errors and correlation the estimated parameter values are linear combinations of the observed values therefore an expression for the estimated variance covariance matrix of the parameter estimates can be obtained by error propagation from the errors in the observations 
let the variance covariance matrix for the observations be denoted by and that of the estimated parameters by 
then when this simplifies to when unit weights are used the identity matrix it is implied that the experimental errors are uncorrelated and all equal where is the priori variance of an observation 
in any case is approximated by the reduced chi squared where is the minimum value of the weighted objective function the denominator is the number of degrees of freedom see effective degrees of freedom for generalizations for the case of correlated observations 
in all cases the variance of the parameter estimate is given by and the covariance between the parameter estimates and is given by the standard deviation is the square root of variance and the correlation coefficient is given by 
these error estimates reflect only random errors in the measurements 
the true uncertainty in the parameters is larger due to the presence of systematic errors which by definition cannot be quantified 
note that even though the observations may be uncorrelated the parameters are typically correlated 
parameter confidence limits it is often assumed for want of any concrete evidence but often appealing to the central limit theorem see normal distribution occurrence and applications that the error on each observation belongs to normal distribution with mean of zero and standard deviation under that assumption the following probabilities can be derived for single scalar parameter estimate in terms of its estimated standard error given here that the interval encompasses the true coefficient value that the interval encompasses the true coefficient value that the interval encompasses the true coefficient valuethe assumption is not unreasonable when if the experimental errors are normally distributed the parameters will belong to student distribution with degrees of freedom 
when student distribution approximates normal distribution 
note however that these confidence limits cannot take systematic error into account 
also parameter errors should be quoted to one significant figure only as they are subject to sampling error when the number of observations is relatively small chebychev inequality can be used for an upper bound on probabilities regardless of any assumptions about the distribution of experimental errors the maximum probabilities that parameter will be more than or standard deviations away from its expectation value are and respectively 
residual values and correlation the residuals are related to the observations by where is the idempotent matrix known as the hat matrix and is the identity matrix 
the variance covariance matrix of the residuals is given by thus the residuals are correlated even if the observations are not 
when the sum of weighted residual values is equal to zero whenever the model function contains constant term 
left multiply the expression for the residuals by xt wt say for example that the first term of the model is constant so that for all in that case it follows that thus in the motivational example above the fact that the sum of residual values is equal to zero is not accidental but is consequence of the presence of the constant term in the model 
if experimental error follows normal distribution then because of the linear relationship between residuals and observations so should residuals but since the observations are only sample of the population of all possible observations the residuals should belong to student distribution 
studentized residuals are useful in making statistical test for an outlier when particular residual appears to be excessively large 
see also iteratively reweighted least squares heteroscedasticity consistent standard errors weighted mean references
linear least squares lls is the least squares approximation of linear functions to data 
it is set of formulations for solving statistical problems involved in linear regression including variants for ordinary unweighted weighted and generalized correlated residuals 
numerical methods for linear least squares include inverting the matrix of the normal equations and orthogonal decomposition methods 
main formulations the three main linear least squares formulations are ordinary least squares ols is the most common estimator 
ols estimates are commonly used to analyze both experimental and observational data 
the ols method minimizes the sum of squared residuals and leads to closed form expression for the estimated value of the unknown parameter vector where is vector whose ith element is the ith observation of the dependent variable and is matrix whose ij element is the ith observation of the jth independent variable 
the estimator is unbiased and consistent if the errors have finite variance and are uncorrelated with the regressors where is the transpose of row of the matrix it is also efficient under the assumption that the errors have finite variance and are homoscedastic meaning that xi does not depend on the condition that the errors are uncorrelated with the regressors will generally be satisfied in an experiment but in the case of observational data it is difficult to exclude the possibility of an omitted covariate that is related to both the observed covariates and the response variable 
the existence of such covariate will generally lead to correlation between the regressors and the response variable and hence to an inconsistent estimator of the condition of homoscedasticity can fail with either experimental or observational data 
if the goal is either inference or predictive modeling the performance of ols estimates can be poor if multicollinearity is present unless the sample size is large 
weighted least squares wls are used when heteroscedasticity is present in the error terms of the model 
generalized least squares gls is an extension of the ols method that allows efficient estimation of when either heteroscedasticity or correlations or both are present among the error terms of the model as long as the form of heteroscedasticity and correlation is known independently of the data 
to handle heteroscedasticity when the error terms are uncorrelated with each other gls minimizes weighted analogue to the sum of squared residuals from ols regression where the weight for the ith case is inversely proportional to var 
this special case of gls is called weighted least squares 
the gls solution to an estimation problem is where is the covariance matrix of the errors 
gls can be viewed as applying linear transformation to the data so that the assumptions of ols are met for the transformed data 
for gls to be applied the covariance structure of the errors must be known up to multiplicative constant 
alternative formulations other formulations include iteratively reweighted least squares irls is used when heteroscedasticity or correlations or both are present among the error terms of the model but where little is known about the covariance structure of the errors independently of the data 
in the first iteration ols or gls with provisional covariance structure is carried out and the residuals are obtained from the fit 
based on the residuals an improved estimate of the covariance structure of the errors can usually be obtained 
subsequent gls iteration is then performed using this estimate of the error structure to define the weights 
the process can be iterated to convergence but in many cases only one iteration is sufficient to achieve an efficient estimate of instrumental variables regression iv can be performed when the regressors are correlated with the errors 
in this case we need the existence of some auxiliary instrumental variables zi such that zi if is the matrix of instruments then the estimator can be given in closed form as optimal instruments regression is an extension of classical iv regression to the situation where zi total least squares tls is an approach to least squares estimation of the linear regression model that treats the covariates and response variable in more geometrically symmetric manner than ols 
it is one approach to handling the errors in variables problem and is also sometimes used even when the covariates are assumed to be error free percentage least squares focuses on reducing percentage errors which is useful in the field of forecasting or time series analysis 
it is also useful in situations where the dependent variable has wide range without constant variance as here the larger residuals at the upper end of the range would dominate if ols were used 
when the percentage or relative error is normally distributed least squares percentage regression provides maximum likelihood estimates 
percentage regression is linked to multiplicative error model whereas ols is linked to models containing an additive error term constrained least squares indicates linear least squares problem with additional constraints on the solution 
objective function in ols assuming unweighted observations the optimal value of the objective function is found by substituting the optimal expression for the coefficient vector where the latter equality holding since is symmetric and idempotent 
it can be shown from this that under an appropriate assignment of weights the expected value of is if instead unit weights are assumed the expected value of is where is the variance of each observation 
if it is assumed that the residuals belong to normal distribution the objective function being sum of weighted squared residuals will belong to chi squared distribution with degrees of freedom 
some illustrative percentile values of are given in the following table 
these values can be used for statistical criterion as to the goodness of fit 
when unit weights are used the numbers should be divided by the variance of an observation 
for wls the ordinary objective function above is replaced for weighted average of residuals 
discussion in statistics and mathematics linear least squares is an approach to fitting mathematical or statistical model to data in cases where the idealized value provided by the model for any data point is expressed linearly in terms of the unknown parameters of the model 
the resulting fitted model can be used to summarize the data to predict unobserved values from the same system and to understand the mechanisms that may underlie the system 
mathematically linear least squares is the problem of approximately solving an overdetermined system of linear equations where is not an element of the column space of the matrix the approximate solution is realized as an exact solution to where is the projection of onto the column space of the best approximation is then that which minimizes the sum of squared differences between the data values and their corresponding modeled values 
the approach is called linear least squares since the assumed function is linear in the parameters to be estimated 
linear least squares problems are convex and have closed form solution that is unique provided that the number of data points used for fitting equals or exceeds the number of unknown parameters except in special degenerate situations 
in contrast non linear least squares problems generally must be solved by an iterative procedure and the problems can be non convex with multiple optima for the objective function 
if prior distributions are available then even an underdetermined system can be solved using the bayesian mmse estimator 
in statistics linear least squares problems correspond to particularly important type of statistical model called linear regression which arises as particular form of regression analysis 
one basic form of such model is an ordinary least squares model 
the present article concentrates on the mathematical aspects of linear least squares problems with discussion of the formulation and interpretation of statistical regression models and statistical inferences related to these being dealt with in the articles just mentioned 
see outline of regression analysis for an outline of the topic 
properties if the experimental errors are uncorrelated have mean of zero and constant variance the gauss markov theorem states that the least squares estimator has the minimum variance of all estimators that are linear combinations of the observations 
in this sense it is the best or optimal estimator of the parameters 
note particularly that this property is independent of the statistical distribution function of the errors 
in other words the distribution function of the errors need not be normal distribution 
however for some probability distributions there is no guarantee that the least squares solution is even possible given the observations still in such cases it is the best estimator that is both linear and unbiased 
for example it is easy to show that the arithmetic mean of set of measurements of quantity is the least squares estimator of the value of that quantity 
if the conditions of the gauss markov theorem apply the arithmetic mean is optimal whatever the distribution of errors of the measurements might be 
however in the case that the experimental errors do belong to normal distribution the least squares estimator is also maximum likelihood estimator these properties underpin the use of the method of least squares for all types of data fitting even when the assumptions are not strictly valid 
limitations an assumption underlying the treatment given above is that the independent variable is free of error 
in practice the errors on the measurements of the independent variable are usually much smaller than the errors on the dependent variable and can therefore be ignored 
when this is not the case total least squares or more generally errors in variables models or rigorous least squares should be used 
this can be done by adjusting the weighting scheme to take into account errors on both the dependent and independent variables and then following the standard procedure in some cases the weighted normal equations matrix xtx is ill conditioned 
when fitting polynomials the normal equations matrix is vandermonde matrix 
vandermonde matrices become increasingly ill conditioned as the order of the matrix increases 
in these cases the least squares estimate amplifies the measurement noise and may be grossly inaccurate 
various regularization techniques can be applied in such cases the most common of which is called ridge regression 
if further information about the parameters is known for example range of possible values of then various techniques can be used to increase the stability of the solution 
for example see constrained least squares 
another drawback of the least squares estimator is the fact that the norm of the residuals is minimized whereas in some cases one is truly interested in obtaining small error in the parameter small value of 
however since the true parameter is necessarily unknown this quantity cannot be directly minimized 
if prior probability on is known then bayes estimator can be used to minimize the mean squared error 
the least squares method is often applied when no prior is known 
surprisingly when several parameters are being estimated jointly better estimators can be constructed an effect known as stein phenomenon 
for example if the measurement error is gaussian several estimators are known which dominate or outperform the least squares technique the best known of these is the james stein estimator 
this is an example of more general shrinkage estimators that have been applied to regression problems 
applications polynomial fitting models are polynomials in an independent variable straight line quadratic cubic quartic and higher polynomials 
for regression with high order polynomials the use of orthogonal polynomials is recommended 
numerical smoothing and differentiation this is an application of polynomial fitting 
multinomials in more than one independent variable including surface fitting curve fitting with splines chemometrics calibration curve standard addition gran plot analysis of mixtures uses in data fitting the primary application of linear least squares is in data fitting 
given set of data points consisting of experimentally measured values taken at values of an independent variable may be scalar or vector quantities and given model function with it is desired to find the parameters such that the model function best fits the data 
in linear least squares linearity is meant to be with respect to parameters so here the functions may be nonlinear with respect to the variable ideally the model function fits the data exactly so for all this is usually not possible in practice as there are more data points than there are parameters to be determined 
the approach chosen then is to find the minimal possible value of the sum of squares of the residuals so to minimize the function after substituting for and then for this minimization problem becomes the quadratic minimization problem above with and the best fit can be found by solving the normal equations 
example as result of an experiment four data points were obtained and shown in red in the diagram on the right 
we hope to find line that best fits these four points 
in other words we would like to find the numbers and that approximately solve the overdetermined linear system of four equations in two unknowns in some best sense 
represents the residual at each point between the curve fit and the data the least squares approach to solving this problem is to try to make the sum of the squares of these residuals as small as possible that is to find the minimum of the function the minimum is determined by calculating the partial derivatives of with respect to and and setting them to zero this results in system of two equations in two unknowns called the normal equations which when solved give and the equation is the line of best fit 
the residuals that is the differences between the values from the observations and the predicated variables by using the line of best fit are then found to be and see the diagram on the right 
the minimum value of the sum of squares of the residuals is more generally one can have regressors and linear model using quadratic model importantly in linear least squares we are not restricted to using line as the model as in the above example 
for instance we could have chosen the restricted quadratic model this model is still linear in the parameter so we can still perform the same analysis constructing system of equations from the data points the partial derivatives with respect to the parameters this time there is only one are again computed and set to and solved leading to the resulting best fit model 
see also line line intersection nearest point to non intersecting lines an application line fitting nonlinear least squares regularized least squares simple linear regression partial least squares regression linear function references further reading bevington philip robinson keith 
data reduction and error analysis for the physical sciences 
external links least squares fitting from mathworld least squares fitting polynomial from mathworld
in signal processing independent component analysis ica is computational method for separating multivariate signal into additive subcomponents 
this is done by assuming that at most one subcomponent is gaussian and that the subcomponents are statistically independent from each other 
ica is special case of blind source separation 
common example application is the cocktail party problem of listening in on one person speech in noisy room 
introduction independent component analysis attempts to decompose multivariate signal into independent non gaussian signals 
as an example sound is usually signal that is composed of the numerical addition at each time of signals from several sources 
the question then is whether it is possible to separate these contributing sources from the observed total signal 
when the statistical independence assumption is correct blind ica separation of mixed signal gives very good results 
it is also used for signals that are not supposed to be generated by mixing for analysis purposes 
simple application of ica is the cocktail party problem where the underlying speech signals are separated from sample data consisting of people talking simultaneously in room 
usually the problem is simplified by assuming no time delays or echoes 
note that filtered and delayed signal is copy of dependent component and thus the statistical independence assumption is not violated 
mixing weights for constructing the observed signals from the components can be placed in an matrix 
an important thing to consider is that if sources are present at least observations 
microphones if the observed signal is audio are needed to recover the original signals 
when there are an equal number of observations and source signals the mixing matrix is square 
other cases of underdetermined and overdetermined have been investigated 
that the ica separation of mixed signals gives very good results is based on two assumptions and three effects of mixing source signals 
two assumptions the source signals are independent of each other 
the values in each source signal have non gaussian distributions three effects of mixing source signals independence as per assumption the source signals are independent however their signal mixtures are not 
this is because the signal mixtures share the same source signals 
normality according to the central limit theorem the distribution of sum of independent random variables with finite variance tends towards gaussian distribution loosely speaking sum of two independent random variables usually has distribution that is closer to gaussian than any of the two original variables 
here we consider the value of each signal as the random variable 
complexity the temporal complexity of any signal mixture is greater than that of its simplest constituent source signal those principles contribute to the basic establishment of ica 
if the signals extracted from set of mixtures are independent and have non gaussian histograms or have low complexity then they must be source signals 
defining component independence ica finds the independent components also called factors latent variables or sources by maximizing the statistical independence of the estimated components 
we may choose one of many ways to define proxy for independence and this choice governs the form of the ica algorithm 
the two broadest definitions of independence for ica are minimization of mutual information maximization of non gaussianitythe minimization of mutual information mmi family of ica algorithms uses measures like kullback leibler divergence and maximum entropy 
the non gaussianity family of ica algorithms motivated by the central limit theorem uses kurtosis and negentropy 
typical algorithms for ica use centering subtract the mean to create zero mean signal whitening usually with the eigenvalue decomposition and dimensionality reduction as preprocessing steps in order to simplify and reduce the complexity of the problem for the actual iterative algorithm 
whitening and dimension reduction can be achieved with principal component analysis or singular value decomposition 
whitening ensures that all dimensions are treated equally priori before the algorithm is run 
well known algorithms for ica include infomax fastica jade and kernel independent component analysis among others 
in general ica cannot identify the actual number of source signals uniquely correct ordering of the source signals nor the proper scaling including sign of the source signals 
ica is important to blind signal separation and has many practical applications 
it is closely related to or even special case of the search for factorial code of the data new vector valued representation of each data vector such that it gets uniquely encoded by the resulting code vector loss free coding but the code components are statistically independent 
mathematical definitions linear independent component analysis can be divided into noiseless and noisy cases where noiseless ica is special case of noisy ica 
nonlinear ica should be considered as separate case 
general definition the data are represented by the observed random vector and the hidden components as the random vector the task is to transform the observed data using linear static transformation as into vector of maximally independent components measured by some function of independence 
generative model linear noiseless ica the components of the observed random vector are generated as sum of the independent components weighted by the mixing weights the same generative model can be written in vector form as where the observed random vector is represented by the basis vectors the basis vectors form the columns of the mixing matrix and the generative formula can be written as where given the model and realizations samples of the random vector the task is to estimate both the mixing matrix and the sources this is done by adaptively calculating the vectors and setting up cost function which either maximizes the non gaussianity of the calculated or minimizes the mutual information 
in some cases priori knowledge of the probability distributions of the sources can be used in the cost function 
the original sources can be recovered by multiplying the observed signals with the inverse of the mixing matrix also known as the unmixing matrix 
here it is assumed that the mixing matrix is square 
if the number of basis vectors is greater than the dimensionality of the observed vectors the task is overcomplete but is still solvable with the pseudo inverse 
linear noisy ica with the added assumption of zero mean and uncorrelated gaussian noise diag the ica model takes the form nonlinear ica the mixing of the sources does not need to be linear 
using nonlinear mixing function with parameters the nonlinear ica model is identifiability the independent components are identifiable up to permutation and scaling of the sources 
this identifiability requires that at most one of the sources is gaussian the number of observed mixtures must be at least as large as the number of estimated components it is equivalent to say that the mixing matrix must be of full rank for its inverse to exist 
binary ica special variant of ica is binary ica in which both signal sources and monitors are in binary form and observations from monitors are disjunctive mixtures of binary independent sources 
the problem was shown to have applications in many domains including medical diagnosis multi cluster assignment network tomography and internet resource management 
let be the set of binary variables from monitors and be the set of binary variables from sources 
source monitor connections are represented by the unknown mixing matrix where indicates that signal from the th source can be observed by the th monitor 
the system works as follows at any time if source is active and it is connected to the monitor then the monitor will observe some activity 
formally we have where is boolean and and is boolean or 
note that noise is not explicitly modelled rather can be treated as independent sources 
the above problem can be heuristically solved by assuming variables are continuous and running fastica on binary observation data to get the mixing matrix real values then apply round number techniques on to obtain the binary values 
this approach has been shown to produce highly inaccurate result another method is to use dynamic programming recursively breaking the observation matrix into its sub matrices and run the inference algorithm on these sub matrices 
the key observation which leads to this algorithm is the sub matrix of where corresponds to the unbiased observation matrix of hidden components that do not have connection to the th monitor 
experimental results from show that this approach is accurate under moderate noise levels 
the generalized binary ica framework introduces broader problem formulation which does not necessitate any knowledge on the generative model 
in other words this method attempts to decompose source into its independent components as much as possible and without losing any information with no prior assumption on the way it was generated 
although this problem appears quite complex it can be accurately solved with branch and bound search tree algorithm or tightly upper bounded with single multiplication of matrix with vector 
methods for blind source separation projection pursuit signal mixtures tend to have gaussian probability density functions and source signals tend to have non gaussian probability density functions 
each source signal can be extracted from set of signal mixtures by taking the inner product of weight vector and those signal mixtures where this inner product provides an orthogonal projection of the signal mixtures 
the remaining challenge is finding such weight vector 
one type of method for doing so is projection pursuit projection pursuit seeks one projection at time such that the extracted signal is as non gaussian as possible 
this contrasts with ica which typically extracts signals simultaneously from signal mixtures which requires estimating unmixing matrix 
one practical advantage of projection pursuit over ica is that fewer than signals can be extracted if required where each source signal is extracted from signal mixtures using an element weight vector 
we can use kurtosis to recover the multiple source signal by finding the correct weight vectors with the use of projection pursuit 
the kurtosis of the probability density function of signal for finite sample is computed as where is the sample mean of the extracted signals 
the constant ensures that gaussian signals have zero kurtosis super gaussian signals have positive kurtosis and sub gaussian signals have negative kurtosis 
the denominator is the variance of and ensures that the measured kurtosis takes account of signal variance 
the goal of projection pursuit is to maximize the kurtosis and make the extracted signal as non normal as possible 
using kurtosis as measure of non normality we can now examine how the kurtosis of signal extracted from set of mixtures varies as the weight vector is rotated around the origin 
given our assumption that each source signal is super gaussian we would expect the kurtosis of the extracted signal to be maximal precisely when the kurtosis of the extracted signal to be maximal when is orthogonal to the projected axes or because we know the optimal weight vector should be orthogonal to transformed axis or for multiple source mixture signals we can use kurtosis and gram schmidt orthogonalization gso to recover the signals 
given signal mixtures in an dimensional space gso project these data points onto an dimensional space by using the weight vector 
we can guarantee the independence of the extracted signals with the use of gso 
in order to find the correct value of we can use gradient descent method 
we first of all whiten the data and transform into new mixture which has unit variance and this process can be achieved by applying singular value decomposition to rescaling each vector and let the signal extracted by weighted vector is if the weight vector has unit length then the variance of is also that is the kurtosis can thus be written as the updating process for is 
where is small constant to guarantee that converges to the optimal solution 
after each update we normalize and set and repeat the updating process until convergence 
we can also use another algorithm to update the weight vector another approach is using negentropy instead of kurtosis 
using negentropy is more robust method than kurtosis as kurtosis is very sensitive to outliers 
the negentropy methods are based on an important property of gaussian distribution gaussian variable has the largest entropy among all continuous random variables of equal variance 
this is also the reason why we want to find the most nongaussian variables 
simple proof can be found in differential entropy 
is gaussian random variable of the same covariance matrix as log an approximation for negentropy is proof can be found in the original papers of comon it has been reproduced in the book independent component analysis by aapo hyv rinen juha karhunen and erkki oja this approximation also suffers from the same problem as kurtosis sensitivity to outliers 
other approaches have been developed 
choice of and are log cosh and exp based on infomax infomax ica is essentially multivariate parallel version of projection pursuit 
whereas projection pursuit extracts series of signals one at time from set of signal mixtures ica extracts signals in parallel 
this tends to make ica more robust than projection pursuit the projection pursuit method uses gram schmidt orthogonalization to ensure the independence of the extracted signal while ica use infomax and maximum likelihood estimate to ensure the independence of the extracted signal 
the non normality of the extracted signal is achieved by assigning an appropriate model or prior for the signal 
the process of ica based on infomax in short is given set of signal mixtures and set of identical independent model cumulative distribution functions cdfs we seek the unmixing matrix which maximizes the joint entropy of the signals where are the signals extracted by given the optimal the signals have maximum entropy and are therefore independent which ensures that the extracted signals are also independent 
is an invertible function and is the signal model 
note that if the source signal model probability density function matches the probability density function of the extracted signal then maximizing the joint entropy of also maximizes the amount of mutual information between and for this reason using entropy to extract independent signals is known as infomax 
consider the entropy of the vector variable where is the set of signals extracted by the unmixing matrix for finite set of values sampled from distribution with pdf the entropy of can be estimated as ln the joint pdf can be shown to be related to the joint pdf of the extracted signals by the multivariate form where is the jacobian matrix 
we have and is the pdf assumed for source signals therefore therefore ln we know that when is of uniform distribution and is maximized 
since where is the absolute value of the determinant of the unmixing matrix therefore ln so ln ln since ln and maximizing does not affect so we can maximize the function ln ln to achieve the independence of extracted signal 
if there are marginal pdfs of the model joint pdf are independent and use the commonly super gaussian model pdf for the source signals tanh then we have ln tanh ln in the sum given an observed signal mixture the corresponding set of extracted signals and source signal model we can find the optimal unmixing matrix and make the extracted signals independent and non gaussian 
like the projection pursuit situation we can use gradient descent method to find the optimal solution of the unmixing matrix 
based on maximum likelihood estimation maximum likelihood estimation mle is standard statistical tool for finding parameter values 
the unmixing matrix that provide the best fit of some data the extracted signals to given model the assumed joint probability density function pdf of source signals the ml model includes specification of pdf which in this case is the pdf of the unknown source signals using ml ica the objective is to find an unmixing matrix that yields extracted signals with joint pdf as similar as possible to the joint pdf of the unknown source signals mle is thus based on the assumption that if the model pdf and the model parameters are correct then high probability should be obtained for the data that were actually observed 
conversely if is far from the correct parameter values then low probability of the observed data would be expected 
using mle we call the probability of the observed data for given set of model parameter values pdf and matrix the likelihood of the model parameter values given the observed data 
we define likelihood function of det 
this equals to the probability density at since thus if we wish to find that is most likely to have generated the observed mixtures from the unknown source signals with pdf then we need only find that which maximizes the likelihood 
the unmixing matrix that maximizes equation is known as the mle of the optimal unmixing matrix 
it is common practice to use the log likelihood because this is easier to evaluate 
as the logarithm is monotonic function the that maximizes the function also maximizes its logarithm ln 
this allows us to take the logarithm of equation above which yields the log likelihood function ln ln ln det if we substitute commonly used high kurtosis model pdf for the source signals tanh then we have ln ln tanh ln det this matrix that maximizes this function is the maximum likelihood estimation 
history and background the early general framework for independent component analysis was introduced by jeanny rault and bernard ans from further developed by christian jutten in and and refined by pierre comon in and popularized in his paper of in tony bell and terry sejnowski introduced fast and efficient ica algorithm based on infomax principle introduced by ralph linsker in there are many algorithms available in the literature which do ica 
largely used one including in industrial applications is the fastica algorithm developed by hyv rinen and oja which uses the negentropy as cost function 
other examples are rather related to blind source separation where more general approach is used 
for example one can drop the independence assumption and separate mutually correlated signals thus statistically dependent signals 
sepp hochreiter and rgen schmidhuber showed how to obtain non linear ica or source separation as by product of regularization 
their method does not require priori knowledge about the number of independent sources 
applications ica can be extended to analyze non physical signals 
for instance ica has been applied to discover discussion topics on bag of news list archives 
some ica applications are listed below optical imaging of neurons neuronal spike sorting face recognition modelling receptive fields of primary visual neurons predicting stock market prices mobile phone communications colour based detection of the ripeness of tomatoes removing artifacts such as eye blinks from eeg data 
predicting decision making using eeg analysis of changes in gene expression over time in single cell rna sequencing experiments 
studies of the resting state network of the brain 
astronomy and cosmology finance availability ica can be applied through the following software sas proc ica scikit learn python implementation sklearn decomposition fastica see also notes references comon pierre independent component analysis new concept 
signal processing the original paper describing the concept of ica hyv rinen karhunen oja independent component analysis new york wiley isbn introductory chapter hyv rinen oja independent component analysis algorithms and application neural networks 
technical but pedagogical introduction 
comon jutten handbook of blind source separation independent component analysis and applications 
academic press oxford uk 
isbn lee independent component analysis theory and applications boston mass kluwer academic publishers isbn acharyya ranjan new approach for blind source separation of convolutive sources wavelet based separation using shrinkage function isbn isbn this book focuses on unsupervised learning with blind source separation external links what is independent component analysis 
by aapo hyv rinen independent component analysis tutorial by aapo hyv rinen tutorial on independent component analysis fastica as package for matlab in language icalab toolboxes for matlab developed at riken high performance signal analysis toolkit provides implementations of fastica and infomax ica toolbox matlab tools for ica with bell sejnowski molgedey schuster and mean field ica 
demonstration of the cocktail party problem eeglab toolbox ica of eeg for matlab developed at ucsd 
fmrlab toolbox ica of fmri for matlab developed at ucsd melodic part of the fmrib software library 
discussion of ica used in biomedical shape representation context fastica cubica jade and tdsep algorithm for python and more group ica toolbox and fusion ica toolbox tutorial using ica for cleaning eeg signals
in mathematics the dimension of vector space is the cardinality the number of vectors of basis of over its base field 
it is sometimes called hamel dimension after georg hamel or algebraic dimension to distinguish it from other types of dimension 
for every vector space there exists basis and all bases of vector space have equal cardinality as result the dimension of vector space is uniquely defined 
we say is finite dimensional if the dimension of is finite and infinite dimensional if its dimension is infinite 
the dimension of the vector space over the field can be written as dim or as read dimension of over 
when can be inferred from context dim is typically written 
examples the vector space has as standard basis and therefore dim more generally dim and even more generally dim for any field the complex numbers are both real and complex vector space we have dim and dim so the dimension depends on the base field 
the only vector space with dimension is the vector space consisting only of its zero element 
properties if is linear subspace of then dim dim 
to show that two finite dimensional vector spaces are equal the following criterion can be used if is finite dimensional vector space and is linear subspace of with dim dim then the space has the standard basis where is the th column of the corresponding identity matrix 
therefore has dimension any two finite dimensional vector spaces over with the same dimension are isomorphic 
any bijective map between their bases can be uniquely extended to bijective linear map between the vector spaces 
if is some set vector space with dimension over can be constructed as follows take the set of all functions such that for all but finitely many in these functions can be added and multiplied with elements of to obtain the desired vector space 
an important result about dimensions is given by the rank nullity theorem for linear maps 
if is field extension then is in particular vector space over furthermore every vector space is also vector space 
the dimensions are related by the formula in particular every complex vector space of dimension is real vector space of dimension some formulae relate the dimension of vector space with the cardinality of the base field and the cardinality of the space itself 
if is vector space over field then and if the dimension of is denoted by dim then if dim is finite then dim if dim is infinite then max dim 
generalizations vector space can be seen as particular case of matroid and in the latter there is well defined notion of dimension 
the length of module and the rank of an abelian group both have several properties similar to the dimension of vector spaces 
the krull dimension of commutative ring named after wolfgang krull is defined to be the maximal number of strict inclusions in an increasing chain of prime ideals in the ring 
trace the dimension of vector space may alternatively be characterized as the trace of the identity operator 
for instance tr id tr this appears to be circular definition but it allows useful generalizations 
firstly it allows for definition of notion of dimension when one has trace but no natural sense of basis 
for example one may have an algebra with maps the inclusion of scalars called the unit and map corresponding to trace called the counit 
the composition is scalar being linear operator on dimensional space corresponds to trace of identity and gives notion of dimension for an abstract algebra 
in practice in bialgebras this map is required to be the identity which can be obtained by normalizing the counit by dividing by dimension tr so in these cases the normalizing constant corresponds to dimension 
alternatively it may be possible to take the trace of operators on an infinite dimensional space in this case finite trace is defined even though no finite dimension exists and gives notion of dimension of the operator 
these fall under the rubric of trace class operators on hilbert space or more generally nuclear operators on banach space 
subtler generalization is to consider the trace of family of operators as kind of twisted dimension 
this occurs significantly in representation theory where the character of representation is the trace of the representation hence scalar valued function on group whose value on the identity is the dimension of the representation as representation sends the identity in the group to the identity matrix tr dim the other values of the character can be viewed as twisted dimensions and find analogs or generalizations of statements about dimensions to statements about characters or representations 
sophisticated example of this occurs in the theory of monstrous moonshine the invariant is the graded dimension of an infinite dimensional graded representation of the monster group and replacing the dimension with the character gives the mckay thompson series for each element of the monster group 
see also fractal dimension ratio providing statistical index of complexity variation with scale krull dimension in mathematics dimension of ring matroid rank maximum size of an independent set of the matroid rank linear algebra dimension of the column space of matrix topological dimension also called lebesgue covering dimension notes references sources axler sheldon 
linear algebra done right 
undergraduate texts in mathematics rd ed 
external links mit linear algebra lecture on independence basis and dimension by gilbert strang at mit opencourseware
in statistics the logistic model or logit model is statistical model that models the probability of an event taking place by having the log odds for the event be linear combination of one or more independent variables 
in regression analysis logistic regression or logit regression is estimating the parameters of logistic model the coefficients in the linear combination 
formally in binary logistic regression there is single binary dependent variable coded by an indicator variable where the two values are labeled and while the independent variables can each be binary variable two classes coded by an indicator variable or continuous variable any real value 
the corresponding probability of the value labeled can vary between certainly the value and certainly the value hence the labeling the function that converts log odds to probability is the logistic function hence the name 
the unit of measurement for the log odds scale is called logit from logistic unit hence the alternative names 
see background and definition for formal mathematics and example for worked example 
binary variables are widely used in statistics to model the probability of certain class or event taking place such as the probability of team winning of patient being healthy etc 
see applications and the logistic model has been the most commonly used model for binary regression since about binary variables can be generalized to categorical variables when there are more than two possible values 
whether an image is of cat dog lion etc 
and the binary logistic regression generalized to multinomial logistic regression 
if the multiple categories are ordered one can use the ordinal logistic regression for example the proportional odds ordinal logistic model 
see extensions for further extensions 
the logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification it is not classifier though it can be used to make classifier for instance by choosing cutoff value and classifying inputs with probability greater than the cutoff as one class below the cutoff as the other this is common way to make binary classifier 
analogous linear models for binary variables with different sigmoid function instead of the logistic function to convert the linear combination to probability can also be used most notably the probit model see alternatives 
the defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at constant rate with each independent variable having its own parameter for binary dependent variable this generalizes the odds ratio 
more abstractly the logistic function is the natural parameter for the bernoulli distribution and in this sense is the simplest way to convert real number to probability 
in particular it maximizes entropy minimizes added information and in this sense makes the fewest assumptions of the data being modeled see maximum entropy 
the parameters of logistic regression are most commonly estimated by maximum likelihood estimation mle 
this does not have closed form expression unlike linear least squares see model fitting 
logistic regression by mle plays similarly basic role for binary or categorical responses as linear regression by ordinary least squares ols plays for scalar responses it is simple well analyzed baseline model see comparison with linear regression for discussion 
the logistic regression as general statistical model was originally developed and popularized primarily by joseph berkson beginning in berkson where he coined logit see history 
applications logistic regression is used in various fields including machine learning most medical fields and social sciences 
for example the trauma and injury severity score triss which is widely used to predict mortality in injured patients was originally developed by boyd et al 
many other medical scales used to assess severity of patient have been developed using logistic regression 
logistic regression may be used to predict the risk of developing given disease 
diabetes coronary heart disease based on observed characteristics of the patient age sex body mass index results of various blood tests etc 
another example might be to predict whether nepalese voter will vote nepali congress or communist party of nepal or any other party based on age income sex race state of residence votes in previous elections etc 
the technique can also be used in engineering especially for predicting the probability of failure of given process system or product 
it is also used in marketing applications such as prediction of customer propensity to purchase product or halt subscription etc 
in economics it can be used to predict the likelihood of person ending up in the labor force and business application would be to predict the likelihood of homeowner defaulting on mortgage 
conditional random fields an extension of logistic regression to sequential data are used in natural language processing 
example problem as simple example we can use logistic regression with one explanatory variable and two categories to answer the following question group of students spends between and hours studying for an exam 
how does the number of hours spent studying affect the probability of the student passing the exam 
the reason for using logistic regression for this problem is that the values of the dependent variable pass and fail while represented by and are not cardinal numbers 
if the problem was changed so that pass fail was replaced with the grade cardinal numbers then simple regression analysis could be used 
the table shows the number of hours each student spent studying and whether they passed or failed 
we wish to fit logistic function to the data consisting of the hours studied xk and the outcome of the test yk for pass for fail 
the data points are indexed by the subscript which runs from to the variable is called the explanatory variable and the variable is called the categorical variable consisting of two categories pass or fail corresponding to the categorical values and respectively 
model the logistic function is of the form where is location parameter the midpoint of the curve where and is scale parameter 
this expression may be rewritten as where and is known as the intercept it is the vertical intercept or intercept of the line and inverse scale parameter or rate parameter these are the intercept and slope of the log odds as function of conversely and 
fit the usual measure of goodness of fit for logistic regression uses logistic loss or log loss the negative log likelihood 
for given xk and yk write 
the are the probabilities that the corresponding will be unity and are the probabilities that they will be zero see bernoulli distribution 
we wish to find the values of and which give the best fit to the data 
in the case of linear regression the sum of the squared deviations of the fit from the data points yk the squared error loss is taken as measure of the goodness of fit and the best fit is obtained when that function is minimized 
the log loss for the th point is ln if ln if the log loss can be interpreted as the surprisal of the actual outcome relative to the prediction and is measure of information content 
note that log loss is always greater than or equal to equals only in case of perfect prediction when and or and and approaches infinity as the prediction gets worse when and or and meaning the actual outcome is more surprising 
since the value of the logistic function is always strictly between zero and one the log loss is always greater than zero and less than infinity 
note that unlike in linear regression where the model can have zero loss at point by passing through data point and zero loss overall if all points are on line in logistic regression it is not possible to have zero loss at any points since is either or but these can be combined into single expression ln ln 
this expression is more formally known as the cross entropy of the predicted distribution from the actual distribution as probability distributions on the two element space of pass fail 
the sum of these the total loss is the overall negative log likelihood and the best fit is obtained for those choices of and for which is minimized 
alternatively instead of minimizing the loss one can maximize its inverse the positive log likelihood ln ln ln ln or equivalently maximize the likelihood function itself which is the probability that the given data set is produced by particular logistic function this method is known as maximum likelihood estimation 
parameter estimation since is nonlinear in and determining their optimum values will require numerical methods 
note that one method of maximizing is to require the derivatives of with respect to and to be zero and the maximization procedure can be accomplished by solving the above two equations for and which again will generally require the use of numerical methods 
the values of and which maximize and using the above data are found to be which yields value for and of predictions the and coefficients may be entered into the logistic regression equation to estimate the probability of passing the exam 
for example for student who studies hours entering the value into the equation gives the estimated probability of passing the exam of probability of passing exam similarly for student who studies hours the estimated probability of passing the exam is probability of passing exam this table shows the estimated probability of passing the exam for several values of hours studying 
model evaluation the logistic regression analysis gives the following output 
by the wald test the output indicates that hours studying is significantly associated with the probability of passing the exam 
rather than the wald method the recommended method to calculate the value for logistic regression is the likelihood ratio test lrt which for this data gives see error and significance of the fit below 
generalizations this simple model is an example of binary logistic regression and has one explanatory variable and binary categorical variable which can assume one of two categorical values 
multinomial logistic regression is the generalization of binary logistic regression to include any number of explanatory variables and any number of categories 
background definition of the logistic function an explanation of logistic regression can begin with an explanation of the standard logistic function 
the logistic function is sigmoid function which takes any real input and outputs value between zero and one 
for the logit this is interpreted as taking input log odds and having output probability 
the standard logistic function is defined as follows graph of the logistic function on the interval is shown in figure let us assume that is linear function of single explanatory variable the case where is linear combination of multiple explanatory variables is treated similarly 
we can then express as follows and the general logistic function can now be written as in the logistic model is interpreted as the probability of the dependent variable equaling success case rather than failure non case 
it clear that the response variables are not identically distributed differs from one data point to another though they are independent given design matrix and shared parameters 
definition of the inverse of the logistic function we can now define the logit log odds function as the inverse of the standard logistic function 
it is easy to see that it satisfies logit ln and equivalently after exponentiating both sides we have the odds 
interpretation of these terms in the above equations the terms are as follows is the logit function 
the equation for illustrates that the logit log odds or natural logarithm of the odds is equivalent to the linear regression expression 
ln denotes the natural logarithm 
is the probability that the dependent variable equals case given some linear combination of the predictors 
the formula for illustrates that the probability of the dependent variable equaling case is equal to the value of the logistic function of the linear regression expression 
this is important in that it shows that the value of the linear regression expression can vary from negative to positive infinity and yet after transformation the resulting expression for the probability ranges between and is the intercept from the linear regression equation the value of the criterion when the predictor is equal to zero 
is the regression coefficient multiplied by some value of the predictor 
base denotes the exponential function 
definition of the odds the odds of the dependent variable equaling case given some linear combination of the predictors is equivalent to the exponential function of the linear regression expression 
this illustrates how the logit serves as link function between the probability and the linear regression expression 
given that the logit ranges between negative and positive infinity it provides an adequate criterion upon which to conduct linear regression and the logit is easily converted back into the odds so we define odds of the dependent variable equaling case given some linear combination of the predictors as follows odds 
the odds ratio for continuous independent variable the odds ratio can be defined as odds odds this exponential relationship provides an interpretation for the odds multiply by for every unit increase in for binary independent variable the odds ratio is defined as where and are cells in contingency table 
multiple explanatory variables if there are multiple explanatory variables the above expression can be revised to then when this is used in the equation relating the log odds of success to the values of the predictors the linear regression will be multiple regression with explanators the parameters for all are all estimated 
again the more traditional equations are log and where usually definition the basic setup of logistic regression is as follows 
we are given dataset containing points 
each point consists of set of input variables xm also called independent variables explanatory variables predictor variables features or attributes and binary outcome variable yi also known as dependent variable response variable output variable or class 
it can assume only the two possible values often meaning no or failure or often meaning yes or success 
the goal of logistic regression is to use the dataset to create predictive model of the outcome variable 
as in linear regression the outcome variables yi are assumed to depend on the explanatory variables xm 
explanatory variablesthe explanatory variables may be of any type real valued binary categorical etc 
the main distinction is between continuous variables and discrete variables 
discrete variables referring to more than two possible choices are typically coded using dummy variables or indicator variables that is separate explanatory variables taking the value or are created for each possible value of the discrete variable with meaning variable does have the given value and meaning variable does not have that value 
outcome variablesformally the outcomes yi are described as being bernoulli distributed data where each outcome is determined by an unobserved probability pi that is specific to the outcome at hand but related to the explanatory variables 
this can be expressed in any of the following equivalent forms bernoulli pr if if pr the meanings of these four lines are the first line expresses the probability distribution of each yi conditioned on the explanatory variables it follows bernoulli distribution with parameters pi the probability of the outcome of for trial as noted above each separate trial has its own probability of success just as each trial has its own explanatory variables 
the probability of success pi is not observed only the outcome of an individual bernoulli trial using that probability 
the second line expresses the fact that the expected value of each yi is equal to the probability of success pi which is general property of the bernoulli distribution 
in other words if we run large number of bernoulli trials using the same probability of success pi then take the average of all the and outcomes then the result would be close to pi 
this is because doing an average this way simply computes the proportion of successes seen which we expect to converge to the underlying probability of success 
the third line writes out the probability mass function of the bernoulli distribution specifying the probability of seeing each of the two possible outcomes 
the fourth line is another way of writing the probability mass function which avoids having to write separate cases and is more convenient for certain types of calculations 
this relies on the fact that yi can take only the value or in each case one of the exponents will be choosing the value under it while the other is canceling out the value under it 
hence the outcome is either pi or pi as in the previous line linear predictor functionthe basic idea of logistic regression is to use the mechanism already developed for linear regression by modeling the probability pi using linear predictor function 
linear combination of the explanatory variables and set of regression coefficients that are specific to the model at hand but the same for all trials 
the linear predictor function for particular data point is written as where are regression coefficients indicating the relative effect of particular explanatory variable on the outcome 
the model is usually put into more compact form as follows the regression coefficients are grouped into single vector of size for each data point an additional explanatory pseudo variable is added with fixed value of corresponding to the intercept coefficient 
the resulting explanatory variables xm are then grouped into single vector xi of size this makes it possible to write the linear predictor function as follows using the notation for dot product between two vectors 
many explanatory variables two categories the above example of binary logistic regression on one explanatory variable can be generalized to binary logistic regression on any number of explanatory variables and any number of categorical values 
to begin with we may consider logistic model with explanatory variables xm and as in the example above two categorical values and 
for the simple binary logistic regression model we assumed linear relationship between the predictor variable and the log odds also called logit of the event that this linear relationship may be extended to the case of explanatory variables log where is the log odds and are parameters of the model 
an additional generalization has been introduced in which the base of the model is not restricted to the euler number in most applications the base of the logarithm is usually taken to be however in some cases it can be easier to communicate results by working in base or base for more compact notation we will specify the explanatory variables and the coefficients as dimensional vectors with an added explanatory variable 
the logit may now be written as solving for the probability that yields where is the sigmoid function with base the above formula shows that once the are fixed we can easily compute either the log odds that for given observation or the probability that for given observation 
the main use case of logistic model is to be given an observation and estimate the probability that the optimum beta coefficients may again be found by maximizing the log likelihood 
for measurements defining xk as the explanatory vector of the th measurement and yk as the categorical outcome of that measurement the log likelihood may be written in form very similar to the simple case above log log as in the simple example above finding the optimum parameters will require numerical methods 
one useful technique is to equate the derivatives of the log likelihood with respect to each of the parameters to zero yielding set of equations which will hold at the maximum of the log likelihood where xmk is the value of the xm explanatory variable from the th measurement 
consider an example with explanatory variables and coefficients and which have been determined by the above method 
to be concrete the model is log where is the probability of the event that this can be interpreted as follows is the intercept 
it is the log odds of the event that when the predictors by exponentiating we can see that when the odds of the event that are to or similarly the probability of the event that when can be computed as means that increasing by increases the log odds by so if increases by the odds that increase by factor of note that the probability of has also increased but it has not increased by as much as the odds have increased 
means that increasing by increases the log odds by so if increases by the odds that increase by factor of note how the effect of on the log odds is twice as great as the effect of but the effect on the odds is times greater 
but the effect on the probability of is not as much as times greater it only the effect on the odds that is times greater 
multinomial logistic regression many explanatory variables and many categories in the above cases of two categories binomial logistic regression the categories were indexed by and and we had two probability distributions the probability that the outcome was in category was given by and the probability that the outcome was in category was given by 
the sum of both probabilities is equal to unity as they must be 
in general if we have explanatory variables including and categories we will need separate probability distributions one for each category indexed by which describe the probability that the categorical outcome for explanatory vector will be in category 
it will also be required that the sum of these probabilities over all categories be equal to unity 
using the mathematically convenient base these probabilities are for each of the probabilities except will have their own set of regression coefficients it can be seen that as required the sum of the over all categories is unity 
note that the selection of to be defined in terms of the other probabilities is artificial 
any of the probabilities could have been selected to be so defined 
this special value of is termed the pivot index and the log odds tn are expressed in terms of the pivot probability and are again expressed as linear combination of the explanatory variables ln note also that for the simple case of the two category case is recovered with and 
the log likelihood that particular set of measurements or data points will be generated by the above probabilities can now be calculated 
indexing each measurement by let the th set of measured explanatory variables be denoted by and their categorical outcomes be denoted by which can be equal to any integer in 
the log likelihood is then ln where is an indicator function which is equal to unity if yk and zero otherwise 
in the case of two explanatory variables this indicator function was defined as yk when and yk when this was convenient but not necessary 
again the optimum beta coefficients may be found by maximizing the log likelihood function generally using numerical methods 
possible method of solution is to set the derivatives of the log likelihood with respect to each beta coefficient equal to zero and solve for the beta coefficients where is the th coefficient of the vector and is the th explanatory variable of the th measurement 
once the beta coefficients have been estimated from the data we will be able to estimate the probability that any subsequent set of explanatory variables will result in any of the possible outcome categories 
interpretations there are various equivalent specifications and interpretations of logistic regression which fit into different types of more general models and allow different generalizations 
as generalized linear model the particular model used by logistic regression which distinguishes it from standard linear regression and from other types of regression analysis used for binary valued outcomes is the way the probability of particular outcome is linked to the linear predictor function logit logit ln written using the more compact notation described above this is logit logit ln this formulation expresses logistic regression as type of generalized linear model which predicts variables with various types of probability distributions by fitting linear predictor function of the above form to some sort of arbitrary transformation of the expected value of the variable 
the intuition for transforming using the logit function the natural log of the odds was explained above 
it also has the practical effect of converting the probability which is bounded to be between and to variable that ranges over thereby matching the potential range of the linear prediction function on the right side of the equation 
note that both the probabilities pi and the regression coefficients are unobserved and the means of determining them is not part of the model itself 
they are typically determined by some sort of optimization procedure 
maximum likelihood estimation that finds values that best fit the observed data 
that give the most accurate predictions for the data already observed usually subject to regularization conditions that seek to exclude unlikely values 
extremely large values for any of the regression coefficients 
the use of regularization condition is equivalent to doing maximum posteriori map estimation an extension of maximum likelihood 
regularization is most commonly done using squared regularizing function which is equivalent to placing zero mean gaussian prior distribution on the coefficients but other regularizers are also possible 
whether or not regularization is used it is usually not possible to find closed form solution instead an iterative numerical method must be used such as iteratively reweighted least squares irls or more commonly these days quasi newton method such as the bfgs method the interpretation of the parameter estimates is as the additive effect on the log of the odds for unit change in the the explanatory variable 
in the case of dichotomous explanatory variable for instance gender is the estimate of the odds of having the outcome for say males compared with females 
an equivalent formula uses the inverse of the logit function which is the logistic function 
logit the formula can also be written as probability distribution specifically using probability mass function pr as latent variable model the logistic model has an equivalent formulation as latent variable model 
this formulation is common in the theory of discrete choice models and makes it easier to extend to certain more complicated models with multiple correlated choices as well as to compare logistic regression to the closely related probit model 
imagine that for each trial there is continuous latent variable yi 
an unobserved random variable that is distributed as follows where logistic 
the latent variable can be written directly in terms of the linear predictor function and an additive random error variable that is distributed according to standard logistic distribution 
then yi can be viewed as an indicator for whether this latent variable is positive if 
the choice of modeling the error variable specifically with standard logistic distribution rather than general logistic distribution with the location and scale set to arbitrary values seems restrictive but in fact it is not 
it must be kept in mind that we can choose the regression coefficients ourselves and very often can use them to offset changes in the parameters of the error variable distribution 
for example logistic error variable distribution with non zero location parameter which sets the mean is equivalent to distribution with zero location parameter where has been added to the intercept coefficient 
both situations produce the same value for yi regardless of settings of explanatory variables 
similarly an arbitrary scale parameter is equivalent to setting the scale parameter to and then dividing all regression coefficients by in the latter case the resulting value of yi will be smaller by factor of than in the former case for all sets of explanatory variables but critically it will always remain on the same side of and hence lead to the same yi choice 
note that this predicts that the irrelevancy of the scale parameter may not carry over into more complex models where more than two choices are available 
it turns out that this formulation is exactly equivalent to the preceding one phrased in terms of the generalized linear model and without any latent variables 
this can be shown as follows using the fact that the cumulative distribution function cdf of the standard logistic distribution is the logistic function which is the inverse of the logit function 
pr logit then pr pr pr pr pr because the logistic distribution is symmetric logit see above this formulation which is standard in discrete choice models makes clear the relationship between logistic regression the logit model and the probit model which uses an error variable distributed according to standard normal distribution instead of standard logistic distribution 
both the logistic and normal distributions are symmetric with basic unimodal bell curve shape 
the only difference is that the logistic distribution has somewhat heavier tails which means that it is less sensitive to outlying data and hence somewhat more robust to model mis specifications or erroneous data 
two way latent variable model yet another formulation uses two separate latent variables where ev ev where ev is standard type extreme value distribution 
pr pr then if otherwise 
this model has separate latent variable and separate set of regression coefficients for each possible outcome of the dependent variable 
the reason for this separation is that it makes it easy to extend logistic regression to multi outcome categorical variables as in the multinomial logit model 
in such model it is natural to model each possible outcome using different set of regression coefficients 
it is also possible to motivate each of the separate latent variables as the theoretical utility associated with making the associated choice and thus motivate logistic regression in terms of utility theory 
in terms of utility theory rational actor always chooses the choice with the greatest associated utility 
this is the approach taken by economists when formulating discrete choice models because it both provides theoretically strong foundation and facilitates intuitions about the model which in turn makes it easy to consider various sorts of extensions 
see the example below 
the choice of the type extreme value distribution seems fairly arbitrary but it makes the mathematics work out and it may be possible to justify its use through rational choice theory 
it turns out that this model is equivalent to the previous model although this seems non obvious since there are now two sets of regression coefficients and error variables and the error variables have different distribution 
in fact this model reduces directly to the previous one with the following substitutions an intuition for this comes from the fact that since we choose based on the maximum of two values only their difference matters not the exact values and this effectively removes one degree of freedom 
another critical fact is that the difference of two type extreme value distributed variables is logistic distribution 
we can demonstrate the equivalent as follows pr pr pr pr pr pr pr substitute as above pr substitute as above pr now same as above model pr logit example as an example consider province level election where the choice is between right of center party left of center party and secessionist party 
the parti qu cois which wants quebec to secede from canada 
we would then use three latent variables one for each choice 
then in accordance with utility theory we can then interpret the latent variables as expressing the utility that results from making each of the choices 
we can also interpret the regression coefficients as indicating the strength that the associated factor 
explanatory variable has in contributing to the utility or more correctly the amount by which unit change in an explanatory variable changes the utility of given choice 
voter might expect that the right of center party would lower taxes especially on rich people 
this would give low income people no benefit 
no change in utility since they usually don pay taxes would cause moderate benefit 
somewhat more money or moderate utility increase for middle incoming people would cause significant benefits for high income people 
on the other hand the left of center party might be expected to raise taxes and offset it with increased welfare and other assistance for the lower and middle classes 
this would cause significant positive benefit to low income people perhaps weak benefit to middle income people and significant negative benefit to high income people 
finally the secessionist party would take no direct actions on the economy but simply secede 
low income or middle income voter might expect basically no clear utility gain or loss from this but high income voter might expect negative utility since he she is likely to own companies which will have harder time doing business in such an environment and probably lose money 
these intuitions can be expressed as follows this clearly shows that separate sets of regression coefficients need to exist for each choice 
when phrased in terms of utility this can be seen very easily 
different choices have different effects on net utility furthermore the effects vary in complex ways that depend on the characteristics of each individual so there need to be separate sets of coefficients for each characteristic not simply single extra per choice characteristic 
even though income is continuous variable its effect on utility is too complex for it to be treated as single variable 
either it needs to be directly split up into ranges or higher powers of income need to be added so that polynomial regression on income is effectively done 
as log linear model yet another formulation combines the two way latent variable formulation above with the original formulation higher up without latent variables and in the process provides link to one of the standard formulations of the multinomial logit 
here instead of writing the logit of the probabilities pi as linear predictor we separate the linear predictor into two one for each of the two outcomes ln pr ln ln pr ln two separate sets of regression coefficients have been introduced just as in the two way latent variable model and the two equations appear form that writes the logarithm of the associated probability as linear predictor with an extra term ln at the end 
this term as it turns out serves as the normalizing factor ensuring that the result is distribution 
this can be seen by exponentiating both sides pr pr in this form it is clear that the purpose of is to ensure that the resulting distribution over yi is in fact probability distribution 
it sums to this means that is simply the sum of all un normalized probabilities and by dividing each probability by the probabilities become normalized 
that is and the resulting equations are pr pr or generally pr this shows clearly how to generalize this formulation to more than two outcomes as in multinomial logit 
note that this general formulation is exactly the softmax function as in pr softmax 
in order to prove that this is equivalent to the previous model note that the above model is overspecified in that pr and pr cannot be independently specified rather pr pr so knowing one automatically determines the other 
as result the model is nonidentifiable in that multiple combinations of and will produce the same probabilities for all possible explanatory variables 
in fact it can be seen that adding any constant vector to both of them will produce the same probabilities pr as result we can simplify matters and restore identifiability by picking an arbitrary value for one of the two vectors 
we choose to set then and so pr which shows that this formulation is indeed equivalent to the previous formulation 
as in the two way latent variable formulation any settings where will produce equivalent results 
note that most treatments of the multinomial logit model start out either by extending the log linear formulation presented here or the two way latent variable formulation presented above since both clearly show the way that the model could be extended to multi way outcomes 
in general the presentation with latent variables is more common in econometrics and political science where discrete choice models and utility theory reign while the log linear formulation here is more common in computer science 
machine learning and natural language processing 
as single layer perceptron the model has an equivalent formulation 
this functional form is commonly called single layer perceptron or single layer artificial neural network 
single layer neural network computes continuous output instead of step function 
the derivative of pi with respect to xk is computed from the general form where is an analytic function in with this choice the single layer neural network is identical to the logistic regression model 
this function has continuous derivative which allows it to be used in backpropagation 
this function is also preferred because its derivative is easily calculated 
in terms of binomial data closely related model assumes that each is associated not with single bernoulli trial but with ni independent identically distributed trials where the observation yi is the number of successes observed the sum of the individual bernoulli distributed random variables and hence follows binomial distribution bin for an example of this distribution is the fraction of seeds pi that germinate after ni are planted 
in terms of expected values this model is expressed as follows so that logit logit ln or equivalently pr this model can be fit using the same sorts of methods as the above more basic model 
model fitting maximum likelihood estimation mle the regression coefficients are usually estimated using maximum likelihood estimation 
unlike linear regression with normally distributed residuals it is not possible to find closed form expression for the coefficient values that maximize the likelihood function so that an iterative process must be used instead for example newton method 
this process begins with tentative solution revises it slightly to see if it can be improved and repeats this revision until no more improvement is made at which point the process is said to have converged in some instances the model may not reach convergence 
non convergence of model indicates that the coefficients are not meaningful because the iterative process was unable to find appropriate solutions 
failure to converge may occur for number of reasons having large ratio of predictors to cases multicollinearity sparseness or complete separation 
having large ratio of variables to cases results in an overly conservative wald statistic discussed below and can lead to non convergence 
regularized logistic regression is specifically intended to be used in this situation 
multicollinearity refers to unacceptably high correlations between predictors 
as multicollinearity increases coefficients remain unbiased but standard errors increase and the likelihood of model convergence decreases 
to detect multicollinearity amongst the predictors one can conduct linear regression analysis with the predictors of interest for the sole purpose of examining the tolerance statistic used to assess whether multicollinearity is unacceptably high 
sparseness in the data refers to having large proportion of empty cells cells with zero counts 
zero cell counts are particularly problematic with categorical predictors 
with continuous predictors the model can infer values for the zero cell counts but this is not the case with categorical predictors 
the model will not converge with zero cell counts for categorical predictors because the natural logarithm of zero is an undefined value so that the final solution to the model cannot be reached 
to remedy this problem researchers may collapse categories in theoretically meaningful way or add constant to all cells 
another numerical problem that may lead to lack of convergence is complete separation which refers to the instance in which the predictors perfectly predict the criterion all cases are accurately classified and the likelihood maximized with infinite coefficients 
in such instances one should re examine the data as there may be some kind of error 
one can also take semi parametric or non parametric approaches via local likelihood or nonparametric quasi likelihood methods which avoid assumptions of parametric form for the index function and is robust to the choice of the link function probit or logit 
iteratively reweighted least squares irls binary logistic regression or can for example be calculated using iteratively reweighted least squares irls which is equivalent to maximizing the log likelihood of bernoulli distributed process using newton method 
if the problem is written in vector matrix form with parameters explanatory variables and expected value of the bernoulli distribution the parameters can be found using the following iterative algorithm where diag is diagonal weighting matrix the vector of expected values the regressor matrix and the vector of response variables 
more details can be found in the literature 
bayesian in bayesian statistics context prior distributions are normally placed on the regression coefficients for example in the form of gaussian distributions 
there is no conjugate prior of the likelihood function in logistic regression 
when bayesian inference was performed analytically this made the posterior distribution difficult to calculate except in very low dimensions 
now though automatic software such as openbugs jags pymc stan or turing jl allows these posteriors to be computed using simulation so lack of conjugacy is not concern 
however when the sample size or the number of parameters is large full bayesian simulation can be slow and people often use approximate methods such as variational bayesian methods and expectation propagation 
rule of ten widely used rule of thumb the one in ten rule states that logistic regression models give stable values for the explanatory variables if based on minimum of about events per explanatory variable epv where event denotes the cases belonging to the less frequent category in the dependent variable 
thus study designed to use explanatory variables for an event 
myocardial infarction expected to occur in proportion of participants in the study will require total of participants 
however there is considerable debate about the reliability of this rule which is based on simulation studies and lacks secure theoretical underpinning 
according to some authors the rule is overly conservative in some circumstances with the authors stating if we somewhat subjectively regard confidence interval coverage less than percent type error greater than percent or relative bias greater than percent as problematic our results indicate that problems are fairly frequent with epv uncommon with epv and still observed with epv 
the worst instances of each problem were not severe with epv and usually comparable to those with epv others have found results that are not consistent with the above using different criteria 
useful criterion is whether the fitted model will be expected to achieve the same predictive discrimination in new sample as it appeared to achieve in the model development sample 
for that criterion events per candidate variable may be required 
also one can argue that observations are needed only to estimate the model intercept precisely enough that the margin of error in predicted probabilities is with confidence level 
error and significance of fit deviance and likelihood ratio test simple case in any fitting procedure the addition of another fitting parameter to model 
the beta parameters in logistic regression model will almost always improve the ability of the model to predict the measured outcomes 
this will be true even if the additional term has no predictive value since the model will simply be overfitting to the noise in the data 
the question arises as to whether the improvement gained by the addition of another fitting parameter is significant enough to recommend the inclusion of the additional term or whether the improvement is simply that which may be expected from overfitting 
in short for logistic regression statistic known as the deviance is defined which is measure of the error between the logistic model fit and the outcome data 
in the limit of large number of data points the deviance is chi squared distributed which allows chi squared test to be implemented in order to determine the significance of the explanatory variables 
linear regression and logistic regression have many similarities 
for example in simple linear regression set of data points xk yk are fitted to proposed model function of the form the fit is obtained by choosing the parameters which minimize the sum of the squares of the residuals the squared error term for each data point the minimum value which constitutes the fit will be denoted by the idea of null model may be introduced in which it is assumed that the variable is of no use in predicting the yk outcomes the data points are fitted to null model function of the form with squared error term the fitting process consists of choosing value of which minimizes of the fit to the null model denoted by where the subscript denotes the null model 
it is seen that the null model is optimized by where is the mean of the yk values and the optimized is which is proportional to the square of the uncorrected sample standard deviation of the yk data points 
we can imagine case where the yk data points are randomly assigned to the various xk and then fitted using the proposed model 
specifically we can consider the fits of the proposed model to every permutation of the yk outcomes 
it can be shown that the optimized error of any of these fits will never be less than the optimum error of the null model and that the difference between these minimum error will follow chi squared distribution distribution with degrees of freedom equal those of the proposed model minus those of the null model which in this case will be 
using the chi squared test we may then estimate how many of these permuted sets of yk will yield an minimum error less than or equal to the minimum error using the original yk and so we can estimate how significant an improvement is given by the inclusion of the variable in the proposed model 
for logistic regression the measure of goodness of fit is the likelihood function or its logarithm the log likelihood the likelihood function is analogous to the in the linear regression case except that the likelihood is maximized rather than minimized 
denote the maximized log likelihood of the proposed model by 
in the case of simple binary logistic regression the set of data points are fitted in probabilistic sense to function of the form where is the probability that the log odds are given by and the log likelihood is ln ln for the null model the probability that is given by the log odds for the null model are given by and the log likelihood is ln ln since we have at the maximum of the maximum log likelihood for the null model is ln ln the optimum is ln where is again the mean of the yk values 
again we can conceptually consider the fit of the proposed model to every permutation of the yk and it can be shown that the maximum log likelihood of these permutation fits will never be smaller than that of the null model also as an analog to the error of the linear regression case we may define the deviance of logistic regression fit as ln which will always be positive or zero 
the reason for this choice is that not only is the deviance good measure of the goodness of fit it is also approximately chi squared distributed with the approximation improving as the number of data points increases becoming exactly chi square distributed in the limit of an infinite number of data points 
as in the case of linear regression we may use this fact to estimate the probability that random set of data points will give better fit than the fit obtained by the proposed model and so have an estimate how significantly the model is improved by including the xk data points in the proposed model 
for the simple model of student test scores described above the maximum value of the log likelihood of the null model is the maximum value of the log likelihood for the simple model is so that the deviance is using the chi squared test of significance the integral of the chi squared distribution with one degree of freedom from to infinity is equal to this effectively means that about out of fits to random yk can be expected to have better fit smaller deviance than the given yk and so we can conclude that the inclusion of the variable and data in the proposed model is very significant improvement over the null model 
in other words we reject the null hypothesis with confidence 
goodness of fit summary goodness of fit in linear regression models is generally measured using 
since this has no direct analog in logistic regression various methods ch including the following can be used instead 
deviance and likelihood ratio tests in linear regression analysis one is concerned with partitioning variance via the sum of squares calculations variance in the criterion is essentially divided into variance accounted for by the predictors and residual variance 
in logistic regression analysis deviance is used in lieu of sum of squares calculations 
deviance is analogous to the sum of squares calculations in linear regression and is measure of the lack of fit to the data in logistic regression model 
when saturated model is available model with theoretically perfect fit deviance is calculated by comparing given model with the saturated model 
this computation gives the likelihood ratio test ln likelihood of the fitted model likelihood of the saturated model 
in the above equation represents the deviance and ln represents the natural logarithm 
the log of this likelihood ratio the ratio of the fitted model to the saturated model will produce negative value hence the need for negative sign 
can be shown to follow an approximate chi squared distribution 
smaller values indicate better fit as the fitted model deviates less from the saturated model 
when assessed upon chi square distribution nonsignificant chi square values indicate very little unexplained variance and thus good model fit 
conversely significant chi square value indicates that significant amount of the variance is unexplained 
when the saturated model is not available common case deviance is calculated simply as log likelihood of the fitted model and the reference to the saturated model log likelihood can be removed from all that follows without harm 
two measures of deviance are particularly important in logistic regression null deviance and model deviance 
the null deviance represents the difference between model with only the intercept which means no predictors and the saturated model 
the model deviance represents the difference between model with at least one predictor and the saturated model 
in this respect the null model provides baseline upon which to compare predictor models 
given that deviance is measure of the difference between given model and the saturated model smaller values indicate better fit 
thus to assess the contribution of predictor or set of predictors one can subtract the model deviance from the null deviance and assess the difference on chi square distribution with degrees of freedom equal to the difference in the number of parameters estimated 
let null ln likelihood of null model likelihood of the saturated model fitted ln likelihood of fitted model likelihood of the saturated model 
then the difference of both is null fitted ln likelihood of null model likelihood of the saturated model ln likelihood of fitted model likelihood of the saturated model ln likelihood of null model likelihood of the saturated model likelihood of fitted model likelihood of the saturated model ln likelihood of the null model likelihood of fitted model 
if the model deviance is significantly smaller than the null deviance then one can conclude that the predictor or set of predictors significantly improve the model fit 
this is analogous to the test used in linear regression analysis to assess the significance of prediction 
pseudo squared in linear regression the squared multiple correlation is used to assess goodness of fit as it represents the proportion of variance in the criterion that is explained by the predictors 
in logistic regression analysis there is no agreed upon analogous measure but there are several competing measures each with limitations four of the most commonly used indices and one less commonly used one are examined on this page likelihood ratio cox and snell cs nagelkerke mcfadden mcf tjur hosmer lemeshow test the hosmer lemeshow test uses test statistic that asymptotically follows distribution to assess whether or not the observed event rates match expected event rates in subgroups of the model population 
this test is considered to be obsolete by some statisticians because of its dependence on arbitrary binning of predicted probabilities and relative low power 
coefficient significance after fitting the model it is likely that researchers will want to examine the contribution of individual predictors 
to do so they will want to examine the regression coefficients 
in linear regression the regression coefficients represent the change in the criterion for each unit change in the predictor 
in logistic regression however the regression coefficients represent the change in the logit for each unit change in the predictor 
given that the logit is not intuitive researchers are likely to focus on predictor effect on the exponential function of the regression coefficient the odds ratio see definition 
in linear regression the significance of regression coefficient is assessed by computing test 
in logistic regression there are several different tests designed to assess the significance of an individual predictor most notably the likelihood ratio test and the wald statistic 
likelihood ratio test the likelihood ratio test discussed above to assess model fit is also the recommended procedure to assess the contribution of individual predictors to given model 
in the case of single predictor model one simply compares the deviance of the predictor model with that of the null model on chi square distribution with single degree of freedom 
if the predictor model has significantly smaller deviance 
chi square using the difference in degrees of freedom of the two models then one can conclude that there is significant association between the predictor and the outcome 
although some common statistical packages 
spss do provide likelihood ratio test statistics without this computationally intensive test it would be more difficult to assess the contribution of individual predictors in the multiple logistic regression case 
to assess the contribution of individual predictors one can enter the predictors hierarchically comparing each new model with the previous to determine the contribution of each predictor 
there is some debate among statisticians about the appropriateness of so called stepwise procedures 
the fear is that they may not preserve nominal statistical properties and may become misleading 
wald statistic alternatively when assessing the contribution of individual predictors in given model one may examine the significance of the wald statistic 
the wald statistic analogous to the test in linear regression is used to assess the significance of coefficients 
the wald statistic is the ratio of the square of the regression coefficient to the square of the standard error of the coefficient and is asymptotically distributed as chi square distribution 
although several statistical packages spss sas report the wald statistic to assess the contribution of individual predictors the wald statistic has limitations 
when the regression coefficient is large the standard error of the regression coefficient also tends to be larger increasing the probability of type ii error 
the wald statistic also tends to be biased when data are sparse 
case control sampling suppose cases are rare 
then we might wish to sample them more frequently than their prevalence in the population 
for example suppose there is disease that affects person in and to collect our data we need to do complete physical 
it may be too expensive to do thousands of physicals of healthy people in order to obtain data for only few diseased individuals 
thus we may evaluate more diseased individuals perhaps all of the rare outcomes 
this is also retrospective sampling or equivalently it is called unbalanced data 
as rule of thumb sampling controls at rate of five times the number of cases will produce sufficient control data logistic regression is unique in that it may be estimated on unbalanced data rather than randomly sampled data and still yield correct coefficient estimates of the effects of each independent variable on the outcome 
that is to say if we form logistic model from such data if the model is correct in the general population the parameters are all correct except for we can correct if we know the true prevalence as follows log log where is the true prevalence and is the prevalence in the sample 
discussion like other forms of regression analysis logistic regression makes use of one or more predictor variables that may be either continuous or categorical 
unlike ordinary linear regression however logistic regression is used for predicting dependent variables that take membership in one of limited number of categories treating the dependent variable in the binomial case as the outcome of bernoulli trial rather than continuous outcome 
given this difference the assumptions of linear regression are violated 
in particular the residuals cannot be normally distributed 
in addition linear regression may make nonsensical predictions for binary dependent variable 
what is needed is way to convert binary variable into continuous one that can take on any real value negative or positive 
to do that binomial logistic regression first calculates the odds of the event happening for different levels of each independent variable and then takes its logarithm to create continuous criterion as transformed version of the dependent variable 
the logarithm of the odds is the logit of the probability the logit is defined as follows although the dependent variable in logistic regression is bernoulli the logit is on an unrestricted scale 
the logit function is the link function in this kind of generalized linear model 
is the bernoulli distributed response variable and is the predictor variable the values are the linear parameters 
the logit of the probability of success is then fitted to the predictors 
the predicted value of the logit is converted back into predicted odds via the inverse of the natural logarithm the exponential function 
thus although the observed dependent variable in binary logistic regression is or variable the logistic regression estimates the odds as continuous variable that the dependent variable is success 
in some applications the odds are all that is needed 
in others specific yes or no prediction is needed for whether the dependent variable is or is not success this categorical prediction can be based on the computed odds of success with predicted odds above some chosen cutoff value being translated into prediction of success 
maximum entropy of all the functional forms used for estimating the probabilities of particular categorical outcome which optimize the fit by maximizing the likelihood function 
probit regression poisson regression etc 
the logistic regression solution is unique in that it is maximum entropy solution 
this is case of general property an exponential family of distributions maximizes entropy given an expected value 
in the case of the logistic model the logistic function is the natural parameter of the bernoulli distribution it is in canonical form and the logistic function is the canonical link function while other sigmoid functions are non canonical link functions this underlies its mathematical elegance and ease of optimization 
see exponential family maximum entropy derivation for details 
proof in order to show this we use the method of lagrange multipliers 
the lagrangian is equal to the entropy plus the sum of the products of lagrange multipliers times various constraint expressions 
the general multinomial case will be considered since the proof is not made that much simpler by considering simpler cases 
equating the derivative of the lagrangian with respect to the various probabilities to zero yields functional form for those probabilities which corresponds to those used in logistic regression as in the above section on multinomial logistic regression we will consider explanatory variables denoted and which include there will be total of data points indexed by and the data points are given by and the xmk will also be represented as an dimensional vector 
there will be possible values of the categorical variable ranging from to let pn be the probability given explanatory variable vector that the outcome will be define which is the probability that for the th measurement the categorical outcome is the lagrangian will be expressed as function of the probabilities pnk and will minimized by equating the derivatives of the lagrangian with respect to these probabilities to zero 
an important point is that the probabilities are treated equally and the fact that they sum to unity is part of the lagrangian formulation rather than being assumed from the beginning 
the first contribution to the lagrangian is the entropy ln the log likelihood is ln assuming the multinomial logistic function the derivative of the log likelihood with respect the beta coefficients was found to be very important point here is that this expression is remarkably not an explicit function of the beta coefficients 
it is only function of the probabilities pnk and the data 
rather than being specific to the assumed multinomial logistic case it is taken to be general statement of the condition at which the log likelihood is maximized and makes no reference to the functional form of pnk 
there are then fitting constraints and the fitting constraint term in the lagrangian is then where the nm are the appropriate lagrange multipliers 
there are normalization constraints which may be written so that the normalization term in the lagrangian is where the are the appropriate lagrange multipliers 
the lagrangian is then the sum of the above three terms setting the derivative of the lagrangian with respect to one of the probabilities to zero yields ln using the more condensed vector notation and dropping the primes on the and indices and then solving for yields where imposing the normalization constraint we can solve for the zk and write the probabilities as the are not all independent 
we can add any constant dimensional vector to each of the without changing the value of the probabilities so that there are only rather than independent in the multinomial logistic regression section above the was subtracted from each which set the exponential term involving to unity and the beta coefficients were given by 
other approaches in machine learning applications where logistic regression is used for binary classification the mle minimises the cross entropy loss function 
logistic regression is an important machine learning algorithm 
the goal is to model the probability of random variable being or given experimental data consider generalized linear model function parameterized by pr therefore pr and since we see that pr is given by pr 
we now calculate the likelihood function assuming that all the observations in the sample are independently bernoulli distributed pr pr typically the log likelihood is maximized log log pr which is maximized using optimization techniques such as gradient descent 
assuming the pairs are drawn uniformly from the underlying distribution then in the limit of large lim log pr pr log pr pr log pr pr log pr kl where is the conditional entropy and kl is the kullback leibler divergence 
this leads to the intuition that by maximizing the log likelihood of model you are minimizing the kl divergence of your model from the maximal entropy distribution 
intuitively searching for the model that makes the fewest assumptions in its parameters 
comparison with linear regression logistic regression can be seen as special case of the generalized linear model and thus analogous to linear regression 
the model of logistic regression however is based on quite different assumptions about the relationship between the dependent and independent variables from those of linear regression 
in particular the key differences between these two models can be seen in the following two features of logistic regression 
first the conditional distribution is bernoulli distribution rather than gaussian distribution because the dependent variable is binary 
second the predicted values are probabilities and are therefore restricted to through the logistic distribution function because logistic regression predicts the probability of particular outcomes rather than the outcomes themselves 
alternatives common alternative to the logistic model logit model is the probit model as the related names suggest 
from the perspective of generalized linear models these differ in the choice of link function the logistic model uses the logit function inverse logistic function while the probit model uses the probit function inverse error function 
equivalently in the latent variable interpretations of these two methods the first assumes standard logistic distribution of errors and the second standard normal distribution of errors 
other sigmoid functions or error distributions can be used instead 
logistic regression is an alternative to fisher method linear discriminant analysis 
if the assumptions of linear discriminant analysis hold the conditioning can be reversed to produce logistic regression 
the converse is not true however because logistic regression does not require the multivariate normal assumption of discriminant analysis the assumption of linear predictor effects can easily be relaxed using techniques such as spline functions 
history detailed history of the logistic regression is given in cramer 
the logistic function was developed as model of population growth and named logistic by pierre fran ois verhulst in the and under the guidance of adolphe quetelet see logistic function history for details 
in his earliest paper verhulst did not specify how he fit the curves to the data 
in his more detailed paper verhulst determined the three parameters of the model by making the curve pass through three observed points which yielded poor predictions the logistic function was independently developed in chemistry as model of autocatalysis wilhelm ostwald 
an autocatalytic reaction is one in which one of the products is itself catalyst for the same reaction while the supply of one of the reactants is fixed 
this naturally gives rise to the logistic equation for the same reason as population growth the reaction is self reinforcing but constrained 
the logistic function was independently rediscovered as model of population growth in by raymond pearl and lowell reed published as pearl reed which led to its use in modern statistics 
they were initially unaware of verhulst work and presumably learned about it from gustave du pasquier but they gave him little credit and did not adopt his terminology 
verhulst priority was acknowledged and the term logistic revived by udny yule in and has been followed since 
pearl and reed first applied the model to the population of the united states and also initially fitted the curve by making it pass through three points as with verhulst this again yielded poor results in the the probit model was developed and systematized by chester ittner bliss who coined the term probit in bliss and by john gaddum in gaddum and the model fit by maximum likelihood estimation by ronald fisher in fisher as an addendum to bliss work 
the probit model was principally used in bioassay and had been preceded by earlier work dating to see probit model history 
the probit model influenced the subsequent development of the logit model and these models competed with each other the logistic model was likely first used as an alternative to the probit model in bioassay by edwin bidwell wilson and his student jane worcester in wilson worcester 
however the development of the logistic model as general alternative to the probit model was principally due to the work of joseph berkson over many decades beginning in berkson where he coined logit by analogy with probit and continuing through berkson and following years 
the logit model was initially dismissed as inferior to the probit model but gradually achieved an equal footing with the logit particularly between and by the logit model achieved parity with the probit model in use in statistics journals and thereafter surpassed it 
this relative popularity was due to the adoption of the logit outside of bioassay rather than displacing the probit within bioassay and its informal use in practice the logit popularity is credited to the logit model computational simplicity mathematical properties and generality allowing its use in varied fields various refinements occurred during that time notably by david cox as in cox the multinomial logit model was introduced independently in cox and thiel which greatly increased the scope of application and the popularity of the logit model 
in daniel mcfadden linked the multinomial logit to the theory of discrete choice specifically luce choice axiom showing that the multinomial logit followed from the assumption of independence of irrelevant alternatives and interpreting odds of alternatives as relative preferences this gave theoretical foundation for the logistic regression 
extensions there are large numbers of extensions multinomial logistic regression or multinomial logit handles the case of multi way categorical dependent variable with unordered values also called classification 
note that the general case of having dependent variables with more than two values is termed polytomous regression 
ordered logistic regression or ordered logit handles ordinal dependent variables ordered values 
mixed logit is an extension of multinomial logit that allows for correlations among the choices of the dependent variable 
an extension of the logistic model to sets of interdependent variables is the conditional random field 
conditional logistic regression handles matched or stratified data when the strata are small 
it is mostly used in the analysis of observational studies 
software most statistical software can do binary logistic regression 
spss for basic logistic regression 
stata sas proc logistic for basic logistic regression 
proc catmod when all the variables are categorical 
proc glimmix for multilevel model logistic regression 
glm in the stats package using family binomial lrm in the rms package glmnet package for an efficient implementation regularized logistic regression lmer for mixed effects logistic regression rfast package command gm logistic for fast and heavy calculations involving large scale data 
arm package for bayesian logistic regression python logit in the statsmodels module 
logisticregression in the scikit learn module 
logisticregressor in the tensorflow module 
full example of logistic regression in the theano tutorial bayesian logistic regression with ard prior code tutorial variational bayes logistic regression with ard prior code tutorial bayesian logistic regression code tutorial ncss logistic regression in ncss matlab mnrfit in the statistics and machine learning toolbox with incorrect coded as instead of fminunc fmincon fitglm mnrfit fitclinear mle can all do logistic regression 
java jvm liblinear apache flink apache spark sparkml supports logistic regression fpga logistic regresesion ip core in hls for fpga notably microsoft excel statistics extension package does not include it 
see also logistic function discrete choice jarrow turnbull model limited dependent variable multinomial logit model ordered logit hosmer lemeshow test brier score mlpack contains implementation of logistic regression local case control sampling logistic model tree references further reading external links media related to logistic regression at wikimedia commons econometrics lecture topic logit model on youtube by mark thoma logistic regression tutorial mlelr software in for teaching purposes
in statistics the coefficient of determination denoted or and pronounced squared is the proportion of the variation in the dependent variable that is predictable from the independent variable 
it is statistic used in the context of statistical models whose main purpose is either the prediction of future outcomes or the testing of hypotheses on the basis of other related information 
it provides measure of how well observed outcomes are replicated by the model based on the proportion of total variation of outcomes explained by the model there are several definitions of that are only sometimes equivalent 
one class of such cases includes that of simple linear regression where is used instead of 
when only an intercept is included then is simply the square of the sample correlation coefficient between the observed outcomes and the observed predictor values 
if additional regressors are included is the square of the coefficient of multiple correlation 
in both such cases the coefficient of determination normally ranges from to there are cases where can yield negative values 
this can arise when the predictions that are being compared to the corresponding outcomes have not been derived from model fitting procedure using those data 
even if model fitting procedure has been used may still be negative for example when linear regression is conducted without including an intercept or when non linear function is used to fit the data 
in cases where negative values arise the mean of the data provides better fit to the outcomes than do the fitted function values according to this particular criterion 
the coefficient of determination can be more intuitively informative than mae mape mse and rmse in regression analysis evaluation as the former can be expressed as percentage whereas the latter measures have arbitrary ranges 
it also proved more robust for poor fits compared to smape on the test datasets in the article when evaluating the goodness of fit of simulated ypred vs measured yobs values it is not appropriate to base this on the of the linear regression yobs ypred 
the quantifies the degree of any linear correlation between yobs and ypred while for the goodness of fit evaluation only one specific linear correlation should be taken into consideration yobs ypred the line 
definitions data set has values marked yn collectively known as yi or as vector yn each associated with fitted or modeled or predicted value fn known as fi or sometimes as vector 
define the residuals as ei yi fi forming vector 
if is the mean of the observed data then the variability of the data set can be measured with two sums of squares formulas the sum of squares of residuals also called the residual sum of squares the total sum of squares proportional to the variance of the data the most general definition of the coefficient of determination is in the best case the modeled values exactly match the observed values which results in res and baseline model which always predicts will have models that have worse predictions than this baseline will have negative 
relation to unexplained variance in general form can be seen to be related to the fraction of variance unexplained fvu since the second term compares the unexplained variance variance of the model errors with the total variance of the data as explained variance suppose this implies that of the variability of the dependent variable in the data set has been accounted for and the remaining of the variability is still unaccounted for 
for regression models the regression sum of squares also called the explained sum of squares is defined as reg in some cases as in simple linear regression the total sum of squares equals the sum of the two other sums of squares defined above res reg tot see partitioning in the general ols model for derivation of this result for one case where the relation holds 
when this relation does hold the above definition of is equivalent to reg tot reg tot where is the number of observations cases on the variables 
in this form is expressed as the ratio of the explained variance variance of the model predictions which is ssreg to the total variance sample variance of the dependent variable which is sstot 
this partition of the sum of squares holds for instance when the model values have been obtained by linear regression 
milder sufficient condition reads as follows the model has the form where the qi are arbitrary values that may or may not depend on or on other free parameters the common choice qi xi is just one special case and the coefficient estimates and are obtained by minimizing the residual sum of squares 
this set of conditions is an important one and it has number of implications for the properties of the fitted residuals and the modelled values 
in particular under these conditions 
as squared correlation coefficient in linear least squares multiple regression with an estimated intercept term equals the square of the pearson correlation coefficient between the observed and modeled predicted data values of the dependent variable 
in linear least squares regression with an intercept term and single explanator this is also equal to the squared pearson correlation coefficient of the dependent variable and explanatory variable it should not be confused with the correlation coefficient between two explanatory variables defined as cov where the covariance between two coefficient estimates as well as their standard deviations are obtained from the covariance matrix of the coefficient estimates under more general modeling conditions where the predicted values might be generated from model different from linear least squares regression an value can be calculated as the square of the correlation coefficient between the original and modeled data values 
in this case the value is not directly measure of how good the modeled values are but rather measure of how good predictor might be constructed from the modeled values by creating revised predictor of the form 
according to everitt this usage is specifically the definition of the term coefficient of determination the square of the correlation between two general variables 
interpretation is measure of the goodness of fit of model 
in regression the coefficient of determination is statistical measure of how well the regression predictions approximate the real data points 
an of indicates that the regression predictions perfectly fit the data 
values of outside the range to occur when the model fits the data worse than the worst possible least squares predictor equivalent to horizontal hyperplane at height equal to the mean of the observed data 
this occurs when wrong model was chosen or nonsensical constraints were applied by mistake 
if equation of kv lseth is used this is the equation used most often can be less than zero 
if equation of kv lseth is used can be greater than one 
in all instances where is used the predictors are calculated by ordinary least squares regression that is by minimizing ssres 
in this case increases as the number of variables in the model is increased is monotone increasing with the number of variables included it will never decrease 
this illustrates drawback to one possible use of where one might keep adding variables kitchen sink regression to increase the value 
for example if one is trying to predict the sales of model of car from the car gas mileage price and engine power one can include such irrelevant factors as the first letter of the model name or the height of the lead engineer designing the car because the will never decrease as variables are added and will likely experience an increase due to chance alone 
this leads to the alternative approach of looking at the adjusted 
the explanation of this statistic is almost the same as but it penalizes the statistic as extra variables are included in the model 
for cases other than fitting by ordinary least squares the statistic can be calculated as above and may still be useful measure 
if fitting is by weighted least squares or generalized least squares alternative versions of can be calculated appropriate to those statistical frameworks while the raw may still be useful if it is more easily interpreted 
values for can be calculated for any type of predictive model which need not have statistical basis 
in multiple linear model consider linear model with more than single explanatory variable of the form where for the ith case is the response variable are regressors and is mean zero error term 
the quantities are unknown coefficients whose values are estimated by least squares 
the coefficient of determination is measure of the global fit of the model 
specifically is an element of and represents the proportion of variability in yi that may be attributed to some linear combination of the regressors explanatory variables in is often interpreted as the proportion of response variation explained by the regressors in the model 
thus indicates that the fitted model explains all variability in while indicates no linear relationship for straight line regression this means that the straight line model is constant line slope intercept between the response variable and regressors 
an interior value such as may be interpreted as follows seventy percent of the variance in the response variable can be explained by the explanatory variables 
the remaining thirty percent can be attributed to unknown lurking variables or inherent variability 
caution that applies to as to other statistical descriptions of correlation and association is that correlation does not imply causation 
in other words while correlations may sometimes provide valuable clues in uncovering causal relationships among variables non zero estimated correlation between two variables is not on its own evidence that changing the value of one variable would result in changes in the values of other variables 
for example the practice of carrying matches or lighter is correlated with incidence of lung cancer but carrying matches does not cause cancer in the standard sense of cause 
in case of single regressor fitted by least squares is the square of the pearson product moment correlation coefficient relating the regressor and the response variable 
more generally is the square of the correlation between the constructed predictor and the response variable 
with more than one regressor the can be referred to as the coefficient of multiple determination 
inflation of in least squares regression using typical data is at least weakly increasing with increases in the number of regressors in the model 
because increases in the number of regressors increase the value of alone cannot be used as meaningful comparison of models with very different numbers of independent variables 
for meaningful comparison between two models an test can be performed on the residual sum of squares similar to the tests in granger causality though this is not always appropriate 
as reminder of this some authors denote by rq where is the number of columns in the number of explanators including the constant 
to demonstrate this property first recall that the objective of least squares linear regression is min res min where xi is row vector of values of explanatory variables for case and is column vector of coefficients of the respective elements of xi 
the optimal value of the objective is weakly smaller as more explanatory variables are added and hence additional columns of the explanatory data matrix whose ith row is xi are added by the fact that less constrained minimization leads to an optimal cost which is weakly smaller than more constrained minimization does 
given the previous conclusion and noting that depends only on the non decreasing property of follows directly from the definition above 
the intuitive reason that using an additional explanatory variable cannot lower the is this minimizing res is equivalent to maximizing 
when the extra variable is included the data always have the option of giving it an estimated coefficient of zero leaving the predicted values and the unchanged 
the only way that the optimization problem will give non zero coefficient is if doing so improves the 
caveats does not indicate whether the independent variables are cause of the changes in the dependent variable omitted variable bias exists the correct regression was used the most appropriate set of independent variables has been chosen there is collinearity present in the data on the explanatory variables the model might be improved by using transformed versions of the existing set of independent variables there are enough data points to make solid conclusion 
extensions adjusted the use of an adjusted one common notation is pronounced bar squared another is or adj is an attempt to account for the phenomenon of the automatically increasing when extra explanatory variables are added to the model 
there are many different ways of adjusting 
by far the most used one to the point that it is typically just referred to as adjusted is the correction proposed by mordecai ezekiel 
the adjusted is defined as res df res tot df tot where dfres is the degrees of freedom of the estimate of the population variance around the model and dftot is the degrees of freedom of the estimate of the population variance around the mean 
dfres is given in terms of the sample size and the number of variables in the model dfres dftot is given in the same way but with being unity for the mean 
inserting the degrees of freedom and using the definition of it can be rewritten as where is the total number of explanatory variables in the model and is the sample size 
the adjusted can be negative and its value will always be less than or equal to that of 
unlike the adjusted increases only when the increase in due to the inclusion of new explanatory variable is more than one would expect to see by chance 
if set of explanatory variables with predetermined hierarchy of importance are introduced into regression one at time with the adjusted computed each time the level at which adjusted reaches maximum and decreases afterward would be the regression with the ideal combination of having the best fit without excess unnecessary terms 
adjusted can be interpreted as less biased estimator of the population whereas the observed sample is positively biased estimate of the population value 
adjusted is more appropriate when evaluating model fit the variance in the dependent variable accounted for by the independent variables and in comparing alternative models in the feature selection stage of model building the principle behind the adjusted statistic can be seen by rewriting the ordinary as var res var tot where var res res and var tot tot are the sample variances of the estimated residuals and the dependent variable respectively which can be seen as biased estimates of the population variances of the errors and of the dependent variable 
these estimates are replaced by statistically unbiased versions var res res and var tot tot 
despite using unbiased estimators for the population variances of the error and the dependent variable adjusted is not an unbiased estimator of the population which results by using the population variances of the errors and the dependent variable instead of estimating them 
ingram olkin and john pratt derived the minimum variance unbiased estimator for the population which is known as olkin pratt estimator 
comparisons of different approaches for adjusting concluded that in most situations either an approximate version of the olkin pratt estimator or the exact olkin pratt estimator should be preferred over ezekiel adjusted 
coefficient of partial determination the coefficient of partial determination can be defined as the proportion of variation that cannot be explained in reduced model but can be explained by the predictors specified in full er model 
this coefficient is used to provide insight into whether or not one or more additional predictors may be useful in more fully specified regression model 
the calculation for the partial is relatively straightforward after estimating two models and generating the anova tables for them 
the calculation for the partial is res reduced res full res reduced which is analogous to the usual coefficient of determination tot res tot 
generalizing and decomposing as explained above model selection heuristics such as the adjusted criterion and the test examine whether the total sufficiently increases to determine if new regressor should be added to the model 
if regressor is added to the model that is highly correlated with other regressors which have already been included then the total will hardly increase even if the new regressor is of relevance 
as result the above mentioned heuristics will ignore relevant regressors when cross correlations are high 
alternatively one can decompose generalized version of to quantify the relevance of deviating from hypothesis 
as hoornweg shows several shrinkage estimators such as bayesian linear regression ridge regression and the adaptive lasso make use of this decomposition of when they gradually shrink parameters from the unrestricted ols solutions towards the hypothesized values 
let us first define the linear regression model as it is assumed that the matrix is standardized with scores and that the column vector is centered to have mean of zero 
let the column vector refer to the hypothesized regression parameters and let the column vector denote the estimated parameters 
we can then define 
an of means that the in sample accuracy improves by if the data optimized solutions are used instead of the hypothesized values 
in the special case that is vector of zeros we obtain the traditional again 
the individual effect on of deviating from hypothesis can be computed with outer 
this times matrix is given by where the diagonal elements of exactly add up to if regressors are uncorrelated and is vector of zeros then the th diagonal element of simply corresponds to the value between and when regressors and are correlated might increase at the cost of decrease in 
as result the diagonal elements of may be smaller than and in more exceptional cases larger than to deal with such uncertainties several shrinkage estimators implicitly take weighted average of the diagonal elements of to quantify the relevance of deviating from hypothesized value 
click on the lasso for an example 
in logistic regression in the case of logistic regression usually fit by maximum likelihood there are several choices of pseudo 
one is the generalized originally proposed by cox snell and independently by magee where is the likelihood of the model with only the intercept is the likelihood of the estimated model the model with given set of parameter estimates and is the sample size 
it is easily rewritten to ln ln where is the test statistic of the likelihood ratio test 
nagelkerke noted that it had the following properties it is consistent with the classical coefficient of determination when both can be computed its value is maximised by the maximum likelihood estimation of model it is asymptotically independent of the sample size the interpretation is the proportion of the variation explained by the model the values are between and with denoting that model does not explain any variation and denoting that it perfectly explains the observed variation it does not have any unit however in the case of logistic model where cannot be greater than is between and max thus nagelkerke suggested the possibility to define scaled as max 
comparison with norm of residuals occasionally the norm of residuals is used for indicating goodness of fit 
this term is calculated as the square root of the sum of squares of residuals norm of residuals res 
both and the norm of residuals have their relative merits 
for least squares analysis varies between and with larger numbers indicating better fits and representing perfect fit 
the norm of residuals varies from to infinity with smaller numbers indicating better fits and zero indicating perfect fit 
one advantage and disadvantage of is the tot term acts to normalize the value 
if the yi values are all multiplied by constant the norm of residuals will also change by that constant but will stay the same 
as basic example for the linear least squares fit to the set of data and norm of residuals if all values of are multiplied by for example in an si prefix change then remains the same but norm of residuals another single parameter indicator of fit is the rmse of the residuals or standard deviation of the residuals 
this would have value of for the above example given that the fit was linear with an unforced intercept 
history the creation of the coefficient of determination has been attributed to the geneticist sewall wright and was first published in 
see also anscombe quartet fraction of variance unexplained goodness of fit nash sutcliffe model efficiency coefficient hydrological applications pearson product moment correlation coefficient proportional reduction in loss regression model validation root mean square deviation stepwise regression notes further reading gujarati damodar porter dawn 
basic econometrics fifth ed 
new york mcgraw hill irwin 
isbn hughes ann grawoig dennis 
statistics foundation for analysis 
elements of econometrics second ed 
isbn lewis beck michael skalaban andrew 
the squared some straight talk 
jstor chicco davide warrens matthijs jurman giuseppe 
the coefficient of determination squared is more informative than smape mae mape mse and rmse in regression analysis evaluation
in mathematics variable from latin variabilis changeable is symbol and placeholder for any mathematical object 
in particular variable may represent number vector matrix function the argument of function set or an element of set algebraic computations with variables as if they were explicit numbers solve range of problems in single computation 
for example the quadratic formula solves any quadratic equation by substituting the numeric values of the coefficients of that equation for the variables that represent them in the quadratic formula 
in mathematical logic variable is either symbol representing an unspecified term of the theory meta variable or basic object of the theory that is manipulated without referring to its possible intuitive interpretation 
history in ancient works such as euclid elements single letters refer to geometric points and shapes 
in the th century brahmagupta used different colours to represent the unknowns in algebraic equations in the br hmasphu asiddh nta 
one section of this book is called equations of several colours at the end of the th century fran ois vi te introduced the idea of representing known and unknown numbers by letters nowadays called variables and the idea of computing with them as if they were numbers in order to obtain the result by simple replacement 
vi te convention was to use consonants for known values and vowels for unknowns in ren descartes invented the convention of representing unknowns in equations by and and knowns by and 
contrarily to vi te convention descartes is still commonly in use 
the history of the letter in math was discussed in scientific american article starting in the isaac newton and gottfried wilhelm leibniz independently developed the infinitesimal calculus which essentially consists of studying how an infinitesimal variation of variable quantity induces corresponding variation of another quantity which is function of the first variable 
almost century later leonhard euler fixed the terminology of infinitesimal calculus and introduced the notation for function its variable and its value until the end of the th century the word variable referred almost exclusively to the arguments and the values of functions 
in the second half of the th century it appeared that the foundation of infinitesimal calculus was not formalized enough to deal with apparent paradoxes such as nowhere differentiable continuous function 
to solve this problem karl weierstrass introduced new formalism consisting of replacing the intuitive notion of limit by formal definition 
the older notion of limit was when the variable varies and tends toward then tends toward without any accurate definition of tends 
weierstrass replaced this sentence by the formula in which none of the five variables is considered as varying 
this static formulation led to the modern notion of variable which is simply symbol representing mathematical object that either is unknown or may be replaced by any element of given set the set of real numbers 
notation variables are generally denoted by single letter most often from the latin alphabet and less often from the greek which may be lowercase or capitalized 
the letter may be followed by subscript number as in another variable xi word or abbreviation of word xtotal or mathematical expression 
under the influence of computer science some variable names in pure mathematics consist of several letters and digits 
following ren descartes letters at the beginning of the alphabet such as are commonly used for known values and parameters and letters at the end of the alphabet such as are commonly used for unknowns and variables of functions 
in printed mathematics the norm is to set variables and constants in an italic typeface for example general quadratic function is conventionally written as where and are parameters also called constants because they are constant functions while is the variable of the function 
more explicit way to denote this function is which clarifies the function argument status of and the constant status of and since occurs in term that is constant function of it is called the constant term specific branches and applications of mathematics have specific naming conventions for variables 
variables with similar roles or meanings are often assigned consecutive letters or the same letter with different subscripts 
for example the three axes in coordinate space are conventionally called and in physics the names of variables are largely determined by the physical quantity they describe but various naming conventions exist 
convention often followed in probability and statistics is to use for the names of random variables keeping for variables representing corresponding better defined values 
specific kinds of variables it is common for variables to play different roles in the same mathematical formula and names or qualifiers have been introduced to distinguish them 
for example the general cubic equation is interpreted as having five variables four which are taken to be given numbers and the fifth variable is understood to be an unknown number 
to distinguish them the variable is called an unknown and the other variables are called parameters or coefficients or sometimes constants although this last terminology is incorrect for an equation and should be reserved for the function defined by the left hand side of this equation 
in the context of functions the term variable refers commonly to the arguments of the functions 
this is typically the case in sentences like function of real variable is the variable of the function is function of the variable meaning that the argument of the function is referred to by the variable 
in the same context variables that are independent of define constant functions and are therefore called constant 
for example constant of integration is an arbitrary constant function that is added to particular antiderivative to obtain the other antiderivatives 
because the strong relationship between polynomials and polynomial function the term constant is often used to denote the coefficients of polynomial which are constant functions of the indeterminates 
this use of constant as an abbreviation of constant function must be distinguished from the normal meaning of the word in mathematics 
constant or mathematical constant is well and unambiguously defined number or other mathematical object as for example the numbers and the identity element of group 
since variable may represent any mathematical object letter that represents constant is often called variable 
this is in particular the case of and even when they represents euler number and other specific names for variables are an unknown is variable in an equation which has to be solved for 
an indeterminate is symbol commonly called variable that appears in polynomial or formal power series 
formally speaking an indeterminate is not variable but constant in the polynomial ring or the ring of formal power series 
however because of the strong relationship between polynomials or power series and the functions that they define many authors consider indeterminates as special kind of variables 
parameter is quantity usually number which is part of the input of problem and remains constant during the whole solution of this problem 
for example in mechanics the mass and the size of solid body are parameters for the study of its movement 
in computer science parameter has different meaning and denotes an argument of function 
free variables and bound variables random variable is kind of variable that is used in probability theory and its applications all these denominations of variables are of semantic nature and the way of computing with them syntax is the same for all 
dependent and independent variables in calculus and its application to physics and other sciences it is rather common to consider variable say whose possible values depend on the value of another variable say in mathematical terms the dependent variable represents the value of function of to simplify formulas it is often useful to use the same symbol for the dependent variable and the function mapping onto for example the state of physical system depends on measurable quantities such as the pressure the temperature the spatial position and all these quantities vary when the system evolves that is they are function of the time 
in the formulas describing the system these quantities are represented by variables which are dependent on the time and thus considered implicitly as functions of the time 
therefore in formula dependent variable is variable that is implicitly function of another or several other variables 
an independent variable is variable that is not dependent the property of variable to be dependent or independent depends often of the point of view and is not intrinsic 
for example in the notation the three variables may be all independent and the notation represents function of three variables 
on the other hand if and depend on are dependent variables then the notation represents function of the single independent variable 
examples if one defines function from the real numbers to the real numbers by sin then is variable standing for the argument of the function being defined which can be any real number 
in the identity the variable is summation variable which designates in turn each of the integers it is also called index because its variation is over discrete set of values while is parameter it does not vary within the formula 
in the theory of polynomials polynomial of degree is generally denoted as ax bx where and are called coefficients they are assumed to be fixed parameters of the problem considered while is called variable 
when studying this polynomial for its polynomial function this stands for the function argument 
when studying the polynomial as an object in itself is taken to be an indeterminate and would often be written with capital letter instead to indicate this status 
example the ideal gas law consider the equation describing the ideal gas law this equation would generally be interpreted to have four variables and one constant 
the constant is the boltzmann constant 
one of the variables the number of particles is positive integer and therefore discrete variable while the other three and for pressure volume and temperature are continuous variables 
one could rearrange this equation to obtain as function of the other variables then as function of the other variables is the dependent variable while its arguments and are independent variables 
one could approach this function more formally and think about its domain and range in function notation here is function however in an experiment in order to determine the dependence of pressure on single one of the independent variables it is necessary to fix all but one of the variables say this gives function where now and are also regarded as constants 
mathematically this constitutes partial application of the earlier function this illustrates how independent variables and constants are largely dependent on the point of view taken 
one could even regard as variable to obtain function moduli spaces considering constants and variables can lead to the concept of moduli spaces 
for illustration consider the equation for parabola where and are all considered to be real 
the set of points in the plane satisfying this equation trace out the graph of parabola 
here and are regarded as constants which specify the parabola while and are variables 
then instead regarding and as variables we observe that each set of tuples corresponds to different parabola 
that is they specify coordinates on the space of parabolas this is known as moduli space of parabolas 
conventional variable names sometimes extended to for parameters or coefficients for situations where distinct letters are inconvenient ai or ui for the th term of sequence or the th coefficient of series for euler number for functions as in for the imaginary unit sometimes or for varying integers or indices in an indexed family or unit vectors and for the length and width of figure also for line or in number theory for prime number not equal to with as second choice for fixed integer such as count of objects or the degree of an equation for prime number or probability for prime power or quotient for radius remainder or correlation coefficient for time for the three cartesian coordinates of point in euclidean geometry or the corresponding axes for complex number or in statistics normal random variable for angle measures with as second choice for an arbitrarily small positive number for an eigenvalue capital sigma for sum or lowercase sigma in statistics for the standard deviation for mean see also lambda calculus observable variable physical constant propositional variable references bibliography
in statistics principal component regression pcr is regression analysis technique that is based on principal component analysis pca 
more specifically pcr is used for estimating the unknown regression coefficients in standard linear regression model 
in pcr instead of regressing the dependent variable on the explanatory variables directly the principal components of the explanatory variables are used as regressors 
one typically uses only subset of all the principal components for regression making pcr kind of regularized procedure and also type of shrinkage estimator 
often the principal components with higher variances the ones based on eigenvectors corresponding to the higher eigenvalues of the sample variance covariance matrix of the explanatory variables are selected as regressors 
however for the purpose of predicting the outcome the principal components with low variances may also be important in some cases even more important one major use of pcr lies in overcoming the multicollinearity problem which arises when two or more of the explanatory variables are close to being collinear 
pcr can aptly deal with such situations by excluding some of the low variance principal components in the regression step 
in addition by usually regressing on only subset of all the principal components pcr can result in dimension reduction through substantially lowering the effective number of parameters characterizing the underlying model 
this can be particularly useful in settings with high dimensional covariates 
also through appropriate selection of the principal components to be used for regression pcr can lead to efficient prediction of the outcome based on the assumed model 
the principle the pcr method may be broadly divided into three major steps perform pca on the observed data matrix for the explanatory variables to obtain the principal components and then usually select subset based on some appropriate criteria of the principal components so obtained for further use 
now regress the observed vector of outcomes on the selected principal components as covariates using ordinary least squares regression linear regression to get vector of estimated regression coefficients with dimension equal to the number of selected principal components now transform this vector back to the scale of the actual covariates using the selected pca loadings the eigenvectors corresponding to the selected principal components to get the final pcr estimator with dimension equal to the total number of covariates for estimating the regression coefficients characterizing the original model 
details of the method data representation let denote the vector of observed outcomes and denote the corresponding data matrix of observed covariates where and denote the size of the observed sample and the number of covariates respectively with each of the rows of denotes one set of observations for the dimensional covariate and the respective entry of denotes the corresponding observed outcome 
data pre processing assume that and each of the columns of have already been centered so that all of them have zero empirical means 
this centering step is crucial at least for the columns of since pcr involves the use of pca on and pca is sensitive to centering of the data 
underlying model following centering the standard gauss markov linear regression model for on can be represented as where denotes the unknown parameter vector of regression coefficients and denotes the vector of random errors with and var for some unknown variance parameter objective the primary goal is to obtain an efficient estimator for the parameter based on the data 
one frequently used approach for this is ordinary least squares regression which assuming is full column rank gives the unbiased estimator of pcr is another technique that may be used for the same purpose of estimating pca step pcr starts by performing pca on the centered data matrix for this let denote the singular value decomposition of where diag with denoting the non negative singular values of while the columns of and are both orthonormal sets of vectors denoting the left and right singular vectors of respectively 
the principal components gives spectral decomposition of where diag diag with denoting the non negative eigenvalues also known as the principal values of while the columns of denote the corresponding orthonormal set of eigenvectors 
then and respectively denote the principal component and the principal component direction or pca loading corresponding to the th largest principal value for each 
derived covariates for any let denote the matrix with orthonormal columns consisting of the first columns of let denote the matrix having the first principal components as its columns 
may be viewed as the data matrix obtained by using the transformed covariates instead of using the original covariates the pcr estimator let denote the vector of estimated regression coefficients obtained by ordinary least squares regression of the response vector on the data matrix then for any the final pcr estimator of based on using the first principal components is given by fundamental characteristics and applications of the pcr estimator two basic properties the fitting process for obtaining the pcr estimator involves regressing the response vector on the derived data matrix which has orthogonal columns for any since the principal components are mutually orthogonal to each other 
thus in the regression step performing multiple linear regression jointly on the selected principal components as covariates is equivalent to carrying out independent simple linear regressions or univariate regressions separately on each of the selected principal components as covariate 
when all the principal components are selected for regression so that then the pcr estimator is equivalent to the ordinary least squares estimator 
thus this is easily seen from the fact that and also observing that is an orthogonal matrix 
variance reduction for any the variance of is given by var diag in particular var var hence for all we have var var thus for all we have var var where indicates that square symmetric matrix is non negative definite 
consequently any given linear form of the pcr estimator has lower variance compared to that of the same linear form of the ordinary least squares estimator 
addressing multicollinearity under multicollinearity two or more of the covariates are highly correlated so that one can be linearly predicted from the others with non trivial degree of accuracy 
consequently the columns of the data matrix that correspond to the observations for these covariates tend to become linearly dependent and therefore tends to become rank deficient losing its full column rank structure 
more quantitatively one or more of the smaller eigenvalues of get very close or become exactly equal to under such situations 
the variance expressions above indicate that these small eigenvalues have the maximum inflation effect on the variance of the least squares estimator thereby destabilizing the estimator significantly when they are close to this issue can be effectively addressed through using pcr estimator obtained by excluding the principal components corresponding to these small eigenvalues 
dimension reduction pcr may also be used for performing dimension reduction 
to see this let denote any matrix having orthonormal columns for any 
suppose now that we want to approximate each of the covariate observations through the rank linear transformation for some 
then it can be shown that is minimized at the matrix with the first principal component directions as columns and the corresponding dimensional derived covariates 
thus the dimensional principal components provide the best linear approximation of rank to the observed data matrix the corresponding reconstruction error is given by thus any potential dimension reduction may be achieved by choosing the number of principal components to be used through appropriate thresholding on the cumulative sum of the eigenvalues of since the smaller eigenvalues do not contribute significantly to the cumulative sum the corresponding principal components may be continued to be dropped as long as the desired threshold limit is not exceeded 
the same criteria may also be used for addressing the multicollinearity issue whereby the principal components corresponding to the smaller eigenvalues may be ignored as long as the threshold limit is maintained 
regularization effect since the pcr estimator typically uses only subset of all the principal components for regression it can be viewed as some sort of regularized procedure 
more specifically for any the pcr estimator denotes the regularized solution to the following constrained minimization problem min subject to 
the constraint may be equivalently written as where 
thus when only proper subset of all the principal components are selected for regression the pcr estimator so obtained is based on hard form of regularization that constrains the resulting solution to the column space of the selected principal component directions and consequently restricts it to be orthogonal to the excluded directions 
optimality of pcr among class of regularized estimators given the constrained minimization problem as defined above consider the following generalized version of it min subject to where denotes any full column rank matrix of order with let denote the corresponding solution 
thus arg min subject to then the optimal choice of the restriction matrix for which the corresponding estimator achieves the minimum prediction error is given by where diag 
quite clearly the resulting optimal estimator is then simply given by the pcr estimator based on the first principal components 
efficiency since the ordinary least squares estimator is unbiased for we have var mse where mse denotes the mean squared error 
now if for some we additionally have then the corresponding is also unbiased for and therefore var mse 
we have already seen that var var which then implies mse mse for that particular thus in that case the corresponding would be more efficient estimator of compared to based on using the mean squared error as the performance criteria 
in addition any given linear form of the corresponding would also have lower mean squared error compared to that of the same linear form of now suppose that for given then the corresponding is biased for however since var var it is still possible that mse mse especially if is such that the excluded principal components correspond to the smaller eigenvalues thereby resulting in lower bias 
in order to ensure efficient estimation and prediction performance of pcr as an estimator of park proposes the following guideline for selecting the principal components to be used for regression drop the principal component if and only if practical implementation of this guideline of course requires estimates for the unknown model parameters and in general they may be estimated using the unrestricted least squares estimates obtained from the original full model 
park however provides slightly modified set of estimates that may be better suited for this purpose unlike the criteria based on the cumulative sum of the eigenvalues of which is probably more suited for addressing the multicollinearity problem and for performing dimension reduction the above criteria actually attempts to improve the prediction and estimation efficiency of the pcr estimator by involving both the outcome as well as the covariates in the process of selecting the principal components to be used in the regression step 
alternative approaches with similar goals include selection of the principal components based on cross validation or the mallow cp criteria 
often the principal components are also selected based on their degree of association with the outcome 
shrinkage effect of pcr in general pcr is essentially shrinkage estimator that usually retains the high variance principal components corresponding to the higher eigenvalues of as covariates in the model and discards the remaining low variance components corresponding to the lower eigenvalues of 
thus it exerts discrete shrinkage effect on the low variance components nullifying their contribution completely in the original model 
in contrast the ridge regression estimator exerts smooth shrinkage effect through the regularization parameter or the tuning parameter inherently involved in its construction 
while it does not completely discard any of the components it exerts shrinkage effect over all of them in continuous manner so that the extent of shrinkage is higher for the low variance components and lower for the high variance components 
frank and friedman conclude that for the purpose of prediction itself the ridge estimator owing to its smooth shrinkage effect is perhaps better choice compared to the pcr estimator having discrete shrinkage effect 
in addition the principal components are obtained from the eigen decomposition of that involves the observations for the explanatory variables only 
therefore the resulting pcr estimator obtained from using these principal components as covariates need not necessarily have satisfactory predictive performance for the outcome 
somewhat similar estimator that tries to address this issue through its very construction is the partial least squares pls estimator 
similar to pcr pls also uses derived covariates of lower dimensions 
however unlike pcr the derived covariates for pls are obtained based on using both the outcome as well as the covariates 
while pcr seeks the high variance directions in the space of the covariates pls seeks the directions in the covariate space that are most useful for the prediction of the outcome 
variant of the classical pcr known as the supervised pcr was proposed 
in spirit similar to that of pls it attempts at obtaining derived covariates of lower dimensions based on criterion that involves both the outcome as well as the covariates 
the method starts by performing set of simple linear regressions or univariate regressions wherein the outcome vector is regressed separately on each of the covariates taken one at time 
then for some the first covariates that turn out to be the most correlated with the outcome based on the degree of significance of the corresponding estimated regression coefficients are selected for further use 
conventional pcr as described earlier is then performed but now it is based on only the data matrix corresponding to the observations for the selected covariates 
the number of covariates used and the subsequent number of principal components used are usually selected by cross validation 
generalization to kernel settings the classical pcr method as described above is based on classical pca and considers linear regression model for predicting the outcome based on the covariates 
however it can be easily generalized to kernel machine setting whereby the regression function need not necessarily be linear in the covariates but instead it can belong to the reproducing kernel hilbert space associated with any arbitrary possibly non linear symmetric positive definite kernel 
the linear regression model turns out to be special case of this setting when the kernel function is chosen to be the linear kernel 
in general under the kernel machine setting the vector of covariates is first mapped into high dimensional potentially infinite dimensional feature space characterized by the kernel function chosen 
the mapping so obtained is known as the feature map and each of its coordinates also known as the feature elements corresponds to one feature may be linear or non linear of the covariates 
the regression function is then assumed to be linear combination of these feature elements 
thus the underlying regression model in the kernel machine setting is essentially linear regression model with the understanding that instead of the original set of covariates the predictors are now given by the vector potentially infinite dimensional of feature elements obtained by transforming the actual covariates using the feature map 
however the kernel trick actually enables us to operate in the feature space without ever explicitly computing the feature map 
it turns out that it is only sufficient to compute the pairwise inner products among the feature maps for the observed covariate vectors and these inner products are simply given by the values of the kernel function evaluated at the corresponding pairs of covariate vectors 
the pairwise inner products so obtained may therefore be represented in the form of symmetric non negative definite matrix also known as the kernel matrix 
pcr in the kernel machine setting can now be implemented by first appropriately centering this kernel matrix say with respect to the feature space and then performing kernel pca on the centered kernel matrix say whereby an eigendecomposition of is obtained 
kernel pcr then proceeds by usually selecting subset of all the eigenvectors so obtained and then performing standard linear regression of the outcome vector on these selected eigenvectors 
the eigenvectors to be used for regression are usually selected using cross validation 
the estimated regression coefficients having the same dimension as the number of selected eigenvectors along with the corresponding selected eigenvectors are then used for predicting the outcome for future observation 
in machine learning this technique is also known as spectral regression 
clearly kernel pcr has discrete shrinkage effect on the eigenvectors of quite similar to the discrete shrinkage effect of classical pcr on the principal components as discussed earlier 
however the feature map associated with the chosen kernel could potentially be infinite dimensional and hence the corresponding principal components and principal component directions could be infinite dimensional as well 
therefore these quantities are often practically intractable under the kernel machine setting 
kernel pcr essentially works around this problem by considering an equivalent dual formulation based on using the spectral decomposition of the associated kernel matrix 
under the linear regression model which corresponds to choosing the kernel function as the linear kernel this amounts to considering spectral decomposition of the corresponding kernel matrix and then regressing the outcome vector on selected subset of the eigenvectors of so obtained 
it can be easily shown that this is the same as regressing the outcome vector on the corresponding principal components which are finite dimensional in this case as defined in the context of the classical pcr 
thus for the linear kernel the kernel pcr based on dual formulation is exactly equivalent to the classical pcr based on primal formulation 
however for arbitrary and possibly non linear kernels this primal formulation may become intractable owing to the infinite dimensionality of the associated feature map 
thus classical pcr becomes practically infeasible in that case but kernel pcr based on the dual formulation still remains valid and computationally scalable 
see also principal component analysis partial least squares regression ridge regression canonical correlation deming regression total sum of squares references further reading amemiya takeshi
independence is fundamental notion in probability theory as in statistics and the theory of stochastic processes 
two events are independent statistically independent or stochastically independent if informally speaking the occurrence of one does not affect the probability of occurrence of the other or equivalently does not affect the odds 
similarly two random variables are independent if the realization of one does not affect the probability distribution of the other 
when dealing with collections of more than two events two notions of independence need to be distinguished 
the events are called pairwise independent if any two events in the collection are independent of each other while mutual independence or collective independence of events means informally speaking that each event is independent of any combination of other events in the collection 
similar notion exists for collections of random variables 
mutual independence implies pairwise independence but not the other way around 
in the standard literature of probability theory statistics and stochastic processes independence without further qualification usually refers to mutual independence 
definition for events two events two events and are independent often written as or where the latter symbol often is also used for conditional independence if and only if their joint probability equals the product of their probabilities indicates that two independent events and have common elements in their sample space so that they are not mutually exclusive mutually exclusive iff 
why this defines independence is made clear by rewriting with conditional probabilities as the probability at which the event occurs provided that the event has or is assumed to have occurred 
thus the occurrence of does not affect the probability of and vice versa 
in other words and are independent to each other 
although the derived expressions may seem more intuitive they are not the preferred definition as the conditional probabilities may be undefined if or are furthermore the preferred definition makes clear by symmetry that when is independent of is also independent of 
log probability and information content stated in terms of log probability two events are independent if and only if the log probability of the joint event is the sum of the log probability of the individual events log log log in information theory negative log probability is interpreted as information content and thus two events are independent if and only if the information content of the combined event equals the sum of information content of the individual events see information content additivity of independent events for details 
odds stated in terms of odds two events are independent if and only if the odds ratio of and is unity 
analogously with probability this is equivalent to the conditional odds being equal to the unconditional odds and or to the odds of one event given the other event being the same as the odds of the event given the other event not occurring and 
the odds ratio can be defined as or symmetrically for odds of given and thus is if and only if the events are independent 
more than two events finite set of events is pairwise independent if every pair of events is independent that is if and only if for all distinct pairs of indices finite set of events is mutually independent if every event is independent of any intersection of the other events that is if and only if for every and for every indices this is called the multiplication rule for independent events 
note that it is not single condition involving only the product of all the probabilities of all single events it must hold true for all subsets of events 
for more than two events mutually independent set of events is by definition pairwise independent but the converse is not necessarily true 
for real valued random variables two random variables two random variables and are independent if and only if iff the elements of the system generated by them are independent that is to say for every and the events and are independent events as defined above in eq 
that is and with cumulative distribution functions and are independent iff the combined random variable has joint cumulative distribution function or equivalently if the probability densities and and the joint probability density exist for all 
more than two random variables finite set of random variables is pairwise independent if and only if every pair of random variables is independent 
even if the set of random variables is pairwise independent it is not necessarily mutually independent as defined next 
finite set of random variables is mutually independent if and only if for any sequence of numbers the events are mutually independent events as defined above in eq 
this is equivalent to the following condition on the joint cumulative distribution function 
finite set of random variables is mutually independent if and only if notice that it is not necessary here to require that the probability distribution factorizes for all possible element subsets as in the case for events 
this is not required because 
the measure theoretically inclined may prefer to substitute events for events in the above definition where is any borel set 
that definition is exactly equivalent to the one above when the values of the random variables are real numbers 
it has the advantage of working also for complex valued random variables or for random variables taking values in any measurable space which includes topological spaces endowed by appropriate algebras 
for real valued random vectors two random vectors and are called independent if where and denote the cumulative distribution functions of and and denotes their joint cumulative distribution function 
independence of and is often denoted by written component wise and are called independent if for all for stochastic processes for one stochastic process the definition of independence may be extended from random vectors to stochastic process 
therefore it is required for an independent stochastic process that the random variables obtained by sampling the process at any times are independent random variables for any formally stochastic process is called independent if and only if for all and for all where 
independence of stochastic process is property within stochastic process not between two stochastic processes 
for two stochastic processes independence of two stochastic processes is property between two stochastic processes and that are defined on the same probability space 
formally two stochastic processes and are said to be independent if for all and for all the random vectors and are independent 
if independent algebras the definitions above eq and eq are both generalized by the following definition of independence for algebras 
let be probability space and let and be two sub algebras of and are said to be independent if whenever and 
likewise finite family of algebras where is an index set is said to be independent if and only if and an infinite family of algebras is said to be independent if all its finite subfamilies are independent 
the new definition relates to the previous ones very directly two events are independent in the old sense if and only if the algebras that they generate are independent in the new sense 
the algebra generated by an event is by definition 
two random variables and defined over are independent in the old sense if and only if the algebras that they generate are independent in the new sense 
the algebra generated by random variable taking values in some measurable space consists by definition of all subsets of of the form where is any measurable subset of using this definition it is easy to show that if and are random variables and is constant then and are independent since the algebra generated by constant random variable is the trivial algebra 
probability zero events cannot affect independence so independence also holds if is only pr almost surely constant 
properties self independence note that an event is independent of itself if and only if or thus an event is independent of itself if and only if it almost surely occurs or its complement almost surely occurs this fact is useful when proving zero one laws 
expectation and covariance if and are independent random variables then the expectation operator has the property and the covariance cov is zero as follows from cov 
the converse does not hold if two random variables have covariance of they still may be not independent 
similarly for two stochastic processes and if they are independent then they are uncorrelated 
characteristic function two random variables and are independent if and only if the characteristic function of the random vector satisfies 
in particular the characteristic function of their sum is the product of their marginal characteristic functions though the reverse implication is not true 
random variables that satisfy the latter condition are called subindependent 
examples rolling dice the event of getting the first time die is rolled and the event of getting the second time are independent 
by contrast the event of getting the first time die is rolled and the event that the sum of the numbers seen on the first and second trial is are not independent 
drawing cards if two cards are drawn with replacement from deck of cards the event of drawing red card on the first trial and that of drawing red card on the second trial are independent 
by contrast if two cards are drawn without replacement from deck of cards the event of drawing red card on the first trial and that of drawing red card on the second trial are not independent because deck that has had red card removed has proportionately fewer red cards 
pairwise and mutual independence consider the two probability spaces shown 
in both cases and the random variables in the first space are pairwise independent because and but the three random variables are not mutually independent 
the random variables in the second space are both pairwise independent and mutually independent 
to illustrate the difference consider conditioning on two events 
in the pairwise independent case although any one event is independent of each of the other two individually it is not independent of the intersection of the other two in the mutually independent case however triple independence but no pairwise independence it is possible to create three event example in which and yet no two of the three events are pairwise independent and hence the set of events are not mutually independent 
this example shows that mutual independence involves requirements on the products of probabilities of all combinations of events not just the single events as in this example 
conditional independence for events the events and are conditionally independent given an event when 
for random variables intuitively two random variables and are conditionally independent given if once is known the value of does not add any additional information about for instance two measurements and of the same underlying quantity are not independent but they are conditionally independent given unless the errors in the two measurements are somehow connected 
the formal definition of conditional independence is based on the idea of conditional distributions 
if and are discrete random variables then we define and to be conditionally independent given if for all and such that on the other hand if the random variables are continuous and have joint probability density function then and are conditionally independent given if for all real numbers and such that if discrete and are conditionally independent given then for any and with that is the conditional distribution for given and is the same as that given alone 
similar equation holds for the conditional probability density functions in the continuous case 
independence can be seen as special kind of conditional independence since probability can be seen as kind of conditional probability given no events 
see also copula statistics independent and identically distributed random variables mutually exclusive events pairwise independent events subindependence conditional independence normally distributed and uncorrelated does not imply independent mean dependence references external links media related to independence probability theory at wikimedia commons
euclidean space is the fundamental space of geometry intended to represent physical space 
originally that is in euclid elements it was the three dimensional space of euclidean geometry but in modern mathematics there are euclidean spaces of any positive integer dimension including the three dimensional space and the euclidean plane dimension two 
the qualifier euclidean is used to distinguish euclidean spaces from other spaces that were later considered in physics and modern mathematics 
ancient greek geometers introduced euclidean space for modeling the physical space 
their work was collected by the ancient greek mathematician euclid in his elements with the great innovation of proving all properties of the space as theorems by starting from few fundamental properties called postulates which either were considered as evident for example there is exactly one straight line passing through two points or seemed impossible to prove parallel postulate 
after the introduction at the end of th century of non euclidean geometries the old postulates were re formalized to define euclidean spaces through axiomatic theory 
another definition of euclidean spaces by means of vector spaces and linear algebra has been shown to be equivalent to the axiomatic definition 
it is this definition that is more commonly used in modern mathematics and detailed in this article 
in all definitions euclidean spaces consist of points which are defined only by the properties that they must have for forming euclidean space 
there is essentially only one euclidean space of each dimension that is all euclidean spaces of given dimension are isomorphic 
therefore in many cases it is possible to work with specific euclidean space which is generally the real space equipped with the dot product 
an isomorphism from euclidean space to associates with each point an tuple of real numbers which locate that point in the euclidean space and are called the cartesian coordinates of that point 
definition history of the definition euclidean space was introduced by ancient greeks as an abstraction of our physical space 
their great innovation appearing in euclid elements was to build and prove all geometry by starting from few very basic properties which are abstracted from the physical world and cannot be mathematically proved because of the lack of more basic tools 
these properties are called postulates or axioms in modern language 
this way of defining euclidean space is still in use under the name of synthetic geometry 
in ren descartes introduced cartesian coordinates and showed that this allows reducing geometric problems to algebraic computations with numbers 
this reduction of geometry to algebra was major change in point of view as until then the real numbers were defined in terms of lengths and distances 
euclidean geometry was not applied in spaces of dimension more than three until the th century 
ludwig schl fli generalized euclidean geometry to spaces of dimension using both synthetic and algebraic methods and discovered all of the regular polytopes higher dimensional analogues of the platonic solids that exist in euclidean spaces of any dimension despite the wide use of descartes approach which was called analytic geometry the definition of euclidean space remained unchanged until the end of th century 
the introduction of abstract vector spaces allowed their use in defining euclidean spaces with purely algebraic definition 
this new definition has been shown to be equivalent to the classical definition in terms of geometric axioms 
it is this algebraic definition that is now most often used for introducing euclidean spaces 
motivation of the modern definition one way to think of the euclidean plane is as set of points satisfying certain relationships expressible in terms of distance and angles 
for example there are two fundamental operations referred to as motions on the plane 
one is translation which means shifting of the plane so that every point is shifted in the same direction and by the same distance 
the other is rotation around fixed point in the plane in which all points in the plane turn around that fixed point through the same angle 
one of the basic tenets of euclidean geometry is that two figures usually considered as subsets of the plane should be considered equivalent congruent if one can be transformed into the other by some sequence of translations rotations and reflections see below 
in order to make all of this mathematically precise the theory must clearly define what is euclidean space and the related notions of distance angle translation and rotation 
even when used in physical theories euclidean space is an abstraction detached from actual physical locations specific reference frames measurement instruments and so on 
purely mathematical definition of euclidean space also ignores questions of units of length and other physical dimensions the distance in mathematical space is number not something expressed in inches or metres 
the standard way to mathematically define euclidean space as carried out in the remainder of this article is as set of points on which real vector space acts the space of translations which is equipped with an inner product 
the action of translations makes the space an affine space and this allows defining lines planes subspaces dimension and parallelism 
the inner product allows defining distance and angles 
the set of tuples of real numbers equipped with the dot product is euclidean space of dimension conversely the choice of point called the origin and an orthonormal basis of the space of translations is equivalent with defining an isomorphism between euclidean space of dimension and viewed as euclidean space 
it follows that everything that can be said about euclidean space can also be said about therefore many authors especially at elementary level call the standard euclidean space of dimension or simply the euclidean space of dimension reason for introducing such an abstract definition of euclidean spaces and for working with it instead of is that it is often preferable to work in coordinate free and origin free manner that is without choosing preferred basis and preferred origin 
another reason is that there is no origin nor any basis in the physical world 
technical definition euclidean vector space is finite dimensional inner product space over the real numbers 
euclidean space is an affine space over the reals such that the associated vector space is euclidean vector space 
euclidean spaces are sometimes called euclidean affine spaces for distinguishing them from euclidean vector spaces if is euclidean space its associated vector space euclidean vector space is often denoted 
the dimension of euclidean space is the dimension of its associated vector space 
the elements of are called points and are commonly denoted by capital letters 
the elements of are called euclidean vectors or free vectors 
they are also called translations although properly speaking translation is the geometric transformation resulting of the action of euclidean vector on the euclidean space 
the action of translation on point provides point that is denoted this action satisfies note the second in the left hand side is vector addition all other denote an action of vector on point 
this notation is not ambiguous as for distinguishing between the two meanings of it suffices to look on the nature of its left argument 
the fact that the action is free and transitive means that for every pair of points there is exactly one displacement vector such that this vector is denoted or 
as previously explained some of the basic properties of euclidean spaces result of the structure of affine space 
they are described in affine structure and its subsections 
the properties resulting from the inner product are explained in metric structure and its subsections 
prototypical examples for any vector space the addition acts freely and transitively on the vector space itself 
thus euclidean vector space can be viewed as euclidean space that has itself as the associated vector space 
typical case of euclidean vector space is viewed as vector space equipped with the dot product as an inner product 
the importance of this particular example of euclidean space lies in the fact that every euclidean space is isomorphic to it 
more precisely given euclidean space of dimension the choice of point called an origin and an orthonormal basis of defines an isomorphism of euclidean spaces from to as every euclidean space of dimension is isomorphic to it the euclidean space is sometimes called the standard euclidean space of dimension affine structure some basic properties of euclidean spaces depend only of the fact that euclidean space is an affine space 
they are called affine properties and include the concepts of lines subspaces and parallelism which are detailed in next subsections 
subspaces let be euclidean space and its associated vector space 
flat euclidean subspace or affine subspace of is subset of such that as the associated vector space of is linear subspace vector subspace of 
euclidean subspace is euclidean space with as the associated vector space 
this linear subspace is also called the direction of if is point of then conversely if is point of and is linear subspace of then is euclidean subspace of direction 
the associated vector space of this subspace is 
euclidean vector space that is euclidean space that is equal to has two sorts of subspaces its euclidean subspaces and its linear subspaces 
linear subspaces are euclidean subspaces and euclidean subspace is linear subspace if and only if it contains the zero vector 
lines and segments in euclidean space line is euclidean subspace of dimension one 
since vector space of dimension one is spanned by any nonzero vector line is set of the form where and are two distinct points of the euclidean space as part of the line 
it follows that there is exactly one line that passes through contains two distinct points 
this implies that two distinct lines intersect in at most one point 
more symmetric representation of the line passing through and is where is an arbitrary point not necessary on the line 
in euclidean vector space the zero vector is usually chosen for this allows simplifying the preceding formula into standard convention allows using this formula in every euclidean space see affine space affine combinations and barycenter 
the line segment or simply segment joining the points and is the subset of points such that in the preceding formulas 
it is denoted pq or qp that is parallelism two subspaces and of the same dimension in euclidean space are parallel if they have the same direction the same associated vector space 
equivalently they are parallel if there is translation vector that maps one to the other given point and subspace there exists exactly one subspace that contains and is parallel to which is 
in the case where is line subspace of dimension one this property is playfair axiom 
it follows that in euclidean plane two lines either meet in one point or are parallel 
the concept of parallel subspaces has been extended to subspaces of different dimensions two subspaces are parallel if the direction of one of them is contained in the direction to the other 
metric structure the vector space associated to euclidean space is an inner product space 
this implies symmetric bilinear form that is positive definite that is is always positive for 
the inner product of euclidean space is often called dot product and denoted this is specially the case when cartesian coordinate system has been chosen as in this case the inner product of two vectors is the dot product of their coordinate vectors 
for this reason and for historical reasons the dot notation is more commonly used than the bracket notation for the inner product of euclidean spaces 
this article will follow this usage that is will be denoted in the remainder of this article 
the euclidean norm of vector is the inner product and the norm allows expressing and proving metric and topological properties of euclidean geometry 
the next subsection describe the most fundamental ones 
in these subsections denotes an arbitrary euclidean space and denotes its vector space of translations 
distance and length the distance more precisely the euclidean distance between two points of euclidean space is the norm of the translation vector that maps one point to the other that is the length of segment pq is the distance between its endpoints and it is often denoted 
the distance is metric as it is positive definite symmetric and satisfies the triangle inequality moreover the equality is true if and only if point belongs to the segment pq 
this inequality means that the length of any edge of triangle is smaller than the sum of the lengths of the other edges 
this is the origin of the term triangle inequality 
with the euclidean distance every euclidean space is complete metric space 
orthogonality two nonzero vectors and of the associated vector space of euclidean space are perpendicular or orthogonal if their inner product is zero two linear subspaces of are orthogonal if every nonzero vector of the first one is perpendicular to every nonzero vector of the second one 
this implies that the intersection of the linear subspaces is reduced to the zero vector 
two lines and more generally two euclidean subspaces line can be considered as an one euclidean subspace 
are orthogonal if their directions the associated vector spaces of the euclidean subspaces are orthogonal 
two orthogonal lines that intersect are said perpendicular 
two segments ab and ac that share common endpoint are perpendicular or form right angle if the vectors and are orthogonal 
if ab and ac form right angle one has this is the pythagorean theorem 
its proof is easy in this context as expressing this in terms of the inner product one has using bilinearity and symmetry of the inner product here is used since these two vectors are orthogonal 
angle the non oriented angle between two nonzero vectors and in is where arccos is the principal value of the arccosine function 
by cauchy schwarz inequality the argument of the arccosine is in the interval 
therefore is real and or if angles are measured in degrees 
angles are not useful in euclidean line as they can be only or in an oriented euclidean plane one can define the oriented angle of two vectors 
the oriented angle of two vectors and is then the opposite of the oriented angle of and in this case the angle of two vectors can have any value modulo an integer multiple of 
in particular reflex angle equals the negative angle the angle of two vectors does not change if they are multiplied by positive numbers 
more precisely if and are two vectors and and are real numbers then if and are three points in euclidean space the angle of the segments ab and ac is the angle of the vectors and 
as the multiplication of vectors by positive numbers do not change the angle the angle of two half lines with initial point can be defined it is the angle of the segments ab and ac where and are arbitrary points one on each half line 
although this is less used one can define similarly the angle of segments or half lines that do not share an initial point 
the angle of two lines is defined as follows 
if is the angle of two segments one on each line the angle of any two other segments one on each line is either or one of these angles is in the interval and the other being in 
the non oriented angle of the two lines is the one in the interval 
in an oriented euclidean plane the oriented angle of two lines belongs to the interval 
cartesian coordinates every euclidean vector space has an orthonormal basis in fact infinitely many in dimension higher than one and two in dimension one that is basis of unit vectors that are pairwise orthogonal for 
more precisely given any basis the gram schmidt process computes an orthonormal basis such that for every the linear spans of and are equal given euclidean space cartesian frame is set of data consisting of an orthonormal basis of and point of called the origin and often denoted cartesian frame allows defining cartesian coordinates for both and in the following way 
the cartesian coordinates of vector of are the coefficients of on the orthonormal basis for example the cartesian coordinates of vector on an orthonormal basis that may be named as as convention in dimensional euclidean space is if as the basis is orthonormal the th coefficient is equal to the dot product the cartesian coordinates of point of are the cartesian coordinates of the vector 
other coordinates as euclidean space is an affine space one can consider an affine frame on it which is the same as euclidean frame except that the basis is not required to be orthonormal 
this define affine coordinates sometimes called skew coordinates for emphasizing that the basis vectors are not pairwise orthogonal 
an affine basis of euclidean space of dimension is set of points that are not contained in hyperplane 
an affine basis define barycentric coordinates for every point 
many other coordinates systems can be defined on euclidean space of dimension in the following way 
let be homeomorphism or more often diffeomorphism from dense open subset of to an open subset of the coordinates of point of are the components of 
the polar coordinate system dimension and the spherical and cylindrical coordinate systems dimension are defined this way 
for points that are outside the domain of coordinates may sometimes be defined as the limit of coordinates of neighbour points but these coordinates may be not uniquely defined and may be not continuous in the neighborhood of the point 
for example for the spherical coordinate system the longitude is not defined at the pole and on the antimeridian the longitude passes discontinuously from to 
this way of defining coordinates extends easily to other mathematical structures and in particular to manifolds 
isometries an isometry between two metric spaces is bijection preserving the distance that is in the case of euclidean vector space an isometry that maps the origin to the origin preserves the norm since the norm of vector is its distance from the zero vector 
it preserves also the inner product since an isometry of euclidean vector spaces is linear isomorphism an isometry of euclidean spaces defines an isometry of the associated euclidean vector spaces 
this implies that two isometric euclidean spaces have the same dimension 
conversely if and are euclidean spaces and is an isometry then the map defined by is an isometry of euclidean spaces 
it follows from the preceding results that an isometry of euclidean spaces maps lines to lines and more generally euclidean subspaces to euclidean subspaces of the same dimension and that the restriction of the isometry on these subspaces are isometries of these subspaces 
isometry with prototypical examples if is euclidean space its associated vector space can be considered as euclidean space 
every point defines an isometry of euclidean spaces which maps to the zero vector and has the identity as associated linear map 
the inverse isometry is the map euclidean frame allows defining the map which is an isometry of euclidean spaces 
the inverse isometry is this means that up to an isomorphism there is exactly one euclidean space of given dimension 
this justifies that many authors talk of as the euclidean space of dimension euclidean group an isometry from euclidean space onto itself is called euclidean isometry euclidean transformation or rigid transformation 
the rigid transformations of euclidean space form group under composition called the euclidean group and often denoted of iso 
the simplest euclidean transformations are translations they are in bijective correspondence with vectors 
this is reason for calling space of translations the vector space associated to euclidean space 
the translations form normal subgroup of the euclidean group 
euclidean isometry of euclidean space defines linear isometry of the associated vector space by linear isometry it is meant an isometry that is also linear map in the following way denoting by the vector if is an arbitrary point of one has it is straightforward to prove that this is linear map that does not depend from the choice of the map is group homomorphism from the euclidean group onto the group of linear isometries called the orthogonal group 
the kernel of this homomorphism is the translation group showing that it is normal subgroup of the euclidean group 
the isometries that fix given point form the stabilizer subgroup of the euclidean group with respect to the restriction to this stabilizer of above group homomorphism is an isomorphism 
so the isometries that fix given point form group isomorphic to the orthogonal group 
let be point an isometry and the translation that maps to 
the isometry fixes so and the euclidean group is the semidirect product of the translation group and the orthogonal group 
the special orthogonal group is the normal subgroup of the orthogonal group that preserves handedness 
it is subgroup of index two of the orthogonal group 
its inverse image by the group homomorphism is normal subgroup of index two of the euclidean group which is called the special euclidean group or the displacement group 
its elements are called rigid motions or displacements 
rigid motions include the identity translations rotations the rigid motions that fix at least point and also screw motions 
typical examples of rigid transformations that are not rigid motions are reflections which are rigid transformations that fix hyperplane and are not the identity 
they are also the transformations consisting in changing the sign of one coordinate over some euclidean frame 
as the special euclidean group is subgroup of index two of the euclidean group given reflection every rigid transformation that is not rigid motion is the product of and rigid motion 
glide reflection is an example of rigid transformation that is not rigid motion or reflection 
all groups that have been considered in this section are lie groups and algebraic groups 
topology the euclidean distance makes euclidean space metric space and thus topological space 
this topology is called the euclidean topology 
in the case of this topology is also the product topology 
the open sets are the subsets that contains an open ball around each of their points 
in other words open balls form base of the topology 
the topological dimension of euclidean space equals its dimension 
this implies that euclidean spaces of different dimensions are not homeomorphic 
moreover the theorem of invariance of domain asserts that subset of euclidean space is open for the subspace topology if and only if it is homeomorphic to an open subset of euclidean space of the same dimension 
euclidean spaces are complete and locally compact 
that is closed subset of euclidean space is compact if it is bounded that is contained in ball 
in particular closed balls are compact 
axiomatic definitions the definition of euclidean spaces that has been described in this article differs fundamentally of euclid one 
in reality euclid did not define formally the space because it was thought as description of the physical world that exists independently of human mind 
the need of formal definition appeared only at the end of th century with the introduction of non euclidean geometries 
two different approaches have been used 
felix klein suggested to define geometries through their symmetries 
the presentation of euclidean spaces given in this article is essentially issued from his erlangen program with the emphasis given on the groups of translations and isometries 
on the other hand david hilbert proposed set of axioms inspired by euclid postulates 
they belong to synthetic geometry as they do not involve any definition of real numbers 
later birkhoff and alfred tarski proposed simpler sets of axioms which use real numbers see birkhoff axioms and tarski axioms 
in geometric algebra emil artin has proved that all these definitions of euclidean space are equivalent 
it is rather easy to prove that all definitions of euclidean spaces satisfy hilbert axioms and that those involving real numbers including the above given definition are equivalent 
the difficult part of artin proof is the following 
in hilbert axioms congruence is an equivalence relation on segments 
one can thus define the length of segment as its equivalence class 
one must thus prove that this length satisfies properties that characterize nonnegative real numbers 
artin proved this with axioms equivalent to those of hilbert 
usage since ancient greeks euclidean space is used for modeling shapes in the physical world 
it is thus used in many sciences such as physics mechanics and astronomy 
it is also widely used in all technical areas that are concerned with shapes figure location and position such as architecture geodesy topography navigation industrial design or technical drawing 
space of dimensions higher than three occurs in several modern theories of physics see higher dimension 
they occur also in configuration spaces of physical systems 
beside euclidean geometry euclidean spaces are also widely used in other areas of mathematics 
tangent spaces of differentiable manifolds are euclidean vector spaces 
more generally manifold is space that is locally approximated by euclidean spaces 
most non euclidean geometries can be modeled by manifold and embedded in euclidean space of higher dimension 
for example an elliptic space can be modeled by an ellipsoid 
it is common to represent in euclidean space mathematical objects that are priori not of geometrical nature 
an example among many is the usual representation of graphs 
other geometric spaces since the introduction at the end of th century of non euclidean geometries many sorts of spaces have been considered about which one can do geometric reasoning in the same way as with euclidean spaces 
in general they share some properties with euclidean spaces but may also have properties that could appear as rather strange 
some of these spaces use euclidean geometry for their definition or can be modeled as subspaces of euclidean space of higher dimension 
when such space is defined by geometrical axioms embedding the space in euclidean space is standard way for proving consistency of its definition or more precisely for proving that its theory is consistent if euclidean geometry is consistent which cannot be proved 
affine space euclidean space is an affine space equipped with metric 
affine spaces have many other uses in mathematics 
in particular as they are defined over any field they allow doing geometry in other contexts 
as soon as non linear questions are considered it is generally useful to consider affine spaces over the complex numbers as an extension of euclidean spaces 
for example circle and line have always two intersection points possibly not distinct in the complex affine space 
therefore most of algebraic geometry is built in complex affine spaces and affine spaces over algebraically closed fields 
the shapes that are studied in algebraic geometry in these affine spaces are therefore called affine algebraic varieties 
affine spaces over the rational numbers and more generally over algebraic number fields provide link between algebraic geometry and number theory 
for example the fermat last theorem can be stated fermat curve of degree higher than two has no point in the affine plane over the rationals 
geometry in affine spaces over finite fields has also been widely studied 
for example elliptic curves over finite fields are widely used in cryptography 
projective space originally projective spaces have been introduced by adding points at infinity to euclidean spaces and more generally to affine spaces in order to make true the assertion two coplanar lines meet in exactly one point 
projective space share with euclidean and affine spaces the property of being isotropic that is there is no property of the space that allows distinguishing between two points or two lines 
therefore more isotropic definition is commonly used which consists as defining projective space as the set of the vector lines in vector space of dimension one more 
as for affine spaces projective spaces are defined over any field and are fundamental spaces of algebraic geometry 
non euclidean geometries non euclidean geometry refers usually to geometrical spaces where the parallel postulate is false 
they include elliptic geometry where the sum of the angles of triangle is more than and hyperbolic geometry where this sum is less than 
their introduction in the second half of th century and the proof that their theory is consistent if euclidean geometry is not contradictory is one of the paradoxes that are at the origin of the foundational crisis in mathematics of the beginning of th century and motivated the systematization of axiomatic theories in mathematics 
curved spaces manifold is space that in the neighborhood of each point resembles euclidean space 
in technical terms manifold is topological space such that each point has neighborhood that is homeomorphic to an open subset of euclidean space 
manifolds can be classified by increasing degree of this resemblance into topological manifolds differentiable manifolds smooth manifolds and analytic manifolds 
however none of these types of resemblance respect distances and angles even approximately 
distances and angles can be defined on smooth manifold by providing smoothly varying euclidean metric on the tangent spaces at the points of the manifold these tangent spaces are thus euclidean vector spaces 
this results in riemannian manifold 
generally straight lines do not exist in riemannian manifold but their role is played by geodesics which are the shortest paths between two points 
this allows defining distances which are measured along geodesics and angles between geodesics which are the angle of their tangents in the tangent space at their intersection 
so riemannian manifolds behave locally like euclidean space that has been bent 
euclidean spaces are trivially riemannian manifolds 
an example illustrating this well is the surface of sphere 
in this case geodesics are arcs of great circle which are called orthodromes in the context of navigation 
more generally the spaces of non euclidean geometries can be realized as riemannian manifolds 
pseudo euclidean space an inner product of real vector space is positive definite bilinear form and so characterized by positive definite quadratic form 
pseudo euclidean space is an affine space with an associated real vector space equipped with non degenerate quadratic form that may be indefinite 
fundamental example of such space is the minkowski space which is the space time of einstein special relativity 
it is four dimensional space where the metric is defined by the quadratic form where the last coordinate is temporal and the other three are spatial 
to take gravity into account general relativity uses pseudo riemannian manifold that has minkowski spaces as tangent spaces 
the curvature of this manifold at point is function of the value of the gravitational field at this point 
see also hilbert space generalization to infinite dimension used in functional analysis footnotes references anton howard elementary linear algebra th ed 
new york wiley isbn artin emil geometric algebra wiley classics library new york john wiley sons inc pp 
doi isbn mr ball rouse 
short account of the history of mathematics th ed 
isbn berger marcel geometry berlin springer isbn coxeter 
regular polytopes rd ed 
schl fli discovered them before time when cayley grassman and bius were the only other people who had ever conceived of the possibility of geometry in more than three dimensions 
euclidean space encyclopedia of mathematics ems press
in statistics ordinary least squares ols is type of linear least squares method for choosing the unknown parameters in linear regression model with fixed level one effects of linear function of set of explanatory variables by the principle of least squares minimizing the sum of the squares of the differences between the observed dependent variable values of the variable being observed in the input dataset and the output of the linear function of the independent variable 
geometrically this is seen as the sum of the squared distances parallel to the axis of the dependent variable between each data point in the set and the corresponding point on the regression surface the smaller the differences the better the model fits the data 
the resulting estimator can be expressed by simple formula especially in the case of simple linear regression in which there is single regressor on the right side of the regression equation 
the ols estimator is consistent for the level one fixed effects when the regressors are exogenous and forms perfect colinearity rank condition consistent for the variance estimate of the residuals when regressors have finite fourth moments and by the gauss markov theorem optimal in the class of linear unbiased estimators when the errors are homoscedastic and serially uncorrelated 
under these conditions the method of ols provides minimum variance mean unbiased estimation when the errors have finite variances 
under the additional assumption that the errors are normally distributed with zero mean ols is the maximum likelihood estimator that outperforms any non linear unbiased estimator 
linear model suppose the data consists of observations each observation includes scalar response and column vector of parameters regressors in linear regression model the response variable is linear function of the regressors or in vector form where as introduced previously is column vector of the th observation of all the explanatory variables is vector of unknown parameters and the scalar represents unobserved random variables errors of the th observation 
accounts for the influences upon the responses from sources other than the explanators this model can also be written in matrix notation as where and are vectors of the response variables and the errors of the observations and is an matrix of regressors also sometimes called the design matrix whose row is and contains the th observations on all the explanatory variables 
as rule the constant term is always included in the set of regressors say by taking for all the coefficient corresponding to this regressor is called the intercept 
regressors do not have to be independent there can be any desired relationship between the regressors so long as it is not linear relationship 
for instance we might suspect the response depends linearly both on value and its square in which case we would include one regressor whose value is just the square of another regressor 
in that case the model would be quadratic in the second regressor but none the less is still considered linear model because the model is still linear in the parameters 
matrix vector formulation consider an overdetermined system of linear equations in unknown coefficients with note for linear model as above not all elements in contains information on the data points 
the first column is populated with ones only the other columns contain actual data 
so here is equal to the number of regressors plus one 
this can be written in matrix form as where 
such system usually has no exact solution so the goal is instead to find the coefficients which fit the equations best in the sense of solving the quadratic minimization problem where the objective function is given by justification for choosing this criterion is given in properties below 
this minimization problem has unique solution provided that the columns of the matrix are linearly independent given by solving the so called normal equations the matrix is known as the normal matrix or gram matrix and the matrix is known as the moment matrix of regressand by regressors 
finally is the coefficient vector of the least squares hyperplane expressed as or 
estimation suppose is candidate value for the parameter vector the quantity yi xitb called the residual for the th observation measures the vertical distance between the data point xi yi and the hyperplane xtb and thus assesses the degree of fit between the actual data and the model 
the sum of squared residuals ssr also called the error sum of squares ess or residual sum of squares rss is measure of the overall model fit where denotes the matrix transpose and the rows of denoting the values of all the independent variables associated with particular value of the dependent variable are xi xit 
the value of which minimizes this sum is called the ols estimator for the function is quadratic in with positive definite hessian and therefore this function possesses unique global minimum at which can be given by the explicit formula proof argmin the product xt is gram matrix and its inverse is the cofactor matrix of closely related to its covariance matrix 
the matrix xt xt xt is called the moore penrose pseudoinverse matrix of this formulation highlights the point that estimation can be carried out if and only if there is no perfect multicollinearity between the explanatory variables which would cause the gram matrix to have no inverse 
after we have estimated the fitted values or predicted values from the regression will be where xtx xt is the projection matrix onto the space spanned by the columns of this matrix is also sometimes called the hat matrix because it puts hat onto the variable another matrix closely related to is the annihilator matrix in this is projection matrix onto the space orthogonal to both matrices and are symmetric and idempotent meaning that and and relate to the data matrix via identities px and mx matrix creates the residuals from the regression using these residuals we can estimate the value of using the reduced chi squared statistic the denominator is the statistical degrees of freedom 
the first quantity is the ols estimate for whereas the second is the mle estimate for 
the two estimators are quite similar in large samples the first estimator is always unbiased while the second estimator is biased but has smaller mean squared error 
in practice is used more often since it is more convenient for the hypothesis testing 
the square root of is called the regression standard error standard error of the regression or standard error of the equation it is common to assess the goodness of fit of the ols regression by comparing how much the initial variation in the sample can be reduced by regressing onto the coefficient of determination is defined as ratio of explained variance to the total variance of the dependent variable in the cases where the regression sum of squares equals the sum of squares of residuals where tss is the total sum of squares for the dependent variable and is an matrix of ones 
is centering matrix which is equivalent to regression on constant it simply subtracts the mean from variable 
in order for to be meaningful the matrix of data on regressors must contain column vector of ones to represent the constant whose coefficient is the regression intercept 
in that case will always be number between and with values close to indicating good degree of fit 
the variance in the prediction of the independent variable as function of the dependent variable is given in the article polynomial least squares 
simple linear regression model if the data matrix contains only two variables constant and scalar regressor xi then this is called the simple regression model 
this case is often considered in the beginner statistics classes as it provides much simpler formulas even suitable for manual calculation 
the parameters are commonly denoted as the least squares estimates in this case are given by simple formulas alternative derivations in the previous section the least squares estimator was obtained as value that minimizes the sum of squared residuals of the model 
however it is also possible to derive the same estimator from other approaches 
in all cases the formula for ols estimator remains the same xtx xty the only difference is in how we interpret this result 
projection for mathematicians ols is an approximate solution to an overdetermined system of linear equations where is the unknown 
assuming the system cannot be solved exactly the number of equations is much larger than the number of unknowns we are looking for solution that could provide the smallest discrepancy between the right and left hand sides 
in other words we are looking for the solution that satisfies min where is the standard norm in the dimensional euclidean space rn 
the predicted quantity is just certain linear combination of the vectors of regressors 
thus the residual vector will have the smallest length when is projected orthogonally onto the linear subspace spanned by the columns of the ols estimator in this case can be interpreted as the coefficients of vector decomposition of py along the basis of in other words the gradient equations at the minimum can be written as geometrical interpretation of these equations is that the vector of residuals is orthogonal to the column space of since the dot product is equal to zero for any conformal vector this means that is the shortest of all possible vectors that is the variance of the residuals is the minimum possible 
this is illustrated at the right 
introducing and matrix with the assumption that matrix is non singular and kt cf 
orthogonal projections the residual vector should satisfy the following equation 
the equation and solution of linear least squares are thus described as follows another way of looking at it is to consider the regression line to be weighted average of the lines passing through the combination of any two points in the dataset 
although this way of calculation is more computationally expensive it provides better intuition on ols 
maximum likelihood the ols estimator is identical to the maximum likelihood estimator mle under the normality assumption for the error terms 
proof this normality assumption has historical importance as it provided the basis for the early work in linear regression analysis by yule and pearson 
from the properties of mle we can infer that the ols estimator is asymptotically efficient in the sense of attaining the cram rao bound for variance if the normality assumption is satisfied 
generalized method of moments in iid case the ols estimator can also be viewed as gmm estimator arising from the moment conditions these moment conditions state that the regressors should be uncorrelated with the errors 
since xi is vector the number of moment conditions is equal to the dimension of the parameter vector and thus the system is exactly identified 
this is the so called classical gmm case when the estimator does not depend on the choice of the weighting matrix 
note that the original strict exogeneity assumption xi implies far richer set of moment conditions than stated above 
in particular this assumption implies that for any vector function the moment condition xi will hold 
however it can be shown using the gauss markov theorem that the optimal choice of function is to take which results in the moment equation posted above 
properties assumptions there are several different frameworks in which the linear regression model can be cast in order to make the ols technique applicable 
each of these settings produces the same formulas and same results 
the only difference is the interpretation and the assumptions which have to be imposed in order for the method to give meaningful results 
the choice of the applicable framework depends mostly on the nature of data in hand and on the inference task which has to be performed 
one of the lines of difference in interpretation is whether to treat the regressors as random variables or as predefined constants 
in the first case random design the regressors xi are random and sampled together with the yi from some population as in an observational study 
this approach allows for more natural study of the asymptotic properties of the estimators 
in the other interpretation fixed design the regressors are treated as known constants set by design and is sampled conditionally on the values of as in an experiment 
for practical purposes this distinction is often unimportant since estimation and inference is carried out while conditioning on all results stated in this article are within the random design framework 
classical linear regression model the classical model focuses on the finite sample estimation and inference meaning that the number of observations is fixed 
this contrasts with the other approaches which study the asymptotic behavior of ols and in which the number of observations is allowed to grow to infinity 
the linear functional form must coincide with the form of the actual data generating process 
the errors in the regression should have conditional mean zero the immediate consequence of the exogeneity assumption is that the errors have mean zero and that the regressors are uncorrelated with the errors xt the exogeneity assumption is critical for the ols theory 
if it holds then the regressor variables are called exogenous 
if it doesn then those regressors that are correlated with the error term are called endogenous and the ols estimator becomes biased 
in such case the method of instrumental variables may be used to carry out inference no linear dependence 
the regressors in must all be linearly independent 
mathematically this means that the matrix must have full column rank almost surely pr rank usually it is also assumed that the regressors have finite moments up to at least the second moment 
then the matrix qxx xtx is finite and positive semi definite 
when this assumption is violated the regressors are called linearly dependent or perfectly multicollinear 
in such case the value of the regression coefficient cannot be learned although prediction of values is still possible for new values of the regressors that lie in the same linearly dependent subspace spherical errors var where in is the identity matrix in dimension and is parameter which determines the variance of each observation 
this is considered nuisance parameter in the model although usually it is also estimated 
if this assumption is violated then the ols estimates are still valid but no longer efficient 
it is customary to split this assumption into two parts homoscedasticity which means that the error term has the same variance in each observation 
when this requirement is violated this is called heteroscedasticity in such case more efficient estimator would be weighted least squares 
if the errors have infinite variance then the ols estimates will also have infinite variance although by the law of large numbers they will nonetheless tend toward the true values so long as the errors have zero mean 
in this case robust estimation techniques are recommended 
no autocorrelation the errors are uncorrelated between observations for this assumption may be violated in the context of time series data panel data cluster samples hierarchical data repeated measures data longitudinal data and other data with dependencies 
in such cases generalized least squares provides better alternative than the ols 
another expression for autocorrelation is serial correlation normality 
it is sometimes additionally assumed that the errors have normal distribution conditional on the regressors 
this assumption is not needed for the validity of the ols method although certain additional finite sample properties can be established in case when it does especially in the area of hypotheses testing 
also when the errors are normal the ols estimator is equivalent to the maximum likelihood estimator mle and therefore it is asymptotically efficient in the class of all regular estimators 
importantly the normality assumption applies only to the error terms contrary to popular misconception the response dependent variable is not required to be normally distributed 
independent and identically distributed iid in some applications especially with cross sectional data an additional assumption is imposed that all observations are independent and identically distributed 
this means that all observations are taken from random sample which makes all the assumptions listed earlier simpler and easier to interpret 
also this framework allows one to state asymptotic results as the sample size which are understood as theoretical possibility of fetching new independent observations from the data generating process 
the list of assumptions in this case is iid observations xi yi is independent from and has the same distribution as xj yj for all no perfect multicollinearity qxx xi xit is positive definite matrix exogeneity xi homoscedasticity var xi 
time series model the stochastic process is co integrating 
the regressors are predetermined xi for all the matrix qxx xi xit is of full rank and hence positive definite is martingale difference sequence with finite matrix of second moments qxx xi xit 
finite sample properties first of all under the strict exogeneity assumption the ols estimators and are unbiased meaning that their expected values coincide with the true values of the parameters proof if the strict exogeneity does not hold as is the case with many time series models where exogeneity is assumed only with respect to the past shocks but not the future ones then these estimators will be biased in finite samples 
the variance covariance matrix or simply covariance matrix of is equal to var in particular the standard error of each coefficient is equal to square root of the th diagonal element of this matrix 
the estimate of this standard error is obtained by replacing the unknown quantity with its estimate 
thus it can also be easily shown that the estimator is uncorrelated with the residuals from the model cov the gauss markov theorem states that under the spherical errors assumption that is the errors should be uncorrelated and homoscedastic the estimator is efficient in the class of linear unbiased estimators 
this is called the best linear unbiased estimator blue 
efficiency should be understood as if we were to find some other estimator which would be linear in and unbiased then var var in the sense that this is nonnegative definite matrix 
this theorem establishes optimality only in the class of linear unbiased estimators which is quite restrictive 
depending on the distribution of the error terms other non linear estimators may provide better results than ols 
assuming normality the properties listed so far are all valid regardless of the underlying distribution of the error terms 
however if you are willing to assume that the normality assumption holds that is that in then additional properties of the ols estimators can be stated 
the estimator is normally distributed with mean and variance as given before 
this estimator reaches the cram rao bound for the model and thus is optimal in the class of all unbiased estimators 
note that unlike the gauss markov theorem this result establishes optimality among both linear and non linear estimators but only in the case of normally distributed error terms 
the estimator will be proportional to the chi squared distribution the variance of this estimator is equal to which does not attain the cram rao bound of 
however it was shown that there are no unbiased estimators of with variance smaller than that of the estimator 
if we are willing to allow biased estimators and consider the class of estimators that are proportional to the sum of squared residuals ssr of the model then the best in the sense of the mean squared error estimator in this class will be ssr which even beats the cram rao bound in case when there is only one regressor moreover the estimators and are independent the fact which comes in useful when constructing the and tests for the regression 
influential observations as was mentioned before the estimator is linear in meaning that it represents linear combination of the dependent variables yi 
the weights in this linear combination are functions of the regressors and generally are unequal 
the observations with high weights are called influential because they have more pronounced effect on the value of the estimator 
to analyze which observations are influential we remove specific th observation and consider how much the estimated quantities are going to change similarly to the jackknife method 
it can be shown that the change in the ols estimator for will be equal to where hj xjt xtx xj is the th diagonal element of the hat matrix and xj is the vector of regressors corresponding to the th observation 
similarly the change in the predicted value for th observation resulting from omitting that observation from the dataset will be equal to from the properties of the hat matrix hj and they sum up to so that on average hj 
these quantities hj are called the leverages and observations with high hj are called leverage points 
usually the observations with high leverage ought to be scrutinized more carefully in case they are erroneous or outliers or in some other way atypical of the rest of the dataset 
partitioned regression sometimes the variables and corresponding parameters in the regression can be logically split into two groups so that the regression takes form where and have dimensions and are and vectors with the frisch waugh lovell theorem states that in this regression the residuals and the ols estimate will be numerically identical to the residuals and the ols estimate for in the following regression where is the annihilator matrix for regressors 
the theorem can be used to establish number of theoretical results 
for example having regression with constant and another regressor is equivalent to subtracting the means from the dependent variable and the regressor and then running the regression for the de meaned variables but without the constant term 
constrained estimation suppose it is known that the coefficients in the regression satisfy system of linear equations where is matrix of full rank and is vector of known constants where in this case least squares estimation is equivalent to minimizing the sum of squared residuals of the model subject to the constraint the constrained least squares cls estimator can be given by an explicit formula 
this expression for the constrained estimator is valid as long as the matrix xtx is invertible 
it was assumed from the beginning of this article that this matrix is of full rank and it was noted that when the rank condition fails will not be identifiable 
however it may happen that adding the restriction makes identifiable in which case one would like to find the formula for the estimator 
the estimator is equal to where is matrix such that the matrix is non singular and rtq such matrix can always be found although generally it is not unique 
the second formula coincides with the first in case when xtx is invertible 
large sample properties the least squares estimators are point estimates of the linear regression model parameters however generally we also want to know how close those estimates might be to the true values of parameters 
in other words we want to construct the interval estimates 
since we haven made any assumption about the distribution of error term it is impossible to infer the distribution of the estimators and nevertheless we can apply the central limit theorem to derive their asymptotic properties as sample size goes to infinity 
while the sample size is necessarily finite it is customary to assume that is large enough so that the true distribution of the ols estimator is close to its asymptotic limit 
we can show that under the model assumptions the least squares estimator for is consistent that is converges in probability to and asymptotically normal proof where 
intervals using this asymptotic distribution approximate two sided confidence intervals for the th component of the vector can be constructed as at the confidence level where denotes the quantile function of standard normal distribution and jj is the th diagonal element of matrix 
similarly the least squares estimator for is also consistent and asymptotically normal provided that the fourth moment of exists with limiting distribution 
these asymptotic distributions can be used for prediction testing hypotheses constructing other estimators etc as an example consider the problem of prediction 
suppose is some point within the domain of distribution of the regressors and one wants to know what the response variable would have been at that point 
the mean response is the quantity whereas the predicted response is 
clearly the predicted response is random variable its distribution can be derived from that of which allows construct confidence intervals for mean response to be constructed at the confidence level 
hypothesis testing two hypothesis tests are particularly widely used 
first one wants to know if the estimated regression equation is any better than simply predicting that all values of the response variable equal its sample mean if not it is said to have no explanatory power 
the null hypothesis of no explanatory value of the estimated regression is tested using an test 
if the calculated value is found to be large enough to exceed its critical value for the pre chosen level of significance the null hypothesis is rejected and the alternative hypothesis that the regression has explanatory power is accepted 
otherwise the null hypothesis of no explanatory power is accepted 
second for each explanatory variable of interest one wants to know whether its estimated coefficient differs significantly from zero that is whether this particular explanatory variable in fact has explanatory power in predicting the response variable 
here the null hypothesis is that the true coefficient is zero 
this hypothesis is tested by computing the coefficient statistic as the ratio of the coefficient estimate to its standard error 
if the statistic is larger than predetermined value the null hypothesis is rejected and the variable is found to have explanatory power with its coefficient significantly different from zero 
otherwise the null hypothesis of zero value of the true coefficient is accepted 
in addition the chow test is used to test whether two subsamples both have the same underlying true coefficient values 
the sum of squared residuals of regressions on each of the subsets and on the combined data set are compared by computing an statistic if this exceeds critical value the null hypothesis of no difference between the two subsets is rejected otherwise it is accepted 
example with real data the following data set gives average heights and weights for american women aged source the world almanac and book of facts 
when only one dependent variable is being modeled scatterplot will suggest the form and strength of the relationship between the dependent variable and regressors 
it might also reveal outliers heteroscedasticity and other aspects of the data that may complicate the interpretation of fitted regression model 
the scatterplot suggests that the relationship is strong and can be approximated as quadratic function 
ols can handle non linear relationships by introducing the regressor height 
the regression model then becomes multiple linear model the output from most popular statistical packages will look similar to this in this table the value column gives the least squares estimates of parameters the std error column shows standard errors of each coefficient estimate the statistic and value columns are testing whether any of the coefficients might be equal to zero 
the statistic is calculated simply as if the errors follow normal distribution follows student distribution 
under weaker conditions is asymptotically normal 
large values of indicate that the null hypothesis can be rejected and that the corresponding coefficient is not zero 
the second column value expresses the results of the hypothesis test as significance level 
conventionally values smaller than are taken as evidence that the population coefficient is nonzero 
squared is the coefficient of determination indicating goodness of fit of the regression 
this statistic will be equal to one if fit is perfect and to zero when regressors have no explanatory power whatsoever 
this is biased estimate of the population squared and will never decrease if additional regressors are added even if they are irrelevant 
adjusted squared is slightly modified version of designed to penalize for the excess number of regressors which do not add to the explanatory power of the regression 
this statistic is always smaller than can decrease as new regressors are added and even be negative for poorly fitting models log likelihood is calculated under the assumption that errors follow normal distribution 
even though the assumption is not very reasonable this statistic may still find its use in conducting lr tests 
durbin watson statistic tests whether there is any evidence of serial correlation between the residuals 
as rule of thumb the value smaller than will be an evidence of positive correlation 
akaike information criterion and schwarz criterion are both used for model selection 
generally when comparing two alternative models smaller values of one of these criteria will indicate better model 
standard error of regression is an estimate of standard error of the error term 
total sum of squares model sum of squared and residual sum of squares tell us how much of the initial variation in the sample were explained by the regression 
statistic tries to test the hypothesis that all coefficients except the intercept are equal to zero 
this statistic has distribution under the null hypothesis and normality assumption and its value indicates probability that the hypothesis is indeed true 
note that when errors are not normal this statistic becomes invalid and other tests such as wald test or lr test should be used 
ordinary least squares analysis often includes the use of diagnostic plots designed to detect departures of the data from the assumed form of the model 
these are some of the common diagnostic plots residuals against the explanatory variables in the model 
non linear relation between these variables suggests that the linearity of the conditional mean function may not hold 
different levels of variability in the residuals for different levels of the explanatory variables suggests possible heteroscedasticity 
residuals against explanatory variables not in the model 
any relation of the residuals to these variables would suggest considering these variables for inclusion in the model 
residuals against the fitted values 
residuals against the preceding residual 
this plot may identify serial correlations in the residuals an important consideration when carrying out statistical inference using regression models is how the data were sampled 
in this example the data are averages rather than measurements on individual women 
the fit of the model is very good but this does not imply that the weight of an individual woman can be predicted with high accuracy based only on her height 
sensitivity to rounding this example also demonstrates that coefficients determined by these calculations are sensitive to how the data is prepared 
the heights were originally given rounded to the nearest inch and have been converted and rounded to the nearest centimetre 
since the conversion factor is one inch to cm this is not an exact conversion 
the original inches can be recovered by round and then re converted to metric without rounding 
if this is done the results become using either of these equations to predict the weight of woman gives similar values kg with rounding vs kg without rounding 
thus seemingly small variation in the data has real effect on the coefficients but small effect on the results of the equation 
while this may look innocuous in the middle of the data range it could become significant at the extremes or in the case where the fitted model is used to project outside the data range extrapolation 
this highlights common error this example is an abuse of ols which inherently requires that the errors in the independent variable in this case height are zero or at least negligible 
the initial rounding to nearest inch plus any actual measurement errors constitute finite and non negligible error 
as result the fitted parameters are not the best estimates they are presumed to be 
though not totally spurious the error in the estimation will depend upon relative size of the and errors 
another example with less real data problem statement we can use the least square mechanism to figure out the equation of two body orbit in polar base co ordinates 
the equation typically used is cos where is the radius of how far the object is from one of the bodies 
in the equation the parameters and are used to determine the path of the orbit 
we have measured the following data 
we need to find the least squares approximation of and for the given data 
solution first we need to represent and in linear form 
so we are going to rewrite the equation as cos 
now we can use this form to represent our observational data as where is and is and is constructed by the first column being the coefficient of and the second column being the coefficient of and is the values for the respective so and 
on solving we get so and see also bayesian least squares fama macbeth regression nonlinear least squares numerical methods for linear least squares nonlinear system identification references further reading dougherty christopher 
introduction to econometrics nd ed 
new york oxford university press 
isbn gujarati damodar porter dawn 
basic econometics fifth ed 
boston mcgraw hill irwin 
isbn heij christiaan boer paul franses philip kloek teun van dijk herman 
econometric methods with applications in business and economics st ed 
oxford oxford university press 
isbn hill carter griffiths william lim guay 
principles of econometrics rd ed 
hoboken nj john wiley sons 
the simple regression model 
introductory econometrics modern approach th ed 
mason oh cengage learning
in the field of multivariate statistics kernel principal component analysis kernel pca is an extension of principal component analysis pca using techniques of kernel methods 
using kernel the originally linear operations of pca are performed in reproducing kernel hilbert space 
background linear pca recall that conventional pca operates on zero centered data that is where is one of the multivariate observations 
it operates by diagonalizing the covariance matrix in other words it gives an eigendecomposition of the covariance matrix which can be rewritten as for see also covariance matrix as linear operator introduction of the kernel to pca to understand the utility of kernel pca particularly for clustering observe that while points cannot in general be linearly separated in dimensions they can almost always be linearly separated in dimensions 
that is given points if we map them to an dimensional space with where it is easy to construct hyperplane that divides the points into arbitrary clusters 
of course this creates linearly independent vectors so there is no covariance on which to perform eigendecomposition explicitly as we would in linear pca 
instead in kernel pca non trivial arbitrary function is chosen that is never calculated explicitly allowing the possibility to use very high dimensional if we never have to actually evaluate the data in that space 
since we generally try to avoid working in the space which we will call the feature space we can create the by kernel which represents the inner product space see gramian matrix of the otherwise intractable feature space 
the dual form that arises in the creation of kernel allows us to mathematically formulate version of pca in which we never actually solve the eigenvectors and eigenvalues of the covariance matrix in the space see kernel trick 
the elements in each column of represent the dot product of one point of the transformed data with respect to all the transformed points points 
some well known kernels are shown in the example below 
because we are never working directly in the feature space the kernel formulation of pca is restricted in that it computes not the principal components themselves but the projections of our data onto those components 
to evaluate the projection from point in the feature space onto the kth principal component where superscript means the component not powers of we note that denotes dot product which is simply the elements of the kernel it seems all that left is to calculate and normalize the which can be done by solving the eigenvector equation where is the number of data points in the set and and are the eigenvalues and eigenvectors of then to normalize the eigenvectors we require that care must be taken regarding the fact that whether or not has zero mean in its original space it is not guaranteed to be centered in the feature space which we never compute explicitly 
since centered data is required to perform an effective principal component analysis we centralize to become where denotes by matrix for which each element takes value we use to perform the kernel pca algorithm described above 
one caveat of kernel pca should be illustrated here 
in linear pca we can use the eigenvalues to rank the eigenvectors based on how much of the variation of the data is captured by each principal component 
this is useful for data dimensionality reduction and it could also be applied to kpca 
however in practice there are cases that all variations of the data are same 
this is typically caused by wrong choice of kernel scale 
large datasets in practice large data set leads to large and storing may become problem 
one way to deal with this is to perform clustering on the dataset and populate the kernel with the means of those clusters 
since even this method may yield relatively large it is common to compute only the top eigenvalues and eigenvectors of the eigenvalues are calculated in this way 
example consider three concentric clouds of points shown we wish to use kernel pca to identify these groups 
the color of the points does not represent information involved in the algorithm but only shows how the transformation relocates the data points 
first consider the kernel applying this to kernel pca yields the next image 
now consider gaussian kernel that is this kernel is measure of closeness equal to when the points coincide and equal to at infinity 
note in particular that the first principal component is enough to distinguish the three different groups which is impossible using only linear pca because linear pca operates only in the given in this case two dimensional space in which these concentric point clouds are not linearly separable 
applications kernel pca has been demonstrated to be useful for novelty detection and image de noising 
see also cluster analysis nonlinear dimensionality reduction spectral clustering references
componential analysis feature analysis or contrast analysis is the analysis of words through structured sets of semantic features which are given as present absent or indifferent with reference to feature 
the method thus departs from the principle of compositionality 
componential analysis is method typical of structural semantics which analyzes the components of word meaning 
thus it reveals the culturally important features by which speakers of the language distinguish different words in semantic field or domain ottenheimer 
examples man male mature or woman male mature or boy male mature or girl male mature or child male mature 
in other words the word girl can have three basic factors or semantic properties human young and female 
another example being edible is an important factor by which plants may be distinguished from one another ottenheimer 
to summarize one word can have basic underlying meanings that are well established depending on the cultural context 
it is crucial to understand these underlying meanings in order to fully understand any language and culture 
historical background structural semantics and the componential analysis were patterned on the phonological methods of the prague school which described sounds by determining the absence and presence of features 
on one hand componential analysis gave birth to various models in generative semantics lexical field theory and transformational grammar 
on the other hand its shortcoming were also visible the discovery procedures for semantic features are not clearly objectifiable 
only part of the vocabulary can be described through more or less structured sets of features 
metalinguistic features are expressed through language again 
features used may not have clear definitions 
limited in focus and mechanical in style as consequence entirely different ways to describe meaning were developed such as prototype semantics 
see also ethnoscience structural linguistics word sense disambiguation references bussmann hadumod routledge dictionary of language and linguistics london routledge 
the anthropology of language 
belmont ca thomson wadsworth
linear discriminant analysis lda normal discriminant analysis nda or discriminant function analysis is generalization of fisher linear discriminant method used in statistics and other fields to find linear combination of features that characterizes or separates two or more classes of objects or events 
the resulting combination may be used as linear classifier or more commonly for dimensionality reduction before later classification 
lda is closely related to analysis of variance anova and regression analysis which also attempt to express one dependent variable as linear combination of other features or measurements 
however anova uses categorical independent variables and continuous dependent variable whereas discriminant analysis has continuous independent variables and categorical dependent variable 
logistic regression and probit regression are more similar to lda than anova is as they also explain categorical variable by the values of continuous independent variables 
these other methods are preferable in applications where it is not reasonable to assume that the independent variables are normally distributed which is fundamental assumption of the lda method 
lda is also closely related to principal component analysis pca and factor analysis in that they both look for linear combinations of variables which best explain the data 
lda explicitly attempts to model the difference between the classes of data 
pca in contrast does not take into account any difference in class and factor analysis builds the feature combinations based on differences rather than similarities 
discriminant analysis is also different from factor analysis in that it is not an interdependence technique distinction between independent variables and dependent variables also called criterion variables must be made 
lda works when the measurements made on independent variables for each observation are continuous quantities 
when dealing with categorical independent variables the equivalent technique is discriminant correspondence analysis discriminant analysis is used when groups are known priori unlike in cluster analysis 
each case must have score on one or more quantitative predictor measures and score on group measure 
in simple terms discriminant function analysis is classification the act of distributing things into groups classes or categories of the same type 
history the original dichotomous discriminant analysis was developed by sir ronald fisher in it is different from an anova or manova which is used to predict one anova or multiple manova continuous dependent variables by one or more independent categorical variables 
discriminant function analysis is useful in determining whether set of variables is effective in predicting category membership 
lda for two classes consider set of observations also called features attributes variables or measurements for each sample of an object or event with known class this set of samples is called the training set 
the classification problem is then to find good predictor for the class of any sample of the same distribution not necessarily from the training set given only an observation 
lda approaches the problem by assuming that the conditional probability density functions and are both the normal distribution with mean and covariance parameters and respectively 
under this assumption the bayes optimal solution is to predict points as being from the second class if the log of the likelihood ratios is bigger than some threshold so that ln ln without any further assumptions the resulting classifier is referred to as quadratic discriminant analysis qda 
lda instead makes the additional simplifying homoscedasticity assumption 
that the class covariances are identical so and that the covariances have full rank 
in this case several terms cancel because is hermitianand the above decision criterion becomes threshold on the dot product for some threshold constant where this means that the criterion of an input being in class is purely function of this linear combination of the known observations 
it is often useful to see this conclusion in geometrical terms the criterion of an input being in class is purely function of projection of multidimensional space point onto vector thus we only consider its direction 
in other words the observation belongs to if corresponding is located on certain side of hyperplane perpendicular to 
the location of the plane is defined by the threshold assumptions the assumptions of discriminant analysis are the same as those for manova 
the analysis is quite sensitive to outliers and the size of the smallest group must be larger than the number of predictor variables 
multivariate normality independent variables are normal for each level of the grouping variable 
homogeneity of variance covariance homoscedasticity variances among group variables are the same across levels of predictors 
can be tested with box statistic 
it has been suggested however that linear discriminant analysis be used when covariances are equal and that quadratic discriminant analysis may be used when covariances are not equal 
multicollinearity predictive power can decrease with an increased correlation between predictor variables 
independence participants are assumed to be randomly sampled and participant score on one variable is assumed to be independent of scores on that variable for all other participants it has been suggested that discriminant analysis is relatively robust to slight violations of these assumptions and it has also been shown that discriminant analysis may still be reliable when using dichotomous variables where multivariate normality is often violated 
discriminant functions discriminant analysis works by creating one or more linear combinations of predictors creating new latent variable for each function 
these functions are called discriminant functions 
the number of functions possible is either where number of groups or the number of predictors whichever is smaller 
the first function created maximizes the differences between groups on that function 
the second function maximizes differences on that function but also must not be correlated with the previous function 
this continues with subsequent functions with the requirement that the new function not be correlated with any of the previous functions 
given group with sets of sample space there is discriminant rule such that if then discriminant analysis then finds good regions of to minimize classification error therefore leading to high percent correct classified in the classification table each function is given discriminant score to determine how well it predicts group placement 
structure correlation coefficients the correlation between each predictor and the discriminant score of each function 
this is zero order correlation not corrected for the other predictors 
standardized coefficients each predictor weight in the linear combination that is the discriminant function 
like in regression equation these coefficients are partial corrected for the other predictors 
indicates the unique contribution of each predictor in predicting group assignment 
functions at group centroids mean discriminant scores for each grouping variable are given for each function 
the farther apart the means are the less error there will be in classification 
discrimination rules maximum likelihood assigns to the group that maximizes population group density 
bayes discriminant rule assigns to the group that maximizes where represents the prior probability of that classification and represents the population density 
fisher linear discriminant rule maximizes the ratio between ssbetween and sswithin and finds linear combination of the predictors to predict group 
eigenvalues an eigenvalue in discriminant analysis is the characteristic root of each function 
it is an indication of how well that function differentiates the groups where the larger the eigenvalue the better the function differentiates 
this however should be interpreted with caution as eigenvalues have no upper limit 
the eigenvalue can be viewed as ratio of ssbetween and sswithin as in anova when the dependent variable is the discriminant function and the groups are the levels of the iv 
this means that the largest eigenvalue is associated with the first function the second largest with the second etc effect size some suggest the use of eigenvalues as effect size measures however this is generally not supported 
instead the canonical correlation is the preferred measure of effect size 
it is similar to the eigenvalue but is the square root of the ratio of ssbetween and sstotal 
it is the correlation between groups and the function 
another popular measure of effect size is the percent of variance for each function 
this is calculated by where is the eigenvalue for the function and is the sum of all eigenvalues 
this tells us how strong the prediction is for that particular function compared to the others 
percent correctly classified can also be analyzed as an effect size 
the kappa value can describe this while correcting for chance agreement kappa normalizes across all categorizes rather than biased by significantly good or poorly performing classes 
canonical discriminant analysis for classes canonical discriminant analysis cda finds axes canonical coordinates being the number of classes that best separate the categories 
these linear functions are uncorrelated and define in effect an optimal space through the dimensional cloud of data that best separates the projections in that space of the groups 
see multiclass lda for details below 
fisher linear discriminant the terms fisher linear discriminant and lda are often used interchangeably although fisher original article actually describes slightly different discriminant which does not make some of the assumptions of lda such as normally distributed classes or equal class covariances 
suppose two classes of observations have means and covariances then the linear combination of features will have means and variances for fisher defined the separation between these two distributions to be the ratio of the variance between the classes to the variance within the classes between within this measure is in some sense measure of the signal to noise ratio for the class labelling 
it can be shown that the maximum separation occurs when when the assumptions of lda are satisfied the above equation is equivalent to lda 
be sure to note that the vector is the normal to the discriminant hyperplane 
as an example in two dimensional problem the line that best divides the two groups is perpendicular to 
generally the data points to be discriminated are projected onto then the threshold that best separates the data is chosen from analysis of the one dimensional distribution 
there is no general rule for the threshold 
however if projections of points from both classes exhibit approximately the same distributions good choice would be the hyperplane between projections of the two means and in this case the parameter in threshold condition can be found explicitly otsu method is related to fisher linear discriminant and was created to binarize the histogram of pixels in grayscale image by optimally picking the black white threshold that minimizes intra class variance and maximizes inter class variance within between grayscales assigned to black and white pixel classes 
multiclass lda in the case where there are more than two classes the analysis used in the derivation of the fisher discriminant can be extended to find subspace which appears to contain all of the class variability 
this generalization is due to rao 
suppose that each of classes has mean and the same covariance then the scatter between class variability may be defined by the sample covariance of the class means where is the mean of the class means 
the class separation in direction in this case will be given by this means that when is an eigenvector of the separation will be equal to the corresponding eigenvalue 
if is diagonalizable the variability between features will be contained in the subspace spanned by the eigenvectors corresponding to the largest eigenvalues since is of rank at most 
these eigenvectors are primarily used in feature reduction as in pca 
the eigenvectors corresponding to the smaller eigenvalues will tend to be very sensitive to the exact choice of training data and it is often necessary to use regularisation as described in the next section 
if classification is required instead of dimension reduction there are number of alternative techniques available 
for instance the classes may be partitioned and standard fisher discriminant or lda used to classify each partition 
common example of this is one against the rest where the points from one class are put in one group and everything else in the other and then lda applied 
this will result in classifiers whose results are combined 
another common method is pairwise classification where new classifier is created for each pair of classes giving classifiers in total with the individual classifiers combined to produce final classification 
incremental lda the typical implementation of the lda technique requires that all the samples are available in advance 
however there are situations where the entire data set is not available and the input data are observed as stream 
in this case it is desirable for the lda feature extraction to have the ability to update the computed lda features by observing the new samples without running the algorithm on the whole data set 
for example in many real time applications such as mobile robotics or on line face recognition it is important to update the extracted lda features as soon as new observations are available 
an lda feature extraction technique that can update the lda features by simply observing new samples is an incremental lda algorithm and this idea has been extensively studied over the last two decades 
chatterjee and roychowdhury proposed an incremental self organized lda algorithm for updating the lda features 
in other work demir and ozmehmet proposed online local learning algorithms for updating lda features incrementally using error correcting and the hebbian learning rules 
later aliyari et al 
derived fast incremental algorithms to update the lda features by observing the new samples 
practical use in practice the class means and covariances are not known 
they can however be estimated from the training set 
either the maximum likelihood estimate or the maximum posteriori estimate may be used in place of the exact value in the above equations 
although the estimates of the covariance may be considered optimal in some sense this does not mean that the resulting discriminant obtained by substituting these values is optimal in any sense even if the assumption of normally distributed classes is correct 
another complication in applying lda and fisher discriminant to real data occurs when the number of measurements of each sample the dimensionality of each data vector exceeds the number of samples in each class 
in this case the covariance estimates do not have full rank and so cannot be inverted 
there are number of ways to deal with this 
one is to use pseudo inverse instead of the usual matrix inverse in the above formulae 
however better numeric stability may be achieved by first projecting the problem onto the subspace spanned by another strategy to deal with small sample size is to use shrinkage estimator of the covariance matrix which can be expressed mathematically as where is the identity matrix and is the shrinkage intensity or regularisation parameter 
this leads to the framework of regularized discriminant analysis or shrinkage discriminant analysis also in many practical cases linear discriminants are not suitable 
lda and fisher discriminant can be extended for use in non linear classification via the kernel trick 
here the original observations are effectively mapped into higher dimensional non linear space 
linear classification in this non linear space is then equivalent to non linear classification in the original space 
the most commonly used example of this is the kernel fisher discriminant 
lda can be generalized to multiple discriminant analysis where becomes categorical variable with possible states instead of only two 
analogously if the class conditional densities are normal with shared covariances the sufficient statistic for are the values of projections which are the subspace spanned by the means affine projected by the inverse covariance matrix 
these projections can be found by solving generalized eigenvalue problem where the numerator is the covariance matrix formed by treating the means as the samples and the denominator is the shared covariance matrix 
see multiclass lda above for details 
applications in addition to the examples given below lda is applied in positioning and product management 
bankruptcy prediction in bankruptcy prediction based on accounting ratios and other financial variables linear discriminant analysis was the first statistical method applied to systematically explain which firms entered bankruptcy vs survived 
despite limitations including known nonconformance of accounting ratios to the normal distribution assumptions of lda edward altman model is still leading model in practical applications 
face recognition in computerised face recognition each face is represented by large number of pixel values 
linear discriminant analysis is primarily used here to reduce the number of features to more manageable number before classification 
each of the new dimensions is linear combination of pixel values which form template 
the linear combinations obtained using fisher linear discriminant are called fisher faces while those obtained using the related principal component analysis are called eigenfaces 
marketing in marketing discriminant analysis was once often used to determine the factors which distinguish different types of customers and or products on the basis of surveys or other forms of collected data 
logistic regression or other methods are now more commonly used 
the use of discriminant analysis in marketing can be described by the following steps formulate the problem and gather data identify the salient attributes consumers use to evaluate products in this category use quantitative marketing research techniques such as surveys to collect data from sample of potential customers concerning their ratings of all the product attributes 
the data collection stage is usually done by marketing research professionals 
survey questions ask the respondent to rate product from one to five or to or to on range of attributes chosen by the researcher 
anywhere from five to twenty attributes are chosen 
they could include things like ease of use weight accuracy durability colourfulness price or size 
the attributes chosen will vary depending on the product being studied 
the same question is asked about all the products in the study 
the data for multiple products is codified and input into statistical program such as spss or sas 
this step is the same as in factor analysis 
estimate the discriminant function coefficients and determine the statistical significance and validity choose the appropriate discriminant analysis method 
the direct method involves estimating the discriminant function so that all the predictors are assessed simultaneously 
the stepwise method enters the predictors sequentially 
the two group method should be used when the dependent variable has two categories or states 
the multiple discriminant method is used when the dependent variable has three or more categorical states 
use wilks lambda to test for significance in spss or stat in sas 
the most common method used to test validity is to split the sample into an estimation or analysis sample and validation or holdout sample 
the estimation sample is used in constructing the discriminant function 
the validation sample is used to construct classification matrix which contains the number of correctly classified and incorrectly classified cases 
the percentage of correctly classified cases is called the hit ratio 
plot the results on two dimensional map define the dimensions and interpret the results 
the statistical program or related module will map the results 
the map will plot each product usually in two dimensional space 
the distance of products to each other indicate either how different they are 
the dimensions must be labelled by the researcher 
this requires subjective judgement and is often very challenging 
biomedical studies the main application of discriminant analysis in medicine is the assessment of severity state of patient and prognosis of disease outcome 
for example during retrospective analysis patients are divided into groups according to severity of disease mild moderate and severe form 
then results of clinical and laboratory analyses are studied in order to reveal variables which are statistically different in studied groups 
using these variables discriminant functions are built which help to objectively classify disease in future patient into mild moderate or severe form 
in biology similar principles are used in order to classify and define groups of different biological objects for example to define phage types of salmonella enteritidis based on fourier transform infrared spectra to detect animal source of escherichia coli studying its virulence factors etc 
earth science this method can be used to separate the alteration zones 
for example when different data from various zones are available discriminant analysis can find the pattern within the data and classify it effectively 
comparison to logistic regression discriminant function analysis is very similar to logistic regression and both can be used to answer the same research questions 
logistic regression does not have as many assumptions and restrictions as discriminant analysis 
however when discriminant analysis assumptions are met it is more powerful than logistic regression 
unlike logistic regression discriminant analysis can be used with small sample sizes 
it has been shown that when sample sizes are equal and homogeneity of variance covariance holds discriminant analysis is more accurate 
despite all these advantages logistic regression has none the less become the common choice since the assumptions of discriminant analysis are rarely met 
linear discriminant in high dimension geometric anomalies in higher dimensions lead to the well known curse of dimensionality 
nevertheless proper utilization of concentration of measure phenomena can make computation easier 
an important case of these blessing of dimensionality phenomena was highlighted by donoho and tanner if sample is essentially high dimensional then each point can be separated from the rest of the sample by linear inequality with high probability even for exponentially large samples 
these linear inequalities can be selected in the standard fisher form of the linear discriminant for rich family of probability distribution 
in particular such theorems are proven for log concave distributions including multidimensional normal distribution the proof is based on the concentration inequalities for log concave measures and for product measures on multidimensional cube this is proven using talagrand concentration inequality for product probability spaces 
data separability by classical linear discriminants simplifies the problem of error correction for artificial intelligence systems in high dimension 
see also data mining decision tree learning factor analysis kernel fisher discriminant analysis logit for logistic regression linear regression multiple discriminant analysis multidimensional scaling pattern recognition preference regression quadratic classifier statistical classification references further reading duda hart stork 
pattern classification nd ed 
chapman hall crc press 
isbn mika et al 
fisher discriminant analysis with kernels 
neural networks for signal processing ix proceedings of the ieee signal processing society workshop cat 
ieee conference on neural networks for signal processing ix 
isbn cid mcfarland richard donald st richards 
exact misclassification probabilities for plug in normal quadratic discriminant functions 
the equal means case 
journal of multivariate analysis 
mcfarland richard donald st richards 
exact misclassification probabilities for plug in normal quadratic discriminant functions 
journal of multivariate analysis 
haghighat abdel mottaleb alhalabi 
discriminant correlation analysis real time feature level fusion for multimodal biometric recognition 
ieee transactions on information forensics and security 
external links discriminant correlation analysis dca of the haghighat article see above alglib contains open source lda implementation in pascal vba 
lda in python lda implementation in python lda tutorial using ms excel biomedical statistics 
discriminant analysis statquest linear discriminant analysis lda clearly explained on youtube course notes discriminant function analysis by david garson nc state university discriminant analysis tutorial in microsoft excel by kardi teknomo course notes discriminant function analysis by david stockburger missouri state university discriminant function analysis da by john poulsen and aaron french san francisco state university
in probability theory and statistics collection of random variables is independent and identically distributed if each random variable has the same probability distribution as the others and all are mutually independent 
this property is usually abbreviated as iid or iid 
iid was first defined in statistics and finds application in different fields such as data mining and signal processing 
introduction in statistics we commonly deal with random samples 
random sample can be thought of as set of objects that are chosen randomly 
or more formally it sequence of independent identically distributed iid random variables 
in other words the terms random sample and iid are basically one and the same 
in statistics we usually say random sample but in probability it more common to say iid identically distributed means that there are no overall trends the distribution doesn fluctuate and all items in the sample are taken from the same probability distribution 
independent means that the sample items are all independent events 
in other words they aren connected to each other in any way 
in other words knowledge of the value of one variable gives no information about the value of the other and vice versa 
application independent and identically distributed random variables are often used as an assumption which tends to simplify the underlying mathematics 
in practical applications of statistical modeling however the assumption may or may not be realistic the 
assumption is also used in central limit theorem which states that the probability distribution of the sum or average of 
variables with finite variance approaches normal distribution often the 
assumption arises in the context of sequences of random variables 
then independent and identically distributed implies that an element in the sequence is independent of the random variables that came before it 
in this way an 
sequence is different from markov sequence where the probability distribution for the nth random variable is function of the previous random variable in the sequence for first order markov sequence 
sequence does not imply the probabilities for all elements of the sample space or event space must be the same 
for example repeated throws of loaded dice will produce sequence that is despite the outcomes being biased 
definition definition for two random variables suppose that the random variables and are defined to assume values in let and be the cumulative distribution functions of and respectively and denote their joint cumulative distribution function by 
two random variables and are identically distributed if and only if two random variables and are independent if and only if 
see further independence probability theory two random variables 
two random variables and are 
if they are independent and identically distributed 
if and only if definition for more than two random variables the definition extends naturally to more than two random variables 
we say that random variables are 
if they are independent see further independence probability theory more than two random variables and identically distributed 
if and only if where denotes the joint cumulative distribution function of definition for independence in probability theory two events are called independent if and only if and 
in the following ab is short for and 
suppose there are the two events of the experiment if there is possibility 
generally the occurrence of has an effect on the probability of which is called conditional probability and only when the occurrence of has no effect on the occurrence of there is 
note if then are mutually independent which cannot be established with mutually incompatible at the same time that is independence must be compatible and mutual exclusion must be related 
suppose are three events 
if ab bc ac abc are satisfied then the events are independent of each other 
more general definition is there are events an 
if the probabilities of the product events for any events are equal to the product of the probabilities of each event then the events an are independent of each other 
examples example sequence of outcomes of spins of fair or unfair roulette wheel is 
one implication of this is that if the roulette ball lands on red for example times in row the next spin is no more or less likely to be black than on any other spin see the gambler fallacy 
sequence of fair or loaded dice rolls is 
sequence of fair or unfair coin flips is 
in signal processing and image processing the notion of transformation to 
implies two specifications the 
the signal level must be balanced on the time axis 
the signal spectrum must be flattened 
transformed by filtering such as deconvolution to white noise signal 
signal where all frequencies are equally present 
example toss coin times and record how many times does the coin lands on head 
independent each outcome of landing will not affect the other outcome which means the results are independent from each other 
identically distributed if the coin is homogeneous material each time the probability for head is which means the probability is identical for each time 
example roll dice times and record how many time the result is independent each outcome of the dice will not affect the next one which means the results are independent from each other 
identically distributed if the dice is homogeneous material each time the probability for the number is which means the probability is identical for each time 
example choose card from standard deck of cards containing cards then place the card back in the deck 
repeat it for times 
record the number of king appears independent each outcome of the card will not affect the next one which means the results are independent from each other 
identically distributed after drawing one card from it each time the probability for king is which means the probability is identical for each time 
generalizations many results that were first proven under the assumption that the random variables are 
have been shown to be true even under weaker distributional assumption 
exchangeable random variables the most general notion which shares the main properties of 
variables are exchangeable random variables introduced by bruno de finetti 
exchangeability means that while variables may not be independent future ones behave like past ones formally any value of finite sequence is as likely as any permutation of those values the joint probability distribution is invariant under the symmetric group 
this provides useful generalization for example sampling without replacement is not independent but is exchangeable 
vy process in stochastic calculus 
variables are thought of as discrete time vy process each variable gives how much one changes from one time to another 
for example sequence of bernoulli trials is interpreted as the bernoulli process 
one may generalize this to include continuous time vy processes and many vy processes can be seen as limits of 
variables for instance the wiener process is the limit of the bernoulli process 
in machine learning why assume the data in machine learning are independent and identically distributed 
machine learning uses currently acquired massive quantities of data to deliver faster more accurate results 
therefore we need to use historical data with overall representativeness 
if the data obtained is not representative of the overall situation then the rules will be summarized badly or wrongly 
hypothesis the number of individual cases in the training sample can be greatly reduced 
this assumption makes maximization very easy to calculate mathematically 
observing the assumption of independent and identical distribution in mathematics simplifies the calculation of the likelihood function in optimization problems 
because of the assumption of independence the likelihood function can be written like this 
in order to maximize the probability of the observed event take the log function and maximize the parameter that is to say to compute log where log log log log 
log the computer is very efficient to calculate multiple additions but it is not efficient to calculate the multiplication 
this simplification is the core reason for the increase in computational efficiency 
and this log transformation is also in the process of maximizing turning many exponential functions into linear functions 
for two reasons this hypothesis is easy to use the central limit theorem in practical applications 
even if the sample comes from more complex non gaussian distribution it can also approximate well 
because it can be simplified from the central limit theorem to gaussian distribution 
for large number of observable samples the sum of many random variables will have an approximately normal distribution 
the second reason is that the accuracy of the model depends on the simplicity and representative power of the model unit as well as the data quality 
because the simplicity of the unit makes it easy to interpret and scale and the representative power scale out of the unit improves the model accuracy 
like in deep neural network each neuron is very simple but has strong representative power layer by layer to represent more complex features to improve model accuracy 
see also de finetti theorem pairwise independent variables central limit theorem references further reading
in mathematics the linear span also called the linear hull or just span of set of vectors from vector space denoted span is defined as the set of all linear combinations of the vectors in it can be characterized either as the intersection of all linear subspaces that contain or as the smallest subspace containing the linear span of set of vectors is therefore vector space itself 
spans can be generalized to matroids and modules 
to express that vector space is linear span of subset one commonly uses the following phrases either spans is spanning set of is spanned generated by or is generator or generator set of definition given vector space over field the span of set of vectors not necessarily infinite is defined to be the intersection of all subspaces of that contain is referred to as the subspace spanned by or by the vectors in conversely is called spanning set of and we say that spans alternatively the span of may be defined as the set of all finite linear combinations of elements vectors of which follows from the above definition 
in the case of infinite infinite linear combinations 
where combination may involve an infinite sum assuming that such sums are defined somehow as in say banach space are excluded by the definition generalization that allows these is not equivalent 
examples the real vector space has as spanning set 
this particular spanning set is also basis 
if were replaced by it would also form the canonical basis of another spanning set for the same space is given by but this set is not basis because it is linearly dependent 
the set is not spanning set of since its span is the space of all vectors in whose last component is zero 
that space is also spanned by the set as is linear combination of and 
it does however span 
when interpreted as subset of 
the empty set is spanning set of since the empty set is subset of all possible vector spaces in and is the intersection of all of these vector spaces 
the set of functions xn where is non negative integer spans the space of polynomials 
theorems equivalence of definitions the set of all linear combinations of subset of vector space over is the smallest linear subspace of containing proof 
we first prove that span is subspace of since is subset of we only need to prove the existence of zero vector in span that span is closed under addition and that span is closed under scalar multiplication 
letting it is trivial that the zero vector of exists in span since adding together two linear combinations of also produces linear combination of where all and multiplying linear combination of by scalar will produce another linear combination of thus is subspace of suppose that is linear subspace of containing it follows that span since every vi is linear combination of trivially 
since is closed under addition and scalar multiplication then every linear combination must be contained in thus span is contained in every subspace of containing and the intersection of all such subspaces or the smallest such subspace is equal to the set of all linear combinations of size of spanning set is at least size of linearly independent set every spanning set of vector space must contain at least as many elements as any linearly independent set of vectors from proof 
let be spanning set and be linearly independent set of vectors from we want to show that since spans then must also span and must be linear combination of thus is linearly dependent and we can remove one vector from that is linear combination of the other elements 
this vector cannot be any of the wi since is linearly indepedent 
the resulting set is which is spanning set of we repeat this step times where the resulting set after the pth step is the union of and vectors of it is ensured until the nth step that there will always be some to remove out of for every adjoint of and thus there are at least as many vi as there are wi 
to verify this we assume by way of contradiction that then at the mth step we have the set and we can adjoin another vector but since is spanning set of is linear combination of 
this is contradiction since is linearly independent 
spanning set can be reduced to basis let be finite dimensional vector space 
any set of vectors that spans can be reduced to basis for by discarding vectors if necessary 
if there are linearly dependent vectors in the set 
if the axiom of choice holds this is true without the assumption that has finite dimension 
this also indicates that basis is minimal spanning set when is finite dimensional 
generalizations generalizing the definition of the span of points in space subset of the ground set of matroid is called spanning set if the rank of equals the rank of the entire ground set 
the vector space definition can also be generalized to modules 
given an module and collection of elements an of the submodule of spanned by an is the sum of cyclic modules consisting of all linear combinations of the elements ai 
as with the case of vector spaces the submodule of spanned by any subset of is the intersection of all submodules containing that subset 
closed linear span functional analysis in functional analysis closed linear span of set of vectors is the minimal closed set which contains the linear span of that set 
suppose that is normed vector space and let be any non empty subset of the closed linear span of denoted by sp or span is the intersection of all the closed linear subspaces of which contain one mathematical formulation of this is sp sp 
the closed linear span of the set of functions xn on the interval where is non negative integer depends on the norm used 
if the norm is used then the closed linear span is the hilbert space of square integrable functions on the interval 
but if the maximum norm is used the closed linear span will be the space of continuous functions on the interval 
in either case the closed linear span contains functions that are not polynomials and so are not in the linear span itself 
however the cardinality of the set of functions in the closed linear span is the cardinality of the continuum which is the same cardinality as for the set of polynomials 
notes the linear span of set is dense in the closed linear span 
moreover as stated in the lemma below the closed linear span is indeed the closure of the linear span 
closed linear spans are important when dealing with closed linear subspaces which are themselves highly important see riesz lemma 
useful lemma let be normed space and let be any non empty subset of then so the usual way to find the closed linear span is to find the linear span first and then the closure of that linear span 
see also affine hull conical combination convex hull citations sources textbooks axler sheldon jay 
linear algebra done right rd ed 
linear algebra th ed 
isbn lane saunders mac birkhoff garrett 
advanced linear algebra nd ed 
isbn rynne brian youngson martin 
isbn lay david linear algebra and its applications th edition 
web lankham isaiah nachtergaele bruno schilling anne february 
linear algebra as an introduction to abstract mathematics pdf 
university of california davis 
retrieved september weisstein eric wolfgang 
retrieved feb cs maint url status link linear hull 
april retrieved feb cs maint url status link external links linear combinations and span understanding linear combinations and spans of vectors khanacademy org 
linear combinations span and basis vectors 
essence of linear algebra 
archived from the original on via youtube
the purpose of this page is to provide supplementary materials for the ordinary least squares article reducing the load of the main article with mathematics and improving its accessibility while at the same time retaining the completeness of exposition 
derivation of the normal equations define the th residual to be then the objective can be rewritten given that is convex it is minimized when its gradient vector is zero this follows by definition if the gradient vector is not zero there is direction in which we can move to minimize it further see maxima and minima 
the elements of the gradient vector are the partial derivatives of with respect to the parameters 
the derivatives are substitution of the expressions for the residuals and the derivatives into the gradient equations gives 
thus if minimizes we have 
upon rearrangement we obtain the normal equations 
the normal equations are written in matrix notation as where xt is the matrix transpose of the solution of the normal equations yields the vector of the optimal parameter values 
derivation directly in terms of matrices the normal equations can be derived directly from matrix representation of the problem as follows 
the objective is to minimize here has the dimension the number of columns of so it is scalar and equal to its own transpose hence and the quantity to minimize becomes differentiating this with respect to and equating to zero to satisfy the first order conditions gives which is equivalent to the above given normal equations 
sufficient condition for satisfaction of the second order conditions for minimum is that have full column rank in which case is positive definite 
derivation without calculus when is positive definite the formula for the minimizing value of can be derived without the use of derivatives 
the quantity can be written as where depends only on and and is the inner product defined by it follows that is equal to and therefore minimized exactly when 
generalization for complex equations in general the coefficients of the matrices and can be complex 
by using hermitian transpose instead of simple transpose it is possible to find vector which minimizes just as for the real matrix case 
in order to get the normal equations we follow similar path as in previous derivations where stands for hermitian transpose 
we should now take derivatives of with respect to each of the coefficients but first we separate real and imaginary parts to deal with the conjugate factors in above expression 
for the we have and the derivatives change into 
after rewriting in the summation form and writing explicitly we can calculate both partial derivatives with result which after adding it together and comparing to zero minimization condition for yields 
in matrix form or 
least squares estimator for using matrix notation the sum of squared residuals is given by 
since this is quadratic expression the vector which gives the global minimum may be found via matrix calculus by differentiating with respect to the vector using denominator layout and setting equal to zero by assumption matrix has full column rank and therefore xtx is invertible and the least squares estimator for is given by unbiasedness and variance of plug into the formula for and then use the law of total expectation where by assumptions of the model 
since the expected value of equals the parameter it estimates it is an unbiased estimator of for the variance let the covariance matrix of be where is the identity matrix and let be known constant 
then where we used the fact that is just an affine transformation of by the matrix for simple linear regression model where is the intercept and is the slope one obtains var 
expected value and biasedness of first we will plug in the expression for into the estimator and use the fact that mx matrix projects onto the space orthogonal to now we can recognize as matrix such matrix is equal to its own trace 
this is useful because by properties of trace operator tr ab tr ba and we can use this to separate disturbance from matrix which is function of regressors tr tr using the law of iterated expectation this can be written as tr tr tr recall that where is the projection onto linear space spanned by columns of matrix by properties of projection matrix it has rank eigenvalues equal to and all other eigenvalues are equal to trace of matrix is equal to the sum of its characteristic values thus tr and tr therefore since the expected value of does not equal the parameter it estimates it is biased estimator of note in the later section maximum likelihood we show that under the additional assumption that errors are distributed normally the estimator is proportional to chi squared distribution with degrees of freedom from which the formula for expected value would immediately follow 
however the result we have shown in this section is valid regardless of the distribution of the errors and thus has importance on its own 
consistency and asymptotic normality of estimator can be written as we can use the law of large numbers to establish that by slutsky theorem and continuous mapping theorem these results can be combined to establish consistency of estimator the central limit theorem tells us that where var applying slutsky theorem again we ll have maximum likelihood approach maximum likelihood estimation is generic technique for estimating the unknown parameters in statistical model by constructing log likelihood function corresponding to the joint distribution of the data then maximizing this function over all possible parameter values 
in order to apply this method we have to make an assumption about the distribution of given so that the log likelihood function can be constructed 
the connection of maximum likelihood estimation to ols arises when this distribution is modeled as multivariate normal 
specifically assume that the errors have multivariate normal distribution with mean and variance matrix 
then the distribution of conditionally on is and the log likelihood function of the data will be ln ln ln differentiating this expression with respect to and we ll find the ml estimates of these parameters we can check that this is indeed maximum by looking at the hessian matrix of the log likelihood function 
finite sample distribution since we have assumed in this section that the distribution of error terms is known to be normal it becomes possible to derive the explicit expressions for the distributions of estimators and so that by the affine transformation properties of multivariate normal distribution 
similarly the distribution of follows from where is the symmetric projection matrix onto subspace orthogonal to and thus mx we have argued before that this matrix rank and thus by properties of chi squared distribution moreover the estimators and turn out to be independent conditional on fact which is fundamental for construction of the classical and tests 
the independence can be easily seen from following the estimator represents coefficients of vector decomposition of by the basis of columns of as such is function of 
at the same time the estimator is norm of vector divided by and thus this estimator is function of 
now random variables are jointly normal as linear transformation of and they are also uncorrelated because pm by properties of multivariate normal distribution this means that and are independent and therefore estimators and will be independent as well 
derivation of simple linear regression estimators we look for and that minimize the sum of squared errors sse min sse min to find minimum take partial derivatives with respect to and sse before taking partial derivative with respect to substitute the previous result for 
min min now take the derivative with respect to sse cov var and finally substitute to determine
in statistics linear regression is linear approach for modelling the relationship between scalar response and one or more explanatory variables also known as dependent and independent variables 
the case of one explanatory variable is called simple linear regression for more than one the process is called multiple linear regression 
this term is distinct from multivariate linear regression where multiple correlated dependent variables are predicted rather than single scalar variable in linear regression the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data 
such models are called linear models 
most commonly the conditional mean of the response given the values of the explanatory variables or predictors is assumed to be an affine function of those values less commonly the conditional median or some other quantile is used 
like all forms of regression analysis linear regression focuses on the conditional probability distribution of the response given the values of the predictors rather than on the joint probability distribution of all of these variables which is the domain of multivariate analysis 
linear regression was the first type of regression analysis to be studied rigorously and to be used extensively in practical applications 
this is because models which depend linearly on their unknown parameters are easier to fit than models which are non linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine 
linear regression has many practical uses 
most applications fall into one of the following two broad categories if the goal is error reduction in prediction or forecasting linear regression can be used to fit predictive model to an observed data set of values of the response and explanatory variables 
after developing such model if additional values of the explanatory variables are collected without an accompanying response value the fitted model can be used to make prediction of the response 
if the goal is to explain variation in the response variable that can be attributed to variation in the explanatory variables linear regression analysis can be applied to quantify the strength of the relationship between the response and the explanatory variables and in particular to determine whether some explanatory variables may have no linear relationship with the response at all or to identify which subsets of explanatory variables may contain redundant information about the response linear regression models are often fitted using the least squares approach but they may also be fitted in other ways such as by minimizing the lack of fit in some other norm as with least absolute deviations regression or by minimizing penalized version of the least squares cost function as in ridge regression norm penalty and lasso norm penalty 
conversely the least squares approach can be used to fit models that are not linear models 
thus although the terms least squares and linear model are closely linked they are not synonymous 
formulation given data set of statistical units linear regression model assumes that the relationship between the dependent variable and the vector of regressors is linear 
this relationship is modeled through disturbance term or error variable an unobserved random variable that adds noise to the linear relationship between the dependent variable and regressors 
thus the model takes the form where denotes the transpose so that xit is the inner product between vectors xi and often these equations are stacked together and written in matrix notation as where 
notation and terminology is vector of observed values of the variable called the regressand endogenous variable response variable measured variable criterion variable or dependent variable 
this variable is also sometimes known as the predicted variable but this should not be confused with predicted values which are denoted 
the decision as to which variable in data set is modeled as the dependent variable and which are modeled as the independent variables may be based on presumption that the value of one of the variables is caused by or directly influenced by the other variables 
alternatively there may be an operational reason to model one of the variables in terms of the others in which case there need be no presumption of causality 
may be seen as matrix of row vectors or of dimensional column vectors which are known as regressors exogenous variables explanatory variables covariates input variables predictor variables or independent variables not to be confused with the concept of independent random variables 
the matrix is sometimes called the design matrix 
usually constant is included as one of the regressors 
in particular for the corresponding element of is called the intercept 
many statistical inference procedures for linear models require an intercept to be present so it is often included even if theoretical considerations suggest that its value should be zero 
sometimes one of the regressors can be non linear function of another regressor or of the data as in polynomial regression and segmented regression 
the model remains linear as long as it is linear in the parameter vector the values xij may be viewed as either observed values of random variables xj or as fixed values chosen prior to observing the dependent variable 
both interpretations may be appropriate in different cases and they generally lead to the same estimation procedures however different approaches to asymptotic analysis are used in these two situations 
is dimensional parameter vector where is the intercept term if one is included in the model otherwise is dimensional 
its elements are known as effects or regression coefficients although the latter term is sometimes reserved for the estimated effects 
in simple linear regression and the coefficient is known as regression slope 
statistical estimation and inference in linear regression focuses on the elements of this parameter vector are interpreted as the partial derivatives of the dependent variable with respect to the various independent variables 
is vector of values this part of the model is called the error term disturbance term or sometimes noise in contrast with the signal provided by the rest of the model 
this variable captures all other factors which influence the dependent variable other than the regressors the relationship between the error term and the regressors for example their correlation is crucial consideration in formulating linear regression model as it will determine the appropriate estimation method fitting linear model to given data set usually requires estimating the regression coefficients such that the error term is minimized 
for example it is common to use the sum of squared errors as measure of for minimization 
example consider situation where small ball is being tossed up in the air and then we measure its heights of ascent hi at various moments in time ti 
physics tells us that ignoring the drag the relationship can be modeled as where determines the initial velocity of the ball is proportional to the standard gravity and is due to measurement errors 
linear regression can be used to estimate the values of and from the measured data 
this model is non linear in the time variable but it is linear in the parameters and if we take regressors xi xi xi ti ti the model takes on the standard form 
assumptions standard linear regression models with standard estimation techniques make number of assumptions about the predictor variables the response variables and their relationship 
numerous extensions have been developed that allow each of these assumptions to be relaxed 
reduced to weaker form and in some cases eliminated entirely 
generally these extensions make the estimation procedure more complex and time consuming and may also require more data in order to produce an equally precise model 
the following are the major assumptions made by standard linear regression models with standard estimation techniques 
ordinary least squares weak exogeneity 
this essentially means that the predictor variables can be treated as fixed values rather than random variables 
this means for example that the predictor variables are assumed to be error free that is not contaminated with measurement errors 
although this assumption is not realistic in many settings dropping it leads to significantly more difficult errors in variables models 
this means that the mean of the response variable is linear combination of the parameters regression coefficients and the predictor variables 
note that this assumption is much less restrictive than it may at first seem 
because the predictor variables are treated as fixed values see above linearity is really only restriction on the parameters 
the predictor variables themselves can be arbitrarily transformed and in fact multiple copies of the same underlying predictor variable can be added each one transformed differently 
this technique is used for example in polynomial regression which uses linear regression to fit the response variable as an arbitrary polynomial function up to given degree of predictor variable 
with this much flexibility models such as polynomial regression often have too much power in that they tend to overfit the data 
as result some kind of regularization must typically be used to prevent unreasonable solutions coming out of the estimation process 
common examples are ridge regression and lasso regression 
bayesian linear regression can also be used which by its nature is more or less immune to the problem of overfitting 
in fact ridge regression and lasso regression can both be viewed as special cases of bayesian linear regression with particular types of prior distributions placed on the regression coefficients 
this means that the variance of the errors does not depend on the values of the predictor variables 
thus the variability of the responses for given fixed values of the predictors is the same regardless of how large or small the responses are 
this is often not the case as variable whose mean is large will typically have greater variance than one whose mean is small 
for example person whose income is predicted to be may easily have an actual income of or standard deviation of around while another person with predicted income of is unlikely to have the same standard deviation since that would imply their actual income could vary anywhere between and 
in fact as this shows in many cases often the same cases where the assumption of normally distributed errors fails the variance or standard deviation should be predicted to be proportional to the mean rather than constant 
the absence of homoscedasticity is called heteroscedasticity 
in order to check this assumption plot of residuals versus predicted values or the values of each individual predictor can be examined for fanning effect increasing or decreasing vertical spread as one moves left to right on the plot 
plot of the absolute or squared residuals versus the predicted values or each predictor can also be examined for trend or curvature 
formal tests can also be used see heteroscedasticity 
the presence of heteroscedasticity will result in an overall average estimate of variance being used instead of one that takes into account the true variance structure 
this leads to less precise but in the case of ordinary least squares not biased parameter estimates and biased standard errors resulting in misleading tests and interval estimates 
the mean squared error for the model will also be wrong 
various estimation techniques including weighted least squares and the use of heteroscedasticity consistent standard errors can handle heteroscedasticity in quite general way 
bayesian linear regression techniques can also be used when the variance is assumed to be function of the mean 
it is also possible in some cases to fix the problem by applying transformation to the response variable fitting the logarithm of the response variable using linear regression model which implies that the response variable itself has log normal distribution rather than normal distribution 
this assumes that the errors of the response variables are uncorrelated with each other 
actual statistical independence is stronger condition than mere lack of correlation and is often not needed although it can be exploited if it is known to hold 
some methods such as generalized least squares are capable of handling correlated errors although they typically require significantly more data unless some sort of regularization is used to bias the model towards assuming uncorrelated errors 
bayesian linear regression is general way of handling this issue 
lack of perfect multicollinearity in the predictors 
for standard least squares estimation methods the design matrix must have full column rank otherwise perfect multicollinearity exists in the predictor variables meaning linear relationship exists between two or more predictor variables 
this can be caused by accidentally duplicating variable in the data using linear transformation of variable along with the original the same temperature measurements expressed in fahrenheit and celsius or including linear combination of multiple variables in the model such as their mean 
it can also happen if there is too little data available compared to the number of parameters to be estimated fewer data points than regression coefficients 
near violations of this assumption where predictors are highly but not perfectly correlated can reduce the precision of parameter estimates see variance inflation factor 
in the case of perfect multicollinearity the parameter vector will be non identifiable it has no unique solution 
in such case only some of the parameters can be identified their values can only be estimated within some linear subspace of the full parameter space rp 
see partial least squares regression 
methods for fitting linear models with multicollinearity have been developed some of which require additional assumptions such as effect sparsity that large fraction of the effects are exactly zero 
note that the more computationally expensive iterated algorithms for parameter estimation such as those used in generalized linear models do not suffer from this problem beyond these assumptions several other statistical properties of the data strongly influence the performance of different estimation methods the statistical relationship between the error terms and the regressors plays an important role in determining whether an estimation procedure has desirable sampling properties such as being unbiased and consistent 
the arrangement or probability distribution of the predictor variables has major influence on the precision of estimates of sampling and design of experiments are highly developed subfields of statistics that provide guidance for collecting data in such way to achieve precise estimate of 
interpretation fitted linear regression model can be used to identify the relationship between single predictor variable xj and the response variable when all the other predictor variables in the model are held fixed 
specifically the interpretation of is the expected change in for one unit change in xj when the other covariates are held fixed that is the expected value of the partial derivative of with respect to xj 
this is sometimes called the unique effect of xj on in contrast the marginal effect of xj on can be assessed using correlation coefficient or simple linear regression model relating only xj to this effect is the total derivative of with respect to xj 
care must be taken when interpreting regression results as some of the regressors may not allow for marginal changes such as dummy variables or the intercept term while others cannot be held fixed recall the example from the introduction it would be impossible to hold ti fixed and at the same time change the value of ti 
it is possible that the unique effect can be nearly zero even when the marginal effect is large 
this may imply that some other covariate captures all the information in xj so that once that variable is in the model there is no contribution of xj to the variation in conversely the unique effect of xj can be large while its marginal effect is nearly zero 
this would happen if the other covariates explained great deal of the variation of but they mainly explain variation in way that is complementary to what is captured by xj 
in this case including the other variables in the model reduces the part of the variability of that is unrelated to xj thereby strengthening the apparent relationship with xj 
the meaning of the expression held fixed may depend on how the values of the predictor variables arise 
if the experimenter directly sets the values of the predictor variables according to study design the comparisons of interest may literally correspond to comparisons among units whose predictor variables have been held fixed by the experimenter 
alternatively the expression held fixed can refer to selection that takes place in the context of data analysis 
in this case we hold variable fixed by restricting our attention to the subsets of the data that happen to have common value for the given predictor variable 
this is the only interpretation of held fixed that can be used in an observational study 
the notion of unique effect is appealing when studying complex system where multiple interrelated components influence the response variable 
in some cases it can literally be interpreted as the causal effect of an intervention that is linked to the value of predictor variable 
however it has been argued that in many cases multiple regression analysis fails to clarify the relationships between the predictor variables and the response variable when the predictors are correlated with each other and are not assigned following study design 
group effects in multiple linear regression model parameter of predictor variable represents the individual effect of it has an interpretation as the expected change in the response variable when increases by one unit with other predictor variables held constant 
when is strongly correlated with other predictor variables it is improbable that can increase by one unit with other variables held constant 
in this case the interpretation of becomes problematic as it is based on an improbable condition and the effect of cannot be evaluated in isolation 
for group of predictor variables say group effect is defined as linear combination of their parameters where is weight vector satisfying because of the constraint on is also referred to as normalized group effect 
group effect has an interpretation as the expected change in when variables in the group change by the amount respectively at the same time with variables not in the group held constant 
it generalizes the individual effect of variable to group of variables in that if then the group effect reduces to an individual effect and if and for then the group effect also reduces to an individual effect 
group effect is said to be meaningful if the underlying simultaneous changes of the variables is probable 
group effects provide means to study the collective impact of strongly correlated predictor variables in linear regression models 
individual effects of such variables are not well defined as their parameters do not have good interpretations 
furthermore when the sample size is not large none of their parameters can be accurately estimated by the least squares regression due to the multicollinearity problem 
nevertheless there are meaningful group effects that have good interpretations and can be accurately estimated by the least squares regression 
simple way to identify these meaningful group effects is to use an all positive correlations apc arrangement of the strongly correlated variables under which pairwise correlations among these variables are all positive and standardize all predictor variables in the model so that they all have mean zero and length one 
to illustrate this suppose that is group of strongly correlated variables in an apc arrangement and that they are not strongly correlated with predictor variables outside the group 
let be the centred and be the standardized then the standardized linear regression model is parameters in the original model including are simple functions of in the standardized model 
the standardization of variables does not change their correlations so is group of strongly correlated variables in an apc arrangement and they are not strongly correlated with other predictor variables in the standardized model 
group effect of is and its minimum variance unbiased linear estimator is where is the least squares estimator of 
in particular the average group effect of the standardized variables is which has an interpretation as the expected change in when all in the strongly correlated group increase by th of unit at the same time with variables outside the group held constant 
with strong positive correlations and in standardized units variables in the group are approximately equal so they are likely to increase at the same time and in similar amount 
thus the average group effect is meaningful effect 
it can be accurately estimated by its minimum variance unbiased linear estimator even when individually none of the can be accurately estimated by 
not all group effects are meaningful or can be accurately estimated 
for example is special group effect with weights and for but it cannot be accurately estimated by 
it is also not meaningful effect 
in general for group of strongly correlated predictor variables in an apc arrangement in the standardized model group effects whose weight vectors are at or near the centre of the simplex are meaningful and can be accurately estimated by their minimum variance unbiased linear estimators 
effects with weight vectors far away from the centre are not meaningful as such weight vectors represent simultaneous changes of the variables that violate the strong positive correlations of the standardized variables in an apc arrangement 
as such they are not probable 
these effects also cannot be accurately estimated 
applications of the group effects include estimation and inference for meaningful group effects on the response variable testing for group significance of the variables via testing versus and characterizing the region of the predictor variable space over which predictions by the least squares estimated model are accurate 
group effect of the original variables can be expressed as constant times group effect of the standardized variables 
the former is meaningful when the latter is 
thus meaningful group effects of the original variables can be found through meaningful group effects of the standardized variables 
extensions numerous extensions of linear regression have been developed which allow some or all of the assumptions underlying the basic model to be relaxed 
simple and multiple linear regression the very simplest case of single scalar predictor variable and single scalar response variable is known as simple linear regression 
the extension to multiple and or vector valued predictor variables denoted with capital is known as multiple linear regression also known as multivariable linear regression not to be confused with multivariate linear regression 
multiple linear regression is generalization of simple linear regression to the case of more than one independent variable and special case of general linear models restricted to one dependent variable 
the basic model for multiple linear regression is for each observation in the formula above we consider observations of one dependent variable and independent variables 
thus yi is the ith observation of the dependent variable xij is ith observation of the jth independent variable the values represent parameters to be estimated and is the ith independent identically distributed normal error 
in the more general multivariate linear regression there is one equation of the above form for each of dependent variables that share the same set of explanatory variables and hence are estimated simultaneously with each other for all observations indexed as and for all dependent variables indexed as nearly all real world regression models involve multiple predictors and basic descriptions of linear regression are often phrased in terms of the multiple regression model 
note however that in these cases the response variable is still scalar 
another term multivariate linear regression refers to cases where is vector the same as general linear regression 
general linear models the general linear model considers the situation when the response variable is not scalar for each observation but vector yi 
conditional linearity of is still assumed with matrix replacing the vector of the classical linear regression model 
multivariate analogues of ordinary least squares ols and generalized least squares gls have been developed 
general linear models are also called multivariate linear models 
these are not the same as multivariable linear models also called multiple linear models 
heteroscedastic models various models have been created that allow for heteroscedasticity 
the errors for different response variables may have different variances 
for example weighted least squares is method for estimating linear regression models when the response variables may have different error variances possibly with correlated errors 
see also weighted linear least squares and generalized least squares 
heteroscedasticity consistent standard errors is an improved method for use with uncorrelated but potentially heteroscedastic errors 
generalized linear models generalized linear models glms are framework for modeling response variables that are bounded or discrete 
this is used for example when modeling positive quantities 
prices or populations that vary over large scale which are better described using skewed distribution such as the log normal distribution or poisson distribution although glms are not used for log normal data instead the response variable is simply transformed using the logarithm function when modeling categorical data such as the choice of given candidate in an election which is better described using bernoulli distribution binomial distribution for binary choices or categorical distribution multinomial distribution for multi way choices where there are fixed number of choices that cannot be meaningfully ordered when modeling ordinal data 
ratings on scale from to where the different outcomes can be ordered but where the quantity itself may not have any absolute meaning 
rating of may not be twice as good in any objective sense as rating of but simply indicates that it is better than or but not as good as generalized linear models allow for an arbitrary link function that relates the mean of the response variable to the predictors 
the link function is often related to the distribution of the response and in particular it typically has the effect of transforming between the range of the linear predictor and the range of the response variable 
some common examples of glms are poisson regression for count data 
logistic regression and probit regression for binary data 
multinomial logistic regression and multinomial probit regression for categorical data 
ordered logit and ordered probit regression for ordinal data single index models allow some degree of nonlinearity in the relationship between and while preserving the central role of the linear predictor as in the classical linear regression model 
under certain conditions simply applying ols to data from single index model will consistently estimate up to proportionality constant 
hierarchical linear models hierarchical linear models or multilevel regression organizes the data into hierarchy of regressions for example where is regressed on and is regressed on it is often used where the variables of interest have natural hierarchical structure such as in educational statistics where students are nested in classrooms classrooms are nested in schools and schools are nested in some administrative grouping such as school district 
the response variable might be measure of student achievement such as test score and different covariates would be collected at the classroom school and school district levels 
errors in variables errors in variables models or measurement error models extend the traditional linear regression model to allow the predictor variables to be observed with error 
this error causes standard estimators of to become biased 
generally the form of bias is an attenuation meaning that the effects are biased toward zero 
others in dempster shafer theory or linear belief function in particular linear regression model may be represented as partially swept matrix which can be combined with similar matrices representing observations and other assumed normal distributions and state equations 
the combination of swept or unswept matrices provides an alternative method for estimating linear regression models 
estimation methods large number of procedures have been developed for parameter estimation and inference in linear regression 
these methods differ in computational simplicity of algorithms presence of closed form solution robustness with respect to heavy tailed distributions and theoretical assumptions needed to validate desirable statistical properties such as consistency and asymptotic efficiency 
some of the more common estimation techniques for linear regression are summarized below 
least squares estimation and related techniques assuming that the independent variable is and the model parameters are then the model prediction would be if is extended to then would become dot product of the parameter and the independent variable 
in the least squares setting the optimum parameter is defined as such that minimizes the sum of mean squared loss arg min arg min now putting the independent and dependent variables in matrices and respectively the loss function can be rewritten as as the loss is convex the optimum solution lies at gradient zero 
the gradient of the loss function is using denominator layout convention setting the gradient to zero produces the optimum parameter note to prove that the obtained is indeed the local minimum one needs to differentiate once more to obtain the hessian matrix and show that it is positive definite 
this is provided by the gauss markov theorem 
linear least squares methods include mainly ordinary least squares weighted least squares generalized least squares maximum likelihood estimation and related techniques maximum likelihood estimation can be performed when the distribution of the error terms is known to belong to certain parametric family of probability distributions 
when is normal distribution with zero mean and variance the resulting estimate is identical to the ols estimate 
gls estimates are maximum likelihood estimates when follows multivariate normal distribution with known covariance matrix 
ridge regression and other forms of penalized estimation such as lasso regression deliberately introduce bias into the estimation of in order to reduce the variability of the estimate 
the resulting estimates generally have lower mean squared error than the ols estimates particularly when multicollinearity is present or when overfitting is problem 
they are generally used when the goal is to predict the value of the response variable for values of the predictors that have not yet been observed 
these methods are not as commonly used when the goal is inference since it is difficult to account for the bias 
least absolute deviation lad regression is robust estimation technique in that it is less sensitive to the presence of outliers than ols but is less efficient than ols when no outliers are present 
it is equivalent to maximum likelihood estimation under laplace distribution model for adaptive estimation 
if we assume that error terms are independent of the regressors then the optimal estimator is the step mle where the first step is used to non parametrically estimate the distribution of the error term 
other estimation techniques bayesian linear regression applies the framework of bayesian statistics to linear regression 
see also bayesian multivariate linear regression 
in particular the regression coefficients are assumed to be random variables with specified prior distribution 
the prior distribution can bias the solutions for the regression coefficients in way similar to but more general than ridge regression or lasso regression 
in addition the bayesian estimation process produces not single point estimate for the best values of the regression coefficients but an entire posterior distribution completely describing the uncertainty surrounding the quantity 
this can be used to estimate the best coefficients using the mean mode median any quantile see quantile regression or any other function of the posterior distribution 
quantile regression focuses on the conditional quantiles of given rather than the conditional mean of given linear quantile regression models particular conditional quantile for example the conditional median as linear function tx of the predictors 
mixed models are widely used to analyze linear regression relationships involving dependent data when the dependencies have known structure 
common applications of mixed models include analysis of data involving repeated measurements such as longitudinal data or data obtained from cluster sampling 
they are generally fit as parametric models using maximum likelihood or bayesian estimation 
in the case where the errors are modeled as normal random variables there is close connection between mixed models and generalized least squares 
fixed effects estimation is an alternative approach to analyzing this type of data 
principal component regression pcr is used when the number of predictor variables is large or when strong correlations exist among the predictor variables 
this two stage procedure first reduces the predictor variables using principal component analysis and then uses the reduced variables in an ols regression fit 
while it often works well in practice there is no general theoretical reason that the most informative linear function of the predictor variables should lie among the dominant principal components of the multivariate distribution of the predictor variables 
the partial least squares regression is the extension of the pcr method which does not suffer from the mentioned deficiency 
least angle regression is an estimation procedure for linear regression models that was developed to handle high dimensional covariate vectors potentially with more covariates than observations 
the theil sen estimator is simple robust estimation technique that chooses the slope of the fit line to be the median of the slopes of the lines through pairs of sample points 
it has similar statistical efficiency properties to simple linear regression but is much less sensitive to outliers 
other robust estimation techniques including the trimmed mean approach and and estimators have been introduced 
applications linear regression is widely used in biological behavioral and social sciences to describe possible relationships between variables 
it ranks as one of the most important tools used in these disciplines 
trend line trend line represents trend the long term movement in time series data after other components have been accounted for 
it tells whether particular data set say gdp oil prices or stock prices have increased or decreased over the period of time 
trend line could simply be drawn by eye through set of data points but more properly their position and slope is calculated using statistical techniques like linear regression 
trend lines typically are straight lines although some variations use higher degree polynomials depending on the degree of curvature desired in the line 
trend lines are sometimes used in business analytics to show changes in data over time 
this has the advantage of being simple 
trend lines are often used to argue that particular action or event such as training or an advertising campaign caused observed changes at point in time 
this is simple technique and does not require control group experimental design or sophisticated analysis technique 
however it suffers from lack of scientific validity in cases where other potential changes can affect the data 
epidemiology early evidence relating tobacco smoking to mortality and morbidity came from observational studies employing regression analysis 
in order to reduce spurious correlations when analyzing observational data researchers usually include several variables in their regression models in addition to the variable of primary interest 
for example in regression model in which cigarette smoking is the independent variable of primary interest and the dependent variable is lifespan measured in years researchers might include education and income as additional independent variables to ensure that any observed effect of smoking on lifespan is not due to those other socio economic factors 
however it is never possible to include all possible confounding variables in an empirical analysis 
for example hypothetical gene might increase mortality and also cause people to smoke more 
for this reason randomized controlled trials are often able to generate more compelling evidence of causal relationships than can be obtained using regression analyses of observational data 
when controlled experiments are not feasible variants of regression analysis such as instrumental variables regression may be used to attempt to estimate causal relationships from observational data 
finance the capital asset pricing model uses linear regression as well as the concept of beta for analyzing and quantifying the systematic risk of an investment 
this comes directly from the beta coefficient of the linear regression model that relates the return on the investment to the return on all risky assets 
economics linear regression is the predominant empirical tool in economics 
for example it is used to predict consumption spending fixed investment spending inventory investment purchases of country exports spending on imports the demand to hold liquid assets labor demand and labor supply 
environmental science linear regression finds application in wide range of environmental science applications 
in canada the environmental effects monitoring program uses statistical analyses on fish and benthic surveys to measure the effects of pulp mill or metal mine effluent on the aquatic ecosystem 
machine learning linear regression plays an important role in the subfield of artificial intelligence known as machine learning 
the linear regression algorithm is one of the fundamental supervised machine learning algorithms due to its relative simplicity and well known properties 
history least squares linear regression as means of finding good rough linear fit to set of points was performed by legendre and gauss for the prediction of planetary movement 
quetelet was responsible for making the procedure well known and for using it extensively in the social sciences 
see also references citations sources further reading pedhazur elazar 
multiple regression in behavioral research explanation and prediction nd ed 
new york holt rinehart and winston 
isbn mathieu rouaud probability statistics and estimation chapter linear regression linear regression with error bars and nonlinear regression 
chapter linear equations and matrices direct methods 
notes on applied science 
her majesty stationery office 
external links least squares regression phet interactive simulations university of colorado at boulder diy linear fit
dependent and independent variables are variables in mathematical modeling statistical modeling and experimental sciences 
dependent variables receive this name because in an experiment their values are studied under the supposition or demand that they depend by some law or rule by mathematical function on the values of other variables 
independent variables in turn are not seen as depending on any other variable in the scope of the experiment in question 
in this sense some common independent variables are time space density mass fluid flow rate and previous values of some observed value of interest 
human population size to predict future values the dependent variable of the two it is always the dependent variable whose variation is being studied by altering inputs also known as regressors in statistical context 
in an experiment any variable that can be attributed value without attributing value to any other variable is called an independent variable 
models and experiments test the effects that the independent variables have on the dependent variables 
sometimes even if their influence is not of direct interest independent variables may be included for other reasons such as to account for their potential confounding effect 
mathematics in mathematics function is rule for taking an input in the simplest case number or set of numbers and providing an output which may also be number 
symbol that stands for an arbitrary input is called an independent variable while symbol that stands for an arbitrary output is called dependent variable 
the most common symbol for the input is and the most common symbol for the output is the function itself is commonly written it is possible to have multiple independent variables or multiple dependent variables 
for instance in multivariable calculus one often encounters functions of the form where is dependent variable and and are independent variables 
functions with multiple outputs are often referred to as vector valued functions 
modeling in mathematical modeling the dependent variable is studied to see if and how much it varies as the independent variables vary 
in the simple stochastic linear model yi bxi ei the term yi is the ith value of the dependent variable and xi is the ith value of the independent variable 
the term ei is known as the error and contains the variability of the dependent variable not explained by the independent variable 
with multiple independent variables the model is yi bxi bxi bxi ei where is the number of independent variables the linear regression model is now discussed 
to use linear regression scatter plot of data is generated with as the independent variable and as the dependent variable 
this is also called bivariate dataset xi yi 
the simple linear regression model takes the form of yi bxi ui for in this case ui un are independent random variables 
this occurs when the measurements do not influence each other 
through propagation of independence the independence of ui implies independence of yi even though each yi has different expectation value 
each ui has an expectation value of and variance of expectation of yi proof the line of best fit for the bivariate dataset takes the form and is called the regression line 
and correspond to the intercept and slope respectively 
simulation in simulation the dependent variable is changed in response to changes in the independent variables 
statistics in an experiment the variable manipulated by an experimenter is something that is proven to work called an independent variable 
the dependent variable is the event expected to change when the independent variable is manipulated in data mining tools for multivariate statistics and machine learning the dependent variable is assigned role as target variable or in some tools as label attribute while an independent variable may be assigned role as regular variable 
known values for the target variable are provided for the training data set and test data set but should be predicted for other data 
the target variable is used in supervised learning algorithms but not in unsupervised learning 
statistics synonyms depending on the context an independent variable is sometimes called predictor variable regressor covariate manipulated variable explanatory variable exposure variable see reliability theory risk factor see medical statistics feature in machine learning and pattern recognition or input variable 
in econometrics the term control variable is usually used instead of covariate 
explanatory variable is preferred by some authors over independent variable when the quantities treated as independent variables may not be statistically independent or independently manipulable by the researcher 
if the independent variable is referred to as an explanatory variable then the term response variable is preferred by some authors for the dependent variable from the economics community the independent variables are also called exogenous 
depending on the context dependent variable is sometimes called response variable regressand criterion predicted variable measured variable explained variable experimental variable responding variable outcome variable output variable target or label 
in economics endogenous variables are usually referencing the target 
explained variable is preferred by some authors over dependent variable when the quantities treated as dependent variables may not be statistically dependent 
if the dependent variable is referred to as an explained variable then the term predictor variable is preferred by some authors for the independent variable variables may also be referred to by their form continuous or categorical which in turn may be binary dichotomous nominal categorical and ordinal categorical among others 
an example is provided by the analysis of trend in sea level by woodworth 
here the dependent variable and variable of most interest was the annual mean sea level at given location for which series of yearly values were available 
the primary independent variable was time 
use was made of covariate consisting of yearly values of annual mean atmospheric pressure at sea level 
the results showed that inclusion of the covariate allowed improved estimates of the trend against time to be obtained compared to analyses which omitted the covariate 
other variables variable may be thought to alter the dependent or independent variables but may not actually be the focus of the experiment 
so that the variable will be kept constant or monitored to try to minimize its effect on the experiment 
such variables may be designated as either controlled variable control variable or fixed variable 
extraneous variables if included in regression analysis as independent variables may aid researcher with accurate response parameter estimation prediction and goodness of fit but are not of substantive interest to the hypothesis under examination 
for example in study examining the effect of post secondary education on lifetime earnings some extraneous variables might be gender ethnicity social class genetics intelligence age and so forth 
variable is extraneous only when it can be assumed or shown to influence the dependent variable 
if included in regression it can improve the fit of the model 
if it is excluded from the regression and if it has non zero covariance with one or more of the independent variables of interest its omission will bias the regression result for the effect of that independent variable of interest 
this effect is called confounding or omitted variable bias in these situations design changes and or controlling for variable statistical control is necessary 
extraneous variables are often classified into three types subject variables which are the characteristics of the individuals being studied that might affect their actions 
these variables include age gender health status mood background etc 
blocking variables or experimental variables are characteristics of the persons conducting the experiment which might influence how person behaves 
gender the presence of racial discrimination language or other factors may qualify as such variables 
situational variables are features of the environment in which the study or research was conducted which have bearing on the outcome of the experiment in negative way 
included are the air temperature level of activity lighting and time of day in modelling variability that is not covered by the independent variable is designated by and is known as the residual side effect error unexplained share residual variable disturbance or tolerance 
examples effect of fertilizer on plant growths in study measuring the influence of different quantities of fertilizer on plant growth the independent variable would be the amount of fertilizer used 
the dependent variable would be the growth in height or mass of the plant 
the controlled variables would be the type of plant the type of fertilizer the amount of sunlight the plant gets the size of the pots etc effect of drug dosage on symptom severity in study of how different doses of drug affect the severity of symptoms researcher could compare the frequency and intensity of symptoms when different doses are administered 
here the independent variable is the dose and the dependent variable is the frequency intensity of symptoms effect of temperature on pigmentation in measuring the amount of color removed from beetroot samples at different temperatures temperature is the independent variable and amount of pigment removed is the dependent variable effect of sugar added in coffee the taste varies with the amount of sugar added in the coffee 
here the sugar is the independent variable while the taste is the dependent variable 
see also abscissa and ordinate blocking statistics latent variable versus observable variable notes references
product distribution is probability distribution constructed as the distribution of the product of random variables having two other known distributions 
given two statistically independent random variables and the distribution of the random variable that is formed as the product is product distribution 
algebra of random variables the product is one type of algebra for random variables related to the product distribution are the ratio distribution sum distribution see list of convolutions of probability distributions and difference distribution 
more generally one may talk of combinations of sums differences products and ratios 
many of these distributions are described in melvin springer book from the algebra of random variables 
derivation for independent random variables if and are two independent continuous random variables described by probability density functions and then the probability density function of is 
proof we first write the cumulative distribution function of starting with its definition def we find the desired probability density function by taking the derivative of both sides with respect to since on the right hand side appears only in the integration limits the derivative is easily performed using the fundamental theorem of calculus and the chain rule 
note the negative sign that is needed when the variable occurs in the lower limit of the integration 
where the absolute value is used to conveniently combine the two terms 
alternate proof faster more compact proof begins with the same step of writing the cumulative distribution of starting with its definition where is the heaviside step function and serves to limit the region of integration to values of and satisfying we find the desired probability density function by taking the derivative of both sides with respect to where we utilize the translation and scaling properties of the dirac delta function more intuitive description of the procedure is illustrated in the figure below 
the joint pdf exists in the plane and an arc of constant value is shown as the shaded line 
to find the marginal probability on this arc integrate over increments of area on this contour 
starting with we have so the probability increment is since implies we can relate the probability increment to the increment namely then integration over yields 
bayesian interpretation let be random sample drawn from probability distribution 
scaling by generates sample from scaled distribution which can be written as conditional distribution 
letting be random variable with pdf the distribution of the scaled sample becomes and integrating out we get so is drawn from this distribution 
however substituting the definition of we also have which has the same form as the product distribution above 
thus the bayesian posterior distribution is the distribution of the product of the two independent random samples and for the case of one variable being discrete let have probability at levels with the conditional density is 
expectation of product of random variables when two random variables are statistically independent the expectation of their product is the product of their expectations 
this can be proved from the law of total expectation in the inner expression is constant 
hence this is true even if and are statistically dependent in which case is function of in the special case in which and are statistically independent it is constant independent of hence variance of the product of independent random variables let be uncorrelated random variables with means and variances if additionally the random variables and are uncorrelated then the variance of the product xy is var in the case of the product of more than two variables if are statistically independent then the variance of their product is var characteristic function of product of random variables assume are independent random variables 
the characteristic function of is and the distribution of is known 
then from the law of total expectation we have if the characteristic functions and distributions of both and are known then alternatively also holds 
mellin transform the mellin transform of distribution with support only on and having random sample is 
the inverse transform is if and are two independent random samples from different distributions then the mellin transform of their product is equal to the product of their mellin transforms if is restricted to integer values simpler result is thus the moments of the random product are the product of the corresponding moments of and and this extends to non integer moments for example 
the pdf of function can be reconstructed from its moments using the saddlepoint approximation method 
further result is that for independent gamma distribution example to illustrate how the product of moments yields much simpler result than finding the moments of the distribution of the product let be sampled from two gamma distributions with parameters whose moments are 
multiplying the corresponding moments gives the mellin transform result independently it is known that the product of two independent gamma distributed samples gamma and gamma has distribution to find the moments of this make the change of variable simplifying similar integrals to thus the definite integral is well documented and we have finally which after some difficulty has agreed with the moment product result above 
if are drawn independently from gamma distributions with shape parameters then this type of result is universally true since for bivariate independent variables thus or equivalently it is clear that and are independent variables 
special cases lognormal distributions the distribution of the product of two random variables which have lognormal distributions is again lognormal 
this is itself special case of more general set of results where the logarithm of the product can be written as the sum of the logarithms 
thus in cases where simple result can be found in the list of convolutions of probability distributions where the distributions to be convolved are those of the logarithms of the components of the product the result might be transformed to provide the distribution of the product 
however this approach is only useful where the logarithms of the components of the product are in some standard families of distributions 
uniformly distributed independent random variables let be the product of two independent variables each uniformly distributed on the interval possibly the outcome of copula transformation 
as noted in lognormal distributions above pdf convolution operations in the log domain correspond to the product of sample values in the original domain 
thus making the transformation ln such that each variate is distributed independently on as and the convolution of the two distributions is the autoconvolution next retransform the variable to yielding the distribution ln on the interval for the product of multiple independent samples the characteristic function route is favorable 
if we define then above is gamma distribution of shape and scale factor and its known cf is note that so the jacobian of the transformation is unity 
the convolution of independent samples from therefore has cf which is known to be the cf of gamma distribution of shape making the inverse transformation we get the pdf of the product of the samples log log 
the following more conventional derivation from stackexchange is consistent with this result 
first of all letting its cdf is pr pr log the density of is then log multiplying by third independent sample gives distribution function pr pr log log log log taking the derivative yields log the author of the note conjectures that in general log 
the figure illustrates the nature of the integrals above 
the shaded area within the unit square and below the line xy represents the cdf of this divides into two parts 
the first is for where the increment of area in the vertical slot is just equal to dx 
the second part lies below the xy line has height and incremental area dx 
independent central normal distributions the product of two independent normal samples follows modified bessel function 
let be samples from normal distribution and then the variance of this distribution could be determined in principle by definite integral from gradsheyn and ryzhik thus much simpler result stated in section above is that the variance of the product of zero mean independent samples is equal to the product of their variances 
since the variance of each normal sample is one the variance of the product is also one 
correlated central normal distributions the product of correlated normal samples case was recently addressed by nadarajaha and pog ny 
let be zero mean unit variance normally distributed variates with correlation coefficient and let then exp mean and variance for the mean we have from the definition of correlation coefficient 
the variance can be found by transforming from two unit variance zero mean uncorrelated variables let then are unit variance variables with correlation coefficient and removing odd power terms whose expectations are obviously zero we get since we have var high correlation asymptote in the highly correlated case the product converges on the square of one sample 
in this case the asymptote is in the limit as and exp exp exp exp as which is chi squared distribution with one degree of freedom 
further show that if are iid random variables sampled from and is their mean then exp 
where is the whittaker function while using the identity see for example the dlmf compilation 
eqn this expression can be somewhat simplified to exp 
the pdf gives the distribution of sample covariance 
the approximate distribution of correlation coefficient can be found via the fisher transformation 
multiple non central correlated samples 
the distribution of the product of correlated non central normal samples was derived by cui et al 
and takes the form of an infinite series of modified bessel functions of the first kind 
moments of product of correlated central normal samples for central normal distribution the moments are exp if is odd 
denotes the double factorial 
if norm are central correlated variables the simplest bivariate case of the multivariate normal moment problem described by kan then if is odd 
if and are even 
if and are odd where is the correlation coefficient and min needs checking correlated non central normal distributions the distribution of the product of non central correlated normal samples was derived by cui et al 
and takes the form of an infinite series 
these product distributions are somewhat comparable to the wishart distribution 
the latter is the joint distribution of the four elements actually only three independent elements of sample covariance matrix 
if are samples from bivariate time series then the is wishart matrix with degrees of freedom 
the product distributions above are the unconditional distribution of the aggregate of samples of 
independent complex valued central normal distributions let be independent samples from normal distribution 
setting and then are independent zero mean complex normal samples with circular symmetry 
their complex variances are var the density functions of are rayleigh distributions defined as of mean and variance the variable is clearly chi squared with two degrees of freedom and has pdf of mean value wells et al 
show that the density function of is and the cumulative distribution function of is pr thus the polar representation of the product of two uncorrelated complex gaussian samples is where is uniform on the first and second moments of this distribution can be found from the integral in normal distributions above thus its variance is var further the density of corresponds to the product of two independent chi square samples each with two dof 
writing these as scaled gamma distributions with then from the gamma products below the density of the product is with expectation independent complex valued noncentral normal distributions the product of non central independent complex gaussians is described by donoughue and moura and forms double infinite series of modified bessel functions of the first and second types 
gamma distributions the product of two independent gamma samples defining follows where beta distributions nagar et al 
define correlated bivariate beta distribution where then the pdf of xy is given by where is the gauss hypergeometric function defined by the euler integral note that multivariate distributions are not generally unique apart from the gaussian case and there may be alternatives 
uniform and gamma distributions the distribution of the product of random variable having uniform distribution on with random variable having gamma distribution with shape parameter equal to is an exponential distribution 
more general case of this concerns the distribution of the product of random variable having beta distribution with random variable having gamma distribution for some cases where the parameters of the two component distributions are related in certain way the result is again gamma distribution but with changed shape parameter the distribution is an example of non standard distribution that can be defined as product distribution where both components have gamma distribution 
gamma and pareto distributions the product of gamma and pareto independent samples was derived by nadarajah 
see also algebra of random variables sum of independent random variables notes references springer melvin dale thompson 
the distribution of products of beta gamma and gaussian random variables 
siam journal on applied mathematics 
jstor springer melvin dale thompson 
the distribution of products of independent random variables 
siam journal on applied mathematics
in mathematics normed vector space or normed space is vector space over the real or complex numbers on which norm is defined 
norm is the formalization and the generalization to real vector spaces of the intuitive notion of length in the real physical world 
norm is real valued function defined on the vector space that is commonly denoted and has the following properties it is nonnegative meaning that for every vector it is positive on nonzero vectors that is for every vector and every scalar the triangle inequality holds that is for every vectors and norm induces distance called its norm induced metric by the formula which makes any normed vector space into metric space and topological vector space 
if this metric is complete then the normed space is banach space 
every normed vector space can be uniquely extended to banach space which makes normed spaces intimately related to banach spaces 
every banach space is normed space but converse is not true 
for example the set of the finite sequences of real numbers can be normed with the euclidean norm but it is not complete for this norm 
an inner product space is normed vector space whose norm is the square root of the inner product of vector and itself 
the euclidean norm of euclidean vector space is special case that allows defining euclidean distance by the formula the study of normed spaces and banach spaces is fundamental part of functional analysis which is major subfield of mathematics 
definition normed vector space is vector space equipped with norm 
seminormed vector space is vector space equipped with seminorm 
useful variation of the triangle inequality is for any vectors and this also shows that vector norm is uniformly continuous function 
property depends on choice of norm on the field of scalars 
when the scalar field is or more generally subset of this is usually taken to be the ordinary absolute value but other choices are possible 
for example for vector space over one could take to be the adic absolute value 
topological structure if is normed vector space the norm induces metric notion of distance and therefore topology on this metric is defined in the natural way the distance between two vectors and is given by 
this topology is precisely the weakest topology which makes continuous and which is compatible with the linear structure of in the following sense the vector addition is jointly continuous with respect to this topology 
this follows directly from the triangle inequality 
the scalar multiplication where is the underlying scalar field of is jointly continuous 
this follows from the triangle inequality and homogeneity of the norm similarly for any seminormed vector space we can define the distance between two vectors and as 
this turns the seminormed space into pseudometric space notice this is weaker than metric and allows the definition of notions such as continuity and convergence 
to put it more abstractly every seminormed vector space is topological vector space and thus carries topological structure which is induced by the semi norm 
of special interest are complete normed spaces which are known as banach spaces 
every normed vector space sits as dense subspace inside some banach space this banach space is essentially uniquely defined by and is called the completion of two norms on the same vector space are called equivalent if they define the same topology 
on finite dimensional vector space all norms are equivalent but this is not true for infinite dimensional vector spaces 
all norms on finite dimensional vector space are equivalent from topological viewpoint as they induce the same topology although the resulting metric spaces need not be the same 
and since any euclidean space is complete we can thus conclude that all finite dimensional normed vector spaces are banach spaces 
normed vector space is locally compact if and only if the unit ball is compact which is the case if and only if is finite dimensional this is consequence of riesz lemma 
in fact more general result is true topological vector space is locally compact if and only if it is finite dimensional 
the point here is that we don assume the topology comes from norm 
the topology of seminormed vector space has many nice properties 
given neighbourhood system around we can construct all other neighbourhood systems as with moreover there exists neighbourhood basis for the origin consisting of absorbing and convex sets 
as this property is very useful in functional analysis generalizations of normed vector spaces with this property are studied under the name locally convex spaces 
norm or seminorm on topological vector space is continuous if and only if the topology that induces on is coarser than meaning which happens if and only if there exists some open ball in such as maybe for example that is open in said different such that 
normable spaces topological vector space is called normable if there exists norm on such that the canonical metric induces the topology on the following theorem is due to kolmogorov kolmogorov normability criterion hausdorff topological vector space is normable if and only if there exists convex von neumann bounded neighborhood of product of family of normable spaces is normable if and only if only finitely many of the spaces are non trivial that is 
furthermore the quotient of normable space by closed vector subspace is normable and if in addition topology is given by norm then the map given by inf is well defined norm on that induces the quotient topology on if is hausdorff locally convex topological vector space then the following are equivalent is normable 
has bounded neighborhood of the origin 
the strong dual space of is normable 
the strong dual space of is metrizable furthermore is finite dimensional if and only if is normable here denotes endowed with the weak topology 
the topology of the fr chet space as defined in the article on spaces of test functions and distributions is defined by countable family of norms but it is not normable space because there does not exist any norm on such that the topology that this norm induces is equal to even if metrizable topological vector space has topology that is defined by family of norms then it may nevertheless still fail to be normable space meaning that its topology can not be defined by any single norm 
an example of such space is the fr chet space whose definition can be found in the article on spaces of test functions and distributions because its topology is defined by countable family of norms but it is not normable space because there does not exist any norm on such that the topology this norm induces is equal to in fact the topology of locally convex space can be defined by family of norms on if and only if there exists at least one continuous norm on 
linear maps and dual spaces the most important maps between two normed vector spaces are the continuous linear maps 
together with these maps normed vector spaces form category 
the norm is continuous function on its vector space 
all linear maps between finite dimensional vector spaces are also continuous 
an isometry between two normed vector spaces is linear map which preserves the norm meaning for all vectors 
isometries are always continuous and injective 
surjective isometry between the normed vector spaces and is called an isometric isomorphism and and are called isometrically isomorphic 
isometrically isomorphic normed vector spaces are identical for all practical purposes 
when speaking of normed vector spaces we augment the notion of dual space to take the norm into account 
the dual of normed vector space is the space of all continuous linear maps from to the base field the complexes or the reals such linear maps are called functionals 
the norm of functional is defined as the supremum of where ranges over all unit vectors that is vectors of norm in this turns into normed vector space 
an important theorem about continuous linear functionals on normed vector spaces is the hahn banach theorem 
normed spaces as quotient spaces of seminormed spaces the definition of many normed spaces in particular banach spaces involves seminorm defined on vector space and then the normed space is defined as the quotient space by the subspace of elements of seminorm zero 
for instance with the spaces the function defined by is seminorm on the vector space of all functions on which the lebesgue integral on the right hand side is defined and finite 
however the seminorm is equal to zero for any function supported on set of lebesgue measure zero 
these functions form subspace which we quotient out making them equivalent to the zero function 
finite product spaces given seminormed spaces with seminorms denote the product space by where vector addition defined as and scalar multiplication defined as define new function by which is seminorm on the function is norm if and only if all are norms 
more generally for each real the map defined by is semi norm 
for each this defines the same topological space 
straightforward argument involving elementary linear algebra shows that the only finite dimensional seminormed spaces are those arising as the product space of normed space and space with trivial seminorm 
consequently many of the more interesting examples and applications of seminormed spaces occur for infinite dimensional vector spaces 
see also banach space normed vector spaces which are complete with respect to the metric induced by the norm banach mazur compactum set of dimensional subspaces of normed space made into compact metric space 
finsler manifold where the length of each tangent vector is determined by norm inner product space normed vector spaces where the norm is given by an inner product kolmogorov normability criterion characterization of normable spaces locally convex topological vector space vector space with topology defined by convex open sets space mathematics mathematical set with some added structure topological vector space vector space with notion of nearness references bibliography rudin walter 
international series in pure and applied mathematics 
new york ny mcgraw hill science engineering math 
isbn oclc banach stefan 
th orie des op rations lin aires theory of linear operations pdf 
monografie matematyczne in french 
warszawa subwencji funduszu kultury narodowej 
zbl archived from the original pdf on retrieved rolewicz stefan functional analysis and control theory linear systems mathematics and its applications east european series vol 
translated from the polish by ewa bednarczuk ed 
dordrecht warsaw reidel publishing co pwn polish scientific publishers pp 
xvi doi isbn mr oclc schaefer 
new york ny springer new york imprint springer 
isbn oclc tr ves fran ois 
topological vector spaces distributions and kernels 
external links media related to normed spaces at wikimedia commons
in statistics generalized least squares gls is technique for estimating the unknown parameters in linear regression model when there is certain degree of correlation between the residuals in regression model 
in these cases ordinary least squares and weighted least squares can be statistically inefficient or even give misleading inferences 
gls was first described by alexander aitken in 
method outline in standard linear regression models we observe data on statistical units 
the response values are placed in vector and the predictor values are placed in the design matrix where is vector of the predictor variables including constant for the ith unit 
the model forces the conditional mean of given to be linear function of and assumes the conditional variance of the error term given is known nonsingular covariance matrix this is usually written as cov here is vector of unknown constants known as regression coefficients that must be estimated from the data 
suppose is candidate estimate for then the residual vector for will be the generalized least squares method estimates by minimizing the squared mahalanobis length of this residual vector argmin argmin where the last two terms evaluate to scalars resulting in argmin this objective is quadratic form in taking the gradient of this quadratic form with respect to and equating it to zero when gives therefore the minimum of the objective function can be computed yielding the explicit formula the quantity is known as the precision matrix or dispersion matrix generalization of the diagonal weight matrix 
properties the gls estimator is unbiased consistent efficient and asymptotically normal with and cov gls is equivalent to applying ordinary least squares to linearly transformed version of the data 
to see this factor for instance using the cholesky decomposition 
then if we pre multiply both sides of the equation by we get an equivalent linear model where and in this model var where is the identity matrix 
thus we can efficiently estimate by applying ordinary least squares ols to the transformed data which requires minimizing 
this has the effect of standardizing the scale of the errors and de correlating them 
since ols is applied to data with homoscedastic errors the gauss markov theorem applies and therefore the gls estimate is the best linear unbiased estimator for 
weighted least squares special case of gls called weighted least squares wls occurs when all the off diagonal entries of are this situation arises when the variances of the observed values are unequal 
heteroscedasticity is present but where no correlations exist among the observed variances 
the weight for unit is proportional to the reciprocal of the variance of the response for unit 
feasible generalized least squares if the covariance of the errors is unknown one can get consistent estimate of say using an implementable version of gls known as the feasible generalized least squares fgls estimator 
in fgls modeling proceeds in two stages the model is estimated by ols or another consistent but inefficient estimator and the residuals are used to build consistent estimator of the errors covariance matrix to do so one often needs to examine the model adding additional constraints for example if the errors follow time series process statistician generally needs some theoretical assumptions on this process to ensure that consistent estimator is available and using the consistent estimator of the covariance matrix of the errors one can implement gls ideas 
whereas gls is more efficient than ols under heteroscedasticity also spelled heteroskedasticity or autocorrelation this is not true for fgls 
the feasible estimator is provided the errors covariance matrix is consistently estimated asymptotically more efficient but for small or medium size sample it can be actually less efficient than ols 
this is why some authors prefer to use ols and reformulate their inferences by simply considering an alternative estimator for the variance of the estimator robust to heteroscedasticity or serial autocorrelation 
but for large samples fgls is preferred over ols under heteroskedasticity or serial correlation 
cautionary note is that the fgls estimator is not always consistent 
one case in which fgls might be inconsistent is if there are individual specific fixed effects in general this estimator has different properties than gls 
for large samples asymptotically all properties are under appropriate conditions common with respect to gls but for finite samples the properties of fgls estimators are unknown they vary dramatically with each particular model and as general rule their exact distributions cannot be derived analytically 
for finite samples fgls may be even less efficient than ols in some cases 
thus while gls can be made feasible it is not always wise to apply this method when the sample is small 
method sometimes used to improve the accuracy of the estimators in finite samples is to iterate 
taking the residuals from fgls to update the errors covariance estimator and then updating the fgls estimation applying the same idea iteratively until the estimators vary less than some tolerance 
but this method does not necessarily improve the efficiency of the estimator very much if the original sample was small 
reasonable option when samples are not too large is to apply ols but throwing away the classical variance estimator which is inconsistent in this framework and using hac heteroskedasticity and autocorrelation consistent estimator 
for example in autocorrelation context we can use the bartlett estimator often known as newey west estimator estimator since these authors popularized the use of this estimator among econometricians in their econometrica article and in heteroskedastic context we can use the eicker white estimator 
this approach is much safer and it is the appropriate path to take unless the sample is large and large is sometimes slippery issue 
if the errors distribution is asymmetric the required sample would be much larger 
the ordinary least squares ols estimator is calculated as usual by ols and estimates of the residuals ols are constructed 
for simplicity consider the model for heteroscedastic and not autocorrelated errors 
assume that the variance covariance matrix of the error vector is diagonal or equivalently that errors from distinct observations are uncorrelated 
then each diagonal entry may be estimated by the fitted residuals so may be constructed by ols diag 
it is important to notice that the squared residuals cannot be used in the previous expression we need an estimator of the errors variances 
to do so we can use parametric heteroskedasticity model or nonparametric estimator 
once this step is fulfilled we can proceed estimate using ols using weighted least squares ols ols the procedure can be iterated 
the first iteration is given by diag this estimation of can be iterated to convergence 
under regularity conditions any of the fgls estimator or that of any of its iterations if we iterate finite number of times is asymptotically distributed as 
where is the sample size and here lim means limit in probability see also confidence region effective degrees of freedom prais winsten estimation references further reading amemiya takeshi 
generalized least squares theory 
econometric methods second ed 
new york mcgraw hill 
generalized linear regression model and its applications 
elements of econometrics second ed 
isbn beck nathaniel katz jonathan september 
what to do and not to do with time series cross section data 
american political science review
in statistics econometrics epidemiology and related disciplines the method of instrumental variables iv is used to estimate causal relationships when controlled experiments are not feasible or when treatment is not successfully delivered to every unit in randomized experiment 
intuitively ivs are used when an explanatory variable of interest is correlated with the error term in which case ordinary least squares and anova give biased results 
valid instrument induces changes in the explanatory variable but has no independent effect on the dependent variable allowing researcher to uncover the causal effect of the explanatory variable on the dependent variable 
instrumental variable methods allow for consistent estimation when the explanatory variables covariates are correlated with the error terms in regression model 
such correlation may occur when changes in the dependent variable change the value of at least one of the covariates reverse causation there are omitted variables that affect both the dependent and independent variables or the covariates are subject to non random measurement error explanatory variables that suffer from one or more of these issues in the context of regression are sometimes referred to as endogenous 
in this situation ordinary least squares produces biased and inconsistent estimates 
however if an instrument is available consistent estimates may still be obtained 
an instrument is variable that does not itself belong in the explanatory equation but is correlated with the endogenous explanatory variables conditionally on the value of other covariates 
in linear models there are two main requirements for using ivs the instrument must be correlated with the endogenous explanatory variables conditionally on the other covariates 
if this correlation is strong then the instrument is said to have strong first stage 
weak correlation may provide misleading inferences about parameter estimates and standard errors 
the instrument cannot be correlated with the error term in the explanatory equation conditionally on the other covariates 
in other words the instrument cannot suffer from the same problem as the original predicting variable 
if this condition is met then the instrument is said to satisfy the exclusion restriction 
history first use of an instrument variable occurred in book by philip wright best known for his excellent description of the production transport and sale of vegetable and animal oils in the early in the united states while in olav reiers applied the same approach in the context of errors in variables models in his dissertation giving the method its name wright attempted to determine the supply and demand for butter using panel data on prices and quantities sold in the united states 
the idea was that regression analysis could produce demand or supply curve because they are formed by the path between prices and quantities demanded or supplied 
the problem was that the observational data did not form demand or supply curve as such but rather cloud of point observations that took different shapes under varying market conditions 
it seemed that making deductions from the data remained elusive 
the problem was that price affected both supply and demand so that function describing only one of the two could not be constructed directly from the observational data 
wright correctly concluded that he needed variable that correlated with either demand or supply but not both that is an instrumental variable 
after much deliberation wright decided to use regional rainfall as his instrumental variable he concluded that rainfall affected grass production and hence milk production and ultimately butter supply but not butter demand 
in this way he was able to construct regression equation with only the instrumental variable of price and supply 
theory while the ideas behind iv extend to broad class of models very common context for iv is in linear regression 
traditionally an instrumental variable is defined as variable that is correlated with the independent variable and uncorrelated with the error term in the linear equation is vector 
is matrix usually with column of ones and perhaps with additional columns for other covariates 
consider how an instrument allows to be recovered 
recall that ols solves for such that cov when we minimize the sum of squared errors min the first order condition is exactly 
if the true model is believed to have cov due to any of the reasons listed above for example if there is an omitted variable which affects both and separately then this ols procedure will not yield the causal impact of on ols will simply pick the parameter that makes the resulting errors appear uncorrelated with consider for simplicity the single variable case 
suppose we are considering regression with one variable and constant perhaps no other covariates are necessary or perhaps we have partialed out any other relevant covariates in this case the coefficient on the regressor of interest is given by cov var 
substituting for gives cov var cov var cov var cov var cov var where is what the estimated coefficient vector would be if were not correlated with in this case it can be shown that is an unbiased estimator of if cov in the underlying model that we believe then ols gives coefficient which does not reflect the underlying causal effect of interest 
iv helps to fix this problem by identifying the parameters not based on whether is uncorrelated with but based on whether another variable is uncorrelated with if theory suggests that is related to the first stage but uncorrelated with the exclusion restriction then iv may identify the causal parameter of interest where ols fails 
because there are multiple specific ways of using and deriving iv estimators even in just the linear case iv sls gmm we save further discussion for the estimation section below 
example informally in attempting to estimate the causal effect of some variable on another an instrument is third variable which affects only through its effect on for example suppose researcher wishes to estimate the causal effect of smoking on general health 
correlation between health and smoking does not imply that smoking causes poor health because other variables such as depression may affect both health and smoking or because health may affect smoking 
it is at best difficult and expensive to conduct controlled experiments on smoking status in the general population 
the researcher may attempt to estimate the causal effect of smoking on health from observational data by using the tax rate for tobacco products as an instrument for smoking 
the tax rate for tobacco products is reasonable choice for an instrument because the researcher assumes that it can only be correlated with health through its effect on smoking 
if the researcher then finds tobacco taxes and state of health to be correlated this may be viewed as evidence that smoking causes changes in health 
angrist and krueger present survey of the history and uses of instrumental variable techniques 
graphical definition of course iv techniques have been developed among much broader class of non linear models 
general definitions of instrumental variables using counterfactual and graphical formalism were given by pearl 
the graphical definition requires that satisfy the following conditions where stands for separation and stands for the graph in which all arrows entering are cut off 
the counterfactual definition requires that satisfies where yx stands for the value that would attain had been and stands for independence 
if there are additional covariates then the above definitions are modified so that qualifies as an instrument if the given criteria hold conditional on the essence of pearl definition is the equations of interest are structural not regression 
the error term stands for all exogenous factors that affect when is held constant 
the instrument should be independent of the instrument should not affect when is held constant exclusion restriction 
the instrument should not be independent of these conditions do not rely on specific functional form of the equations and are applicable therefore to nonlinear equations where can be non additive see non parametric analysis 
they are also applicable to system of multiple equations in which and other factors affect through several intermediate variables 
an instrumental variable need not be cause of proxy of such cause may also be used if it satisfies conditions 
the exclusion restriction condition is redundant it follows from conditions and 
selecting suitable instruments since is unobserved the requirement that be independent of cannot be inferred from data and must instead be determined from the model structure the data generating process 
causal graphs are representation of this structure and the graphical definition given above can be used to quickly determine whether variable qualifies as an instrumental variable given set of covariates to see how consider the following example 
suppose that we wish to estimate the effect of university tutoring program on grade point average gpa 
the relationship between attending the tutoring program and gpa may be confounded by number of factors 
students who attend the tutoring program may care more about their grades or may be struggling with their work 
this confounding is depicted in the figures on the right through the bidirected arc between tutoring program and gpa 
if students are assigned to dormitories at random the proximity of the student dorm to the tutoring program is natural candidate for being an instrumental variable 
however what if the tutoring program is located in the college library 
in that case proximity may also cause students to spend more time at the library which in turn improves their gpa see figure 
using the causal graph depicted in the figure we see that proximity does not qualify as an instrumental variable because it is connected to gpa through the path proximity library hours gpa in 
however if we control for library hours by adding it as covariate then proximity becomes an instrumental variable since proximity is separated from gpa given library hours in 
now suppose that we notice that student natural ability affects his or her number of hours in the library as well as his or her gpa as in figure using the causal graph we see that library hours is collider and conditioning on it opens the path proximity library hours gpa 
as result proximity cannot be used as an instrumental variable 
finally suppose that library hours does not actually affect gpa because students who do not study in the library simply study elsewhere as in figure in this case controlling for library hours still opens spurious path from proximity to gpa 
however if we do not control for library hours and remove it as covariate then proximity can again be used an instrumental variable 
estimation we now revisit and expand upon the mechanics of iv in greater detail 
suppose the data are generated by process of the form where indexes observations is the th value of the dependent variable is vector of the th values of the independent variable and constant is the th value of an unobserved error term representing all causes of other than and is an unobserved parameter vector the parameter vector is the causal effect on of one unit change in each element of holding all other causes of constant 
the econometric goal is to estimate for simplicity sake assume the draws of are uncorrelated and that they are drawn from distributions with the same variance that is that the errors are serially uncorrelated and homoskedastic 
suppose also that regression model of nominally the same form is proposed 
given random sample of observations from this process the ordinary least squares estimator is where and denote column vectors of length this equation is similar to the equation involving cov in the introduction this is the matrix version of that equation 
when and are uncorrelated under certain regularity conditions the second term has an expected value conditional on of zero and converges to zero in the limit so the estimator is unbiased and consistent 
when and the other unmeasured causal variables collapsed into the term are correlated however the ols estimator is generally biased and inconsistent for in this case it is valid to use the estimates to predict values of given values of but the estimate does not recover the causal effect of on to recover the underlying parameter we introduce set of variables that is highly correlated with each endogenous component of but in our underlying model is not correlated with for simplicity one might consider to be matrix composed of column of constants and one endogenous variable and to be consisting of column of constants and one instrumental variable 
however this technique generalizes to being matrix of constant and say endogenous variables with being matrix composed of constant and instruments 
in the discussion that follows we will assume that is matrix and leave this value unspecified 
an estimator in which and are both matrices is referred to as just identified 
suppose that the relationship between each endogenous component xi and the instruments is given by the most common iv specification uses the following estimator this specification approaches the true parameter as the sample gets large so long as in the true model as long as in the underlying process which generates the data the appropriate use of the iv estimator will identify this parameter 
this works because iv solves for the unique parameter that satisfies and therefore hones in on the true underlying parameter as the sample size grows 
now an extension suppose that there are more instruments than there are covariates in the equation of interest so that is matrix with this is often called the over identified case 
in this case the generalized method of moments gmm can be used 
the gmm iv estimator is where refers to the projection matrix this expression collapses to the first when the number of instruments is equal to the number of covariates in the equation of interest 
the over identified iv is therefore generalization of the just identified iv 
there is an equivalent under identified estimator for the case where since the parameters are the solutions to set of linear equations an under identified model using the set of equations does not have unique solution 
interpretation as two stage least squares one computational method which can be used to calculate iv estimates is two stage least squares sls or tsls 
in the first stage each explanatory variable that is an endogenous covariate in the equation of interest is regressed on all of the exogenous variables in the model including both exogenous covariates in the equation of interest and the excluded instruments 
the predicted values from these regressions are obtained stage regress each column of on errors and save the predicted values in the second stage the regression of interest is estimated as usual except that in this stage each endogenous covariate is replaced with the predicted values from the first stage stage regress on the predicted values from the first stage which gives sls this method is only valid in linear models 
for categorical endogenous covariates one might be tempted to use different first stage than ordinary least squares such as probit model for the first stage followed by ols for the second 
this is commonly known in the econometric literature as the forbidden regression because second stage iv parameter estimates are consistent only in special cases 
the resulting estimator of is numerically identical to the expression displayed above 
small correction must be made to the sum of squared residuals in the second stage fitted model in order that the covariance matrix of is calculated correctly 
non parametric analysis when the form of the structural equations is unknown an instrumental variable can still be defined through the equations where and are two arbitrary functions and is independent of unlike linear models however measurements of and do not allow for the identification of the average causal effect of on denoted ace ace pr do 
balke and pearl derived tight bounds on ace and showed that these can provide valuable information on the sign and size of ace in linear analysis there is no test to falsify the assumption the is instrumental relative to the pair 
this is not the case when is discrete 
pearl has shown that for all and the following constraint called instrumental inequality must hold whenever satisfies the two equations above max max pr 
interpretation under treatment effect heterogeneity the exposition above assumes that the causal effect of interest does not vary across observations that is that is constant 
generally different subjects will respond in different ways to changes in the treatment when this possibility is recognized the average effect in the population of change in on may differ from the effect in given subpopulation 
for example the average effect of job training program may substantially differ across the group of people who actually receive the training and the group which chooses not to receive training 
for these reasons iv methods invoke implicit assumptions on behavioral response or more generally assumptions over the correlation between the response to treatment and propensity to receive treatment the standard iv estimator can recover local average treatment effects late rather than average treatment effects ate 
imbens and angrist demonstrate that the linear iv estimate can be interpreted under weak conditions as weighted average of local average treatment effects where the weights depend on the elasticity of the endogenous regressor to changes in the instrumental variables 
roughly that means that the effect of variable is only revealed for the subpopulations affected by the observed changes in the instruments and that subpopulations which respond most to changes in the instruments will have the largest effects on the magnitude of the iv estimate 
for example if researcher uses presence of land grant college as an instrument for college education in an earnings regression she identifies the effect of college on earnings in the subpopulation which would obtain college degree if college is present but which would not obtain degree if college is not present 
this empirical approach does not without further assumptions tell the researcher anything about the effect of college among people who would either always or never get college degree regardless of whether local college exists 
weak instruments problem as bound jaeger and baker note problem is caused by the selection of weak instruments instruments that are poor predictors of the endogenous question predictor in the first stage equation 
in this case the prediction of the question predictor by the instrument will be poor and the predicted values will have very little variation 
consequently they are unlikely to have much success in predicting the ultimate outcome when they are used to replace the question predictor in the second stage equation 
in the context of the smoking and health example discussed above tobacco taxes are weak instruments for smoking if smoking status is largely unresponsive to changes in taxes 
if higher taxes do not induce people to quit smoking or not start smoking then variation in tax rates tells us nothing about the effect of smoking on health 
if taxes affect health through channels other than through their effect on smoking then the instruments are invalid and the instrumental variables approach may yield misleading results 
for example places and times with relatively health conscious populations may both implement high tobacco taxes and exhibit better health even holding smoking rates constant so we would observe correlation between health and tobacco taxes even if it were the case that smoking has no effect on health 
in this case we would be mistaken to infer causal effect of smoking on health from the observed correlation between tobacco taxes and health 
testing for weak instruments the strength of the instruments can be directly assessed because both the endogenous covariates and the instruments are observable 
common rule of thumb for models with one endogenous regressor is the statistic against the null that the excluded instruments are irrelevant in the first stage regression should be larger than 
statistical inference and hypothesis testing when the covariates are exogenous the small sample properties of the ols estimator can be derived in straightforward manner by calculating moments of the estimator conditional on when some of the covariates are endogenous so that instrumental variables estimation is implemented simple expressions for the moments of the estimator cannot be so obtained 
generally instrumental variables estimators only have desirable asymptotic not finite sample properties and inference is based on asymptotic approximations to the sampling distribution of the estimator 
even when the instruments are uncorrelated with the error in the equation of interest and when the instruments are not weak the finite sample properties of the instrumental variables estimator may be poor 
for example exactly identified models produce finite sample estimators with no moments so the estimator can be said to be neither biased nor unbiased the nominal size of test statistics may be substantially distorted and the estimates may commonly be far away from the true value of the parameter 
testing the exclusion restriction the assumption that the instruments are not correlated with the error term in the equation of interest is not testable in exactly identified models 
if the model is overidentified there is information available which may be used to test this assumption 
the most common test of these overidentifying restrictions called the sargan hansen test is based on the observation that the residuals should be uncorrelated with the set of exogenous variables if the instruments are truly exogenous 
the sargan hansen test statistic can be calculated as the number of observations multiplied by the coefficient of determination from the ols regression of the residuals onto the set of exogenous variables 
this statistic will be asymptotically chi squared with degrees of freedom under the null that the error term is uncorrelated with the instruments 
see also control function econometrics optimal instruments references further reading greene william 
econometric analysis sixth ed 
upper saddle river pearson prentice hall 
isbn gujarati damodar porter dawn 
basic econometrics fifth ed 
new york mcgraw hill irwin 
lectures on advanced econometric theory 
introductory econometrics modern approach fifth international ed 
mason oh south western 
quasi likelihood methods for count data handbook of applied econometrics volume ed 
pesaran and schmidt oxford blackwell pp 
terza estimating count models with endogenous switching sample selection and endogenous treatment effects 
journal of econometrics pp 
econometric analysis of cross section and panel data mit press cambridge massachusetts 
external links chapter from daniel mcfadden textbook econometrics lecture topic instrumental variable on youtube by mark thoma 
econometrics lecture topic two stages least square on youtube by mark thoma
robust principal component analysis rpca is modification of the widely used statistical procedure of principal component analysis pca which works well with respect to grossly corrupted observations 
number of different approaches exist for robust pca including an idealized version of robust pca which aims to recover low rank matrix from highly corrupted measurements 
this decomposition in low rank and sparse matrices can be achieved by techniques such as principal component pursuit method pcp stable pcp quantized pcp block based pcp and local pcp 
then optimization methods are used such as the augmented lagrange multiplier method alm alternating direction method adm fast alternating minimization fam iteratively reweighted least squares irls or alternating projections ap 
algorithms non convex method the guaranteed algorithm for the robust pca problem with the input matrix being is an alternating minimization type algorithm 
the computational complexity is log where the input is the superposition of low rank of rank and sparse matrix of dimension and is the desired accuracy of the recovered solution where is the true low rank component and is the estimated or recovered low rank component 
intuitively this algorithm performs projections of the residual on to the set of low rank matrices via the svd operation and sparse matrices via entry wise hard thresholding in an alternating manner that is low rank projection of the difference the input matrix and the sparse matrix obtained at given iteration followed by sparse projection of the difference of the input matrix and the low rank matrix obtained in the previous step and iterating the two steps until convergence 
this alternating projections algorithm is later improved by an accelerated version coined accaltproj 
the acceleration is achieved by applying tangent space projection before project the residue onto the set of low rank matrices 
this trick improves the computational complexity to log with much smaller constant in front while it maintains the theoretically guaranteed linear convergence 
another fast version of accelerated alternating projections algorithm is ircur 
it uses the structure of cur decomposition in alternating projections framework to dramatically reduces the compuational complexity of rpca to max log log log convex relaxation this method consists of relaxing the rank constraint in the optimization problem to the nuclear norm and the sparsity constraint to norm the resulting program can be solved using methods such as the method of augmented lagrange multipliers 
deep learning augmented method some recent works propose rpca algorithms with learnable training parameters 
such learnable trainable algorithm can be unfolded as deep neural network whose parameters can be learned via machine learning techniques from given dataset or problem distribution 
the learned algorithm will have superior performance on the corresponding problem distribution 
applications rpca has many real life important applications particularly when the data under study can naturally be modeled as low rank plus sparse contribution 
following examples are inspired by contemporary challenges in computer science and depending on the applications either the low rank component or the sparse component could be the object of interest video surveillance given sequence of surveillance video frames it is often required to identify the activities that stand out from the background 
if we stack the video frames as columns of matrix then the low rank component naturally corresponds to the stationary background and the sparse component captures the moving objects in the foreground 
face recognition images of convex lambertian surface under varying illuminations span low dimensional subspace 
this is one of the reasons for effectiveness of low dimensional models for imagery data 
in particular it is easy to approximate images of human face by low dimensional subspace 
to be able to correctly retrieve this subspace is crucial in many applications such as face recognition and alignment 
it turns out that rpca can be applied successfully to this problem to exactly recover the face 
surveys robust pca dynamic rpca decomposition into low rank plus additive matrices low rank models books journals and workshops books bouwmans aybat and zahzah 
handbook on robust low rank and sparse matrix decomposition applications in image and video processing crc press taylor and francis group may 
more information http www crcpress com product isbn lin zhang low rank models in visual analysis theories algorithms and applications academic press elsevier june 
more information https www elsevier com books low rank models in visual analysis lin journals vaswani chi bouwmans special issue on rethinking pca for modern datasets theory algorithms and applications proceedings of the ieee bouwmans vaswani rodriguez vidal lin special issue on robust subspace learning and tracking theory algorithms and applications ieee journal of selected topics in signal processing december 
workshops rsl cv workshop on robust subspace learning and computer vision in conjunction with iccv for more information http rsl cv univ lr fr workshop rsl cv workshop on robust subspace learning and computer vision in conjunction with iccv for more information http rsl cv univ lr fr rsl cv workshop on robust subspace learning and computer vision in conjunction with iccv for more information https rsl cv univ lr fr sessions special session on online algorithms for static and dynamic robust pca and compressive sensing in conjunction with ssp 
more information https ssp org resources and libraries websites background subtraction website dlam website documentation from the university of illinois archive link libraries the lrs library developed by andrews sobral provides collection of low rank and sparse decomposition algorithms in matlab 
the library was designed for moving object detection in videos but it can be also used for other computer vision machine learning tasks 
currently the lrslibrary offers more than algorithms based on matrix and tensor methods 
references external links lrslibrary
principal component analysis pca is popular technique for analyzing large datasets containing high number of dimensions features per observation increasing the interpretability of data while preserving the maximum amount of information and enabling the visualization of multidimensional data 
formally pca is statistical technique for reducing the dimensionality of dataset 
this is accomplished by linearly transforming the data into new coordinate system where most of the variation in the data can be described with fewer dimensions than the initial data 
many studies use the first two principal components in order to plot the data in two dimensions and to visually identify clusters of closely related data points 
principal component analysis has applications in many fields such as population genetics microbiome studies and atmospheric science 
the principal components of collection of points in real coordinate space are sequence of unit vectors where the th vector is the direction of line that best fits the data while being orthogonal to the first vectors 
here best fitting line is defined as one that minimizes the average squared perpendicular distance from the points to the line 
these directions constitute an orthonormal basis in which different individual dimensions of the data are linearly uncorrelated 
principal component analysis pca is the process of computing the principal components and using them to perform change of basis on the data sometimes using only the first few principal components and ignoring the rest 
in data analysis the first principal component of set of variables presumed to be jointly normally distributed is the derived variable formed as linear combination of the original variables that explains the most variance 
the second principal component explains the most variance in what is left once the effect of the first component is removed and we may proceed through iterations until all the variance is explained 
pca is most commonly used when many of the variables are highly correlated with each other and it is desirable to reduce their number to an independent set 
pca is used in exploratory data analysis and for making predictive models 
it is commonly used for dimensionality reduction by projecting each data point onto only the first few principal components to obtain lower dimensional data while preserving as much of the data variation as possible 
the first principal component can equivalently be defined as direction that maximizes the variance of the projected data 
the th principal component can be taken as direction orthogonal to the first principal components that maximizes the variance of the projected data 
for either objective it can be shown that the principal components are eigenvectors of the data covariance matrix 
thus the principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix 
pca is the simplest of the true eigenvector based multivariate analyses and is closely related to factor analysis 
factor analysis typically incorporates more domain specific assumptions about the underlying structure and solves eigenvectors of slightly different matrix 
pca is also related to canonical correlation analysis cca 
cca defines coordinate systems that optimally describe the cross covariance between two datasets while pca defines new orthogonal coordinate system that optimally describes variance in single dataset 
robust and norm based variants of standard pca have also been proposed 
history pca was invented in by karl pearson as an analogue of the principal axis theorem in mechanics it was later independently developed and named by harold hotelling in the 
depending on the field of application it is also named the discrete karhunen lo ve transform klt in signal processing the hotelling transform in multivariate quality control proper orthogonal decomposition pod in mechanical engineering singular value decomposition svd of invented in the last quarter of the th century eigenvalue decomposition evd of xtx in linear algebra factor analysis for discussion of the differences between pca and factor analysis see ch 
of jolliffe principal component analysis eckart young theorem harman or empirical orthogonal functions eof in meteorological science empirical eigenfunction decomposition sirovich empirical component analysis lorenz quasiharmonic modes brooks et al spectral decomposition in noise and vibration and empirical modal analysis in structural dynamics 
intuition pca can be thought of as fitting dimensional ellipsoid to the data where each axis of the ellipsoid represents principal component 
if some axis of the ellipsoid is small then the variance along that axis is also small 
to find the axes of the ellipsoid we must first center the values of each variable in the dataset on by subtracting the mean of the variable observed values from each of those values 
these transformed values are used instead of the original observed values for each of the variables 
then we compute the covariance matrix of the data and calculate the eigenvalues and corresponding eigenvectors of this covariance matrix 
then we must normalize each of the orthogonal eigenvectors to turn them into unit vectors 
once this is done each of the mutually orthogonal unit eigenvectors can be interpreted as an axis of the ellipsoid fitted to the data 
this choice of basis will transform the covariance matrix into diagonalized form in which the diagonal elements represent the variance of each axis 
the proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues 
biplots and scree plots degree of explained variance are used to explain findings of the pca 
details pca is defined as an orthogonal linear transformation that transforms the data to new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate called the first principal component the second greatest variance on the second coordinate and so on consider an data matrix with column wise zero empirical mean the sample mean of each column has been shifted to zero where each of the rows represents different repetition of the experiment and each of the columns gives particular kind of feature say the results from particular sensor 
mathematically the transformation is defined by set of size of dimensional vectors of weights or coefficients that map each row vector of to new vector of principal component scores given by in such way that the individual variables of considered over the data set successively inherit the maximum possible variance from with each coefficient vector constrained to be unit vector where is usually selected to be strictly less than to reduce dimensionality 
first component in order to maximize variance the first weight vector thus has to satisfy arg max arg max equivalently writing this in matrix form gives arg max arg max since has been defined to be unit vector it equivalently also satisfies arg max the quantity to be maximised can be recognised as rayleigh quotient 
standard result for positive semidefinite matrix such as xtx is that the quotient maximum possible value is the largest eigenvalue of the matrix which occurs when is the corresponding eigenvector 
with found the first principal component of data vector can then be given as score in the transformed co ordinates or as the corresponding vector in the original variables 
further components the th component can be found by subtracting the first principal components from and then finding the weight vector which extracts the maximum variance from this new data matrix arg max it turns out that this gives the remaining eigenvectors of xtx with the maximum values for the quantity in brackets given by their corresponding eigenvalues 
thus the weight vectors are eigenvectors of xtx 
the th principal component of data vector can therefore be given as score tk in the transformed coordinates or as the corresponding vector in the space of the original variables where is the kth eigenvector of xtx 
the full principal components decomposition of can therefore be given as where is by matrix of weights whose columns are the eigenvectors of xtx 
the transpose of is sometimes called the whitening or sphering transformation 
columns of multiplied by the square root of corresponding eigenvalues that is eigenvectors scaled up by the variances are called loadings in pca or in factor analysis 
covariances xtx itself can be recognized as proportional to the empirical sample covariance matrix of the dataset xt 
the sample covariance between two of the different principal components over the dataset is given by where the eigenvalue property of has been used to move from line to line however eigenvectors and corresponding to eigenvalues of symmetric matrix are orthogonal if the eigenvalues are different or can be orthogonalised if the vectors happen to share an equal repeated value 
the product in the final line is therefore zero there is no sample covariance between different principal components over the dataset 
another way to characterise the principal components transformation is therefore as the transformation to coordinates which diagonalise the empirical sample covariance matrix 
in matrix form the empirical covariance matrix for the original variables can be written the empirical covariance matrix between the principal components becomes where is the diagonal matrix of eigenvalues of xtx 
is equal to the sum of the squares over the dataset associated with each component that is tk 
dimensionality reduction the transformation maps data vector from an original space of variables to new space of variables which are uncorrelated over the dataset 
however not all the principal components need to be kept 
keeping only the first principal components produced by using only the first eigenvectors gives the truncated transformation where the matrix tl now has rows but only columns 
in other words pca learns linear transformation where the columns of matrix form an orthogonal basis for the features the components of representation that are decorrelated 
by construction of all the transformed data matrices with only columns this score matrix maximises the variance in the original data that has been preserved while minimising the total squared reconstruction error or such dimensionality reduction can be very useful step for visualising and processing high dimensional datasets while still retaining as much of the variance in the dataset as possible 
for example selecting and keeping only the first two principal components finds the two dimensional plane through the high dimensional dataset in which the data is most spread out so if the data contains clusters these too may be most spread out and therefore most visible to be plotted out in two dimensional diagram whereas if two directions through the data or two of the original variables are chosen at random the clusters may be much less spread apart from each other and may in fact be much more likely to substantially overlay each other making them indistinguishable 
similarly in regression analysis the larger the number of explanatory variables allowed the greater is the chance of overfitting the model producing conclusions that fail to generalise to other datasets 
one approach especially when there are strong correlations between different possible explanatory variables is to reduce them to few principal components and then run the regression against them method called principal component regression 
dimensionality reduction may also be appropriate when the variables in dataset are noisy 
if each column of the dataset contains independent identically distributed gaussian noise then the columns of will also contain similarly identically distributed gaussian noise such distribution is invariant under the effects of the matrix which can be thought of as high dimensional rotation of the co ordinate axes 
however with more of the total variance concentrated in the first few principal components compared to the same noise variance the proportionate effect of the noise is less the first few components achieve higher signal to noise ratio 
pca thus can have the effect of concentrating much of the signal into the first few principal components which can usefully be captured by dimensionality reduction while the later principal components may be dominated by noise and so disposed of without great loss 
if the dataset is not too large the significance of the principal components can be tested using parametric bootstrap as an aid in determining how many principal components to retain 
singular value decomposition the principal components transformation can also be associated with another matrix factorization the singular value decomposition svd of here is an by rectangular diagonal matrix of positive numbers called the singular values of is an by matrix the columns of which are orthogonal unit vectors of length called the left singular vectors of and is by whose columns are orthogonal unit vectors of length and called the right singular vectors of in terms of this factorization the matrix xtx can be written where is the square diagonal matrix with the singular values of and the excess zeros chopped off that satisfies comparison with the eigenvector factorization of xtx establishes that the right singular vectors of are equivalent to the eigenvectors of xtx while the singular values of are equal to the square root of the eigenvalues of xtx 
using the singular value decomposition the score matrix can be written so each column of is given by one of the left singular vectors of multiplied by the corresponding singular value 
this form is also the polar decomposition of efficient algorithms exist to calculate the svd of without having to form the matrix xtx so computing the svd is now the standard way to calculate principal components analysis from data matrix unless only handful of components are required 
as with the eigen decomposition truncated score matrix tl can be obtained by considering only the first largest singular values and their singular vectors the truncation of matrix or using truncated singular value decomposition in this way produces truncated matrix that is the nearest possible matrix of rank to the original matrix in the sense of the difference between the two having the smallest possible frobenius norm result known as the eckart young theorem 
further considerations the singular values in are the square roots of the eigenvalues of the matrix xtx 
each eigenvalue is proportional to the portion of the variance more correctly of the sum of the squared distances of the points from their multidimensional mean that is associated with each eigenvector 
the sum of all the eigenvalues is equal to the sum of the squared distances of the points from their multidimensional mean 
pca essentially rotates the set of points around their mean in order to align with the principal components 
this moves as much of the variance as possible using an orthogonal transformation into the first few dimensions 
the values in the remaining dimensions therefore tend to be small and may be dropped with minimal loss of information see below 
pca is often used in this manner for dimensionality reduction 
pca has the distinction of being the optimal orthogonal transformation for keeping the subspace that has largest variance as defined above 
this advantage however comes at the price of greater computational requirements if compared for example and when applicable to the discrete cosine transform and in particular to the dct ii which is simply known as the dct 
nonlinear dimensionality reduction techniques tend to be more computationally demanding than pca 
pca is sensitive to the scaling of the variables 
if we have just two variables and they have the same sample variance and are completely correlated then the pca will entail rotation by and the weights they are the cosines of rotation for the two variables with respect to the principal component will be equal 
but if we multiply all values of the first variable by then the first principal component will be almost the same as that variable with small contribution from the other variable whereas the second component will be almost aligned with the second original variable 
this means that whenever the different variables have different units like temperature and mass pca is somewhat arbitrary method of analysis 
different results would be obtained if one used fahrenheit rather than celsius for example 
pearson original paper was entitled on lines and planes of closest fit to systems of points in space in space implies physical euclidean space where such concerns do not arise 
one way of making the pca less arbitrary is to use variables scaled so as to have unit variance by standardizing the data and hence use the autocorrelation matrix instead of the autocovariance matrix as basis for pca 
however this compresses or expands the fluctuations in all dimensions of the signal space to unit variance 
mean centering is necessary for performing classical pca to ensure that the first principal component describes the direction of maximum variance 
if mean subtraction is not performed the first principal component might instead correspond more or less to the mean of the data 
mean of zero is needed for finding basis that minimizes the mean square error of the approximation of the data mean centering is unnecessary if performing principal components analysis on correlation matrix as the data are already centered after calculating correlations 
correlations are derived from the cross product of two standard scores scores or statistical moments hence the name pearson product moment correlation 
also see the article by kromrey foster johnson on mean centering in moderated regression much ado about nothing 
since covariances are correlations of normalized variables or standard scores pca based on the correlation matrix of is equal to pca based on the covariance matrix of the standardized version of pca is popular primary technique in pattern recognition 
it is not however optimized for class separability 
however it has been used to quantify the distance between two or more classes by calculating center of mass for each class in principal component space and reporting euclidean distance between center of mass of two or more classes 
the linear discriminant analysis is an alternative which is optimized for class separability 
table of symbols and abbreviations properties and limitations of pca properties some properties of pca include property for any integer consider the orthogonal linear transformation where is element vector and is matrix and let be the variance covariance matrix for then the trace of denoted tr is maximized by taking where consists of the first columns of is the transpose of property consider again the orthonormal transformation with and defined as before 
then tr is minimized by taking where consists of the last columns of the statistical implication of this property is that the last few pcs are not simply unstructured left overs after removing the important pcs 
because these last pcs have variances as small as possible they are useful in their own right 
they can help to detect unsuspected near constant linear relationships between the elements of and they may also be useful in regression in selecting subset of variables from and in outlier detection 
property spectral decomposition of before we look at its usage we first look at diagonal elements var then perhaps the main statistical implication of the result is that not only can we decompose the combined variances of all the elements of into decreasing contributions due to each pc but we can also decompose the whole covariance matrix into contributions from each pc 
although not strictly decreasing the elements of will tend to become smaller as increases as is nonincreasing for increasing whereas the elements of tend to stay about the same size because of the normalization constraints limitations as noted above the results of pca depend on the scaling of the variables 
this can be cured by scaling each feature by its standard deviation so that one ends up with dimensionless features with unital variance the applicability of pca as described above is limited by certain tacit assumptions made in its derivation 
in particular pca can capture linear correlations between the features but fails when this assumption is violated see figure in the reference 
in some cases coordinate transformations can restore the linearity assumption and pca can then be applied see kernel pca 
another limitation is the mean removal process before constructing the covariance matrix for pca 
in fields such as astronomy all the signals are non negative and the mean removal process will force the mean of some astrophysical exposures to be zero which consequently creates unphysical negative fluxes and forward modeling has to be performed to recover the true magnitude of the signals 
as an alternative method non negative matrix factorization focusing only on the non negative elements in the matrices which is well suited for astrophysical observations 
see more at relation between pca and non negative matrix factorization 
pca is at disadvantage if the data has not been standardized before applying the algorithm to it 
pca transforms original data into data that is relevant to the principal components of that data which means that the new data variables cannot be interpreted in the same ways that the originals were 
they are linear interpretations of the original variables 
also if pca is not performed properly there is high likelihood of information loss pca relies on linear model 
if dataset has pattern hidden inside it that is nonlinear then pca can actually steer the analysis in the complete opposite direction of progress 
researchers at kansas state university discovered that the sampling error in their experiments impacted the bias of pca results 
if the number of subjects or blocks is smaller than and or the researcher is interested in pc beyond the first it may be better to first correct for the serial correlation before pca is conducted 
the researchers at kansas state also found that pca could be seriously biased if the autocorrelation structure of the data is not correctly handled 
pca and information theory dimensionality reduction results in loss of information in general 
pca based dimensionality reduction tends to minimize that information loss under certain signal and noise models 
under the assumption that that is that the data vector is the sum of the desired information bearing signal and noise signal one can show that pca can be optimal for dimensionality reduction from an information theoretic point of view 
in particular linsker showed that if is gaussian and is gaussian noise with covariance matrix proportional to the identity matrix the pca maximizes the mutual information between the desired information and the dimensionality reduced output if the noise is still gaussian and has covariance matrix proportional to the identity matrix that is the components of the vector are iid but the information bearing signal is non gaussian which is common scenario pca at least minimizes an upper bound on the information loss which is defined as 
the optimality of pca is also preserved if the noise is iid and at least more gaussian in terms of the kullback leibler divergence than the information bearing signal in general even if the above signal model holds pca loses its information theoretic optimality as soon as the noise becomes dependent 
computing pca using the covariance method the following is detailed description of pca using the covariance method see also here as opposed to the correlation method the goal is to transform given data set of dimension to an alternative data set of smaller dimension equivalently we are seeking to find the matrix where is the karhunen lo ve transform klt of matrix organize the data setsuppose you have data comprising set of observations of variables and you want to reduce the data so that each observation can be described with only variables suppose further that the data are arranged as set of data vectors with each representing single grouped observation of the variables 
write as row vectors each with elements 
place the row vectors into single matrix of dimensions calculate the empirical meanfind the empirical mean along each column place the calculated mean values into an empirical mean vector of dimensions calculate the deviations from the meanmean subtraction is an integral part of the solution towards finding principal component basis that minimizes the mean square error of approximating the data 
hence we proceed by centering the data as follows subtract the empirical mean vector from each row of the data matrix store mean subtracted data in the matrix where is an column vector of all for in some applications each variable column of may also be scaled to have variance equal to see score 
this step affects the calculated principal components but makes them independent of the units used to measure the different variables 
find the covariance matrixfind the empirical covariance matrix from matrix where is the conjugate transpose operator 
if consists entirely of real numbers which is the case in many applications the conjugate transpose is the same as the regular transpose 
the reasoning behind using instead of to calculate the covariance is bessel correction find the eigenvectors and eigenvalues of the covariance matrixcompute the matrix of eigenvectors which diagonalizes the covariance matrix where is the diagonal matrix of eigenvalues of this step will typically involve the use of computer based algorithm for computing eigenvectors and eigenvalues 
these algorithms are readily available as sub components of most matrix algebra systems such as sas matlab mathematica scipy idl interactive data language or gnu octave as well as opencv 
matrix will take the form of an diagonal matrix where is the jth eigenvalue of the covariance matrix and matrix also of dimension contains column vectors each of length which represent the eigenvectors of the covariance matrix the eigenvalues and eigenvectors are ordered and paired 
the jth eigenvalue corresponds to the jth eigenvector 
matrix denotes the matrix of right eigenvectors as opposed to left eigenvectors 
in general the matrix of right eigenvectors need not be the conjugate transpose of the matrix of left eigenvectors rearrange the eigenvectors and eigenvaluessort the columns of the eigenvector matrix and eigenvalue matrix in order of decreasing eigenvalue 
make sure to maintain the correct pairings between the columns in each matrix compute the cumulative energy content for each eigenvectorthe eigenvalues represent the distribution of the source data energy among each of the eigenvectors where the eigenvectors form basis for the data 
the cumulative energy content for the jth eigenvector is the sum of the energy content across all of the eigenvalues from through for select subset of the eigenvectors as basis vectorssave the first columns of as the matrix where use the vector as guide in choosing an appropriate value for the goal is to choose value of as small as possible while achieving reasonably high value of on percentage basis 
for example you may want to choose so that the cumulative energy is above certain threshold like percent 
in this case choose the smallest value of such that project the data onto the new basisthe projected data points are the rows of the matrix that is the first column of is the projection of the data points onto the first principal component the second column is the projection onto the second principal component etc 
derivation of pca using the covariance method let be dimensional random vector expressed as column vector 
without loss of generality assume has zero mean 
we want to find orthonormal transformation matrix so that px has diagonal covariance matrix that is px is random vector with all its distinct components pairwise uncorrelated 
quick computation assuming were unitary yields cov cov hence holds if and only if cov were diagonalisable by this is very constructive as cov is guaranteed to be non negative definite matrix and thus is guaranteed to be diagonalisable by some unitary matrix 
covariance free computation in practical implementations especially with high dimensional data large the naive covariance method is rarely used because it is not efficient due to high computational and memory costs of explicitly determining the covariance matrix 
the covariance free approach avoids the np operations of explicitly calculating and storing the covariance matrix xtx instead utilizing one of matrix free methods for example based on the function evaluating the product xt at the cost of np operations 
iterative computation one way to compute the first principal component efficiently is shown in the following pseudo code for data matrix with zero mean without ever computing its covariance matrix 
random vector of length norm do times vector of length for each row in rts is the eigenvalue error norm exit if error tolerance return this power iteration algorithm simply calculates the vector xt normalizes and places the result back in the eigenvalue is approximated by rt xtx which is the rayleigh quotient on the unit vector for the covariance matrix xtx 
if the largest singular value is well separated from the next largest one the vector gets close to the first principal component of within the number of iterations which is small relative to at the total cost cnp 
the power iteration convergence can be accelerated without noticeably sacrificing the small cost per iteration using more advanced matrix free methods such as the lanczos algorithm or the locally optimal block preconditioned conjugate gradient lobpcg method 
subsequent principal components can be computed one by one via deflation or simultaneously as block 
in the former approach imprecisions in already computed approximate principal components additively affect the accuracy of the subsequently computed principal components thus increasing the error with every new computation 
the latter approach in the block power method replaces single vectors and with block vectors matrices and every column of approximates one of the leading principal components while all columns are iterated simultaneously 
the main calculation is evaluation of the product xt 
implemented for example in lobpcg efficient blocking eliminates the accumulation of the errors allows using high level blas matrix matrix product functions and typically leads to faster convergence compared to the single vector one by one technique 
the nipals method non linear iterative partial least squares nipals is variant the classical power iteration with matrix deflation by subtraction implemented for computing the first few components in principal component or partial least squares analysis 
for very high dimensional datasets such as those generated in the omics sciences for example genomics metabolomics it is usually only necessary to compute the first few pcs 
the non linear iterative partial least squares nipals algorithm updates iterative approximations to the leading scores and loadings and by the power iteration multiplying on every iteration by on the left and on the right that is calculation of the covariance matrix is avoided just as in the matrix free implementation of the power iterations to xtx based on the function evaluating the product xt tx the matrix deflation by subtraction is performed by subtracting the outer product from leaving the deflated residual matrix used to calculate the subsequent leading pcs 
for large data matrices or matrices that have high degree of column collinearity nipals suffers from loss of orthogonality of pcs due to machine precision round off errors accumulated in each iteration and matrix deflation by subtraction 
gram schmidt re orthogonalization algorithm is applied to both the scores and the loadings at each iteration step to eliminate this loss of orthogonality 
nipals reliance on single vector multiplications cannot take advantage of high level blas and results in slow convergence for clustered leading singular values both these deficiencies are resolved in more sophisticated matrix free block solvers such as the locally optimal block preconditioned conjugate gradient lobpcg method 
online sequential estimation in an online or streaming situation with data arriving piece by piece rather than being stored in single batch it is useful to make an estimate of the pca projection that can be updated sequentially 
this can be done efficiently but requires different algorithms 
pca and qualitative variables in pca it is common that we want to introduce qualitative variables as supplementary elements 
for example many quantitative variables have been measured on plants 
for these plants some qualitative variables are available as for example the species to which the plant belongs 
these data were subjected to pca for quantitative variables 
when analyzing the results it is natural to connect the principal components to the qualitative variable species 
for this the following results are produced 
identification on the factorial planes of the different species for example using different colors 
representation on the factorial planes of the centers of gravity of plants belonging to the same species 
for each center of gravity and each axis value to judge the significance of the difference between the center of gravity and origin these results are what is called introducing qualitative variable as supplementary element 
this procedure is detailed in and husson pag and pag few software offer this option in an automatic way 
this is the case of spad that historically following the work of ludovic lebart was the first to propose this option and the package factominer 
applications intelligence the earliest application of factor analysis was in locating and measuring components of human intelligence 
it was believed that intelligence had various uncorrelated components such as spatial intelligence verbal intelligence induction deduction etc and that scores on these could be adduced by factor analysis from results on various tests to give single index known as the intelligence quotient iq 
the pioneering statistical psychologist spearman actually developed factor analysis in for his two factor theory of intelligence adding formal technique to the science of psychometrics 
in thurstone looked for factors of intelligence developing the notion of mental age 
standard iq tests today are based on this early work 
residential differentiation in shevky and williams introduced the theory of factorial ecology which dominated studies of residential differentiation from the to the 
neighbourhoods in city were recognizable or could be distinguished from one another by various characteristics which could be reduced to three by factor analysis 
these were known as social rank an index of occupational status familism or family size and ethnicity cluster analysis could then be applied to divide the city into clusters or precincts according to values of the three key factor variables 
an extensive literature developed around factorial ecology in urban geography but the approach went out of fashion after as being methodologically primitive and having little place in postmodern geographical paradigms 
one of the problems with factor analysis has always been finding convincing names for the various artificial factors 
in flood revived the factorial ecology approach to show that principal components analysis actually gave meaningful answers directly without resorting to factor rotation 
the principal components were actually dual variables or shadow prices of forces pushing people together or apart in cities 
the first component was accessibility the classic trade off between demand for travel and demand for space around which classical urban economics is based 
the next two components were disadvantage which keeps people of similar status in separate neighbourhoods mediated by planning and ethnicity where people of similar ethnic backgrounds try to co locate about the same time the australian bureau of statistics defined distinct indexes of advantage and disadvantage taking the first principal component of sets of key variables that were thought to be important 
these seifa indexes are regularly published for various jurisdictions and are used frequently in spatial analysis 
development indexes pca has been the only formal method available for the development of indexes which are otherwise hit or miss ad hoc undertaking 
the city development index was developed by pca from about indicators of city outcomes in survey of global cities 
the first principal component was subject to iterative regression adding the original variables singly until about of its variation was accounted for 
the index ultimately used about indicators but was good predictor of many more variables 
its comparative value agreed very well with subjective assessment of the condition of each city 
the coefficients on items of infrastructure were roughly proportional to the average costs of providing the underlying services suggesting the index was actually measure of effective physical and social investment in the city 
the country level human development index hdi from undp which has been published since and is very extensively used in development studies has very similar coefficients on similar indicators strongly suggesting it was originally constructed using pca 
population genetics in cavalli sforza and others pioneered the use of principal components analysis pca to summarise data on variation in human gene frequencies across regions 
the components showed distinctive patterns including gradients and sinusoidal waves 
they interpreted these patterns as resulting from specific ancient migration events 
since then pca has been ubiquitous in population genetics with thousands of papers using pca as display mechanism 
genetics varies largely according to proximity so the first two principal components actually show spatial distribution and may be used to map the relative geographical location of different population groups thereby showing individuals who have wandered from their original locations pca in genetics has been technically controversial in that the technique has been performed on discrete non normal variables and often on binary allele markers 
the lack of any measures of standard error in pca are also an impediment to more consistent usage 
in august the molecular biologist eran elhaik published theoretical paper in scientific reports analyzing pca applications 
he concluded that it was easy to manipulate the method which in his view generated results that were erroneous contradictory and absurd 
specifically he argued the results achieved in population genetics were characterized by cherry picking and circular reasoning 
market research and indexes of attitude market research has been an extensive user of pca 
it is used to develop customer satisfaction or customer loyalty scores for products and with clustering to develop market segments that may be targeted with advertising campaigns in much the same way as factorial ecology will locate geographical areas with similar characteristics pca rapidly transforms large amounts of data into smaller easier to digest variables that can be more rapidly and readily analyzed 
in any consumer questionnaire there are series of questions designed to elicit consumer attitudes and principal components seek out latent variables underlying these attitudes 
for example the oxford internet survey in asked people about their attitudes and beliefs and from these analysts extracted four principal component dimensions which they identified as escape social networking efficiency and problem creating another example from joe flood in extracted an attitudinal index toward housing from attitude questions in national survey of households in australia 
the first principal component represented general attitude toward property and home ownership 
the index or the attitude questions it embodied could be fed into general linear model of tenure choice 
the strongest determinant of private renting by far was the attitude index rather than income marital status or household type 
quantitative finance in quantitative finance principal component analysis can be directly applied to the risk management of interest rate derivative portfolios 
trading multiple swap instruments which are usually function of other market quotable swap instruments is sought to be reduced to usually or principal components representing the path of interest rates on macro basis 
converting risks to be represented as those to factor loadings or multipliers provides assessments and understanding beyond that available to simply collectively viewing risks to individual buckets 
pca has also been applied to equity portfolios in similar fashion both to portfolio risk and to risk return 
one application is to reduce portfolio risk where allocation strategies are applied to the principal portfolios instead of the underlying stocks 
second is to enhance portfolio return using the principal components to select stocks with upside potential 
neuroscience variant of principal components analysis is used in neuroscience to identify the specific properties of stimulus that increases neuron probability of generating an action potential 
this technique is known as spike triggered covariance analysis 
in typical application an experimenter presents white noise process as stimulus usually either as sensory input to test subject or as current injected directly into the neuron and records train of action potentials or spikes produced by the neuron as result 
presumably certain features of the stimulus make the neuron more likely to spike 
in order to extract these features the experimenter calculates the covariance matrix of the spike triggered ensemble the set of all stimuli defined and discretized over finite time window typically on the order of ms that immediately preceded spike 
the eigenvectors of the difference between the spike triggered covariance matrix and the covariance matrix of the prior stimulus ensemble the set of all stimuli defined over the same length time window then indicate the directions in the space of stimuli along which the variance of the spike triggered ensemble differed the most from that of the prior stimulus ensemble 
specifically the eigenvectors with the largest positive eigenvalues correspond to the directions along which the variance of the spike triggered ensemble showed the largest positive change compared to the varince of the prior 
since these were the directions in which varying the stimulus led to spike they are often good approximations of the sought after relevant stimulus features 
in neuroscience pca is also used to discern the identity of neuron from the shape of its action potential 
spike sorting is an important procedure because extracellular recording techniques often pick up signals from more than one neuron 
in spike sorting one first uses pca to reduce the dimensionality of the space of action potential waveforms and then performs clustering analysis to associate specific action potentials with individual neurons 
pca as dimension reduction technique is particularly suited to detect coordinated activities of large neuronal ensembles 
it has been used in determining collective variables that is order parameters during phase transitions in the brain 
relation with other methods correspondence analysis correspondence analysis ca was developed by jean paul benz cri and is conceptually similar to pca but scales the data which should be non negative so that rows and columns are treated equivalently 
it is traditionally applied to contingency tables 
ca decomposes the chi squared statistic associated to this table into orthogonal factors 
because ca is descriptive technique it can be applied to tables for which the chi squared statistic is appropriate or not 
several variants of ca are available including detrended correspondence analysis and canonical correspondence analysis 
one special extension is multiple correspondence analysis which may be seen as the counterpart of principal component analysis for categorical data 
factor analysis principal component analysis creates variables that are linear combinations of the original variables 
the new variables have the property that the variables are all orthogonal 
the pca transformation can be helpful as pre processing step before clustering 
pca is variance focused approach seeking to reproduce the total variable variance in which components reflect both common and unique variance of the variable 
pca is generally preferred for purposes of data reduction that is translating variable space into optimal factor space but not when the goal is to detect the latent construct or factors 
factor analysis is similar to principal component analysis in that factor analysis also involves linear combinations of variables 
different from pca factor analysis is correlation focused approach seeking to reproduce the inter correlations among variables in which the factors represent the common variance of variables excluding unique variance 
in terms of the correlation matrix this corresponds with focusing on explaining the off diagonal terms that is shared co variance while pca focuses on explaining the terms that sit on the diagonal 
however as side result when trying to reproduce the on diagonal terms pca also tends to fit relatively well the off diagonal correlations 
results given by pca and factor analysis are very similar in most situations but this is not always the case and there are some problems where the results are significantly different 
factor analysis is generally used when the research purpose is detecting data structure that is latent constructs or factors or causal modeling 
if the factor model is incorrectly formulated or the assumptions are not met then factor analysis will give erroneous results 
means clustering it has been asserted that the relaxed solution of means clustering specified by the cluster indicators is given by the principal components and the pca subspace spanned by the principal directions is identical to the cluster centroid subspace 
however that pca is useful relaxation of means clustering was not new result and it is straightforward to uncover counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions 
non negative matrix factorization non negative matrix factorization nmf is dimension reduction method where only non negative elements in the matrices are used which is therefore promising method in astronomy in the sense that astrophysical signals are non negative 
the pca components are orthogonal to each other while the nmf components are all non negative and therefore constructs non orthogonal basis 
in pca the contribution of each component is ranked based on the magnitude of its corresponding eigenvalue which is equivalent to the fractional residual variance frv in analyzing empirical data 
for nmf its components are ranked based only on the empirical frv curves 
the residual fractional eigenvalue plots that is as function of component number given total of components for pca has flat plateau where no data is captured to remove the quasi static noise then the curves dropped quickly as an indication of over fitting and captures random noise 
the frv curves for nmf is decreasing continuously when the nmf components are constructed sequentially indicating the continuous capturing of quasi static noise then converge to higher levels than pca indicating the less over fitting property of nmf 
iconography of correlations it is often difficult to interpret the principal components when the data include many variables of various origins or when some variables are qualitative 
this leads the pca user to delicate elimination of several variables 
if observations or variables have an excessive impact on the direction of the axes they should be removed and then projected as supplementary elements 
in addition it is necessary to avoid interpreting the proximities between the points close to the center of the factorial plane 
the iconography of correlations on the contrary which is not projection on system of axes does not have these drawbacks 
we can therefore keep all the variables 
the principle of the diagram is to underline the remarkable correlations of the correlation matrix by solid line positive correlation or dotted line negative correlation 
strong correlation is not remarkable if it is not direct but caused by the effect of third variable 
conversely weak correlations can be remarkable 
for example if variable depends on several independent variables the correlations of with each of them are weak and yet remarkable 
generalizations sparse pca particular disadvantage of pca is that the principal components are usually linear combinations of all input variables 
sparse pca overcomes this disadvantage by finding linear combinations that contain just few input variables 
it extends the classic method of principal component analysis pca for the reduction of dimensionality of data by adding sparsity constraint on the input variables 
several approaches have been proposed including regression framework convex relaxation semidefinite programming framework generalized power method framework an alternating maximization framework forward backward greedy search and exact methods using branch and bound techniques bayesian formulation framework the methodological and theoretical developments of sparse pca as well as its applications in scientific studies were recently reviewed in survey paper 
nonlinear pca most of the modern methods for nonlinear dimensionality reduction find their theoretical and algorithmic roots in pca or means 
pearson original idea was to take straight line or plane which will be the best fit to set of data points 
trevor hastie expanded on this concept by proposing principal curves as the natural extension for the geometric interpretation of pca which explicitly constructs manifold for data approximation followed by projecting the points onto it as is illustrated by fig 
see also the elastic map algorithm and principal geodesic analysis 
another popular generalization is kernel pca which corresponds to pca performed in reproducing kernel hilbert space associated with positive definite kernel 
in multilinear subspace learning pca is generalized to multilinear pca mpca that extracts features directly from tensor representations 
mpca is solved by performing pca in each mode of the tensor iteratively 
mpca has been applied to face recognition gait recognition etc 
mpca is further extended to uncorrelated mpca non negative mpca and robust mpca 
way principal component analysis may be performed with models such as tucker decomposition parafac multiple factor analysis co inertia analysis statis and distatis 
robust pca while pca finds the mathematically optimal method as in minimizing the squared error it is still sensitive to outliers in the data that produce large errors something that the method tries to avoid in the first place 
it is therefore common practice to remove outliers before computing pca 
however in some contexts outliers can be difficult to identify 
for example in data mining algorithms like correlation clustering the assignment of points to clusters and outliers is not known beforehand 
recently proposed generalization of pca based on weighted pca increases robustness by assigning different weights to data objects based on their estimated relevancy 
outlier resistant variants of pca have also been proposed based on norm formulations pca robust principal component analysis rpca via decomposition in low rank and sparse matrices is modification of pca that works well with respect to grossly corrupted observations 
similar techniques independent component analysis independent component analysis ica is directed to similar problems as principal component analysis but finds additively separable components rather than successive approximations 
network component analysis given matrix it tries to decompose it into two matrices such that key difference from techniques such as pca and ica is that some of the entries of are constrained to be here is termed the regulatory layer 
while in general such decomposition can have multiple solutions they prove that if the following conditions are satisfied has full column rank each column of must have at least zeroes where is the number of columns of or alternatively the number of rows of 
the justification for this criterion is that if node is removed from the regulatory layer along with all the output nodes connected to it the result must still be characterized by connectivity matrix with full column rank 
must have full row rank then the decomposition is unique up to multiplication by scalar 
discriminant analysis of principal components discriminant analysis of principal components dapc is multivariate method used to identify and describe clusters of genetically related individuals 
genetic variation is partitioned into two components variation between groups and within groups and it maximizes the former 
linear discriminants are linear combinations of alleles which best separate the clusters 
alleles that most contribute to this discrimination are therefore those that are the most markedly different across groups 
the contributions of alleles to the groupings identified by dapc can allow identifying regions of the genome driving the genetic divergence among groups in dapc data is first transformed using principal components analysis pca and subsequently clusters are identified using discriminant analysis da 
dapc can be realized on using the package adegenet 
more info adegenet on the web software source code alglib and library that implements pca and truncated pca analytica the built in eigendecomp function computes principal components 
elki includes pca for projection including robust variants of pca as well as pca based clustering algorithms 
gretl principal component analysis can be performed either via the pca command or via the princomp function 
julia supports pca with the pca function in the multivariatestats package knime java based nodal arranging software for analysis in this the nodes called pca pca compute pca apply pca inverse make it easily 
mathematica implements principal component analysis with the principalcomponents command using both covariance and correlation methods 
mathphp php mathematics library with support for pca 
matlab the svd function is part of the basic system 
in the statistics toolbox the functions princomp and pca give the principal components while the function pcares gives the residuals and reconstructed matrix for low rank pca approximation 
matplotlib python library have pca package in the mlab module 
mlpack provides an implementation of principal component analysis in 
nag library principal components analysis is implemented via the aa routine available in both the fortran versions of the library 
nmath proprietary numerical library containing pca for the net framework 
gnu octave free software computational environment mostly compatible with matlab the function princomp gives the principal component 
opencv oracle database implemented via dbms data mining svds scoring mode by specifying setting value svds scoring pca orange software integrates pca in its visual programming environment 
pca displays scree plot degree of explained variance where user can interactively select the number of principal components 
origin contains pca in its pro version 
qlucore commercial software for analyzing multivariate data with instant response using pca 
free statistical package the functions princomp and prcomp can be used for principal component analysis prcomp uses singular value decomposition which generally gives better numerical accuracy 
some packages that implement pca in include but are not limited to ade vegan exposition dimred and factominer 
sas proprietary software for example see scikit learn python library for machine learning which contains pca probabilistic pca kernel pca sparse pca and other techniques in the decomposition module 
spss proprietary software most commonly used by social scientists for pca factor analysis and associated cluster analysis 
weka java library for machine learning which contains modules for computing principal components 
see also references further reading jackson 
user guide to principal components wiley 
springer series in statistics 
springer series in statistics 
new york springer verlag 
isbn husson fran ois bastien pag me 
exploratory multivariate analysis by example using chapman hall crc the series london 
multiple factor analysis by example using chapman hall crc the series london external links university of copenhagen video by rasmus bro on youtube stanford university video by andrew ng on youtube tutorial on principal component analysis layman introduction to principal component analysis on youtube video of less than seconds 
statquest principal component analysis pca clearly explained on youtube see also the list of software implementations
in the theory of vector spaces set of vectors is said to be linearly dependent if there is nontrivial linear combination of the vectors that equals the zero vector 
if no such linear combination exists then the vectors are said to be linearly independent 
these concepts are central to the definition of dimension vector space can be of finite dimension or infinite dimension depending on the maximum number of linearly independent vectors 
the definition of linear dependence and the ability to determine whether subset of vectors in vector space is linearly dependent are central to determining the dimension of vector space 
definition sequence of vectors from vector space is said to be linearly dependent if there exist scalars not all zero such that where denotes the zero vector 
this implies that at least one of the scalars is nonzero say and the above equation is able to be written as if and if thus set of vectors is linearly dependent if and only if one of them is zero or linear combination of the others 
sequence of vectors is said to be linearly independent if it is not linearly dependent that is if the equation can only be satisfied by for this implies that no vector in the sequence can be represented as linear combination of the remaining vectors in the sequence 
in other words sequence of vectors is linearly independent if the only representation of as linear combination of its vectors is the trivial representation in which all the scalars are zero 
even more concisely sequence of vectors is linearly independent if and only if can be represented as linear combination of its vectors in unique way 
if sequence of vectors contains the same vector twice it is necessarily dependent 
the linear dependency of sequence of vectors does not depend of the order of the terms in the sequence 
this allows defining linear independence for finite set of vectors finite set of vectors is linearly independent if the sequence obtained by ordering them is linearly independent 
in other words one has the following result that is often useful 
sequence of vectors is linearly independent if and only if it does not contain the same vector twice and the set of its vectors is linearly independent 
infinite case an infinite set of vectors is linearly independent if every nonempty finite subset is linearly independent 
conversely an infinite set of vectors is linearly dependent if it contains finite subset that is linearly dependent or equivalently if some vector in the set is linear combination of other vectors in the set 
an indexed family of vectors is linearly independent if it does not contain the same vector twice and if the set of its vectors is linearly independent 
otherwise the family is said linearly dependent 
set of vectors which is linearly independent and spans some vector space forms basis for that vector space 
for example the vector space of all polynomials in over the reals has the infinite subset as basis 
geometric examples and are independent and define the plane and are dependent because all three are contained in the same plane 
and are dependent because they are parallel to each other 
and are independent because and are independent of each other and is not linear combination of them or equivalently because they do not belong to common plane 
the three vectors define three dimensional space 
the vectors null vector whose components are equal to zero and are dependent since geographic location person describing the location of certain place might say it is miles north and miles east of here 
this is sufficient information to describe the location because the geographic coordinate system may be considered as dimensional vector space ignoring altitude and the curvature of the earth surface 
the person might add the place is miles northeast of here 
this last statement is true but it is not necessary to find the location 
in this example the miles north vector and the miles east vector are linearly independent 
that is to say the north vector cannot be described in terms of the east vector and vice versa 
the third miles northeast vector is linear combination of the other two vectors and it makes the set of vectors linearly dependent that is one of the three vectors is unnecessary to define specific location on plane 
also note that if altitude is not ignored it becomes necessary to add third vector to the linearly independent set 
in general linearly independent vectors are required to describe all locations in dimensional space 
evaluating linear independence the zero vector if one or more vectors from given sequence of vectors is the zero vector then the vector are necessarily linearly dependent and consequently they are not linearly independent 
to see why suppose that is an index 
an element of such that then let alternatively letting be equal any other non zero scalar will also work and then let all other scalars be explicitly this means that for any index other than 
for let so that consequently 
simplifying gives because not all scalars are zero in particular this proves that the vectors are linearly dependent 
as consequence the zero vector can not possibly belong to any collection of vectors that is linearly independent 
now consider the special case where the sequence of has length 
collection of vectors that consists of exactly one vector is linearly dependent if and only if that vector is zero 
explicitly if is any vector then the sequence which is sequence of length is linearly dependent if and only if alternatively the collection is linearly independent if and only if 
linear dependence and independence of two vectors this example considers the special case where there are exactly two vector and from some real or complex vector space 
the vectors and are linearly dependent if and only if at least one of the following is true is scalar multiple of explicitly this means that there exists scalar such that or is scalar multiple of explicitly this means that there exists scalar such that if then by setting we have this equality holds no matter what the value of is which shows that is true in this particular case 
similarly if then is true because if for instance if they are both equal to the zero vector then both and are true by using for both 
if then is only possible if and in this case it is possible to multiply both sides by to conclude this shows that if and then is true if and only if is true that is in this particular case either both and are true and the vectors are linearly dependent or else both and are false and the vectors are linearly independent 
if but instead then at least one of and must be zero 
moreover if exactly one of and is while the other is non zero then exactly one of and is true with the other being false 
the vectors and are linearly independent if and only if is not scalar multiple of and is not scalar multiple of 
vectors in three vectors consider the set of vectors and then the condition for linear dependence seeks set of non zero scalars such that or 
row reduce this matrix equation by subtracting the first row from the second to obtain 
continue the row reduction by dividing the second row by and then ii multiplying by and adding to the first row that is 
rearranging this equation allows us to obtain 
which shows that non zero ai exist such that can be defined in terms of and 
thus the three vectors are linearly dependent 
two vectors now consider the linear dependence of the two vectors and and check or 
the same row reduction presented above yields 
this shows that which means that the vectors and are linearly independent 
vectors in in order to determine if the three vectors in 
are linearly dependent form the matrix equation 
row reduce this equation to obtain 
rearrange to solve for and obtain 
this equation is easily solved to define non zero ai where can be chosen arbitrarily 
thus the vectors and are linearly dependent 
alternative method using determinants an alternative method relies on the fact that vectors in are linearly independent if and only if the determinant of the matrix formed by taking the vectors as its columns is non zero 
in this case the matrix formed by the vectors is 
we may write linear combination of the columns as 
we are interested in whether for some nonzero vector this depends on the determinant of which is det since the determinant is non zero the vectors and are linearly independent 
otherwise suppose we have vectors of coordinates with then is an matrix and is column vector with entries and we are again interested in as we saw previously this is equivalent to list of equations 
consider the first rows of the first equations any solution of the full list of equations must also be true of the reduced list 
in fact if im is any list of rows then the equation must be true for those rows 
furthermore the reverse is true 
that is we can test whether the vectors are linearly dependent by testing whether det for all possible lists of rows 
in case this requires only one determinant as above 
if then it is theorem that the vectors must be linearly dependent 
this fact is valuable for theory in practical calculations more efficient methods are available 
more vectors than dimensions if there are more vectors than dimensions the vectors are linearly dependent 
this is illustrated in the example above of three vectors in 
natural basis vectors let and consider the following elements in known as the natural basis vectors 
then are linearly independent 
linear independence of functions let be the vector space of all differentiable functions of real variable then the functions and in are linearly independent 
proof suppose and are two real numbers such that take the first derivative of the above equation for all values of we need to show that and in order to do this we subtract the first equation from the second giving since is not zero for some it follows that too 
therefore according to the definition of linear independence and are linearly independent 
space of linear dependencies linear dependency or linear relation among vectors vn is tuple an with scalar components such that if such linear dependence exists with at least nonzero component then the vectors are linearly dependent 
linear dependencies among vn form vector space 
if the vectors are expressed by their coordinates then the linear dependencies are the solutions of homogeneous system of linear equations with the coordinates of the vectors as coefficients 
basis of the vector space of linear dependencies can therefore be computed by gaussian elimination 
generalizations affine independence set of vectors is said to be affinely dependent if at least one of the vectors in the set can be defined as an affine combination of the others 
otherwise the set is called affinely independent 
any affine combination is linear combination therefore every affinely dependent set is linearly dependent 
conversely every linearly independent set is affinely independent 
consider set of vectors of size each and consider the set of augmented vectors of size each 
the original vectors are affinely independent if and only if the augmented vectors are linearly independent 
linearly independent vector subspaces two vector subspaces and of vector space are said to be linearly independent if 
more generally collection of subspaces of are said to be linearly independent if for every index where for all span the vector space is said to be direct sum of if these subspaces are linearly independent and 
see also matroid abstraction of linear independence of vectors references bachman george narici lawrence 
functional analysis second ed 
mineola new york dover publications 
external links linear independence encyclopedia of mathematics ems press linearly dependent functions at wolframmathworld 
tutorial and interactive program on linear independence 
introduction to linear independence at khanacademy
in statistical modeling regression analysis is set of statistical processes for estimating the relationships between dependent variable often called the outcome or response variable or label in machine learning parlance and one or more independent variables often called predictors covariates explanatory variables or features 
the most common form of regression analysis is linear regression in which one finds the line or more complex linear combination that most closely fits the data according to specific mathematical criterion 
for example the method of ordinary least squares computes the unique line or hyperplane that minimizes the sum of squared differences between the true data and that line or hyperplane 
for specific mathematical reasons see linear regression this allows the researcher to estimate the conditional expectation or population average value of the dependent variable when the independent variables take on given set of values 
less common forms of regression use slightly different procedures to estimate alternative location parameters quantile regression or necessary condition analysis or estimate the conditional expectation across broader collection of non linear models nonparametric regression 
regression analysis is primarily used for two conceptually distinct purposes 
first regression analysis is widely used for prediction and forecasting where its use has substantial overlap with the field of machine learning 
second in some situations regression analysis can be used to infer causal relationships between the independent and dependent variables 
importantly regressions by themselves only reveal relationships between dependent variable and collection of independent variables in fixed dataset 
to use regressions for prediction or to infer causal relationships respectively researcher must carefully justify why existing relationships have predictive power for new context or why relationship between two variables has causal interpretation 
the latter is especially important when researchers hope to estimate causal relationships using observational data 
history the earliest form of regression was the method of least squares which was published by legendre in and by gauss in legendre and gauss both applied the method to the problem of determining from astronomical observations the orbits of bodies about the sun mostly comets but also later the then newly discovered minor planets 
gauss published further development of the theory of least squares in including version of the gauss markov theorem 
the term regression was coined by francis galton in the th century to describe biological phenomenon 
the phenomenon was that the heights of descendants of tall ancestors tend to regress down towards normal average phenomenon also known as regression toward the mean 
for galton regression had only this biological meaning but his work was later extended by udny yule and karl pearson to more general statistical context 
in the work of yule and pearson the joint distribution of the response and explanatory variables is assumed to be gaussian 
this assumption was weakened by fisher in his works of and fisher assumed that the conditional distribution of the response variable is gaussian but the joint distribution need not be 
in this respect fisher assumption is closer to gauss formulation of in the and economists used electromechanical desk calculators to calculate regressions 
before it sometimes took up to hours to receive the result from one regression regression methods continue to be an area of active research 
in recent decades new methods have been developed for robust regression regression involving correlated responses such as time series and growth curves regression in which the predictor independent variable or response variables are curves images graphs or other complex data objects regression methods accommodating various types of missing data nonparametric regression bayesian methods for regression regression in which the predictor variables are measured with error regression with more predictor variables than observations and causal inference with regression 
regression model in practice researchers first select model they would like to estimate and then use their chosen method ordinary least squares to estimate the parameters of that model 
regression models involve the following components the unknown parameters often denoted as scalar or vector the independent variables which are observed in data and are often denoted as vector where denotes row of data 
the dependent variable which are observed in data and often denoted using the scalar the error terms which are not directly observed in data and are often denoted using the scalar in various fields of application different terminologies are used in place of dependent and independent variables 
most regression models propose that is function of and with representing an additive error term that may stand in for un modeled determinants of or random statistical noise the researchers goal is to estimate the function that most closely fits the data 
to carry out regression analysis the form of the function must be specified 
sometimes the form of this function is based on knowledge about the relationship between and that does not rely on the data 
if no such knowledge is available flexible or convenient form for is chosen 
for example simple univariate regression may propose suggesting that the researcher believes to be reasonable approximation for the statistical process generating the data 
once researchers determine their preferred statistical model different forms of regression analysis provide tools to estimate the parameters for example least squares including its most common variant ordinary least squares finds the value of that minimizes the sum of squared errors given regression method will ultimately provide an estimate of usually denoted to distinguish the estimate from the true unknown parameter value that generated the data 
using this estimate the researcher can then use the fitted value for prediction or to assess the accuracy of the model in explaining the data 
whether the researcher is intrinsically interested in the estimate or the predicted value will depend on context and their goals 
as described in ordinary least squares least squares is widely used because the estimated function approximates the conditional expectation 
however alternative variants least absolute deviations or quantile regression are useful when researchers want to model other functions 
it is important to note that there must be sufficient data to estimate regression model 
for example suppose that researcher has access to rows of data with one dependent and two independent variables 
suppose further that the researcher wants to estimate bivariate linear model via least squares if the researcher only has access to data points then they could find infinitely many combinations that explain the data equally well any combination can be chosen that satisfies all of which lead to and are therefore valid solutions that minimize the sum of squared residuals 
to understand why there are infinitely many options note that the system of equations is to be solved for unknowns which makes the system underdetermined 
alternatively one can visualize infinitely many dimensional planes that go through fixed points 
more generally to estimate least squares model with distinct parameters one must have distinct data points 
if then there does not generally exist set of parameters that will perfectly fit the data 
the quantity appears often in regression analysis and is referred to as the degrees of freedom in the model 
moreover to estimate least squares model the independent variables 
must be linearly independent one must not be able to reconstruct any of the independent variables by adding and multiplying the remaining independent variables 
as discussed in ordinary least squares this condition ensures that is an invertible matrix and therefore that unique solution exists 
underlying assumptions by itself regression is simply calculation using the data 
in order to interpret the output of regression as meaningful statistical quantity that measures real world relationships researchers often rely on number of classical assumptions 
these assumptions often include the sample is representative of the population at large 
the independent variables are measured with no error 
deviations from the model have an expected value of zero conditional on covariates the variance of the residuals is constant across observations homoscedasticity 
the residuals are uncorrelated with one another 
mathematically the variance covariance matrix of the errors is diagonal handful of conditions are sufficient for the least squares estimator to possess desirable properties in particular the gauss markov assumptions imply that the parameter estimates will be unbiased consistent and efficient in the class of linear unbiased estimators 
practitioners have developed variety of methods to maintain some or all of these desirable properties in real world settings because these classical assumptions are unlikely to hold exactly 
for example modeling errors in variables can lead to reasonable estimates independent variables are measured with errors 
heteroscedasticity consistent standard errors allow the variance of to change across values of correlated errors that exist within subsets of the data or follow specific patterns can be handled using clustered standard errors geographic weighted regression or newey west standard errors among other techniques 
when rows of data correspond to locations in space the choice of how to model within geographic units can have important consequences 
the subfield of econometrics is largely focused on developing techniques that allow researchers to make reasonable real world conclusions in real world settings where classical assumptions do not hold exactly 
linear regression in linear regression the model specification is that the dependent variable is linear combination of the parameters but need not be linear in the independent variables 
for example in simple linear regression for modeling data points there is one independent variable and two parameters and straight line in multiple linear regression there are several independent variables or functions of independent variables 
adding term in to the preceding regression gives parabola this is still linear regression although the expression on the right hand side is quadratic in the independent variable it is linear in the parameters and in both cases is an error term and the subscript indexes particular observation 
returning our attention to the straight line case given random sample from the population we estimate the population parameters and obtain the sample linear regression model the residual is the difference between the value of the dependent variable predicted by the model and the true value of the dependent variable one method of estimation is ordinary least squares 
this method obtains parameter estimates that minimize the sum of squared residuals ssr minimization of this function results in set of normal equations set of simultaneous linear equations in the parameters which are solved to yield the parameter estimators in the case of simple regression the formulas for the least squares estimates are where is the mean average of the values and is the mean of the values 
under the assumption that the population error term has constant variance the estimate of that variance is given by this is called the mean square error mse of the regression 
the denominator is the sample size reduced by the number of model parameters estimated from the same data for regressors or if an intercept is used 
in this case so the denominator is the standard errors of the parameter estimates are given by under the further assumption that the population error term is normally distributed the researcher can use these estimated standard errors to create confidence intervals and conduct hypothesis tests about the population parameters 
general linear model in the more general multiple regression model there are independent variables where is the th observation on the th independent variable 
if the first independent variable takes the value for all then is called the regression intercept 
the least squares parameter estimates are obtained from normal equations 
the residual can be written as the normal equations are in matrix notation the normal equations are written as where the element of is the element of the column vector is and the element of is thus is is and is the solution is 
diagnostics once regression model has been constructed it may be important to confirm the goodness of fit of the model and the statistical significance of the estimated parameters 
commonly used checks of goodness of fit include the squared analyses of the pattern of residuals and hypothesis testing 
statistical significance can be checked by an test of the overall fit followed by tests of individual parameters 
interpretations of these diagnostic tests rest heavily on the model assumptions 
although examination of the residuals can be used to invalidate model the results of test or test are sometimes more difficult to interpret if the model assumptions are violated 
for example if the error term does not have normal distribution in small samples the estimated parameters will not follow normal distributions and complicate inference 
with relatively large samples however central limit theorem can be invoked such that hypothesis testing may proceed using asymptotic approximations 
limited dependent variables limited dependent variables which are response variables that are categorical variables or are variables constrained to fall only in certain range often arise in econometrics 
the response variable may be non continuous limited to lie on some subset of the real line 
for binary zero or one variables if analysis proceeds with least squares linear regression the model is called the linear probability model 
nonlinear models for binary dependent variables include the probit and logit model 
the multivariate probit model is standard method of estimating joint relationship between several binary dependent variables and some independent variables 
for categorical variables with more than two values there is the multinomial logit 
for ordinal variables with more than two values there are the ordered logit and ordered probit models 
censored regression models may be used when the dependent variable is only sometimes observed and heckman correction type models may be used when the sample is not randomly selected from the population of interest 
an alternative to such procedures is linear regression based on polychoric correlation or polyserial correlations between the categorical variables 
such procedures differ in the assumptions made about the distribution of the variables in the population 
if the variable is positive with low values and represents the repetition of the occurrence of an event then count models like the poisson regression or the negative binomial model may be used 
nonlinear regression when the model function is not linear in the parameters the sum of squares must be minimized by an iterative procedure 
this introduces many complications which are summarized in differences between linear and non linear least squares 
interpolation and extrapolation regression models predict value of the variable given known values of the variables 
prediction within the range of values in the dataset used for model fitting is known informally as interpolation 
prediction outside this range of the data is known as extrapolation 
performing extrapolation relies strongly on the regression assumptions 
the further the extrapolation goes outside the data the more room there is for the model to fail due to differences between the assumptions and the sample data or the true values 
it is generally advised that when performing extrapolation one should accompany the estimated value of the dependent variable with prediction interval that represents the uncertainty 
such intervals tend to expand rapidly as the values of the independent variable moved outside the range covered by the observed data 
for such reasons and others some tend to say that it might be unwise to undertake extrapolation however this does not cover the full set of modeling errors that may be made in particular the assumption of particular form for the relation between and properly conducted regression analysis will include an assessment of how well the assumed form is matched by the observed data but it can only do so within the range of values of the independent variables actually available 
this means that any extrapolation is particularly reliant on the assumptions being made about the structural form of the regression relationship 
best practice advice here is that linear in variables and linear in parameters relationship should not be chosen simply for computational convenience but that all available knowledge should be deployed in constructing regression model 
if this knowledge includes the fact that the dependent variable cannot go outside certain range of values this can be made use of in selecting the model even if the observed dataset has no values particularly near such bounds 
the implications of this step of choosing an appropriate functional form for the regression can be great when extrapolation is considered 
at minimum it can ensure that any extrapolation arising from fitted model is realistic or in accord with what is known 
power and sample size calculations there are no generally agreed methods for relating the number of observations versus the number of independent variables in the model 
one method conjectured by good and hardin is where is the sample size is the number of independent variables and is the number of observations needed to reach the desired precision if the model had only one independent variable 
for example researcher is building linear regression model using dataset that contains patients 
if the researcher decides that five observations are needed to precisely define straight line then the maximum number of independent variables the model can support is because log log 
other methods although the parameters of regression model are usually estimated using the method of least squares other methods which have been used include bayesian methods 
bayesian linear regression percentage regression for situations where reducing percentage errors is deemed more appropriate 
least absolute deviations which is more robust in the presence of outliers leading to quantile regression nonparametric regression requires large number of observations and is computationally intensive scenario optimization leading to interval predictor models distance metric learning which is learned by the search of meaningful distance metric in given input space 
software all major statistical software packages perform least squares regression analysis and inference 
simple linear regression and multiple regression using least squares can be done in some spreadsheet applications and on some calculators 
while many statistical software packages can perform various types of nonparametric and robust regression these methods are less standardized 
different software packages implement different methods and method with given name may be implemented differently in different packages 
specialized regression software has been developed for use in fields such as survey analysis and neuroimaging 
see also references further reading william kruskal and judith tanur ed 
linear hypotheses international encyclopedia of statistics 
free press evan williams regression pp 
analysis of variance pp 
regression and correlation analysis new palgrave dictionary of economics pp 
birkes david and dodge alternative methods of regression 
isbn chatfield calculating interval forecasts journal of business and economic statistics pp 
applied regression analysis rd ed 
applied regression analysis linear models and related methods 
sage hardle applied nonparametric regression isbn meade nigel islam towhidul 
prediction intervals for growth curve forecasts 
sen srivastava regression analysis theory methods and applications springer verlag berlin th printing 
strutz data fitting and uncertainty practical introduction to weighted least squares and beyond 
vieweg teubner isbn stulp freek and olivier sigaud 
many regression algorithms one unified model review 
https doi org neunet 
operations and production systems with multiple objectives 
external links regression analysis encyclopedia of mathematics ems press earliest uses regression basic history and references what is multiple regression used for 
multiple regression regression of weakly correlated data how linear regression mistakes can appear when range is much smaller than range
functional principal component analysis fpca is statistical method for investigating the dominant modes of variation of functional data 
using this method random function is represented in the eigenbasis which is an orthonormal basis of the hilbert space that consists of the eigenfunctions of the autocovariance operator 
fpca represents functional data in the most parsimonious way in the sense that when using fixed number of basis functions the eigenfunction basis explains more variation than any other basis expansion 
fpca can be applied for representing random functions or in functional regression and classification 
formulation for square integrable stochastic process let and cov where 
are the eigenvalues and are the orthonormal eigenfunctions of the linear hilbert schmidt operator by the karhunen lo ve theorem one can express the centered process in the eigenbasis where is the principal component associated with the th eigenfunction with the properties var and for the centered process is then equivalent to common assumption is that can be represented by only the first few eigenfunctions after subtracting the mean function 
interpretation of eigenfunctions the first eigenfunction depicts the dominant mode of variation of var where the th eigenfunction is the dominant mode of variation orthogonal to for var where for 
estimation let yij xi tij ij be the observations made at locations usually time points tij where xi is the th realization of the smooth stochastic process that generates the data and ij are identically and independently distributed normal random variable with mean and variance mi 
to obtain an estimate of the mean function tij if dense sample on regular grid is available one may take the average at each location tij if the observations are sparse one needs to smooth the data pooled from all observations to obtain the mean estimate using smoothing methods like local linear smoothing or spline smoothing 
then the estimate of the covariance function is obtained by averaging in the dense case or smoothing in the sparse case the raw covariances note that the diagonal elements of gi should be removed because they contain measurement error in practice is discretized to an equal spaced dense grid and the estimation of eigenvalues and eigenvectors vk is carried out by numerical linear algebra 
the eigenfunction estimates can then be obtained by interpolating the eigenvectors 
the fitted covariance should be positive definite and symmetric and is then obtained as 
let be smoothed version of the diagonal elements gi tij tij of the raw covariance matrices 
then is an estimate of 
an estimate of is obtained by if otherwise if the observations xij mi are dense in then the th fpc can be estimated by numerical integration implementing 
however if the observations are sparse this method will not work 
instead one can use best linear unbiased predictors yielding where and is evaluated at the grid points generated by tij mi 
the algorithm pace has an available matlab package and packageasymptotic convergence properties of these estimates have been investigated 
applications fpca can be applied for displaying the modes of functional variation in scatterplots of fpcs against each other or of responses against fpcs for modeling sparse longitudinal data or for functional regression and classification functional linear regression 
scree plots and other methods can be used to determine the number of included components 
functional principal component analysis has varied applications in time series analysis 
nowadays this methodology is being adapted from traditional multi variate techniques to carry out analysis on financial data sets such as stock market indices generation of implied volatility graphs and so on 
very nice example of the advantages of the functional approach is the smoothed fpca spca proposed by silverman and studied by pezzulli and silverman that enables direct combination of the fpca analysis together with general smoothing approach that makes the use of the information stored in some linear differential operators possible 
an important application of the fpca already known from multivariate pca is motivated by the karhunen lo ve decomposition of random function to the set of functional parameters factor functions and corresponding factor loadings scalar random variables 
this application is much more important than in the standard multivariate pca since the distribution of the random function is in general too complex to be directly analyzed and the karhunen lo ve decomposition reduces the analysis to the interpretation of the factor functions and the distribution of scalar random variables 
due to dimensionality reduction as well as its accuracy to represent data there is wide scope for further developments of functional principal component techniques in the financial field 
connection with principal component analysis the following table shows comparison of various elements of principal component analysis pca and fpca 
the two methods are both used for dimensionality reduction 
in implementations fpca uses pca step 
however pca and fpca differ in some critical aspects 
first the order of multivariate data in pca can be permuted which has no effect on the analysis but the order of functional data carries time or space information and cannot be reordered 
second the spacing of observations in fpca matters while there is no spacing issue in pca 
third regular pca does not work for high dimensional data without regularization while fpca has built in regularization due to the smoothness of the functional data and the truncation to finite number of included components 
see also principal component analysis notes references james ramsay silverman june
control variable or scientific constant in scientific experimentation is an experimental element which is constant controlled and unchanged throughout the course of the investigation 
control variables could strongly influence experimental results were they not held constant during the experiment in order to test the relative relationship of the dependent variable dv and independent variable iv 
the control variables themselves are not of primary interest to the experimenter 
usage variable in an experiment which is held constant in order to assess the relationship between multiple variables is control variable 
control variable is an element that is not changed throughout an experiment because its unchanging state allows better understanding of the relationship between the other variables being tested in any system existing in natural state many variables may be interdependent with each affecting the other 
scientific experiments test the relationship of an iv or independent variable that element that is manipulated by the experimenter to the dv or dependent variable that element affected by the manipulation of the iv 
any additional independent variable can be control variable control variable is an experimental condition or element that is kept the same throughout the experiment and it is not of primary concern in the experiment nor will it influence the outcome of the experiment 
uncontrolled change in control variable during an experiment would invalidate the correlation of dependent variables dv to the independent variable iv thus skewing the results and invalidating the working hypothesis 
this indicates the presence of spurious relationship existing within experimental parameters 
unexpected results may result from the presence of confounding variable thus requiring re working of the initial experimental hypothesis 
confounding variables are threat to the internal validity of an experiment 
this situation may be resolved by first identifying the confounding variable and then redesigning the experiment taking that information into consideration 
one way to this is to control the confounding variable thus making it control variable 
if however the spurious relationship cannot be identified the working hypothesis may have to be abandoned 
experimental examples take for example the well known combined gas law which is stated mathematically as where is the pressure is the volume is the thermodynamic temperature measured in kelvins is constant with units of energy divided by temperature which shows that the ratio between the pressure volume product and the temperature of system remains constant in an experimental verification of parts of the combined gas law where pressure temperature and volume are all variables to test the resultant changes to any of these variables requires at least one to be kept constant 
this is in order to see comparable experimental results in the remaining variables 
if temperature is made the control variable and it is not allowed to change throughout the course of the experiment the relationship between the dependent variables pressure and volume can quickly be established by changing the value for one or the other and this is boyle law 
for instance if the pressure is raised then the volume must decrease 
if however volume is made the control variable and it is not allowed to change throughout the course of the experiment the relationship between dependent variables pressure and temperature can quickly be established by changing the value for one or the other and this is gay lussac law 
for instance if the pressure is raised then the temperature must increase 
notes references external links definitions science buddies science fair projects
linear combination of atomic orbitals or lcao is quantum superposition of atomic orbitals and technique for calculating molecular orbitals in quantum chemistry 
in quantum mechanics electron configurations of atoms are described as wavefunctions 
in mathematical sense these wave functions are the basis set of functions the basis functions which describe the electrons of given atom 
in chemical reactions orbital wavefunctions are modified 
the electron cloud shape is changed according to the type of atoms participating in the chemical bond 
it was introduced in by sir john lennard jones with the description of bonding in the diatomic molecules of the first main row of the periodic table but had been used earlier by linus pauling for 
mathematical description an initial assumption is that the number of molecular orbitals is equal to the number of atomic orbitals included in the linear expansion 
in sense atomic orbitals combine to form molecular orbitals which can be numbered to and which may not all be the same 
the expression linear expansion for the th molecular orbital would be or where is molecular orbital represented as the sum of atomic orbitals each multiplied by corresponding coefficient and numbered to represents which atomic orbital is combined in the term 
the coefficients are the weights of the contributions of the atomic orbitals to the molecular orbital 
the hartree fock method is used to obtain the coefficients of the expansion 
the orbitals are thus expressed as linear combinations of basis functions and the basis functions are single electron functions which may or may not be centered on the nuclei of the component atoms of the molecule 
in either case the basis functions are usually also referred to as atomic orbitals even though only in the former case this name seems to be adequate 
the atomic orbitals used are typically those of hydrogen like atoms since these are known analytically 
slater type orbitals but other choices are possible such as the gaussian functions from standard basis sets or the pseudo atomic orbitals from plane wave pseudopotentials 
by minimizing the total energy of the system an appropriate set of coefficients of the linear combinations is determined 
this quantitative approach is now known as the hartree fock method 
however since the development of computational chemistry the lcao method often refers not to an actual optimization of the wave function but to qualitative discussion which is very useful for predicting and rationalizing results obtained via more modern methods 
in this case the shape of the molecular orbitals and their respective energies are deduced approximately from comparing the energies of the atomic orbitals of the individual atoms or molecular fragments and applying some recipes known as level repulsion and the like 
the graphs that are plotted to make this discussion clearer are called correlation diagrams 
the required atomic orbital energies can come from calculations or directly from experiment via koopmans theorem 
this is done by using the symmetry of the molecules and orbitals involved in bonding and thus is sometimes called symmetry adapted linear combination salc 
the first step in this process is assigning point group to the molecule 
each operation in the point group is performed upon the molecule 
the number of bonds that are unmoved is the character of that operation 
this reducible representation is decomposed into the sum of irreducible representations 
these irreducible representations correspond to the symmetry of the orbitals involved 
molecular orbital diagrams provide simple qualitative lcao treatment 
the ckel method the extended ckel method and the pariser parr pople method provide some quantitative theories 
see also quantum chemistry computer programs hartree fock method basis set chemistry tight binding holstein herring method external links lcao chemistry umeche maine edu link references
in statistics mediation model seeks to identify and explain the mechanism or process that underlies an observed relationship between an independent variable and dependent variable via the inclusion of third hypothetical variable known as mediator variable also mediating variable intermediary variable or intervening variable 
rather than direct causal relationship between the independent variable and the dependent variable mediation model proposes that the independent variable influences the mediator variable which in turn influences the dependent variable 
thus the mediator variable serves to clarify the nature of the relationship between the independent and dependent variables mediation analyses are employed to understand known relationship by exploring the underlying mechanism or process by which one variable influences another variable through mediator variable 
in particular mediation analysis can contribute to better understanding the relationship between an independent variable and dependent variable when these variables do not have an obvious direct connection 
baron and kenny steps for mediation analysis baron and kenny laid out several requirements that must be met to form true mediation relationship 
they are outlined below using real world example 
see the diagram above for visual representation of the overall mediating relationship to be explained 
note hayes critiqued baron and kenny mediation steps approach and as of david kenny on his website stated that mediation can exist in the absence of significant total effect and therefore step below may not be needed 
this situation is sometimes referred to as inconsistent mediation 
later publications by hayes also questioned the concepts of full or partial mediation and advocated for these terms along with the classical mediation steps approach outlined below to be abandoned 
step regress the dependent variable on the independent variable to confirm that the independent variable is significant predictor of the dependent variable independent variable dependent variable is significant step regress the mediator on the independent variable to confirm that the independent variable is significant predictor of the mediator 
if the mediator is not associated with the independent variable then it couldn possibly mediate anything independent variable mediator is significant step regress the dependent variable on both the mediator and independent variable to confirm that the mediator is significant predictor of the dependent variable and the strength of the coefficient of the previously significant independent variable in step is now greatly reduced if not rendered nonsignificant 
is significant should be smaller in absolute value than the original effect for the independent variable above example the following example drawn from howell explains each step of baron and kenny requirements to understand further how mediation effect is characterized 
step and step use simple regression analysis whereas step uses multiple regression analysis 
how you were parented independent variable predicts how confident you feel about parenting your own children dependent variable 
how you were parented independent variable predicts your feelings of competence and self esteem mediator 
your feelings of competence and self esteem mediator predict how confident you feel about parenting your own children dependent variable while controlling for how you were parented independent variable such findings would lead to the conclusion implying that your feelings of competence and self esteem mediate the relationship between how you were parented and how confident you feel about parenting your own children 
if step does not yield significant result one may still have grounds to move to step sometimes there is actually significant relationship between independent and dependent variables but because of small sample sizes or other extraneous factors there could not be enough power to predict the effect that actually exists 
direct versus indirect effects in the diagram shown above the indirect effect is the product of path coefficients and 
the direct effect is the coefficient 
the direct effect measures the extent to which the dependent variable changes when the independent variable increases by one unit and the mediator variable remains unaltered 
in contrast the indirect effect measures the extent to which the dependent variable changes when the independent variable is held constant and the mediator variable changes by the amount it would have changed had the independent variable increased by one unit 
in linear systems the total effect is equal to the sum of the direct and indirect ab in the model above 
in nonlinear models the total effect is not generally equal to the sum of the direct and indirect effects but to modified combination of the two 
full versus partial mediation mediator variable can either account for all or some of the observed relationship between two variables 
full mediation maximum evidence for mediation also called full mediation would occur if inclusion of the mediation variable drops the relationship between the independent variable and dependent variable see pathway in diagram above to zero 
partial mediation partial mediation maintains that the mediating variable accounts for some but not all of the relationship between the independent variable and dependent variable 
partial mediation implies that there is not only significant relationship between the mediator and the dependent variable but also some direct relationship between the independent and dependent variable 
in order for either full or partial mediation to be established the reduction in variance explained by the independent variable must be significant as determined by one of several tests such as the sobel test 
the effect of an independent variable on the dependent variable can become nonsignificant when the mediator is introduced simply because trivial amount of variance is explained not true mediation 
thus it is imperative to show significant reduction in variance explained by the independent variable before asserting either full or partial mediation 
it is possible to have statistically significant indirect effects in the absence of total effect 
this can be explained by the presence of several mediating paths that cancel each other out and become noticeable when one of the cancelling mediators is controlled for 
this implies that the terms partial and full mediation should always be interpreted relative to the set of variables that are present in the model 
in all cases the operation of fixing variable must be distinguished from that of controlling for variable which has been inappropriately used in the literature 
the former stands for physically fixing while the latter stands for conditioning on adjusting for or adding to the regression model 
the two notions coincide only when all error terms not shown in the diagram are statistically uncorrelated 
when errors are correlated adjustments must be made to neutralize those correlations before embarking on mediation analysis see bayesian networks 
sobel test sobel test is performed to determine if the relationship between the independent variable and dependent variable has been significantly reduced after inclusion of the mediator variable 
in other words this test assesses whether mediation effect is significant 
it examines the relationship between the independent variable and the dependent variable compared to the relationship between the independent variable and dependent variable including the mediation factor 
the sobel test is more accurate than the baron and kenny steps explained above however it does have low statistical power 
as such large sample sizes are required in order to have sufficient power to detect significant effects 
this is because the key assumption of sobel test is the assumption of normality 
because sobel test evaluates given sample on the normal distribution small sample sizes and skewness of the sampling distribution can be problematic see normal distribution for more details 
thus the rule of thumb as suggested by mackinnon et al is that sample size of is required to detect small effect sample size of is sufficient in detecting medium effect and sample size of is required to detect large effect 
the equation for sobel is preacher hayes bootstrap method the bootstrapping method provides some advantages to the sobel test primarily an increase in power 
the preacher and hayes bootstrapping method is non parametric test 
as such the bootstrap method does not violate assumptions of normality and is therefore recommended for small sample sizes 
bootstrapping involves repeatedly randomly sampling observations with replacement from the data set to compute the desired statistic in each resample 
computing over hundreds or thousands of bootstrap resamples provide an approximation of the sampling distribution of the statistic of interest 
the preacher hayes method provides point estimates and confidence intervals by which one can assess the significance or nonsignificance of mediation effect 
point estimates reveal the mean over the number of bootstrapped samples and if zero does not fall between the resulting confidence intervals of the bootstrapping method one can confidently conclude that there is significant mediation effect to report 
significance of mediation as outlined above there are few different options one can choose from to evaluate mediation model 
bootstrapping is becoming the most popular method of testing mediation because it does not require the normality assumption to be met and because it can be effectively utilized with smaller sample sizes 
however mediation continues to be most frequently determined using the logic of baron and kenny or the sobel test 
it is becoming increasingly more difficult to publish tests of mediation based purely on the baron and kenny method or tests that make distributional assumptions such as the sobel test 
thus it is important to consider your options when choosing which test to conduct 
approaches to mediation while the concept of mediation as defined within psychology is theoretically appealing the methods used to study mediation empirically have been challenged by statisticians and epidemiologists and interpreted formally 
experimental causal chain design an experimental causal chain design is used when the proposed mediator is experimentally manipulated 
such design implies that one manipulates some controlled third variable that they have reason to believe could be the underlying mechanism of given relationship 
measurement of mediation design measurement of mediation design can be conceptualized as statistical approach 
such design implies that one measures the proposed intervening variable and then uses statistical analyses to establish mediation 
this approach does not involve manipulation of the hypothesized mediating variable but only involves measurement 
criticisms of mediation measurement experimental approaches to mediation must be carried out with caution 
first it is important to have strong theoretical support for the exploratory investigation of potential mediating variable 
criticism of mediation approach rests on the ability to manipulate and measure mediating variable 
thus one must be able to manipulate the proposed mediator in an acceptable and ethical fashion 
as such one must be able to measure the intervening process without interfering with the outcome 
the mediator must also be able to establish construct validity of manipulation 
one of the most common criticisms of the measurement of mediation approach is that it is ultimately correlational design 
consequently it is possible that some other third variable independent from the proposed mediator could be responsible for the proposed effect 
however researchers have worked hard to provide counter evidence to this disparagement 
specifically the following counter arguments have been put forward temporal precedence for example if the independent variable precedes the dependent variable in time this would provide evidence suggesting directional and potentially causal link from the independent variable to the dependent variable 
nonspuriousness and or no confounds for example should one identify other third variables and prove that they do not alter the relationship between the independent variable and the dependent variable he she would have stronger argument for their mediation effect 
see other rd variables below mediation can be an extremely useful and powerful statistical test however it must be used properly 
it is important that the measures used to assess the mediator and the dependent variable are theoretically distinct and that the independent variable and mediator cannot interact 
should there be an interaction between the independent variable and the mediator one would have grounds to investigate moderation 
other third variables confounding another model that is often tested is one in which competing variables in the model are alternative potential mediators or an unmeasured cause of the dependent variable 
an additional variable in causal model may obscure or confound the relationship between the independent and dependent variables 
potential confounders are variables that may have causal impact on both the independent variable and dependent variable 
they include common sources of measurement error as discussed above as well as other influences shared by both the independent and dependent variables 
in experimental studies there is special concern about aspects of the experimental manipulation or setting that may account for study effects rather than the motivating theoretical factor 
any of these problems may produce spurious relationships between the independent and dependent variables as measured 
ignoring confounding variable may bias empirical estimates of the causal effect of the independent variable 
suppression suppressor variable increases the predictive validity of another variable when included in regression equation 
suppression can occur when single causal variable is related to an outcome variable through two separate mediator variables and when one of those mediated effects is positive and one is negative 
in such case each mediator variable suppresses or conceals the effect that is carried through the other mediator variable 
for example higher intelligence scores causal variable may cause an increase in error detection mediator variable which in turn may cause decrease in errors made at work on an assembly line an outcome variable at the same time intelligence could also cause an increase in boredom which in turn may cause an increase in errors 
thus in one causal path intelligence decreases errors and in the other it increases them 
when neither mediator is included in the analysis intelligence appears to have no effect or weak effect on errors 
however when boredom is controlled intelligence will appear to decrease errors and when error detection is controlled intelligence will appear to increase errors 
if intelligence could be increased while only boredom was held constant errors would decrease if intelligence could be increased while holding only error detection constant errors would increase 
in general the omission of suppressors or confounders will lead to either an underestimation or an overestimation of the effect of on thereby either reducing or artificially inflating the magnitude of relationship between two variables 
moderators other important third variables are moderators 
moderators are variables that can make the relationship between two variables either stronger or weaker 
such variables further characterize interactions in regression by affecting the direction and or strength of the relationship between and moderating relationship can be thought of as an interaction 
it occurs when the relationship between variables and depends on the level of see moderation for further discussion 
moderated mediation mediation and moderation can co occur in statistical models 
it is possible to mediate moderation and moderate mediation 
moderated mediation is when the effect of the treatment on the mediator and or the partial effect on the dependent variable depend in turn on levels of another variable moderator 
essentially in moderated mediation mediation is first established and then one investigates if the mediation effect that describes the relationship between the independent variable and dependent variable is moderated by different levels of another variable moderator 
this definition has been outlined by muller judd and yzerbyt and preacher rucker and hayes 
models of moderated mediation there are five possible models of moderated mediation as illustrated in the diagrams below 
in the first model the independent variable also moderates the relationship between the mediator and the dependent variable 
the second possible model of moderated mediation involves new variable which moderates the relationship between the independent variable and the mediator the path 
the third model of moderated mediation involves new moderator variable which moderates the relationship between the mediator and the dependent variable the path 
moderated mediation can also occur when one moderating variable affects both the relationship between the independent variable and the mediator the path and the relationship between the mediator and the dependent variable the path 
the fifth and final possible model of moderated mediation involves two new moderator variables one moderating the path and the other moderating the path 
in addition to the models mentioned above new variable can also exist which moderates the relationship between the independent variable and mediator the path while at the same time have the new variable moderate the relationship between the independent variable and dependent variable the path 
mediated moderation mediated moderation is variant of both moderation and mediation 
this is where there is initially overall moderation and the direct effect of the moderator variable on the outcome is mediated 
the main difference between mediated moderation and moderated mediation is that for the former there is initial overall moderation and this effect is mediated and for the latter there is no moderation but the effect of either the treatment on the mediator path is moderated or the effect of the mediator on the outcome path is moderated in order to establish mediated moderation one must first establish moderation meaning that the direction and or the strength of the relationship between the independent and dependent variables path differs depending on the level of third variable the moderator variable 
researchers next look for the presence of mediated moderation when they have theoretical reason to believe that there is fourth variable that acts as the mechanism or process that causes the relationship between the independent variable and the moderator path or between the moderator and the dependent variable path 
example the following is published example of mediated moderation in psychological research 
participants were presented with an initial stimulus prime that made them think of morality or made them think of might 
they then participated in the prisoner dilemma game pdg in which participants pretend that they and their partner in crime have been arrested and they must decide whether to remain loyal to their partner or to compete with their partner and cooperate with the authorities 
the researchers found that prosocial individuals were affected by the morality and might primes whereas proself individuals were not 
thus social value orientation proself vs prosocial moderated the relationship between the prime independent variable morality vs might and the behaviour chosen in the pdg dependent variable competitive vs cooperative 
the researchers next looked for the presence of mediated moderation effect 
regression analyses revealed that the type of prime morality vs might mediated the moderating relationship of participants social value orientation on pdg behaviour 
prosocial participants who experienced the morality prime expected their partner to cooperate with them so they chose to cooperate themselves 
prosocial participants who experienced the might prime expected their partner to compete with them which made them more likely to compete with their partner and cooperate with the authorities 
in contrast participants with pro self social value orientation always acted competitively 
regression equations for moderated mediation and mediated moderation muller judd and yzerbyt outline three fundamental models that underlie both moderated mediation and mediated moderation 
mo represents the moderator variable me represents the mediator variable and represents the measurement error of each regression equation 
step moderation of the relationship between the independent variable and the dependent variable also called the overall treatment effect path in the diagram 
to establish overall moderation the regression weight must be significant first step for establishing mediated moderation 
establishing moderated mediation requires that there be no moderation effect so the regression weight must not be significant 
step moderation of the relationship between the independent variable and the mediator path 
if the regression weight is significant the moderator affects the relationship between the independent variable and the mediator 
step moderation of both the relationship between the independent and dependent variables path and the relationship between the mediator and the dependent variable path 
if both in step and in step are significant the moderator affects the relationship between the independent variable and the mediator path 
if both in step and in step are significant the moderator affects the relationship between the mediator and the dependent variable path 
either or both of the conditions above may be true 
causal mediation analysis fixing versus conditioning mediation analysis quantifies the extent to which variable participates in the transmittance of change from cause to its effect 
it is inherently causal notion hence it cannot be defined in statistical terms 
traditionally however the bulk of mediation analysis has been conducted within the confines of linear regression with statistical terminology masking the causal character of the relationships involved 
this led to difficulties biases and limitations that have been alleviated by modern methods of causal analysis based on causal diagrams and counterfactual logic 
the source of these difficulties lies in defining mediation in terms of changes induced by adding third variables into regression equation 
such statistical changes are epiphenomena which sometimes accompany mediation but in general fail to capture the causal relationships that mediation analysis aims to quantify 
the basic premise of the causal approach is that it is not always appropriate to control for the mediator when we seek to estimate the direct effect of on see the figure above 
the classical rationale for controlling for is that if we succeed in preventing from changing then whatever changes we measure in are attributable solely to variations in and we are justified then in proclaiming the effect observed as direct effect of on 
unfortunately controlling for does not physically prevent from changing it merely narrows the analyst attention to cases of equal values 
moreover the language of probability theory does not possess the notation to express the idea of preventing from changing or physically holding constant 
the only operator probability provides is conditioning which is what we do when we control for or add as regressor in the equation for the result is that instead of physically holding constant say at and comparing for units under to those under we allow to vary but ignore all units except those in which achieves the value these two operations are fundamentally different and yield different results except in the case of no omitted variables 
to illustrate assume that the error terms of and are correlated 
under such conditions the structural coefficient and between and and between and can no longer be estimated by regressing on and in fact the regression slopes may both be nonzero even when is zero 
this has two consequences 
first new strategies must be devised for estimating the structural coefficients and second the basic definitions of direct and indirect effects must go beyond regression analysis and should invoke an operation that mimics fixing rather than conditioning on definitions such an operator denoted do was defined in pearl and it operates by removing the equation of and replacing it by constant for example if the basic mediation model consists of the equations then after applying the operator do the model becomes and after applying the operator do the model becomes where the functions and as well as the distributions of the error terms and remain unaltered 
if we further rename the variables and resulting from do as and respectively we obtain what came to be known as potential outcomes or structural counterfactuals 
these new variables provide convenient notation for defining direct and indirect effects 
in particular four types of effects have been defined for the transition from to total effect controlled direct effect natural direct effect natural indirect effect where stands for expectation taken over the error terms 
these effects have the following interpretations te measures the expected increase in the outcome as changes from to while the mediator is allowed to track the change in as dictated by the function 
cde measures the expected increase in the outcome as changes from to while the mediator is fixed at pre specified level uniformly over the entire population nde measures the expected increase in as changes from to while setting the mediator variable to whatever value it would have obtained under before the change 
nie measures the expected increase in when the is held constant at and changes to whatever value it would have attained for each individual under the difference te nde measures the extent to which mediation is necessary for explaining the effect while the nie measures the extent to which mediation is sufficient for sustaining it controlled version of the indirect effect does not exist because there is no way of disabling the direct effect by fixing variable to constant 
according to these definitions the total effect can be decomposed as sum where nier stands for the reverse transition from to it becomes additive in linear systems where reversal of transitions entails sign reversal 
the power of these definitions lies in their generality they are applicable to models with arbitrary nonlinear interactions arbitrary dependencies among the disturbances and both continuous and categorical variables 
the mediation formula in linear analysis all effects are determined by sums of products of structural coefficients giving independent of therefore all effects are estimable whenever the model is identified 
in non linear systems more stringent conditions are needed for estimating the direct and indirect effects 
for example if no confounding exists and are mutually independent the following formulas can be derived 
the last two equations are called mediation formulas and have become the target of estimation in many studies of mediation 
they give distribution free expressions for direct and indirect effects and demonstrate that despite the arbitrary nature of the error distributions and the functions and mediated effects can nevertheless be estimated from data using regression 
the analyses of moderated mediation and mediating moderators fall as special cases of the causal mediation analysis and the mediation formulas identify how various interactions coefficients contribute to the necessary and sufficient components of mediation 
example assume the model takes the form where the parameter quantifies the degree to which modifies the effect of on even when all parameters are estimated from data it is still not obvious what combinations of parameters measure the direct and indirect effect of on or more practically how to assess the fraction of the total effect that is explained by mediation and the fraction of that is owed to mediation 
in linear analysis the former fraction is captured by the product the latter by the difference and the two quantities coincide 
in the presence of interaction however each fraction demands separate analysis as dictated by the mediation formula which yields thus the fraction of output response for which mediation would be sufficient is while the fraction for which mediation would be necessary is 
these fractions involve non obvious combinations of the model parameters and can be constructed mechanically with the help of the mediation formula 
significantly due to interaction direct effect can be sustained even when the parameter vanishes and moreover total effect can be sustained even when both the direct and indirect effects vanish 
this illustrates that estimating parameters in isolation tells us little about the effect of mediation and more generally mediation and moderation are intertwined and cannot be assessed separately 
references as of june this article is derived in whole or in part from causal analysis in theory and practice 
the copyright holder has licensed the content in manner that permits reuse under cc by sa and gfdl 
all relevant terms must be followed 
notes bibliographypreacher kristopher hayes andrew 
spss and sas procedures for estimating indirect effects in simple mediation models 
behavior research methods instruments and computers 
pmid preacher kristopher hayes andrew 
asymptotic and resampling strategies for assessing and comparing indirect effects in multiple mediator models 
pmid preacher zyphur zhang 
general multilevel sem framework for assessing multilevel mediation 
pmid baron and kenny 
the moderator mediator variable distinction in social psychological research conceptual strategic and statistical considerations journal of personality and social psychology vol 
statistical power analysis for the behavioral sciences nd ed 
new york ny academic press 
beyond baron and kenny statistical mediation analysis in the new millennium 
statistical methods for psychology th ed 
belmot ca cengage learning 
advances in statistical methods for substance abuse prevention research 
doi pmc pmid preacher kelley 
effect sizes measures for mediation models quantitative strategies for communicating indirect effects 
pmid rucker preacher tormala 
mediation analysis in social psychology current practices and new recommendations 
social and personality psychology compass 
asymptotic confidence intervals for indirect effects in structural equation models 
jstor spencer zanna fong 
establishing causal chain why experiments are often more effective than mediational analyses in examining psychological processes 
journal of personality and social psychology 
the mediation formula guide to the assessment of causal pathways in nonlinear models 
in berzuini dawid bernardinelli 
causality statistical perspectives and applications 
chichester uk john wiley and sons ltd pp 
research methods in psychology th ed pp 
new york mcgraw hill 
the determiners of behavior at choice point 
degrees of hunger reward and nonreward and maze learning in rats 
university of california publications in psychology 
explanation in causal inference 
external links summary of mediation methods at psychwiki example of causal mediation using propensity scores the methodology center penn state university book on moderation and mediation analysis including an introduction to the process macro for spss and sas andrew hayes ohio state university online text of the determiner of behavior at choice point kenneth maccorquodale and paul meehl on distinction between hypothetical constructs and intervening variables classics in the history of psychology retr