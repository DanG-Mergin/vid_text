
the sample mean or empirical mean and the sample covariance are statistics computed from sample of data on one or more random variables
the sample mean is the average value or mean value of sample of numbers taken from larger population of numbers where population indicates not number of people but the entirety of relevant data whether collected or not
sample of companies sales from the fortune might be used for convenience instead of looking at the population all companies sales
the sample mean is used as an estimator for the population mean the average value in the entire population where the estimate is more likely to be close to the population mean if the sample is large and representative
the reliability of the sample mean is estimated using the standard error which in turn is calculated using the variance of the sample
if the sample is random the standard error falls with the size of the sample and the sample mean distribution approaches the normal distribution as the sample size increases
the term sample mean can also be used to refer to vector of average values when the statistician is looking at the values of several variables in the sample
the sales profits and employees of sample of fortune companies
in this case there is not just sample variance for each variable but sample variance covariance matrix or simply covariance matrix showing also the relationship between each pair of variables
this would be matrix when variables are being considered
the sample covariance is useful in judging the reliability of the sample means as estimators and is also useful as an estimate of the population covariance matrix
due to their ease of calculation and other desirable characteristics the sample mean and sample covariance are widely used in statistics to represent the location and dispersion of the distribution of values in the sample and to estimate the values for the population
definition of the sample mean the sample mean is the average of the values of variable in sample which is the sum of those values divided by the number of values
using mathematical notation if sample of observations on variable is taken from the population the sample mean is under this definition if the sample is taken from the population then the sample mean is as compared to the population mean of even if sample is random it is rarely perfectly representative and other samples would have other sample means even if the samples were all from the same population
the sample for example would have sample mean of if the statistician is interested in variables rather than one each observation having value for each of those variables the overall sample mean consists of sample means for individual variables
let be the ith independently drawn observation on the jth random variable
these observations can be arranged into column vectors each with entries with the column vector giving the th observations of all variables being denoted
the sample mean vector is column vector whose th element is the average value of the observations of the jth variable thus the sample mean vector contains the average of the observations for each variable and is written definition of sample covariance the sample covariance matrix is by matrix with entries where is an estimate of the covariance between the jth variable and the kth variable of the population underlying the data
in terms of the observation vectors the sample covariance is
alternatively arranging the observation vectors as the columns of matrix so that which is matrix of rows and columns
here the sample covariance matrix can be computed as where is an by vector of ones
if the observations are arranged as rows instead of columns so is now row vector and is an matrix whose column is the vector of observations on variable then applying transposes in the appropriate places yields
like covariance matrices for random vector sample covariance matrices are positive semi definite
to prove it note that for any matrix the matrix is positive semi definite
furthermore covariance matrix is positive definite if and only if the rank of the
vectors is unbiasedness the sample mean and the sample covariance matrix are unbiased estimates of the mean and the covariance matrix of the random vector row vector whose jth element is one of the random variables
the sample covariance matrix has in the denominator rather than due to variant of bessel correction in short the sample covariance relies on the difference between each observation and the sample mean but the sample mean is slightly correlated with each observation since it is defined in terms of all observations
if the population mean is known the analogous unbiased estimate using the population mean has in the denominator
this is an example of why in probability and statistics it is essential to distinguish between random variables upper case letters and realizations of the random variables lower case letters
the maximum likelihood estimate of the covariance for the gaussian distribution case has in the denominator as well
the ratio of to approaches for large so the maximum likelihood estimate approximately equals the unbiased estimate when the sample is large
distribution of the sample mean for each random variable the sample mean is good estimator of the population mean where good estimator is defined as being efficient and unbiased
of course the estimator will likely not be the true value of the population mean since different samples drawn from the same distribution will give different sample means and hence different estimates of the true mean
thus the sample mean is random variable not constant and consequently has its own distribution
for random sample of observations on the jth random variable the sample mean distribution itself has mean equal to the population mean and variance equal to where is the population variance
the arithmetic mean of population or population mean is often denoted the sample mean the arithmetic mean of sample of values drawn from the population makes good estimator of the population mean as its expected value is equal to the population mean that is it is an unbiased estimator
the sample mean is random variable not constant since its calculated value will randomly differ depending on which members of the population are sampled and consequently it will have its own distribution
for random sample of independent observations the expected value of the sample mean is and the variance of the sample mean is var if the samples are not independent but correlated then special care has to be taken in order to avoid the problem of pseudoreplication
if the population is normally distributed then the sample mean is normally distributed as follows
if the population is not normally distributed the sample mean is nonetheless approximately normally distributed if is large and
this is consequence of the central limit theorem
weighted samples in weighted sample each vector each set of single observations on each of the random variables is assigned weight without loss of generality assume that the weights are normalized
if they are not divide the weights by their sum
then the weighted mean vector is given by and the elements of the weighted covariance matrix are
if all weights are the same the weighted mean and covariance reduce to the biased sample mean and covariance mentioned above
criticism the sample mean and sample covariance are not robust statistics meaning that they are sensitive to outliers
as robustness is often desired trait particularly in real world applications robust alternatives may prove desirable notably quantile based statistics such as the sample median for location and interquartile range iqr for dispersion
other alternatives include trimming and winsorising as in the trimmed mean and the winsorized mean
see also estimation of covariance matrices scatter matrix unbiased estimation of standard deviation references
viktor amazaspovich ambartsumian russian armenian viktor hamazaspi hambardzumyan september
september august was soviet armenian astrophysicist and science administrator
one of the th century top astronomers he is widely regarded as the founder of theoretical astrophysics in the soviet union
educated at leningrad state university lsu and the pulkovo observatory ambartsumian taught at lsu and founded the soviet union first department of astrophysics there in he subsequently moved to soviet armenia where he founded the byurakan observatory in it became his institutional base for the decades to come and major center of astronomical research
he also co founded the armenian academy of sciences and led it for almost half century the entire post war period
one commentator noted that science in armenia was synonymous with the name ambartsumian
in ambartsumian founded the journal astrofizika and served as its editor for over years
ambartsumian began retiring from the various positions he held only from the age of he died at his house in byurakan and was buried on the grounds of the observatory
he was declared national hero of armenia in
background ambartsumian was born in tiflis on september september in old style to hripsime khakhanian and hamazasp hambardzumyan
hripsime father was an armenian apostolic priest from tskhinvali while hamazasp hailed from vardenis basargechar
his ancestors moved from diyadin what is now turkey to the southern shores of lake sevan in in the aftermath of the russo turkish war
hamazasp russified amazasp was an educated man of letters who studied law at saint petersburg university
he was also writer and translator and notably translated homer iliad into armenian from classical greek
in he co founded the caucasian society of armenian writers which lasted until ambartsumian was the secretary while hovhannes tumanyan the famed poet served as its president ambartsumian parents married in he had brother levon and sister gohar
his brother geophysics student died at while on an expedition in the urals
gohar was mathematician and chair of probability theory and mathematical statistics at yerevan state university towards the end of her life
education ambartsumian developed an early interest in mathematics and was able to multiply by the age of his interest in astronomy began with reading russian translation of book by ormsby mitchel at according to himself he became an astronomer at the age of between and he studied at tiflis gymnasiums and where schooling was done in both russian and armenian
in he transferred to gymnasium to study under nikolay ignatievich sudakov moscow educated astronomer whom ambartsumian called very serious teacher of astronomy
ambartsumian worked with sudakov at the school observatory the latter had built
at school ambartsumian wrote several papers on astronomy and delivered lectures on the origin of the solar system and extraterrestrial life at first in school and then in the various clubs and houses of culture beginning at
in ambartsumian delivered lecture at yerevan state university about the theory of relativity
he also met ashot hovhannisyan and alexander miasnikian armenia communist leaders in ambartsumian moved to leningrad where he began attending the herzen pedagogical institute
according to shakhbazyan it was his non peasant and non proletarian background that kept him from attending leningrad state university lsu
however in an interview ambartsumian stated that it was too late for him to apply to lsu because he arrived in august and admissions were already closed
not to lose year he instead enrolled in the physics and mathematics department at the pedagogical institute
after year he transferred to lsu department of physics and mathematics
at university ambartsumian was interested in both astronomy and mathematics
loved mathematics but at the same time felt that my profession would be astronomy
mathematics was like hobby but did complete the full mathematics curriculum
thus you could say that graduated with major in mathematics but in fact it is recorded that graduated as an astronomer he said in an interview in at lsu among his professors were the physicist orest khvolson and mathematician vladimir smirnov
he studied alongside other major soviet scientists such as lev landau sergei sobolev sergey khristianovich and george gamow
in he published the first of his scholarly papers as student
he graduated in although he received his diploma only fifty years later in his undergraduate thesis was devoted to study of radiative transfer radiative equilibrium
he completed his postgraduate studies at the pulkovo observatory under aristarkh belopolsky between and
career leningrad after completing his postgraduate studies in ambartsumian began working at the pulkovo observatory and teaching part time at lsu
in ambartsumian began reading the first course on theoretical astrophysics in the soviet union
he also served as pulkovo scientific secretary in which involved mostly administrative work
ambartsumian later characterized pulkovo as being very old institution and for this reason there were certain elements of ossification and stagnation
nevertheless this was the best qualified astronomical institution in the soviet union
in ambartsumian was fired by pulkovo director boris gerasimovich for alleged laziness
gerasimovich viewed ambartsumian and other young astrophysicists as undisciplined and in too much of rush to publish untested theories and poorly documented research
gerasimovich himself had tendency to non cooperativeness
gerasimovich was not taken seriously by them
when in subrahmanyan chandrasekhar visited leningrad he was told by ambartsumian look here here is set of papers by gerasimovich
turn to an arbitrary paper and to an arbitrary line
am sure you will find mistake
chandrasekhar stated in that during his visit in ambartsumian was very free and very open
he was extremely critical of his seniors
after leaving pulkovo ambartsumian founded the first department of astrophysics in the soviet union at leningrad state university in in he was named professor at lsu and in he was named doctor of physical mathematical sciences without having to defend thesis based on his scientific work through that date
he headed the department until or between and ambartsumian was the director of the astronomical observatory of lsu
he was simultaneously prorector deputy president of the university
among his graduate students were viktor sobolev benjamin markarian grigor gurzadyan and others
ambartsumian considered sobolev his most brilliant graduate student
stalin purgesmany of ambartsumian colleagues and friends suffered during the great purge under stalin most notably nikolai aleksandrovich kozyrev with whom he became close friends in the mid
kozyrev was sentenced to ten years in forced labor camp but survived the repressions
others such as matvei petrovich bronstein and pulkovo director boris gerasimovich did not survive
ambartsumian relations with kozyrev were strained for the remainder of his life
mccutcheon notes that while in the west some have questioned ambartsumian possible role in the terror there is no hard evidence to suggest that he was guilty of anything more serious than surviving at time when others did not
world war iiambartsumian led the evacuation of part of the faculty of leningrad state university to elabuga yelabuga tatarstan in after the nazi invasion of the soviet union
there branch of lsu operated under ambartsumian leadership until he served as the dean of the branch
armenia in ambartsumian moved with his family to yerevan soviet armenia where he lived until the end of his life
in the same year he co founded the armenian academy of sciences along with scientists and scholars hovsep orbeli hrachia acharian artem alikhanian abram alikhanov manuk abeghian and others
he served as vice president of the academy until and as president from to since ambartsumian served as director of the yerevan astronomical observatory
the small observatory was affiliated with yerevan state university
ambartsumian had secured nine inch telescope from leningrad for the observatory
ambartsumian said that before the war this observatory did not rise significantly above the level of amateur variable star observations
during the war they also carried out photographic observations of variable stars using small camera
in ambartsumian founded the department of astrophysicists at yerevan state university ysu
he was named professor of astrophysics at ysu in he served as chair of the department until in ambartsumian founded the journal astrofizika armenian russian which has been published by the armenian academy of sciences since then
it was originally published in russian subsequently articles in english began to appear
he served as its editor in chief until the journal has also been published since the first issue in english by springer in the us as astrophysics
byurakan in ambartsumian founded the byurakan astrophysical observatory in the village of byurakan at an altitude of ft on the slopes of mount aragats some km mi from yerevan
the first buildings were completed in though the official inauguration took place in observations began to be carried out simultaneous with the construction of the observatory
our instruments stood under the open sky covered with tarpaulin said ambartsumian
ambartsumian initially lived at house in the village of byurakan then build house within the observatory grounds with the money awarded with the stalin prize
ambartsumian directed the byurakan observatory until and was named its honorary director that year
from until his death in the byurakan observatory served as ambartsumian institutional base
in ambartsumian secured schmidt telescope with cm correcting plate and cm mirror for byurakan
the telescope was reportedly made by carl zeiss ag in nazi germany in the and was transferred to leningrad as spoils of war
it was completed in leningrad and sent to armenia
beginning with on ambartsumian initiative benjamin markarian started the first byurakan survey that resulted in the discovery of the markarian galaxies
number of international symposiums and meetings were held at byurakan under ambartsumian supervision
in the observatory was awarded the order of lenin the soviet union highest civilian order for its great merit to the development of science
in ambartsumian supervised the establishment of an astrophysical station of leningrad state university his alma mater within the grounds of the byurakan observatory
it is where graduate students of the lsu did their summer internships until the late
it was shut down in ambartsumian and his disciples at the byurakan observatory became known in the scholarly literature as the byurakan school
from to ambartsumian headed specialized council for theses defenses at byurakan
over scientists defended their phd candidate and doctoral theses on astronomy astrophysics and theoretical physics in those years under ambartsumian
though most of the students were graduates of the astrophysics department of yerevan state university many came from russia georgia ukraine azerbaijan hungary bulgaria and elsewhere
several symposiums of the international astronomical union and numerous conferences were held in byurakan in attendance of jan oort fritz zwicky subrahmanyan chandrasekhar pyotr kapitsa vitaly ginzburg and others
it was also visited by soviet leaders nikita khrushchev and leonid brezhnev with the byurakan observatory ambartsumian put armenia on the astronomical map and made soviet armenia one of the world centers for the study of astrophysics
by the time of his death in the new york times described byurakan as one of the world leading astronomical research centers
as of the byurakan observatory maintained regular contact with research institutions and with scientists from countries
research ambartsumian carried out basic research in astronomy and cosmogony
his research covered astrophysics theoretical physics and mathematical physics
most of his research focused on physics of nebulae star systems and extragalactic astronomy
he is best known for having discovered stellar associations and predicted activity of galactic nuclei
in his later career ambartsumian held views in contradiction to the consequences of the general relativity such as rejecting the existence of black holes
stellar associations in ambartsumian discovered stellar associations new type of stellar system which led to the conclusion that star formation continues to take place in the milky way galaxy
at the time the idea of star formation as an ongoing process was regarded as very speculative
his discovery was announced in short publication by the armenian academy sciences
ambartsumian discovery was based on his observation of stars of and spectral types and tauri and flare stars that cluster very loosely
this is significantly different from open clusters which have higher density of stars while stellar associations have lower than average density
ambartsumian divided stellar associations into ob and groups and concluded that the associations have to be dynamically unstable configurations and must expand subsequently dissolving to form field stars
he thus argued that star forming is ongoing in the galaxy and that stars are born explosively and in groups ambartsumian concept was not immediately accepted
chandrasekhar noted the early scepticism with which this discovery was received by the astronomers of the establishment when first gave an account of ambartsumian paper at the colloquium at the yerkes observatory in late
chandrasekhar noted that ambartsumian discovery of stellar associations had far reaching implications for subsequent theories relating to star formation
mccutcheon noted that the discovery opened an entirely new field of astrophysical research
active galactic nuclei agn ambartsumian began studying nuclei of galaxies in the mid
he found that clusters of galaxies are unstable and that galaxy formation is still ongoing
at the solvay conference on physics in brussels he gave famous lecture in which he claimed enormous explosions take place in galactic nuclei and as result huge amount of mass is expelled
in addition if this is so these galactic nuclei must contain bodies of huge mass and unknown nature
ambartsumian report essentially introduced active galactic nucleus agn as major theory of galactic evolution
the concept of agn was widely accepted some years later
astronomy from space ambartsumian was pioneer of astronomical research from soviet spacecraft
the program was directed by his disciple grigor gurzadyan and was launched in in april the salyut space station carried orion the first space telescope with an objective prism into orbit
in december the manned soyuz mission operated the orion ultraviolet cassegrain telescope with quartz objective prism built in the byurakan observatory
spectra of thousands of stars to as faint as thirteenth magnitude were obtained as was the first satellite ultraviolet spectrogram of planetary nebula revealing lines of aluminium and titanium elements not previously observed in planetary nebulae
these activities especially the space missions when for example special manned spaceship had to be devoted to an experiment from the smallest soviet republic needed powerful backing both in kremlin corridors and within the top secret rocket industry establishment
this was achieved due to ambartsumian political skills with the active support of mstislav keldish the then president of the academy of sciences of the ussr
mathematics ambartsumian also made contributions to mathematics most notably with his paper in zeitschrift physik
in it ambartsumian first introduced the inverse sturm liouville problem
he proved that among all vibrating strings only the homogeneous vibrating string has eigenvalues that are specific to it that is homogeneous vibrating strings have spectrum of eigenvalues
it was only in the mid when his paper received attention and became significant research topic in the ensuing decades
he commented when an astronomer is publishing mathematical paper in physical journal he cannot expect to attract too many readers
science administration soviet academy of sciences ambartsumian was elected corresponding member of the ussr academy of sciences in and full member academician in in he became member of the academy presidium the governing body
he also chaired the academy joint coordinating scientific council on astronomy which was responsible for the priorities and all major decisions in all of astronomy
he was also chairman of the academy commissions on astronomy and cosmogony in these positions ambartsumian was one of the most powerful scientists of his time
mccutcheon noted that ambartsumian towering authority as an astrophysicist combined with his position in the soviet establishment made him arguably the most powerful soviet astronomer of his day
he was often the official head of soviet delegations at many conferences not only on astronomy but also on natural philosophy
from to ambartsumian was member of the editorial board of astronomicheskii zhurnal also known as astronomy reports the soviet union main astronomy journal
he was also on the editorial board of doklady akademii nauk sssr proceedings of the ussr academy of sciences
armenian academy of sciences although the armenian branch of the soviet academy of sciences was established in it was not until that the national academy of sciences of the armenian ssr was founded
ambartsumian was one of its original co founders along with other prominent scholars and scientists including hovsep orbeli who became its first president
ambartsumian initially served as vice president and in he became the academy second president serving for years until when he stepped down in he was declared honorary president of the academy rouben paul adalian wrote that ambartsumian exercised enormous influence in the advancement of science in soviet armenia and was revered as his country leading scientist
mccutcheon went on to note that from that point forward science in armenia was synonymous with the name ambartsumian
as president of the principal coordinating body for scientific research in soviet armenia ambartsumian played significant role in promoting the sciences in the country
he actively promoted the natural and exact sciences including physics and mathematics radioelectronics chemistry mechanics and engineering
artashes shahinian noted that ambartsumian played significant role in the development of the physical and mathematical sciences
he played an instrumental role in the establishment and development of the yerevan scientific research institute of mathematical machines yerniimm in popularly known as the mergelyan institute after its first director mathematician sergey mergelyan
apoyan rejects that ambartsumian had direct involvement in its creation and characterizes his role as favorable neutrality
overall apoyan criticizes ambartsumian role in science administration
he wrote that he had tendency to fail projects that did not directly serve his fame
he went as far as call ambartsumian role similar to that of tyrant
ambartsumian and mergelyan had complicated relationship
in ambartsumian persuaded him to return to armenia from moscow and become vice president of the armenian academy of sciences
however in mergelyan was not reelected to the presidium of the academy and was forced to leave it
some academicians called for revote but ambartsumian rejected any such attempts
oganjanyan and silantiev note that ambartsumian was rumored to have seen mergelyan as rival for the academy president and decided to get rid of the competitor forever
ambartsumian was the chairman of the editorial board of the armenian soviet encyclopedia haykakan sovetakan hanragitaran published in volumes in
supplementary volume devoted to soviet armenia was published in works on the encyclopedia began in although it reflected the government marxist leninist viewpoint is in the most comprehensive encyclopedia in the armenian language to this day
each volume was published in copies
international according to jean claude pecker ambartsumian had very strong influence on world astropolitics and is one of the few astronomers who have had such deep influence on the life of the international bodies devoted to the promotion and defense of astronomy and science in general
international astronomical unionambartsumian was member of the international astronomical union iau since he served as vice president of the iau from to then as president from to as vice president ambartsumian attempted to have the iau general assembly be held in leningrad in however the iau executive committee canceled the assembly increasing tensions within the iau
an iau general assembly eventually took place in moscow in ambartsumian headed the organizing committee
blaauw noted that during these years ambartsumian although violently opposing the iau policy remained loyal to the executive committee majority decisions for the sake of safeguarding international collaboration an attitude that contributed to his election as president of the iau in
he continued to support it as the world wide organization embracing astronomers from all countries
his election as president of the iau in reflected both the appreciation for his efforts in this respect and his outstanding scientific achievements
ambartsumian was outspoken about the importance of international cooperation
at the iau general assembly in rome he declared we believe that the joint study of such large problems as that of the evolution of celestial bodies will contribute to the cultural rapprochement of different nations and to better understanding among them
this is our modest contribution to the noble efforts toward maintaining peace throughout the world
at the iau symposium in sydney he stated that while competition between nations is important it should be associated with co operation
international council of scientific unionsambartsumian also served as president of the international council of scientific unions icsu between and being elected twice for two year terms in and he was the first individual from the eastern bloc to be elected to that post
philosophical and cosmological views ambartsumian published several books and articles on philosophy including philosophical questions about the science of the universe
in paper ambartsumian wrote that he believes in close collaboration of philosophy and the natural sciences to solve the main scientific problems about nature
ambartsumian became member of the administration of the philosophical society of the soviet union when it was established in in he became honorary president of the philosophical society of armenia which was created through his efforts
science and religion ambartsumian was an atheist and believed that science and religion are irreconcilable
ambartsumian wrote in for over four decades he headed gitelik the armenian branch of the all soviet organization znaniye knowledge founded in to continue the pre war atheist work of the league of militant godless
it published atheist novels and journals produced films and organized lectures on the supremacy of science over religion
the organization engaged in what it called scientific atheistic propaganda
despite his atheism ambartsumian reportedly felt that christianity has been important in preserving armenian identity
according to one associate ambartsumian self identified as an armenian christian but was not religious
ambartsumian had friendly relations with vazgen the long time head catholicos of the armenian apostolic church especially since at least the late
in ambartsumian visited san lazzaro degli armeni in venice home of the armenian catholic congregation of the mekhitarists and was declared an honorary member of the san lazzaro armenian academy that year
marxism leninism and dialectical materialism ambartsumian accepted and followed marxist leninist philosophy and staunchly promoted dialectical materialism and projected it on his astrophysical interpretations
helge kragh described ambartsumian as convinced marxist
he wrote on marxism leninism and dialectical materialism in dialectical materialism influenced ambartsumian cosmological views and ideas
according to loren graham perhaps no great soviet scientist has made more outspoken statements in favor of dialectical materialism than ambartsumian
mark teeter wrote in report that ambartsumian is one of rather limited group of soviet scholars of international stature who claim that dialectical materialism has assisted them in their work
kragh noted that ambartsumian was not cosmologist but an astrophysicist and that his ideas of the universe were influenced both by his background in astrophysics and his adherence to marxist leninist philosophy
graham notes that his praise of dialectical materialism has been voiced again and again over the years these affirmations have come when political controls were rather lax as well as when they were tight
we have every reason to believe that they reflect at root his own approach to nature
political career and views ambartsumian is often referred to as politician donald lynden bell called him skillful one
in interview subrahmanyan chandrasekhar went as far as to opine that ambartsumian has been much more of politician than an astronomer since the mid lyudvig mirzoyan colleague and friend wrote that ambartsumian was true patriot of his native land soviet armenia and all the soviet union and simultaneously he was convinced internationalist
he was described by us based soviet government printed magazine as an ardent advocate of the widest possible international scientific exchange
soviet politics mccutcheon noted that ambartsumian life was shaped and directed by the soviet system and he was politically loyal to the soviet authorities
loren graham noted that at the same time ambartsumian was not afraid to reprimand the communist party ideologues when they obstructed his research
ronald doel noted that ambartsumian was in favor with the communist party and enjoyed the freedom to travel to the west
adriaan blaauw wrote that his political views harmonized to considerable degree with those of soviet rulers
mccutcheon wrote the following on his relationship with the soviet system ambartsumian jointed the communist party of the soviet union cpsu in in he became member of the central committee the executive branch of the communist party of the armenian ssr
ambartsumian was also member of the supreme soviet from to rd to th convocation sessions
in he was elected as representative from armenia to the congress of people deputies of the soviet union in the first relatively free elections ambartsumian was delegate to the th th nd rd th th and th congresses of the cpsu
cold war politics ambartsumian often signed open letters in support of the official line of the soviet authorities
in he was among leading soviet scientists who signed letter to president richard nixon in support of black militant communist angela davis and appealed him to give her an opportunity of continuing her scientific work
in ambartsumian was among soviet scientists who signed statement attacking president ronald reagan strategic defense initiative star wars namely reagan plan for an effective defense against nuclear attack
the scientists stated that reagan is creating most dangerous illusion that may turn into an even more threatening spiral of the arms race
ambartsumian relationship with dissidents was complicated
in he refused to meet yuri orlov nuclear physicist and prominent dissident after having offered him job in yerevan
ambartsumian told him through subordinate that there are situations when even an academy member is helpless
in he was among soviet scientists who denounced the award of the nobel peace prize to soviet physicist and dissident andrei sakharov
armenian causes ambartsumian revered the armenian language and supported its usage
he insisted all internal communication of the armenian academy of sciences be done in armenian when he became president in as president of the armenian academy of sciences ambartsumian often gave speeches at major events such as during the commemorations of the th anniversary of mesrop mashtots the inventor of the armenian alphabet in and the th anniversary of hovhannes tumanyan armenia national poet in
armenian genocide ambartsumian delivered speech on april on the th anniversary of the armenian genocide describing it as extermination of the armenian population of western armenia
he linked it to the th anniversary of soviet armenia and the revival of the armenian people as result of the october revolution
in an article published in pravda on april ambartsumian linked the armenian genocide to the holocaust and blamed german imperialism during world war for inspiring the young turks and the capitalist states for failing to defend the innocent armenian population and praised the october revolution for saving the armenian nation
nagorno karabakh in november the armenian academy of sciences led by ambartsumian issued statement protesting the decision of the supreme soviet of the soviet union to return nagorno karabakh to the direct jurisdiction of soviet azerbaijan in september ambartsumian and four other armenians including writer zori balayan and actor sos sargsyan went on hunger strike at the hotel moskva in moscow to protest the military rule over nagorno karabakh declared by mikhail gorbachev
ambartsumian celebrated his nd birthday hunger striking
he insisted that gorbachev had violated the soviet constitution by keeping nagorno karabakh under direct rule from moscow
this is bad thing when government does not abide by its own laws he argued
he also stated my desire is that karabakh be part of armenia
this is problem that has to be solved with long process and with concessions
ambartsumian stated that his only demand is that the elected leaders of nagorno karabakh regain control
ambartsumian called the hunger strike modest step aimed at making huge resonance in the world to let the world know
the soviet authorities totally ignored the strike
he ended it after days only when catholicos vazgen persuaded him to do so on may ambartsumian and number of members of the armenian academy of sciences wrote letter to soviet president mikhail gorbachev expressing their concern with the forced expulsion of ethnic armenians from parts of nkao and shahumian rayon as part of operation ring
soviet collapse and independence of armenia in june the session of the armenian academy of sciences issued statement on its views on armenian independence and the future of the soviet union
the academy stated its unconditional support for the independence of armenia pushed at the time by the pan armenian national movement hhsh
however it argued that because armenia is economically interconnected with and dependent on other soviet republics an abrupt disruption in the existing relations would result in unimaginable levels of economic collapse unemployment and emigration
thus they called for armenia to join the new union treaty proposed by gorbachev
the session also argued that leaving the soviet union would mean to abandon nagorno karabakh as communist ambartsumian reportedly regretted the collapse of the soviet union but voted for armenia independence in the referendum
he appreciated independent armenia but reminded armenians that they will be paying high price for it
in he congratulated armenians worldwide with armenia independence and stated that the newly independent republic is moving forward
according to yuri shahbazyan friend and biographer of ambartsumian he remained sympathetic towards the communist party of russia and was critical of western sponsored economic liberalization in russia and other post soviet countries
personal life when ambartsumian was referred to by foreigners as russian scientist he corrected them by saying he was armenian
he spoke perfect armenian albeit with an accent between and ambartsumian mostly divided his time between yerevan and byurakan
he built himself house within the byurakan observatory with the award money that came with his second stalin prize in since he also maintained house next to the building of the academy of sciences in yerevan on baghramyan avenue
personality donald lynden bell characterized ambartsumian as broad shouldered thickset man of medium height quick intellect and strong character
lynden bell and vahe gurzadyan wrote that ambartsumian was modest in private life and behaved simply in public
fadey sargsyan described ambartsumian as an extremely modest man
anthony astrachan wrote in the new yorker that ambartsumian is by all reports an engaging human being
ambartsumian admitted to not having any hobbies my only passion is science astronomy
like jealous wife it expects man to give all of himself
however he loved poetry and music and could enliven even the most abstract mathematical lectures with quotations from classical and contemporary poets
family in or ambartsumian married vera fyodorovna klochikhina an ethnic russian who was the niece and the adopted daughter of pelageya shajn the wife of grigory shajn both russian astronomers
she was an english teacher who taught him to read his papers in english when he visited the and britain
however she could not reconcile with his barbarous pronunciation as she described it
he was deeply depressed by her death in they had four children daughters karine
all four became either mathematicians or physicists
as of he had eight grandchildren
retirement and death ambartsumian began retiring from the various positions he held in at he left the position of the director of the byurakan observatory that year
in he stepped down as president of the armenian academy of sciences and in as chair of astrophysics at yerevan state university ambartsumian died at his house at the byurakan observatory complex on august month before his th birthday
the house was opened as his museum in august he was buried at the observatory grounds next to his wife and parents
his funeral was attended by thousands of people including armenia president levon ter petrosyan
recognition ambartsumian was one of the leading astrophysicists and astronomers of the th century
in subrahmanyan chandrasekhar stated my own impression has always been that he was when he was in his prime one of the most perceptive and elegant of astronomers
chandrasekhar wrote in ambartsumian was arguably the leading astronomer of the soviet union and is universally recognized as the founder of the soviet school of theoretical astrophysics
he was also well regarded internationally
loren graham called him one of the best known abroad of all soviet scientists
he was an honorary or foreign member of academies of sciences of over countries despite being soviet scientist he was well regarded in the united states
during the cold war ambartsumian was the first soviet scientist to become foreign honorary member of the american academy of arts and sciences and foreign associate of the national academy of sciences in and respectively
in january ambartsumian was invited to the house committee on science and astronautics where he was introduced by fred lawrence whipple as man who is rated the world greatest astronomer or at least among the very greatest
in armenia ambartsumian is recognized as the greatest scientist in th century armenia
he is considered the greatest armenian scientist since anania shirakatsi the seventh century astronomer
fadey sargsyan ambartsumian successor as president of the armenian academy of sciences stated in that ambartsumian is one of those scientists who in his merits and reputation goes beyond the limits of his scientific fields and in his own lifetime becomes great national figure
he can truly be called great armenian
on october armenia president levon ter petrosyan awarded ambartsumian the title of national hero of armenia for his scientific work of international significance science administration and patriotic activism
his official obituary was signed by armenia president government and parliament
tribute an asteroid discovered at the crimean astrophysical observatory in by tamara smirnova is named ambartsumian in ambartsumian th anniversary was celebrated in armenia the international astronomical union held symposium at the byurakan observatory and the central bank of armenia issued dram banknote depicting ambartsumian and the byurakan observatory
the byurakan observatory was officially named after ambartsumian that year
other things named after ambartsumian include chair of general physics and astrophysics at yerevan state university street park and public school in yerevan and the pedagogical institute of vardenis in metre ft bronze statue of ambartsumian was unveiled in yerevan at the park around the yerevan observatory in attendance of president serzh sargsyan and other officials
busts of ambartsumian stand at the byurakan observatory the city of vardenis and at the central campus of yerevan state university
viktor ambartsumian international prize in president of armenia serzh sargsyan signed decree to establish an international prize in ambartsumian memory
it was first awarded in and is awarded every two years
the prize was initially but was reduced to in it is considered one of the prestigious awards in astronomy and related fields
awards and honors membership soviet unioncorresponding member of the ussr academy of sciences full member academician of the armenian ssr academy of sciences full member academician of the ussr academy of sciences honorary member of the academies of sciences of the georgian ssr and azerbaijan ssrabroadambartsumian was elected honorary and foreign member of academies of sciences including honorary member of the american astronomical society associate of the royal astronomical society corresponding member and foreign associate of the french academy of sciences foreign honorary member of the american academy of arts and sciences foreign associate of the national academy of sciences honorary member of the royal astronomical society of canada foreign member of the royal society honorary degrees ambartsumian received honorary doctorates from several universities australian national university university of paris university of li ge charles university in prague nicolaus copernicus university in toru national university of la plata
publications throughout his career ambartsumian authored some books and booklets and over academic papers in he published the first systematic textbook in russian on theoretical astrophysics based on his lectures at leningrad state university
theoretical astrophysics ambartsumian served as editor and senior author of the book teoreticheskaia astrofizika
it was translated into number of languages including english german and chinese
the english translation appeared in as theoretical astrophysics
roderick oliver redman noted in that it has found many appreciative readers in both german and english speaking countries
it became bible for generation of astronomers and astrophysicists
the book received critical acclaim by contemporary astronomers
cecilia payne gaposchkin wrote that it is the only advanced book of this scope in english it will be of the greatest value
george field described the book as comprehensively and competently constructed
redman wrote it is welcome addition to the comparatively few general texts of solid worth which are now available
see also armenians in tbilisi references notes citations bibliography books on ambartsumianshakhbazyan yuri
ambartsumian stages of life and scientific concepts pdf in russian
isbn archived from the original pdf on december journal articleslynden bell gurzadyan
biographical memoirs of fellows of the royal society
mother see of holy etchmiadzin
armenian academy of sciences
the purge of soviet astronomers
translated in editorial board of journal astrofizika
ambartsumian life in science
cid general booksgraham loren
science and philosophy in the soviet union
new york columbia university press
einstein and soviet ideology
external links oral history interview transcript with viktor amazaspovich ambartsumian on october american institute of physics niels bohr library archives ambartsumian bibliography at sonoma state university ambartsumian bibliography at ambartsumian ru further reading harutyunian haik sedrakian david kalloghlian arsen nikoghossian arthur eds
ambartsumian legacy and active universe
new york springer verlag
cornelius cornel lanczos hungarian nczos korn pronounced la nt so korne born as korn wy until wy wy korn february june was hungarian american and later hungarian irish mathematician and physicist
according to gy rgy marx he was one of the martians
biography he was born in feh rv alba regia fej county kingdom of hungary to roly wy and ad hahn
lanczos ph thesis was on relativity theory
he sent his thesis copy to albert einstein and einstein wrote back saying studied your paper as far as my present overload allowed
believe may say this much this does involve competent and original brainwork on the basis of which doctorate should be obtainable gladly accept the honorable dedication
in he discovered an exact solution of the einstein field equation representing cylindrically symmetric rigidly rotating configuration of dust particles
this was later rediscovered by willem jacob van stockum and is known today as the van stockum dust
it is one of the simplest known exact solutions in general relativity and is regarded as an important example in part because it exhibits closed timelike curves
lanczos served as assistant to albert einstein during the period of
in lanczos married maria rupp
he was offered one year visiting professorship from purdue university
for dozen years lanczos split his life between two continents
his wife maria rupp stayed with lanczos parents in sz kesfeh rv year around while lanczos went to purdue for half the year teaching graduate students matrix mechanics and tensor analysis
in his son elmar was born elmar came to lafayette indiana with his father in august just before ww ii broke out
maria was too ill to travel and died several weeks later from tuberculosis
when the nazis purged hungary of jews in of lanczos family only his sister and nephew survived
elmar married moved to seattle and raised two sons
when elmar looked at his own firstborn son he said for me it proves that hitler did not win
during the mccarthy era lanczos came under suspicion for possible communist links
in he left the and moved to the school of theoretical physics at the dublin institute for advanced studies in ireland where he succeeded erwin schr dinger and stayed until his death in in lanczos published applied analysis
the topics covered include algebraic equations matrices and eigenvalue problems large scale linear systems harmonic analysis data analysis quadrature and power expansions illustrated by numerical examples worked out in detail
the contents of the book are stylized parexic analysis lies between classical analysis and numerical analysis it is roughly the theory of approximation by finite or truncated infinite algorithms
research lanczos did pioneering work along with danielson on what is now called the fast fourier transform fft but the significance of his discovery was not appreciated at the time and today the fft is credited to cooley and tukey
as matter of fact similar claims can be made for several other mathematicians including carl friedrich gauss
lanczos was the one who introduced chebyshev polynomials to numerical computing
he discovered the diagonalizable matrix
working in washington dc at the national bureau of standards after lanczos developed number of techniques for mathematical calculations using digital computers including the lanczos algorithm for finding eigenvalues of large symmetric matrices the lanczos approximation for the gamma function the conjugate gradient method for solving systems of linear equations in lanczos showed that the weyl tensor which plays fundamental role in general relativity can be obtained from tensor potential that is now called the lanczos potential
lanczos resampling is based on windowed sinc function as practical upsampling filter approximating the ideal sinc function
lanczos resampling is widely used in video up sampling for digital zoom applications and image scaling
books such as the variational principles of mechanics is classic graduate text on mechanics
he shows his explanatory ability and enthusiasm as physics teacher in the preface of the first edition he says it is taught for two semester graduate course of three hours weekly
publications books the variational principles of mechanics dedicated to albert einstein university of toronto press isbn followed by editions
isbn applied analysis prentice hall linear differential operators van nostrand company isbn the variational principles of mechanics nd ed
the variational principles of mechanics rd ed
albert einstein and the cosmic world order six lectures delivered at the university of michigan in the spring of interscience publishers discourse on fourier series oliver boyd numbers without end edinburgh oliver boyd the variational principles of mechanics th ed
judaism and science leeds university press isbn pages brodetsky memorial lecture space through the ages the evolution of the geometric ideas from pythagoras to hilbert and einstein academic press isbn review by max jammer on science magazine december
the einstein decade granada publishing isbn william davis editor cornelius lanczos collected published papers with commentaries north carolina state university isbn articles lanczos kornel
ber eine station re kosmologie im sinne der einsteinschen gravitationstheorie
zeitschrift physik in german
springer science and business media llc
an iteration method for the solution of the eigenvalue problem of linear differential and integral operators journal of research of the national bureau of standards journal of research of the national bureau of standards research paper vol
october los angeles september lanczos
the splitting of the riemann tensor
reviews of modern physics
american physical society aps
see also the martians scientists references brendan scaife
studies in numerical analysis papers in honour of cornelius lanczos
dublin london new york academic press
external links connor john robertson edmund cornelius lanczos mactutor history of mathematics archive university of st andrews cornelius lanczos at the mathematics genealogy project cornelius lanczos collected published papers with commentaries published by north carolina state university photo gallery of lanczos by nicholas higham series of historic video tapes produced in digitalized on the occasion of the th anniversary of cornelius lanczos birth
in coding theory parity check matrix of linear block code is matrix which describes the linear relations that the components of codeword must satisfy
it can be used to decide whether particular vector is codeword and is also used in decoding algorithms
definition formally parity check matrix of linear code is generator matrix of the dual code
this means that codeword is in if and only if the matrix vector product hc some authors would write this in an equivalent form ch
the rows of parity check matrix are the coefficients of the parity check equations
that is they show how linear combinations of certain digits components of each codeword equal zero
for example the parity check matrix compactly represents the parity check equations that must be satisfied for the vector to be codeword of from the definition of the parity check matrix it directly follows the minimum distance of the code is the minimum number such that every columns of parity check matrix are linearly independent while there exist columns of that are linearly dependent
creating parity check matrix the parity check matrix for given code can be derived from its generator matrix and vice versa
if the generator matrix for an code is in standard form then the parity check matrix is given by because negation is performed in the finite field fq
note that if the characteristic of the underlying field is in that field as in binary codes then so the negation is unnecessary
for example if binary code has the generator matrix then its parity check matrix is it can be verified that is matrix while is matrix
syndromes for any row vector of the ambient vector space hx is called the syndrome of the vector is codeword if and only if the calculation of syndromes is the basis for the syndrome decoding algorithm
see also hamming code notes references hill raymond
first course in coding theory
oxford applied mathematics and computing science series
isbn pless vera introduction to the theory of error correcting codes rd ed
wiley interscience isbn roman steven coding and information theory gtm vol
introduction to coding theory
in linear algebra rotation matrix is transformation matrix that is used to perform rotation in euclidean space
for example using the convention below the matrix cos sin sin cos rotates points in the xy plane counterclockwise through an angle with respect to the positive axis about the origin of two dimensional cartesian coordinate system
to perform the rotation on plane point with standard coordinates it should be written as column vector and multiplied by the matrix cos sin sin cos cos sin sin cos
if and are the endpoint coordinates of vector where is cosine and is sine then the above equations become the trigonometric summation angle formulae
indeed rotation matrix can be seen as the trigonometric summation angle formulae in matrix form
one way to understand this is say we have vector at an angle from the axis and we wish to rotate that angle by further
we simply need to compute the vector endpoint coordinates at
the examples in this article apply to active rotations of vectors counterclockwise in right handed coordinate system counterclockwise from by pre multiplication on the left
if any one of these is changed such as rotating axes instead of vectors passive transformation then the inverse of the example matrix should be used which coincides with its transpose
since matrix multiplication has no effect on the zero vector the coordinates of the origin rotation matrices describe rotations about the origin
rotation matrices provide an algebraic description of such rotations and are used extensively for computations in geometry physics and computer graphics
in some literature the term rotation is generalized to include improper rotations characterized by orthogonal matrices with determinant of instead of
these combine proper rotations with reflections which invert orientation
in other cases where reflections are not being considered the label proper may be dropped
the latter convention is followed in this article
rotation matrices are square matrices with real entries
more specifically they can be characterized as orthogonal matrices with determinant that is square matrix is rotation matrix if and only if rt and det the set of all orthogonal matrices of size with determinant is representation of group known as the special orthogonal group so one example of which is the rotation group so
the set of all orthogonal matrices of size with determinant or is representation of the general orthogonal group
in two dimensions in two dimensions the standard rotation matrix has the following form cos sin sin cos
this rotates column vectors by means of the following matrix multiplication cos sin sin cos
thus the new coordinates of point after rotation are cos sin sin cos
examples for example when the vector is rotated by an angle its new coordinates are cos sin and when the vector is rotated by an angle its new coordinates are sin cos
direction the direction of vector rotation is counterclockwise if is positive
and clockwise if is negative
thus the clockwise rotation matrix is found as cos sin sin cos
the two dimensional case is the only non trivial
not one dimensional case where the rotation matrices group is commutative so that it does not matter in which order multiple rotations are performed
an alternative convention uses rotating axes and the above matrices also represent rotation of the axes clockwise through an angle
non standard orientation of the coordinate system if standard right handed cartesian coordinate system is used with the axis to the right and the axis up the rotation is counterclockwise
if left handed cartesian coordinate system is used with directed to the right but directed down is clockwise
such non standard orientations are rarely used in mathematics but are common in computer graphics which often have the origin in the top left corner and the axis down the screen or page see below for other alternative conventions which may change the sense of the rotation produced by rotation matrix
common rotations particularly useful are the matrices for and counter clockwise rotations
relationship with complex plane since the matrices of the shape form ring isomorphic to the field of the complex numbers under this isomorphism the rotation matrices correspond to circle of the unit complex numbers the complex numbers of modulus if one identifies with through the linear isomorphism the action of matrix of the above form on vectors of corresponds to the multiplication by the complex number iy and rotations correspond to multiplication by complex numbers of modulus as every rotation matrix can be written cos sin sin cos the above correspondence associates such matrix with the complex number cos sin this last equality is euler formula
in three dimensions basic rotations basic rotation also called elemental rotation is rotation about one of the axes of coordinate system
the following three basic rotation matrices rotate vectors by an angle about the or axis in three dimensions using the right hand rule which codifies their alternating signs
the same matrices can also represent clockwise rotation of the axes
cos sin sin cos cos sin sin cos cos sin sin cos for column vectors each of these basic vector rotations appears counterclockwise when the axis about which they occur points toward the observer the coordinate system is right handed and the angle is positive
rz for instance would rotate toward the axis vector aligned with the axis as can easily be checked by operating with rz on the vector cos sin sin cos this is similar to the rotation produced by the above mentioned two dimensional rotation matrix
see below for alternative conventions which may apparently or actually invert the sense of the rotation produced by these matrices
general rotations other rotation matrices can be obtained from these three using matrix multiplication
for example the product cos sin sin cos yaw cos sin sin cos pitch cos sin sin cos roll cos cos cos sin sin sin cos cos sin cos sin sin sin cos sin sin sin cos cos sin sin cos cos sin sin cos sin cos cos represents rotation whose yaw pitch and roll angles are and respectively
more formally it is an intrinsic rotation whose tait bryan angles are about axes respectively
similarly the product cos sin sin cos cos sin sin cos cos sin sin cos cos cos sin sin cos cos sin cos sin cos sin sin cos sin sin sin sin cos cos cos sin sin sin cos sin sin cos cos cos represents an extrinsic rotation whose improper euler angles are about axes these matrices produce the desired effect only if they are used to premultiply column vectors and since in general matrix multiplication is not commutative only if they are applied in the specified order see ambiguities for more details
the order of rotation operations is from right to left the matrix adjacent to the column vector is the first to be applied and then the one to the left
conversion from rotation matrix to axis angle every rotation in three dimensions is defined by its axis vector along this axis is unchanged by the rotation and its angle the amount of rotation about that axis euler rotation theorem
there are several methods to compute the axis and angle from rotation matrix see also axis angle representation
here we only describe the method based on the computation of the eigenvectors and eigenvalues of the rotation matrix
it is also possible to use the trace of the rotation matrix
determining the axis given rotation matrix vector parallel to the rotation axis must satisfy since the rotation of around the rotation axis must result in the equation above may be solved for which is unique up to scalar factor unless further the equation may be rewritten which shows that lies in the null space of viewed in another way is an eigenvector of corresponding to the eigenvalue every rotation matrix must have this eigenvalue the other two eigenvalues being complex conjugates of each other
it follows that general rotation matrix in three dimensions has up to multiplicative constant only one real eigenvector
one way to determine the rotation axis is by showing that since rt is skew symmetric matrix we can choose such that
the matrix vector product becomes cross product of vector with itself ensuring that the result is zero therefore if then
the magnitude of computed this way is sin where is the angle of rotation
this does not work if is symmetric
above if rt is zero then all subsequent steps are invalid
in this case it is necessary to diagonalize and find the eigenvector corresponding to an eigenvalue of
determining the angle to find the angle of rotation once the axis of the rotation is known select vector perpendicular to the axis
then the angle of the rotation is the angle between and rv
more direct method however is to simply calculate the trace the sum of the diagonal elements of the rotation matrix
care should be taken to select the right sign for the angle to match the chosen axis tr cos from which follows that the angle absolute value is arccos tr
rotation matrix from axis and angle the matrix of proper rotation by angle around the axis ux uy uz unit vector with is given by cos cos cos sin cos sin cos sin cos cos cos sin cos sin cos sin cos cos
derivation of this matrix from first principles can be found in section here
the basic idea to derive this matrix is dividing the problem into few known simple steps
first rotate the given axis and the point such that the axis lies in one of the coordinate planes xy yz or zx then rotate the given axis and the point such that the axis is aligned with one of the two coordinate axes for that particular coordinate plane or use one of the fundamental rotation matrices to rotate the point depending on the coordinate axis with which the rotation axis is aligned
reverse rotate the axis point pair such that it attains the final configuration as that was in step undoing step reverse rotate the axis point pair which was done in step undoing step this can be written more concisely as cos sin cos where is the cross product matrix of the expression is the outer product and is the identity matrix
alternatively the matrix entries are cos sin if sin sin if where jkl is the levi civita symbol with this is matrix form of rodrigues rotation formula or the equivalent differently parametrized euler rodrigues formula with
in the rotation of vector around the axis by an angle can be written as cos sin if the space is right handed and this rotation will be counterclockwise when points towards the observer right hand rule
explicitly with right handed orthonormal basis cos sin sin cos note the striking merely apparent differences to the equivalent lie algebraic formulation below
properties for any dimensional rotation matrix acting on the rotation is an orthogonal matrix it follows that det rotation is termed proper if det and improper or roto reflection if det
for even dimensions the eigenvalues of proper rotation occur as pairs of complex conjugates which are roots of unity for which is real only for
therefore there may be no vectors fixed by the rotation and thus no axis of rotation
any fixed eigenvectors occur in pairs and the axis of rotation is an even dimensional subspace
for odd dimensions proper rotation will have an odd number of eigenvalues with at least one and the axis of rotation will be an odd dimensional subspace
proof det det det det det det det det
here is the identity matrix and we use det rt det as well as since is odd
therefore det meaning there is null vector with that is rv fixed eigenvector
there may also be pairs of fixed eigenvectors in the even dimensional subspace orthogonal to so the total dimension of fixed eigenvectors is odd
for example in space rotation by angle has eigenvalues ei and so there is no axis of rotation except when the case of the null rotation
in space the axis of non null proper rotation is always unique line and rotation around this axis by angle has eigenvalues ei
in space the four eigenvalues are of the form
the null rotation has the case of is called simple rotation with two unit eigenvalues forming an axis plane and two dimensional rotation orthogonal to the axis plane
otherwise there is no axis plane
the case of is called an isoclinic rotation having eigenvalues repeated twice so every vector is rotated through an angle the trace of rotation matrix is equal to the sum of its eigenvalues
for rotation by angle has trace cos for rotation around any axis by angle has trace cos for and the trace is cos cos which becomes cos for an isoclinic rotation
examples geometry in euclidean geometry rotation is an example of an isometry transformation that moves points without changing the distances between them
rotations are distinguished from other isometries by two additional properties they leave at least one point fixed and they leave handedness unchanged
in contrast translation moves every point reflection exchanges left and right handed ordering glide reflection does both and an improper rotation combines change in handedness with normal rotation
if fixed point is taken as the origin of cartesian coordinate system then every point can be given coordinates as displacement from the origin
thus one may work with the vector space of displacements instead of the points themselves
now suppose pn are the coordinates of the vector from the origin to point choose an orthonormal basis for our coordinates then the squared distance to by pythagoras is which can be computed using the matrix multiplication geometric rotation transforms lines to lines and preserves ratios of distances between points
from these properties it can be shown that rotation is linear transformation of the vectors and thus can be written in matrix form qp
the fact that rotation preserves not just ratios but distances themselves is stated as or because this equation holds for all vectors one concludes that every rotation matrix satisfies the orthogonality condition rotations preserve handedness because they cannot change the ordering of the axes which implies the special matrix condition det equally important it can be shown that any matrix satisfying these two conditions acts as rotation
multiplication the inverse of rotation matrix is its transpose which is also rotation matrix det det the product of two rotation matrices is rotation matrix det det det for multiplication of rotation matrices is generally not commutative
noting that any identity matrix is rotation matrix and that matrix multiplication is associative we may summarize all these properties by saying that the rotation matrices form group which for is non abelian called special orthogonal group and denoted by so so son or son the group of rotation matrices is isomorphic to the group of rotations in an dimensional space
this means that multiplication of rotation matrices corresponds to composition of rotations applied in left to right order of their corresponding matrices
ambiguities the interpretation of rotation matrix can be subject to many ambiguities
in most cases the effect of the ambiguity is equivalent to the effect of rotation matrix inversion for these orthogonal matrices equivalently matrix transpose
alias or alibi passive or active transformation the coordinates of point may change due to either rotation of the coordinate system cs alias or rotation of the point alibi
in the latter case the rotation of also produces rotation of the vector representing in other words either and are fixed while cs rotates alias or cs is fixed while and rotate alibi
any given rotation can be legitimately described both ways as vectors and coordinate systems actually rotate with respect to each other about the same axis but in opposite directions
throughout this article we chose the alibi approach to describe rotations
for instance cos sin sin cos represents counterclockwise rotation of vector by an angle or rotation of cs by the same angle but in the opposite direction
alibi and alias transformations are also known as active and passive transformations respectively
pre multiplication or post multiplication the same point can be represented either by column vector or row vector rotation matrices can either pre multiply column vectors rv or post multiply row vectors wr
however rv produces rotation in the opposite direction with respect to wr
throughout this article rotations produced on column vectors are described by means of pre multiplication
to obtain exactly the same rotation
the same final coordinates of point the equivalent row vector must be post multiplied by the transpose of
right or left handed coordinates the matrix and the vector can be represented with respect to right handed or left handed coordinate system
throughout the article we assumed right handed orientation unless otherwise specified
vectors or forms the vector space has dual space of linear forms and the matrix can act on either vectors or forms
decompositions independent planes consider the rotation matrix
if acts in certain direction purely as scaling by factor then we have so that thus is root of the characteristic polynomial for det
two features are noteworthy
first one of the roots or eigenvalues is which tells us that some direction is unaffected by the matrix
for rotations in three dimensions this is the axis of the rotation concept that has no meaning in any other dimension
second the other two roots are pair of complex conjugates whose product is the constant term of the quadratic and whose sum is cos the negated linear term
this factorization is of interest for rotation matrices because the same thing occurs for all of them
as special cases for null rotation the complex conjugates are both and for rotation they are both
furthermore similar factorization holds for any rotation matrix
if the dimension is odd there will be dangling eigenvalue of and for any dimension the rest of the polynomial factors into quadratic terms like the one here with the two special cases noted
we are guaranteed that the characteristic polynomial will have degree and thus eigenvalues
and since rotation matrix commutes with its transpose it is normal matrix so can be diagonalized
we conclude that every rotation matrix when expressed in suitable coordinate system partitions into independent rotations of two dimensional subspaces at most of them
the sum of the entries on the main diagonal of matrix is called the trace it does not change if we reorient the coordinate system and always equals the sum of the eigenvalues
this has the convenient implication for and rotation matrices that the trace reveals the angle of rotation in the two dimensional space or subspace
for matrix the trace is cos and for matrix it is cos in the three dimensional case the subspace consists of all vectors perpendicular to the rotation axis the invariant direction with eigenvalue
thus we can extract from any rotation matrix rotation axis and an angle and these completely determine the rotation
sequential angles the constraints on rotation matrix imply that it must have the form with therefore we may set cos and sin for some angle to solve for it is not enough to look at alone or alone we must consider both together to place the angle in the correct quadrant using two argument arctangent function
now consider the first column of rotation matrix
although will probably not equal but some value we can use slight variation of the previous computation to find so called givens rotation that transforms the column to zeroing this acts on the subspace spanned by the and axes
we can then repeat the process for the xz subspace to zero acting on the full matrix these two rotations produce the schematic form
shifting attention to the second column givens rotation of the yz subspace can now zero the value
this brings the full matrix to the form which is an identity matrix
thus we have decomposed as an rotation matrix will have or entries below the diagonal to zero
we can zero them by extending the same idea of stepping through the columns with series of rotations in fixed sequence of planes
we conclude that the set of rotation matrices each of which has entries can be parameterized by angles
in three dimensions this restates in matrix form an observation made by euler so mathematicians call the ordered sequence of three angles euler angles
however the situation is somewhat more complicated than we have so far indicated
despite the small dimension we actually have considerable freedom in the sequence of axis pairs we use and we also have some freedom in the choice of angles
thus we find many different conventions employed when three dimensional rotations are parameterized for physics or medicine or chemistry or other disciplines
when we include the option of world axes or body axes different sequences are possible
and while some disciplines call any sequence euler angles others give different names cardano tait bryan roll pitch yaw to different sequences
one reason for the large number of options is that as noted previously rotations in three dimensions and higher do not commute
if we reverse given sequence of rotations we get different outcome
this also implies that we cannot compose two rotations by adding their corresponding angles
thus euler angles are not vectors despite similarity in appearance as triplet of numbers
nested dimensions rotation matrix such as cos sin sin cos suggests rotation matrix cos sin sin cos is embedded in the upper left corner
this is no illusion not just one but many copies of dimensional rotations are found within dimensional rotations as subgroups
each embedding leaves one direction fixed which in the case of matrices is the rotation axis
for example we have cos sin sin cos cos sin sin cos cos sin sin cos fixing the axis the axis and the axis respectively
the rotation axis need not be coordinate axis if is unit vector in the desired direction then sin cos where cos sin is rotation by angle leaving axis fixed
direction in dimensional space will be unit magnitude vector which we may consider point on generalized sphere sn
thus it is natural to describe the rotation group so as combining so and sn
suitable formalism is the fiber bundle where for every direction in the base space sn the fiber over it in the total space so is copy of the fiber space so namely the rotations that keep that direction fixed
thus we can build an rotation matrix by starting with matrix aiming its fixed axis on the ordinary sphere in three dimensional space aiming the resulting rotation on and so on up through sn
point on sn can be selected using numbers so we again have numbers to describe any rotation matrix
in fact we can view the sequential angle decomposition discussed previously as reversing this process
the composition of givens rotations brings the first column and row to so that the remainder of the matrix is rotation matrix of dimension one less embedded so as to leave fixed
skew parameters via cayley formula when an rotation matrix does not include eigenvalue thus none of the planar rotations which it comprises are rotations then is an invertible matrix
most rotation matrices fit this description and for them it can be shown that is skew symmetric matrix thus at and since the diagonal is necessarily zero and since the upper triangle determines the lower one contains independent numbers
conveniently is invertible whenever is skew symmetric thus we can recover the original matrix using the cayley transform which maps any skew symmetric matrix to rotation matrix
in fact aside from the noted exceptions we can produce any rotation matrix in this way
although in practical applications we can hardly afford to ignore rotations the cayley transform is still potentially useful tool giving parameterization of most rotation matrices without trigonometric functions
in three dimensions for example we have cayley
if we condense the skew entries into vector then we produce rotation around the axis for around the axis for and around the axis for
the rotations are just out of reach for in the limit as does approach rotation around the axis and similarly for other directions
decomposition into shears for the case rotation matrix can be decomposed into three shear matrices paeth tan sin tan this is useful for instance in computer graphics since shears can be implemented with fewer multiplication instructions than rotating bitmap directly
on modern computers this may not matter but it can be relevant for very old or low end microprocessors
rotation can also be written as two shears and scaling daubechies sweldens tan sin cos cos cos group theory below follow some basic facts about the role of the collection of all rotation matrices of fixed dimension here mostly in mathematics and particularly in physics where rotational symmetry is requirement of every truly fundamental law due to the assumption of isotropy of space and where the same symmetry when present is simplifying property of many problems of less fundamental nature
examples abound in classical mechanics and quantum mechanics
knowledge of the part of the solutions pertaining to this symmetry applies with qualifications to all such problems and it can be factored out of specific problem at hand thus reducing its complexity
prime example in mathematics and physics would be the theory of spherical harmonics
their role in the group theory of the rotation groups is that of being representation space for the entire set of finite dimensional irreducible representations of the rotation group so
for this topic see rotation group so spherical harmonics
the main articles listed in each subsection are referred to for more detail
lie group the rotation matrices for each form group the special orthogonal group so
this algebraic structure is coupled with topological structure inherited from gl in such way that the operations of multiplication and taking the inverse are analytic functions of the matrix entries
thus so is for each lie group
it is compact and connected but not simply connected
it is also semi simple group in fact simple group with the exception so
the relevance of this is that all theorems and all machinery from the theory of analytic manifolds analytic manifolds are in particular smooth manifolds apply and the well developed representation theory of compact semi simple groups is ready for use
lie algebra the lie algebra so of so is given by and is the space of skew symmetric matrices of dimension see classical group where is the lie algebra of the orthogonal group
for reference the most common basis for so is
exponential map connecting the lie algebra to the lie group is the exponential map which is defined using the standard matrix exponential series for ea for any skew symmetric matrix exp is always rotation matrix an important practical example is the case
in rotation group so it is shown that one can identify every so with an euler vector where is unit magnitude vector
by the properties of the identification is in the null space of thus is left invariant by exp and is hence rotation axis
according to rodrigues rotation formula on matrix form one obtains exp exp exp sin cos where
this is the matrix for rotation around axis by the angle for full detail see exponential map so
baker campbell hausdorff formula the bch formula provides an explicit expression for log exey in terms of series expansion of nested commutators of and this general expansion unfolds as
in the case the general infinite expansion has compact form for suitable trigonometric function coefficients detailed in the baker campbell hausdorff formula for so
as group identity the above holds for all faithful representations including the doublet spinor representation which is simpler
the same explicit formula thus follows straightforwardly through pauli matrices see the derivation for su
for the general case one might use ref
spin group the lie group of rotation matrices so is not simply connected so lie theory tells us it is homomorphic image of universal covering group
often the covering group which in this case is called the spin group denoted by spin is simpler and more natural to work with in the case of planar rotations so is topologically circle
its universal covering group spin is isomorphic to the real line under addition
whenever angles of arbitrary magnitude are used one is taking advantage of the convenience of the universal cover
every rotation matrix is produced by countable infinity of angles separated by integer multiples of
correspondingly the fundamental group of so is isomorphic to the integers in the case of spatial rotations so is topologically equivalent to three dimensional real projective space rp
its universal covering group spin is isomorphic to the sphere
every rotation matrix is produced by two opposite points on the sphere
correspondingly the fundamental group of so is isomorphic to the two element group
we can also describe spin as isomorphic to quaternions of unit norm under multiplication or to certain real matrices or to complex special unitary matrices namely su
the covering maps for the first and the last case are given by and
for detailed account of the su covering and the quaternionic covering see spin group so
many features of these cases are the same for higher dimensions
the coverings are all two to one with so having fundamental group
the natural setting for these groups is within clifford algebra
one type of action of the rotations is produced by kind of sandwich denoted by qvq
more importantly in applications to physics the corresponding spin representation of the lie algebra sits inside the clifford algebra
it can be exponentiated in the usual way to give rise to valued representation also known as projective representation of the rotation group
this is the case with so and su where the valued representation can be viewed as an inverse of the covering map
by properties of covering maps the inverse can be chosen ono to one as local section but not globally
infinitesimal rotations the matrices in the lie algebra are not themselves rotations the skew symmetric matrices are derivatives proportional differences of rotations
an actual differential rotation or infinitesimal rotation matrix has the form where is vanishingly small and so for instance with lx
the computation rules are as usual except that infinitesimals of second order are routinely dropped
with these rules these matrices do not satisfy all the same properties as ordinary finite rotation matrices under the usual treatment of infinitesimals
it turns out that the order in which infinitesimal rotations are applied is irrelevant
to see this exemplified consult infinitesimal rotations so
conversions we have seen the existence of several decompositions that apply in any dimension namely independent planes sequential angles and nested dimensions
in all these cases we can either decompose matrix or construct one
we have also given special attention to rotation matrices and these warrant further attention in both directions stuelpnagel
quaternion given the unit quaternion xi yj zk the equivalent pre multiplied to be used with column vectors rotation matrix is
now every quaternion component appears multiplied by two in term of degree two and if all such terms are zero what is left is an identity matrix
this leads to an efficient robust conversion from any quaternion whether unit or non unit to rotation matrix
given if otherwise we can calculate freed from the demand for unit quaternion we find that nonzero quaternions act as homogeneous coordinates for rotation matrices
the cayley transform discussed earlier is obtained by scaling the quaternion so that its component is for rotation around any axis will be zero which explains the cayley limitation
the sum of the entries along the main diagonal the trace plus one equals which is
thus we can write the trace itself as and from the previous version of the matrix we see that the diagonal entries themselves have the same form and so we can easily compare the magnitudes of all four quaternion components using the matrix diagonal
we can in fact obtain all four magnitudes using sums and square roots and choose consistent signs using the skew symmetric part of the off diagonal entries tr the trace of sgn sgn sgn alternatively use single square root and division tr this is numerically stable so long as the trace is not negative otherwise we risk dividing by nearly zero
in that case suppose qxx is the largest diagonal entry so will have the largest magnitude the other cases are derived by cyclic permutation then the following is safe
if the matrix contains significant error such as accumulated numerical error we may construct symmetric matrix and find the eigenvector of its largest magnitude eigenvalue
if is truly rotation matrix that value will be
the quaternion so obtained will correspond to the rotation matrix closest to the given matrix bar itzhack note formulation of the cited article is post multiplied works with row vectors
polar decomposition if the matrix is nonsingular its columns are linearly independent vectors thus the gram schmidt process can adjust them to be an orthonormal basis
stated in terms of numerical linear algebra we convert to an orthogonal matrix using qr decomposition
however we often prefer closest to which this method does not accomplish
for that the tool we want is the polar decomposition fan hoffman higham
to measure closeness we may use any matrix norm invariant under orthogonal transformations
convenient choice is the frobenius norm squared which is the sum of the squares of the element differences
writing this in terms of the trace tr our goal is find minimizing tr subject to qtq though written in matrix terms the objective function is just quadratic polynomial
we can minimize it in the usual way by finding where its derivative is zero
for matrix the orthogonality constraint implies six scalar equalities that the entries of must satisfy
to incorporate the constraint we may employ standard technique lagrange multipliers assembled as symmetric matrix thus our method is differentiate tr qtq with respect to the entries of and equate to zero
in general we obtain the equation so that where is orthogonal and is symmetric
to ensure minimum the matrix and hence must be positive definite
linear algebra calls qs the polar decomposition of with the positive square root of mtm
when is non singular the and factors of the polar decomposition are uniquely determined
however the determinant of is positive because is positive definite so inherits the sign of the determinant of that is is only guaranteed to be orthogonal not rotation matrix
this is unavoidable an with negative determinant has no uniquely defined closest rotation matrix
axis and angle to efficiently construct rotation matrix from an angle and unit axis we can take advantage of symmetry and skew symmetry within the entries
if and are the components of the unit vector representing the axis and cos sin then determining an axis and angle like determining quaternion is only possible up to the sign that is and correspond to the same rotation matrix just like and
additionally axis angle extraction presents additional difficulties
the angle can be restricted to be from to but angles are formally ambiguous by multiples of
when the angle is zero the axis is undefined
when the angle is the matrix becomes symmetric which has implications in extracting the axis
near multiples of care is needed to avoid numerical problems in extracting the angle two argument arctangent with atan sin cos equal to avoids the insensitivity of arccos and in computing the axis magnitude in order to force unit magnitude brute force approach can lose accuracy through underflow moler morrison
partial approach is as follows atan the and components of the axis would then be divided by fully robust approach will use different algorithm when the trace of the matrix is negative as with quaternion extraction
when is zero because the angle is zero an axis must be provided from some source other than the matrix
euler angles complexity of conversion escalates with euler angles used here in the broad sense
the first difficulty is to establish which of the twenty four variations of cartesian axis order we will use
suppose the three angles are physics and chemistry may interpret these as while aircraft dynamics may use
one systematic approach begins with choosing the rightmost axis
among all permutations of only two place that axis first one is an even permutation and the other odd
choosing parity thus establishes the middle axis
that leaves two choices for the left most axis either duplicating the first or not
these three choices gives us variations we double that to by choosing static or rotating axes
this is enough to construct matrix from angles but triples differing in many ways can give the same rotation matrix
for example suppose we use the zyz convention above then we have the following equivalent pairs angles for any order can be found using concise common routine herter lott shoemake
the problem of singular alignment the mathematical analog of physical gimbal lock occurs when the middle rotation aligns the axes of the first and last rotations
it afflicts every axis order at either even or odd multiples of
these singularities are not characteristic of the rotation matrix as such and only occur with the usage of euler angles
the singularities are avoided when considering and manipulating the rotation matrix as orthonormal row vectors in applications often named the right vector up vector and out vector instead of as angles
the singularities are also avoided when working with quaternions
vector to vector formulation in some instances it is interesting to describe rotation by specifying how vector is mapped into another through the shortest path smallest angle
in this completely describes the associated rotation matrix
in general given the matrix belongs to so and maps to
uniform random rotation matrices we sometimes need to generate uniformly distributed random rotation matrix
it seems intuitively clear in two dimensions that this means the rotation angle is uniformly distributed between and
that intuition is correct but does not carry over to higher dimensions
for example if we decompose rotation matrices in axis angle form the angle should not be uniformly distributed the probability that the magnitude of the angle is at most should be sin for since so is connected and locally compact lie group we have simple standard criterion for uniformity namely that the distribution be unchanged when composed with any arbitrary rotation lie group translation
this definition corresponds to what is called haar measure
le mass rivest show how to use the cayley transform to generate and test matrices according to this criterion
we can also generate uniform distribution in any dimension using the subgroup algorithm of diaconis shashahani
this recursively exploits the nested dimensions group structure of so as follows
generate uniform angle and construct rotation matrix
to step from to generate vector uniformly distributed on the sphere sn embed the matrix in the next larger size with last column and rotate the larger matrix so the last column becomes as usual we have special alternatives for the case
each of these methods begins with three independent random scalars uniformly distributed on the unit interval
arvo takes advantage of the odd dimension to change householder reflection to rotation by negation and uses that to aim the axis of uniform planar rotation
another method uses unit quaternions
multiplication of rotation matrices is homomorphic to multiplication of quaternions and multiplication by unit quaternion rotates the unit sphere
since the homomorphism is local isometry we immediately conclude that to produce uniform distribution on so we may use uniform distribution on
in practice create four element vector where each element is sampling of normal distribution
normalize its length and you have uniformly sampled random unit quaternion which represents uniformly sampled random rotation
note that the aforementioned only applies to rotations in dimension for generalised idea of quaternions one should look into rotors
euler angles can also be used though not with each angle uniformly distributed murnaghan miles
for the axis angle form the axis is uniformly distributed over the unit sphere of directions while the angle has the nonuniform distribution over noted previously miles
see also remarks notes references arvo james fast random rotation matrices in david kirk ed
graphics gems iii san diego academic press professional pp
bibcode grge book isbn baker andrew matrix groups an introduction to lie group theory springer isbn bar itzhack itzhack
nov dec new method for extracting the quaternion from rotation matrix journal of guidance control and dynamics bibcode jgcd doi issn bj rck ke bowie clazett june an iterative algorithm for computing the best estimate of an orthogonal matrix siam journal on numerical analysis bibcode sjna doi issn cayley arthur sur quelques propri des terminants gauches journal die reine und angewandte mathematik doi crll issn cid reprinted as article in cayley arthur the collected mathematical papers of arthur cayley vol
cambridge university press pp
diaconis persi shahshahani mehrdad the subgroup algorithm for generating uniform random variables probability in the engineering and informational sciences doi issn cid eng kenth june on the bch formula in so bit numerical mathematics doi issn cid fan ky hoffman alan
february some metric inequalities in the space of matrices proceedings of the american mathematical society doi issn jstor fulton william harris joe representation theory first course graduate texts in mathematics vol
new york berlin heidelberg springer isbn mr goldstein herbert poole charles safko john classical mechanics third ed
addison wesley isbn hall brian lie groups lie algebras and representations an elementary introduction springer isbn gtm herter thomas lott klaus september october algorithms for decomposing orthogonal matrices into primitive rotations computers graphics doi issn higham nicholas
october matrix nearness problems and applications in gover michael barnett stephen eds
applications of matrix theory oxford university press pp
isbn le carlos mass jean claude rivest louis paul february statistical model for random rotations journal of multivariate analysis doi jmva issn miles roger december on random rotations in biometrika doi issn jstor moler cleve morrison donald replacing square roots by pythagorean sums ibm journal of research and development doi rd issn murnaghan francis the element of volume of the rotation group proceedings of the national academy of sciences bibcode pnas doi pnas issn pmc pmid murnaghan francis the unitary and rotation groups lectures on applied mathematics washington spartan books cayley arthur the collected mathematical papers of arthur cayley vol
cambridge university press pp
paeth alan fast algorithm for general raster rotation pdf proceedings graphics interface daubechies ingrid sweldens wim factoring wavelet transforms into lifting steps pdf journal of fourier analysis and applications doi bf cid pique michael rotation tools in andrew glassner ed
graphics gems san diego academic press professional pp
isbn press william teukolsky saul vetterling william flannery brian section picking random rotation matrix numerical recipes the art of scientific computing rd ed
new york cambridge university press isbn shepperd stanley may june quaternion from rotation matrix journal of guidance and control doi shoemake ken euler angle conversion in paul heckbert ed
graphics gems iv san diego academic press professional pp
isbn stuelpnagel john october on the parameterization of the three dimensional rotation group siam review bibcode siamr doi issn cid also nasa cr
varadarajan veeravalli lie groups lie algebras and their representation springer isbn gtm wedderburn joseph lectures on matrices ams isbn external links rotation encyclopedia of mathematics ems press rotation matrices at mathworld math awareness month interactive demo requires java rotation matrices at mathpages in italian parametrization of son by generalized euler angles rotation about any point
in coding theory parity check matrix of linear block code is matrix which describes the linear relations that the components of codeword must satisfy
it can be used to decide whether particular vector is codeword and is also used in decoding algorithms
definition formally parity check matrix of linear code is generator matrix of the dual code
this means that codeword is in if and only if the matrix vector product hc some authors would write this in an equivalent form ch
the rows of parity check matrix are the coefficients of the parity check equations
that is they show how linear combinations of certain digits components of each codeword equal zero
for example the parity check matrix compactly represents the parity check equations that must be satisfied for the vector to be codeword of from the definition of the parity check matrix it directly follows the minimum distance of the code is the minimum number such that every columns of parity check matrix are linearly independent while there exist columns of that are linearly dependent
creating parity check matrix the parity check matrix for given code can be derived from its generator matrix and vice versa
if the generator matrix for an code is in standard form then the parity check matrix is given by because negation is performed in the finite field fq
note that if the characteristic of the underlying field is in that field as in binary codes then so the negation is unnecessary
for example if binary code has the generator matrix then its parity check matrix is it can be verified that is matrix while is matrix
syndromes for any row vector of the ambient vector space hx is called the syndrome of the vector is codeword if and only if the calculation of syndromes is the basis for the syndrome decoding algorithm
see also hamming code notes references hill raymond
first course in coding theory
oxford applied mathematics and computing science series
isbn pless vera introduction to the theory of error correcting codes rd ed
wiley interscience isbn roman steven coding and information theory gtm vol
introduction to coding theory
in probability theory and statistics variance is the expectation of the squared deviation of random variable from its population mean or sample mean
variance is measure of dispersion meaning it is measure of how far set of numbers is spread out from their average value
variance has central role in statistics where some ideas that use it include descriptive statistics statistical inference hypothesis testing goodness of fit and monte carlo sampling
variance is an important tool in the sciences where statistical analysis of data is common
the variance is the square of the standard deviation the second central moment of distribution and the covariance of the random variable with itself and it is often represented by var or an advantage of variance as measure of dispersion is that it is more amenable to algebraic manipulation than other measures of dispersion such as the expected absolute deviation for example the variance of sum of uncorrelated random variables is equal to the sum of their variances
disadvantage of the variance for practical applications is that unlike the standard deviation its units differ from the random variable which is why the standard deviation is more commonly reported as measure of dispersion once the calculation is finished
there are two distinct concepts that are both called variance
one as discussed above is part of theoretical probability distribution and is defined by an equation
the other variance is characteristic of set of observations
when variance is calculated from observations those observations are typically measured from real world system
if all possible observations of the system are present then the calculated variance is called the population variance
normally however only subset is available and the variance calculated from this is called the sample variance
the variance calculated from sample is considered an estimate of the full population variance
there are multiple ways to calculate an estimate of the population variance as discussed in the section below
the two kinds of variance are closely related
to see how consider that theoretical probability distribution can be used as generator of hypothetical observations
if an infinite number of observations are generated using distribution then the sample variance calculated from that infinite set will match the value calculated using the distribution equation for variance
etymology the term variance was first introduced by ronald fisher in his paper the correlation between relatives on the supposition of mendelian inheritance the great body of available statistics show us that the deviations of human measurement from its mean follow very closely the normal law of errors and therefore that the variability may be uniformly measured by the standard deviation corresponding to the square root of the mean square error
when there are two independent causes of variability capable of producing in an otherwise uniform population distributions with standard deviations and it is found that the distribution when both causes act together has standard deviation it is therefore desirable in analysing the causes of variability to deal with the square of the standard deviation as the measure of variability
we shall term this quantity the variance definition the variance of random variable is the expected value of the squared deviation from the mean of var
this definition encompasses random variables that are generated by processes that are discrete continuous neither or mixed
the variance can also be thought of as the covariance of random variable with itself var cov
the variance is also equivalent to the second cumulant of probability distribution that generates the variance is typically designated as var or sometimes as or or symbolically as or simply pronounced sigma squared
the expression for the variance can be expanded as follows var in other words the variance of is equal to the mean of the square of minus the square of the mean of this equation should not be used for computations using floating point arithmetic because it suffers from catastrophic cancellation if the two components of the equation are similar in magnitude
for other numerically stable alternatives see algorithms for calculating variance
discrete random variable if the generator of random variable is discrete with probability mass function then var where is the expected value
when such discrete weighted variance is specified by weights whose sum is not then one divides by the sum of the weights
the variance of collection of equally likely values can be written as var where is the average value
that is the variance of set of equally likely values can be equivalently expressed without directly referring to the mean in terms of squared deviations of all pairwise squared distances of points from each other var
absolutely continuous random variable if the random variable has probability density function and is the corresponding cumulative distribution function then var or equivalently var where is the expected value of given by
in these formulas the integrals with respect to and are lebesgue and lebesgue stieltjes integrals respectively
if the function is riemann integrable on every finite interval then var where the integral is an improper riemann integral
examples exponential distribution the exponential distribution with parameter is continuous distribution whose probability density function is given by on the interval
its mean can be shown to be using integration by parts and making use of the expected value already calculated we have thus the variance of is given by var
fair die fair six sided die can be modeled as discrete random variable with outcomes through each with equal probability
the expected value of is therefore the variance of is var the general formula for the variance of the outcome of an sided die is var
commonly used probability distributions the following table lists the variance for some commonly used probability distributions
properties basic properties variance is non negative because the squares are positive or zero var the variance of constant is zero
var conversely if the variance of random variable is then it is almost surely constant
that is it always has the same value var
issues of finiteness if distribution does not have finite expected value as is the case for the cauchy distribution then the variance cannot be finite either
however some distributions may not have finite variance despite their expected value being finite
an example is pareto distribution whose index satisfies
decomposition the general formula for variance decomposition or the law of total variance is if and are two random variables and the variance of exists then var var var
the conditional expectation of given and the conditional variance var may be understood as follows
given any particular value of the random variable there is conditional expectation given the event this quantity depends on the particular value it is function
that same function evaluated at the random variable is the conditional expectation
in particular if is discrete random variable assuming possible values with corresponding probabilities then in the formula for total variance the first term on the right hand side becomes var where var
similarly the second term on the right hand side becomes var where and thus the total variance is given by var
similar formula is applied in analysis of variance where the corresponding formula is total between within here refers to the mean of the squares
in linear regression analysis the corresponding formula is total regression residual
this can also be derived from the additivity of variances since the total observed score is the sum of the predicted score and the error score where the latter two are uncorrelated
similar decompositions are possible for the sum of squared deviations sum of squares total between within total regression residual
calculation from the cdf the population variance for non negative random variable can be expressed in terms of the cumulative distribution function using this expression can be used to calculate the variance in situations where the cdf but not the density can be conveniently expressed
characteristic property the second moment of random variable attains the minimum value when taken around the first moment mean of the random variable
conversely if continuous function satisfies for all random variables then it is necessarily of the form where this also holds in the multidimensional case
units of measurement unlike the expected absolute deviation the variance of variable has units that are the square of the units of the variable itself
for example variable measured in meters will have variance measured in meters squared
for this reason describing data sets via their standard deviation or root mean square deviation is often preferred over using the variance
in the dice example the standard deviation is slightly larger than the expected absolute deviation of the standard deviation and the expected absolute deviation can both be used as an indicator of the spread of distribution
the standard deviation is more amenable to algebraic manipulation than the expected absolute deviation and together with variance and its generalization covariance is used frequently in theoretical statistics however the expected absolute deviation tends to be more robust as it is less sensitive to outliers arising from measurement anomalies or an unduly heavy tailed distribution
propagation addition and multiplication by constant variance is invariant with respect to changes in location parameter
that is if constant is added to all values of the variable the variance is unchanged var var
if all values are scaled by constant the variance is scaled by the square of that constant var var
the variance of sum of two random variables is given by var var var cov var var var cov where cov is the covariance
linear combinations in general for the sum of random variables the variance becomes var cov var cov see also general bienaym identity
these results lead to the variance of linear combination as var cov var cov var cov
if the random variables are such that cov then they are said to be uncorrelated
it follows immediately from the expression given earlier that if the random variables are uncorrelated then the variance of their sum is equal to the sum of their variances or expressed symbolically var var
since independent random variables are always uncorrelated see covariance uncorrelatedness and independence the equation above holds in particular when the random variables are independent
thus independence is sufficient but not necessary for the variance of the sum to equal the sum of the variances
matrix notation for the variance of linear combination define as column vector of random variables and as column vector of scalars therefore is linear combination of these random variables where denotes the transpose of also let be the covariance matrix of the variance of is then given by var this implies that the variance of the mean can be written as with column vector of ones var var
sum of variables sum of uncorrelated variables one reason for the use of the variance in preference to other measures of dispersion is that the variance of the sum or the difference of uncorrelated random variables is the sum of their variances var var
this statement is called the bienaym formula and was discovered in it is often made with the stronger condition that the variables are independent but being uncorrelated suffices
so if all the variables have the same variance then since division by is linear transformation this formula immediately implies that the variance of their mean is var var var that is the variance of the mean decreases when increases
this formula for the variance of the mean is used in the definition of the standard error of the sample mean which is used in the central limit theorem
to prove the initial statement it suffices to show that var var var
the general result then follows by induction
starting with the definition var using the linearity of the expectation operator and the assumption of independence or uncorrelatedness of and this further simplifies as follows var var var
sum of correlated variables sum of correlated variables with fixed sample size in general the variance of the sum of variables is the sum of their covariances var cov var cov
note the second equality comes from the fact that cov xi xi var xi
here cov is the covariance which is zero for independent random variables if it exists
the formula states that the variance of sum is equal to the sum of all elements in the covariance matrix of the components
the next expression states equivalently that the variance of the sum is the sum of the diagonal of covariance matrix plus two times the sum of its upper triangular elements or its lower triangular elements this emphasizes that the covariance matrix is symmetric
this formula is used in the theory of cronbach alpha in classical test theory
so if the variables have equal variance and the average correlation of distinct variables is then the variance of their mean is var this implies that the variance of the mean increases with the average of the correlations
in other words additional correlated observations are not as effective as additional independent observations at reducing the uncertainty of the mean
moreover if the variables have unit variance for example if they are standardized then this simplifies to var this formula is used in the spearman brown prediction formula of classical test theory
this converges to if goes to infinity provided that the average correlation remains constant or converges too
so for the variance of the mean of standardized variables with equal correlations or converging average correlation we have lim var therefore the variance of the mean of large number of standardized variables is approximately equal to their average correlation
this makes clear that the sample mean of correlated variables does not generally converge to the population mean even though the law of large numbers states that the sample mean will converge for independent variables
sum of uncorrelated variables with random sample size there are cases when sample is taken without knowing in advance how many observations will be acceptable according to some criterion
in such cases the sample size is random variable whose variation adds to the variation of such that var var var which follows from the law of total variance
if has poisson distribution then var with estimator so the estimator of var becomes giving se weighted sum of variables the scaling property and the bienaym formula along with the property of the covariance cov ax by ab cov jointly imply that var var var cov
this implies that in weighted sum of variables the variable with the largest weight will have disproportionally large weight in the variance of the total
for example if and are uncorrelated and the weight of is two times the weight of then the weight of the variance of will be four times the weight of the variance of the expression above can be extended to weighted sum of multiple variables var var cov product of variables product of independent variables if two variables and are independent the variance of their product is given by var var var var var
equivalently using the basic properties of expectation it is given by var
product of statistically dependent variables in general if two variables are statistically dependent then the variance of their product is given by var cov cov var var cov arbitrary functions the delta method uses second order taylor expansions to approximate the variance of function of one or more random variables see taylor expansions for the moments of functions of random variables
for example the approximate variance of function of one variable is given by var var provided that is twice differentiable and that the mean and variance of are finite
population variance and sample variance real world observations such as the measurements of yesterday rain throughout the day typically cannot be complete sets of all possible observations that could be made
as such the variance calculated from the finite set will in general not match the variance that would have been calculated from the full population of possible observations
this means that one estimates the mean and variance from limited set of observations by using an estimator equation
the estimator is function of the sample of observations drawn without observational bias from the whole population of potential observations
in this example that sample would be the set of actual measurements of yesterday rainfall from available rain gauges within the geography of interest
the simplest estimators for population mean and population variance are simply the mean and variance of the sample the sample mean and uncorrected sample variance these are consistent estimators they converge to the correct value as the number of samples increases but can be improved
estimating the population variance by taking the sample variance is close to optimal in general but can be improved in two ways
most simply the sample variance is computed as an average of squared deviations about the sample mean by dividing by however using values other than improves the estimator in various ways
four common values for the denominator are and is the simplest population variance of the sample eliminates bias minimizes mean squared error for the normal distribution and mostly eliminates bias in unbiased estimation of standard deviation for the normal distribution
firstly if the true population mean is unknown then the sample variance which uses the sample mean in place of the true mean is biased estimator it underestimates the variance by factor of correcting by this factor dividing by instead of is called bessel correction
the resulting estimator is unbiased and is called the corrected sample variance or unbiased sample variance
for example when the variance of single observation about the sample mean itself is obviously zero regardless of the population variance
if the mean is determined in some other way than from the same samples used to estimate the variance then this bias does not arise and the variance can safely be estimated as that of the samples about the independently known mean
secondly the sample variance does not generally minimize mean squared error between sample variance and population variance
correcting for bias often makes this worse one can always choose scale factor that performs better than the corrected sample variance though the optimal scale factor depends on the excess kurtosis of the population see mean squared error variance and introduces bias
this always consists of scaling down the unbiased estimator dividing by number larger than and is simple example of shrinkage estimator one shrinks the unbiased estimator towards zero
for the normal distribution dividing by instead of or minimizes mean squared error
the resulting estimator is biased however and is known as the biased sample variation
population variance in general the population variance of finite population of size with values xi is given by where the population mean is the population variance can also be computed using this is true because the population variance matches the variance of the generating probability distribution
in this sense the concept of population can be extended to continuous random variables with infinite populations
sample variance biased sample variance in many practical situations the true variance of population is not known priori and must be computed somehow
when dealing with extremely large populations it is not possible to count every object in the population so the computation must be performed on sample of the population
sample variance can also be applied to the estimation of the variance of continuous distribution from sample of that distribution
we take sample with replacement of values yn from the population where and estimate the variance on the basis of this sample
directly taking the variance of the sample data gives the average of the squared deviations here denotes the sample mean since the yi are selected randomly both and are random variables
their expected values can be evaluated by averaging over the ensemble of all possible samples of size from the population
for this gives hence gives an estimate of the population variance that is biased by factor of for this reason is referred to as the biased sample variance
unbiased sample variance correcting for this bias yields the unbiased sample variance denoted either estimator may be simply referred to as the sample variance when the version can be determined by context
the same proof is also applicable for samples taken from continuous probability distribution
the use of the term is called bessel correction and it is also used in sample covariance and the sample standard deviation the square root of variance
the square root is concave function and thus introduces negative bias by jensen inequality which depends on the distribution and thus the corrected sample standard deviation using bessel correction is biased
the unbiased estimation of standard deviation is technically involved problem though for the normal distribution using the term yields an almost unbiased estimator
the unbiased sample variance is statistic for the function meaning that it is obtained by averaging sample statistic over element subsets of the population
distribution of the sample variance being function of random variables the sample variance is itself random variable and it is natural to study its distribution
in the case that yi are independent observations from normal distribution cochran theorem shows that follows scaled chi squared distribution see also asymptotic properties as direct consequence it follows that and var var var if the yi are independent and identically distributed but not necessarily normally distributed then var where is the kurtosis of the distribution and is the fourth central moment
if the conditions of the law of large numbers hold for the squared observations is consistent estimator of
one can see indeed that the variance of the estimator tends asymptotically to zero
an asymptotically equivalent formula was given in kenney and keeping rose and smith and weisstein
samuelson inequality samuelson inequality is result that states bounds on the values that individual observations in sample can take given that the sample mean and biased variance have been calculated
values must lie within the limits
relations with the harmonic and arithmetic means it has been shown that for sample of positive real numbers max where ymax is the maximum of the sample is the arithmetic mean is the harmonic mean of the sample and is the biased variance of the sample
this bound has been improved and it is known that variance is bounded by max max max min min min where ymin is the minimum of the sample
tests of equality of variances the test of equality of variances and the chi square tests are adequate when the sample is normally distributed
non normality makes testing for the equality of two or more variances more difficult
several non parametric tests have been proposed these include the barton david ansari freund siegel tukey test the capon test mood test the klotz test and the sukhatme test
the sukhatme test applies to two variances and requires that both medians be known and equal to zero
the mood klotz capon and barton david ansari freund siegel tukey tests also apply to two variances
they allow the median to be unknown but do require that the two medians are equal
the lehmann test is parametric test of two variances
of this test there are several variants known
other tests of the equality of variances include the box test the box anderson test and the moses test
resampling methods which include the bootstrap and the jackknife may be used to test the equality of variances
moment of inertia the variance of probability distribution is analogous to the moment of inertia in classical mechanics of corresponding mass distribution along line with respect to rotation about its center of mass
it is because of this analogy that such things as the variance are called moments of probability distributions
the covariance matrix is related to the moment of inertia tensor for multivariate distributions
the moment of inertia of cloud of points with covariance matrix of is given by tr
this difference between moment of inertia in physics and in statistics is clear for points that are gathered along line
suppose many points are close to the axis and distributed along it
the covariance matrix might look like
that is there is the most variance in the direction
physicists would consider this to have low moment about the axis so the moment of inertia tensor is
semivariance the semivariance is calculated in the same manner as the variance but only those observations that fall below the mean are included in the calculation it is also described as specific measure in different fields of application
for skewed distributions the semivariance can provide additional information that variance does not for inequalities associated with the semivariance see chebyshev inequality semivariances
generalizations for complex variables if is scalar complex valued random variable with values in then its variance is where is the complex conjugate of this variance is real scalar
for vector valued random variables as matrix if is vector valued random variable with values in and thought of as column vector then natural generalization of variance is where and is the transpose of and so is row vector
the result is positive semi definite square matrix commonly referred to as the variance covariance matrix or simply as the covariance matrix
if is vector and complex valued random variable with values in then the covariance matrix is where is the conjugate transpose of this matrix is also positive semi definite and square
as scalar another generalization of variance for vector valued random variables which results in scalar value rather than in matrix is the generalized variance det the determinant of the covariance matrix
the generalized variance can be shown to be related to the multidimensional scatter of points around their mean different generalization is obtained by considering the euclidean distance between the random variable and its mean
this results in tr which is the trace of the covariance matrix
see also bhatia davis inequality coefficient of variation homoscedasticity least squares spectral analysis for computing frequency spectrum with spectral magnitudes in of variance or in db popoviciu inequality on variances measures for statistical dispersion variance stabilizing transformation types of variance correlation distance variance explained variance pooled variance pseudo variance references
in probability theory and statistics covariance matrix also known as auto covariance matrix dispersion matrix variance matrix or variance covariance matrix is square matrix giving the covariance between each pair of elements of given random vector
any covariance matrix is symmetric and positive semi definite and its main diagonal contains variances the covariance of each element with itself
intuitively the covariance matrix generalizes the notion of variance to multiple dimensions
as an example the variation in collection of random points in two dimensional space cannot be characterized fully by single number nor would the variances in the and directions contain all of the necessary information matrix would be necessary to fully characterize the two dimensional variation
the covariance matrix of random vector is typically denoted by or
definition throughout this article boldfaced unsubscripted and are used to refer to random vectors and unboldfaced subscripted and are used to refer to scalar random variables
if the entries in the column vector
are random variables each with finite variance and expected value then the covariance matrix is the matrix whose entry is the covariance cov where the operator denotes the expected value mean of its argument
conflicting nomenclatures and notations nomenclatures differ
some statisticians following the probabilist william feller in his two volume book an introduction to probability theory and its applications call the matrix the variance of the random vector because it is the natural generalization to higher dimensions of the dimensional variance
others call it the covariance matrix because it is the matrix of covariances between the scalar components of the vector var cov
both forms are quite standard and there is no ambiguity between them
the matrix is also often called the variance covariance matrix since the diagonal terms are in fact variances
by comparison the notation for the cross covariance matrix between two vectors is cov
properties relation to the autocorrelation matrix the auto covariance matrix is related to the autocorrelation matrix by where the autocorrelation matrix is defined as
relation to the correlation matrix an entity closely related to the covariance matrix is the matrix of pearson product moment correlation coefficients between each of the random variables in the random vector which can be written as corr diag diag where diag is the matrix of the diagonal elements of diagonal matrix of the variances of for
equivalently the correlation matrix can be seen as the covariance matrix of the standardized random variables for corr
each element on the principal diagonal of correlation matrix is the correlation of random variable with itself which always equals each off diagonal element is between and inclusive
inverse of the covariance matrix the inverse of this matrix if it exists is the inverse covariance matrix or inverse concentration matrix also known as the precision matrix or concentration matrix just as the covariance matrix can be written as the rescaling of correlation matrix by the marginal variances cov so using the idea of partial correlation and partial variance the inverse covariance matrix can be expressed analogously cov this duality motivates number of other dualities between marginalizing and conditioning for gaussian random variables
basic properties for var and where is dimensional random variable the following basic properties apply is positive semidefinite
for all is symmetric
non random matrix and constant vector one has var var if is another random vector with the same dimension as then var var cov cov var where cov is the cross covariance matrix of and
block matrices the joint mean and joint covariance matrix of and can be written in block form where var var and cov
and can be identified as the variance matrices of the marginal distributions for and respectively
if and are jointly normally distributed then the conditional distribution for given is given by defined by conditional mean and conditional variance the matrix is known as the matrix of regression coefficients while in linear algebra is the schur complement of in the matrix of regression coefficients may often be given in transpose form suitable for post multiplying row vector of explanatory variables rather than pre multiplying column vector in this form they correspond to the coefficients obtained by inverting the matrix of the normal equations of ordinary least squares ols
partial covariance matrix covariance matrix with all non zero elements tells us that all the individual random variables are interrelated
this means that the variables are not only directly correlated but also correlated via other variables indirectly
often such indirect common mode correlations are trivial and uninteresting
they can be suppressed by calculating the partial covariance matrix that is the part of covariance matrix that shows only the interesting part of correlations
if two vectors of random variables and are correlated via another vector the latter correlations are suppressed in matrix pcov cov cov cov cov
the partial covariance matrix is effectively the simple covariance matrix as if the uninteresting random variables were held constant
covariance matrix as parameter of distribution if column vector of possibly correlated random variables is jointly normally distributed or more generally elliptically distributed then its probability density function can be expressed in terms of the covariance matrix as follows exp where and is the determinant of
covariance matrix as linear operator applied to one vector the covariance matrix maps linear combination of the random variables onto vector of covariances with those variables cov
treated as bilinear form it yields the covariance between the two linear combinations cov
the variance of linear combination is then its covariance with itself
similarly the pseudo inverse covariance matrix provides an inner product which induces the mahalanobis distance measure of the unlikelihood of which matrices are covariance matrices
from the identity just above let be real valued vector then var var which must always be nonnegative since it is the variance of real valued random variable so covariance matrix is always positive semidefinite matrix
the above argument can be expanded as follows where the last inequality follows from the observation that is scalar
conversely every symmetric positive semi definite matrix is covariance matrix
to see this suppose is symmetric positive semidefinite matrix
from the finite dimensional case of the spectral theorem it follows that has nonnegative symmetric square root which can be denoted by
let be any column vector valued random variable whose covariance matrix is the identity matrix
then var var complex random vectors the variance of complex scalar valued random variable with expected value is conventionally defined using complex conjugation var where the complex conjugate of complex number is denoted thus the variance of complex random variable is real number
if is column vector of complex valued random variables then the conjugate transpose is formed by both transposing and conjugating
in the following expression the product of vector with its conjugate transpose results in square matrix called the covariance matrix as its expectation cov the matrix so obtained will be hermitian positive semidefinite with real numbers in the main diagonal and complex numbers off diagonal
propertiesthe covariance matrix is hermitian matrix
the diagonal elements of the covariance matrix are real
pseudo covariance matrix for complex random vectors another kind of second central moment the pseudo covariance matrix also called relation matrix is defined as follows cov in contrast to the covariance matrix defined above hermitian transposition gets replaced by transposition in the definition
its diagonal elements may be complex valued it is complex symmetric matrix
estimation if and are centred data matrices of dimension and respectively
with columns of observations of and rows of variables from which the row means have been subtracted then if the row means were estimated from the data sample covariance matrices and can be defined to be or if the row means were known priori these empirical sample covariance matrices are the most straightforward and most often used estimators for the covariance matrices but other estimators also exist including regularised or shrinkage estimators which may have better properties
applications the covariance matrix is useful tool in many different areas
from it transformation matrix can be derived called whitening transformation that allows one to completely decorrelate the data or from different point of view to find an optimal basis for representing the data in compact way see rayleigh quotient for formal proof and additional properties of covariance matrices
this is called principal component analysis pca and the karhunen lo ve transform kl transform
the covariance matrix plays key role in financial economics especially in portfolio theory and its mutual fund separation theorem and in the capital asset pricing model
the matrix of covariances among various assets returns is used to determine under certain assumptions the relative amounts of different assets that investors should in normative analysis or are predicted to in positive analysis choose to hold in context of diversification
use in optimization the evolution strategy particular family of randomized search heuristics fundamentally relies on covariance matrix in its mechanism
the characteristic mutation operator draws the update step from multivariate normal distribution using an evolving covariance matrix
there is formal proof that the evolution strategy covariance matrix adapts to the inverse of the hessian matrix of the search landscape up to scalar factor and small random fluctuations proven for single parent strategy and static model as the population size increases relying on the quadratic approximation
intuitively this result is supported by the rationale that the optimal covariance distribution can offer mutation steps whose equidensity probability contours match the level sets of the landscape and so they maximize the progress rate
covariance mapping in covariance mapping the values of the cov or pcov matrix are plotted as dimensional map
when vectors and are discrete random functions the map shows statistical relations between different regions of the random functions
statistically independent regions of the functions show up on the map as zero level flatland while positive or negative correlations show up respectively as hills or valleys
in practice the column vectors and are acquired experimentally as rows of samples
where is the th discrete value in sample of the random function
the expected values needed in the covariance formula are estimated using the sample mean
and the covariance matrix is estimated by the sample covariance matrix cov where the angular brackets denote sample averaging as before except that the bessel correction should be made to avoid bias
using this estimation the partial covariance matrix can be calculated as pcov cov cov cov cov where the backslash denotes the left matrix division operator which bypasses the requirement to invert matrix and is available in some computational packages such as matlab
illustrates how partial covariance map is constructed on an example of an experiment performed at the flash free electron laser in hamburg
the random function is the time of flight spectrum of ions from coulomb explosion of nitrogen molecules multiply ionised by laser pulse
since only few hundreds of molecules are ionised at each laser pulse the single shot spectra are highly fluctuating
however collecting typically such spectra and averaging them over produces smooth spectrum which is shown in red at the bottom of fig
the average spectrum reveals several nitrogen ions in form of peaks broadened by their kinetic energy but to find the correlations between the ionisation stages and the ion momenta requires calculating covariance map
in the example of fig
spectra and are the same except that the range of the time of flight differs
panel shows panel shows and panel shows their difference which is cov note change in the colour scale
unfortunately this map is overwhelmed by uninteresting common mode correlations induced by laser intensity fluctuating from shot to shot
to suppress such correlations the laser intensity is recorded at every shot put into and pcov is calculated as panels and show
the suppression of the uninteresting correlations is however imperfect because there are other sources of common mode fluctuations than the laser intensity and in principle all these sources should be monitored in vector yet in practice it is often sufficient to overcompensate the partial covariance correction as panel shows where interesting correlations of ion momenta are now clearly visible as straight lines centred on ionisation stages of atomic nitrogen
two dimensional infrared spectroscopy two dimensional infrared spectroscopy employs correlation analysis to obtain spectra of the condensed phase
there are two versions of this analysis synchronous and asynchronous
mathematically the former is expressed in terms of the sample covariance matrix and the technique is equivalent to covariance mapping
see also covariance function multivariate statistics lewandowski kurowicka joe distribution gramian matrix eigenvalue decomposition quadratic form statistics principal components references further reading covariance matrix encyclopedia of mathematics ems press covariance matrix explained with pictures an easy way to visualize covariance matrices
weisstein eric covariance matrix
stochastic processes in physics and chemistry
new york north holland
in quantum chemistry electron valence state perturbation theory nevpt is perturbative treatment applicable to multireference casci type wavefunctions
it can be considered as generalization of the well known second order ller plesset perturbation theory to multireference complete active space cases
the theory is directly integrated into many quantum chemistry packages such as molcas molpro dalton pyscf and orca
the research performed into the development of this theory led to various implementations
the theory here presented refers to the deployment for the single state nevpt where the perturbative correction is applied to single electronic state
research implementations has been also developed for quasi degenerate cases where set of electronic states undergo the perturbative correction at the same time allowing interaction among themselves
the theory development makes use of the quasi degenerate formalism by lindgren and the hamiltonian multipartitioning technique from zaitsevskii and malrieu
theory let be zero order casci wavefunction defined as linear combination of slater determinants obtained diagonalizing the true hamiltonian inside the casci space where is the projector inside the casci space
it is possible to define perturber wavefunctions in nevpt as zero order wavefunctions of the outer space external to cas where electrons are removed from the inactive part core and virtual orbitals and added to the valence part active orbitals
at second order of perturbation decomposing the zero order casci wavefunction as an antisymmetrized product of the inactive part and valence part then the perturber wavefunctions can be written as the pattern of inactive orbitals involved in the procedure can be grouped as collective index so to represent the various perturber wavefunctions as with an enumerator index for the different wavefunctions
the number of these functions is relative to the degree of contraction of the resulting perturbative space
supposing indexes and referring to core orbitals and referring to active orbitals and and referring to virtual orbitals the possible excitation schemes are two electrons from core orbitals to virtual orbitals the active space is not enriched nor depleted of electrons therefore one electron from core orbital to virtual orbital and one electron from core orbital to an active orbital the active space is enriched with one electron therefore one electron from core orbital to virtual orbital and one electron from an active orbital to virtual orbital the active space is depleted with one electron therefore two electrons from core orbitals to active orbitals active space enriched with two electrons two electrons from active orbitals to virtual orbitals active space depleted with two electrons these cases always represent situations where interclass electronic excitations happen
other three excitation schemes involve single interclass excitation plus an intraclass excitation internal to the active space one electron from core orbital to virtual orbital and an internal active active excitation one electron from core orbital to an active orbital and an internal active active excitation one electron from an active orbital to virtual orbital and an internal active active excitation totally uncontracted approach possible approach is to define the perturber wavefunctions into hilbert spaces defined by those determinants with given and labels
the determinants characterizing these spaces can be written as partition comprising the same inactive core virtual part and all possible valence active parts the full dimensionality of these spaces can be exploited to obtain the definition of the perturbers by diagonalizing the hamiltonian inside them this procedure is impractical given its high computational cost for each space diagonalization of the true hamiltonian must be performed
computationally is preferable to improve the theoretical development making use of the modified dyall hamiltonian this hamiltonian behaves like the true hamiltonian inside the cas space having the same eigenvalues and eigenvectors of the true hamiltonian projected onto the cas space
also given the decomposition for the wavefunction defined before the action of the dyall hamiltonian can be partitioned into stripping out the constant contribution of the inactive part and leaving subsystem to be solved for the valence part the total energy is the sum of and the energies of the orbitals involved in the definition of the inactive part this introduces the possibility to perform single diagonalization of the valence dyall hamiltonian on the casci zero order wavefunction and evaluate the perturber energies using the property depicted above
strongly contracted approach different choice in the development of the nevpt approach is to choose single function for each space leading to the strongly contracted sc scheme
set of perturbative operators are used to produce single function for each space defined as the projection inside each space of the application of the hamiltonian to the contracted zero order wavefunction
in other words where is the projector onto the subspace
this can be equivalently written as the application of specific part of the hamiltonian to the zero order wavefunction for each space appropriate operators can be devised
we will not present their definition as it could result overkilling
suffice to say that the resulting perturbers are not normalized and their norm plays an important role in the strongly contracted development
to evaluate these norms the spinless density matrix of rank not higher than three between the functions are needed
an important property of the is that any other function of the space which is orthogonal to do not interact with the zero order wavefunction through the true hamiltonian
it is possible to use the functions as basis set for the expansion of the first order correction to the wavefunction and also for the expression of the zero order hamiltonian by means of spectral decomposition where are the normalized
the expression for the first order correction to the wavefunction is therefore and for the energy is this result still misses definition of the perturber energies which can be defined in computationally advantageous approach by means of the dyall hamiltonian leading to developing the first term and extracting the inactive part of the dyall hamiltonian it can be obtained with equal to the sum of the orbital energies of the newly occupied virtual orbitals minus the orbital energies of the unoccupied core orbitals
the term that still needs to be evaluated is the bracket involving the commutator
this can be obtained developing each operator and substituting
to obtain the final result it is necessary to evaluate koopmans matrices and density matrices involving only active indexes
an interesting case is represented by the contribution for the case which is trivial and can be demonstrated identical to the ller plesset second order contribution nevpt can therefore be seen as generalized form of mp to multireference wavefunctions
partially contracted approach an alternative approach named partially contracted pc is to define the perturber wavefunctions in subspace of with dimensionality higher than one like in case of the strongly contracted approach
to define this subspace set of functions is generated by means of the operators after decontraction of their formulation
for example in the case of the operator the partially contracted approach makes use of functions and
these functions must be orthonormalized and purged of linear dependencies which may arise
the resulting set spans the space
once all the spaces have been defined we can obtain as usual set of perturbers from the diagonalization of the hamiltonian true or dyall inside this space as usual the evaluation of the partially contracted perturbative correction by means of the dyall hamiltonian involves simply manageable entities for nowadays computers
although the strongly contracted approach makes use of perturbative space with very low flexibility in general it provides values in very good agreement with those obtained by the more decontracted space defined for the partially contracted approach
this can be probably explained by the fact that the strongly contracted perturbers are good average of the totally decontracted perturbative space
the partially contracted evaluation has very little overhead in computational cost with respect to the strongly contracted one therefore they are normally evaluated together
properties nevpt is blessed with many important properties making the approach very solid and reliable
these properties arise both from the theoretical approach used and on the dyall hamiltonian particular structure size consistency nevpt is size consistent strict separable
briefly if and are two non interacting systems the energy of the supersystem is equal to the sum of the energy of plus the energy of taken by themselves
this property is of particular importance to obtain correctly behaving dissociation curves
absence of intruder states in perturbation theory divergencies can occur if the energy of some perturber happens to be nearly equal to the energy of the zero order wavefunction
this situation which is due to the presence of an energy difference at the denominator can be avoided if the energies associated to the perturbers are guaranteed to be never nearly equal to the zero order energy
nevpt satisfies this requirement
invariance under active orbital rotation the nevpt results are stable if an intraclass active active orbital mixing occurs
this arises both from the structure of the dyall hamiltonian and the properties of casscf wavefunction
this property has been also extended to the intraclass core core and virtual virtual mixing thanks to the non canonical nevpt approach allowing to apply nevpt evaluation without performing an orbital canonization which is required as we saw previously spin purity is guaranteed the resulting wave functions are guaranteed to be spin pure due to the spin free formalism
efficiency although not formal theoretical property computational efficiency is highly important for the evaluation on medium size molecular systems
the current limit of the nevpt application is largely dependent on the feasibility of the previous casscf evaluation which scales factorially with respect to the active space size
the nevpt implementation using the dyall hamiltonian involves the evaluation of koopmans matrices and density matrices up to the four particle density matrix spanning only active orbitals
this is particularly convenient given the small size of currently used active spaces
partitioning into additive classes the perturbative correction to the energy is additive on eight different contributions
although the evaluation of each contribution has different computational cost this fact can be used to improve performance by parallelizing each contribution to different processor
see also electron correlation perturbation theory quantum mechanics post hartree fock references angeli cimiraglia evangelisti leininger malrieu
introduction of electron valence states for multireference perturbation theory
the journal of chemical physics
electron valence state perturbation theory fast implementation of the strongly contracted variant
doi angeli cimiraglia malrieu
electron valence state perturbation theory spinless formulation and an efficient implementation of the strongly contracted and of the partially contracted variants
the journal of chemical physics
in mathematics matrix norm is vector norm in vector space whose elements vectors are matrices of given dimensions
preliminaries given field of either real or complex numbers let be the vector space of matrices with rows and columns and entries in the field matrix norm is norm on this article will always write such norms with double vertical bars like so
thus the matrix norm is function that must satisfy the following properties for all scalars and matrices positive valued definite absolutely homogeneous sub additive or satisfying the triangle inequality the only feature distinguishing matrices from rearranged vectors is multiplication
matrix norms are particularly useful if they are also sub multiplicative every norm on kn can be rescaled to be sub multiplicative in some books the terminology matrix norm is reserved for sub multiplicative norms
matrix norms induced by vector norms suppose vector norm on and vector norm on are given
any matrix induces linear operator from to with respect to the standard basis and one defines the corresponding induced norm or operator norm or subordinate norm on the space of all matrices as follows where sup denotes the supremum
this norm measures how much the mapping induced by can stretch vectors
depending on the vector norms used notation other than can be used for the operator norm
matrix norms induced by vector norms if the norm for vectors is used for both spaces and then the corresponding operator norm is these induced norms are different from the entry wise norms and the schatten norms for matrices treated below which are also usually denoted by in the special cases of the induced matrix norms can be computed or estimated by which is simply the maximum absolute column sum of the matrix which is simply the maximum absolute row sum of the matrix
in the special case of the euclidean norm or norm for vectors the induced matrix norm is the spectral norm
the two values do not coincide in infinite dimensions see spectral radius for further discussion
the spectral norm of matrix is the largest singular value of the square root of the largest eigenvalue of the matrix where denotes the conjugate transpose of where max represents the largest singular value of matrix also since max max and similarly by singular value decomposition svd
there is another important inequality where is the frobenius norm
equality holds if and only if the matrix is rank one matrix or zero matrix
this inequality can be derived from the fact that the trace of matrix is equal to the sum of its eigenvalues
when we have an equivalent definition for as sup with
it can be shown to be equivalent to the above definitions using the cauchy schwarz inequality
for example for we have that properties any operator norm is consistent with the vector norms that induce it giving suppose and are operator norms induced by the respective pairs of vector norms and
then this follows from and square matrices suppose is an operator norm on the space of square matrices induced by vector norms and then the operator norm is sub multiplicative matrix norm moreover any such norm satisfies the inequality for all positive integers where is the spectral radius of for symmetric or hermitian we have equality in for the norm since in this case the norm is precisely the spectral radius of for an arbitrary matrix we may not have equality for any norm counterexample would be which has vanishing spectral radius
in any case for any matrix norm we have the spectral radius formula consistent and compatible norms matrix norm on is called consistent with vector norm on and vector norm on if for all and all in the special case of and is also called compatible with all induced norms are consistent by definition
also any sub multiplicative matrix norm on induces compatible vector norm on by defining
entry wise matrix norms these norms treat an matrix as vector of size and use one of the familiar vector norms
for example using the norm for vectors we get this is different norm from the induced norm see above and the schatten norm see below but the notation is the same
the special case is the frobenius norm and yields the maximum norm
and lp norms let be the columns of matrix from the original definition the matrix presents data points in dimensional space
the norm is the sum of the euclidean norms of the columns of the matrix the norm as an error function is more robust since the error for each data point column is not squared
it is used in robust data analysis and sparse coding
for the norm can be generalized to the norm as follows
frobenius norm when for the norm it is called the frobenius norm or the hilbert schmidt norm though the latter term is used more frequently in the context of operators on possibly infinite dimensional hilbert space
this norm can be defined in various ways trace min where are the singular values of recall that the trace function returns the sum of diagonal entries of square matrix
the frobenius norm is an extension of the euclidean norm to and comes from the frobenius inner product on the space of all matrices
the frobenius norm is sub multiplicative and is very useful for numerical linear algebra
the sub multiplicativity of frobenius norm can be proved using cauchy schwarz inequality
frobenius norm is often easier to compute than induced norms and has the useful property of being invariant under rotations and unitary operations in general
that is for any unitary matrix this property follows from the cyclic nature of the trace trace trace trace trace trace trace and analogously trace trace trace where we have used the unitary nature of that is
it also satisfies and where is the frobenius inner product and re is the real part of complex number irrelevant for real matrices max norm the max norm is the elementwise norm in the limit as goes to infinity max max
this norm is not sub multiplicative
note that in some literature such as communication complexity an alternative definition of max norm also called the norm refers to the factorization norm min min max schatten norms the schatten norms arise when applying the norm to the vector of singular values of matrix
if the singular values of the matrix are denoted by then the schatten norm is defined by min these norms again share the notation with the induced and entry wise norms but they are different
all schatten norms are sub multiplicative
they are also unitarily invariant which means that for all matrices and all unitary matrices and the most familiar cases are
the case yields the frobenius norm introduced before
the case yields the spectral norm which is the operator norm induced by the vector norm see above
finally yields the nuclear norm also known as the trace norm or the ky fan norm defined as trace min where denotes positive semidefinite matrix such that more precisely since is positive semidefinite matrix its square root is well defined
the nuclear norm is convex envelope of the rank function rank so it is often used in mathematical optimization to search for low rank matrices
monotone norms matrix norm is called monotone if it is monotonic with respect to the loewner order
thus matrix norm is increasing if
the frobenius norm and spectral norm are examples of monotone norms
cut norms another source of inspiration for matrix norms arises from considering matrix as the adjacency matrix of weighted directed graph
the so called cut norm measures how close the associated graph is to being bipartite where km
equivalent definitions up to constant factor impose the conditions or the cut norm is equivalent to the induced operator norm which is itself equivalent to the another norm called the grothendieck norm to define the grothendieck norm first note that linear operator is just scalar and thus extends to linear operator on any kk kk
moreover given any choice of basis for kn and km any linear operator kn km extends to linear operator kk kk by letting each matrix element on elements of kk via scalar multiplication
the grothendieck norm is the norm of that extended operator in symbols the grothendieck norm depends on choice of basis usually taken to be the standard basis and equivalence of norms for any two matrix norms and we have that for some positive numbers and for all matrices in other words all norms on are equivalent they induce the same topology on this is true because the vector space has the finite dimension moreover for every vector norm on there exists unique positive real number such that is sub multiplicative matrix norm for every sub multiplicative matrix norm is said to be minimal if there exists no other sub multiplicative matrix norm satisfying
examples of norm equivalence let once again refer to the norm induced by the vector norm as above in the induced norm section
for matrix of rank the following inequalities hold max max another useful inequality between matrix norms is which is special case of lder inequality
see also dual norm logarithmic norm notes references bibliography james demmel applied numerical linear algebra section published by siam carl meyer matrix analysis and applied linear algebra published by siam
john watrous theory of quantum information norms of operators lecture notes university of waterloo kendall atkinson an introduction to numerical analysis published by john wiley sons inc
in statistics sometimes the covariance matrix of multivariate random variable is not known but has to be estimated
estimation of covariance matrices then deals with the question of how to approximate the actual covariance matrix on the basis of sample from the multivariate distribution
simple cases where observations are complete can be dealt with by using the sample covariance matrix
the sample covariance matrix scm is an unbiased and efficient estimator of the covariance matrix if the space of covariance matrices is viewed as an extrinsic convex cone in rp however measured using the intrinsic geometry of positive definite matrices the scm is biased and inefficient estimator
in addition if the random variable has normal distribution the sample covariance matrix has wishart distribution and slightly differently scaled version of it is the maximum likelihood estimate
cases involving missing data heteroscedasticity or autocorrelated residuals require deeper considerations
another issue is the robustness to outliers to which sample covariance matrices are highly sensitive statistical analyses of multivariate data often involve exploratory studies of the way in which the variables change in relation to one another and this may be followed up by explicit statistical models involving the covariance matrix of the variables
thus the estimation of covariance matrices directly from observational data plays two roles to provide initial estimates that can be used to study the inter relationships to provide sample estimates that can be used for model checking estimates of covariance matrices are required at the initial stages of principal component analysis and factor analysis and are also involved in versions of regression analysis that treat the dependent variables in data set jointly with the independent variable as the outcome of random sample
estimation in general context given sample consisting of independent observations xn of dimensional random vector rp column vector an unbiased estimator of the covariance matrix is the sample covariance matrix where is the th observation of the dimensional random vector and the vector is the sample mean
this is true regardless of the distribution of the random variable provided of course that the theoretical means and covariances exist
the reason for the factor rather than is essentially the same as the reason for the same factor appearing in unbiased estimates of sample variances and sample covariances which relates to the fact that the mean is not known and is replaced by the sample mean see bessel correction
in cases where the distribution of the random variable is known to be within certain family of distributions other estimates may be derived on the basis of that assumption
well known instance is when the random variable is normally distributed in this case the maximum likelihood estimator of the covariance matrix is slightly different from the unbiased estimate and is given by derivation of this result is given below
clearly the difference between the unbiased estimator and the maximum likelihood estimator diminishes for large in the general case the unbiased estimate of the covariance matrix provides an acceptable estimate when the data vectors in the observed data set are all complete that is they contain no missing elements
one approach to estimating the covariance matrix is to treat the estimation of each variance or pairwise covariance separately and to use all the observations for which both variables have valid values
assuming the missing data are missing at random this results in an estimate for the covariance matrix which is unbiased
however for many applications this estimate may not be acceptable because the estimated covariance matrix is not guaranteed to be positive semi definite
this could lead to estimated correlations having absolute values which are greater than one and or non invertible covariance matrix
when estimating the cross covariance of pair of signals that are wide sense stationary missing samples do not need be random sub sampling by an arbitrary factor is valid
maximum likelihood estimation for the multivariate normal distribution random vector rp column vector has multivariate normal distribution with nonsingular covariance matrix precisely if rp is positive definite matrix and the probability density function of is det exp where rp is the expected value of the covariance matrix is the multidimensional analog of what in one dimension would be the variance and det normalizes the density so that it integrates to suppose now that xn are independent and identically distributed samples from the distribution above
based on the observed values xn of this sample we wish to estimate
first steps the likelihood function is det exp it is fairly readily shown that the maximum likelihood estimate of the mean vector is the sample mean vector see the section on estimation in the article on the normal distribution for details the process here is similar
since the estimate does not depend on we can just substitute it for in the likelihood function getting det exp and then seek the value of that maximizes the likelihood of the data in practice it is easier to work with log
the trace of matrix now we come to the first surprising step regard the scalar as the trace of matrix
this makes it possible to use the identity tr ab tr ba whenever and are matrices so shaped that both products exist
we get det exp det exp tr det exp tr det exp tr where is sometimes called the scatter matrix and is positive definite if there exists subset of the data consisting of affinely independent observations which we will assume
using the spectral theorem it follows from the spectral theorem of linear algebra that positive definite symmetric matrix has unique positive definite symmetric square root
we can again use the cyclic property of the trace to write det exp tr
then the expression above becomes det det exp tr
the positive definite matrix can be diagonalized and then the problem of finding the value of that maximizes det exp tr since the trace of square matrix equals the sum of eigenvalues trace and eigenvalues the equation reduces to the problem of finding the eigenvalues that maximize exp
this is just calculus problem and we get for all thus assume is the matrix of eigen vectors then times the identity matrix
concluding steps finally we get the sample covariance matrix is the maximum likelihood estimator of the population covariance matrix at this point we are using capital rather than lower case because we are thinking of it as an estimator rather than as an estimate as something random whose probability distribution we could profit by knowing
the random matrix can be shown to have wishart distribution with degrees of freedom
alternative derivation an alternative derivation of the maximum likelihood estimator can be performed via matrix calculus formulae see also differential of determinant and differential of the inverse matrix
it also verifies the aforementioned fact about the maximum likelihood estimate of the mean
re write the likelihood in the log form using the trace trick ln const ln det tr
the differential of this log likelihood is ln tr tr
it naturally breaks down into the part related to the estimation of the mean and to the part related to the estimation of the variance
the first order condition for maximum ln is satisfied when the terms multiplying and are identically zero
assuming the maximum likelihood estimate of is non singular the first order condition for the estimate of the mean vector is which leads to the maximum likelihood estimator this lets us simplify as defined above
then the terms involving in ln can be combined as tr
the first order condition ln will hold when the term in the square bracket is matrix valued zero
pre multiplying the latter by and dividing by gives which of course coincides with the canonical derivation given earlier
dwyer points out that decomposition into two terms such as appears above is unnecessary and derives the estimator in two lines of working
note that it may be not trivial to show that such derived estimator is the unique global maximizer for likelihood function
intrinsic covariance matrix estimation intrinsic expectation given sample of independent observations xn of dimensional zero mean gaussian random variable with covariance the maximum likelihood estimator of is given by the parameter belongs to the set of positive definite matrices which is riemannian manifold not vector space hence the usual vector space notions of expectation
and estimator bias must be generalized to manifolds to make sense of the problem of covariance matrix estimation
this can be done by defining the expectation of manifold valued estimator with respect to the manifold valued point as exp exp where exp exp exp log are the exponential map and inverse exponential map respectively exp and log denote the ordinary matrix exponential and matrix logarithm and is the ordinary expectation operator defined on vector space in this case the tangent space of the manifold
bias of the sample covariance matrix the intrinsic bias vector field of the scm estimator is defined to be exp exp the intrinsic estimator bias is then given by exp
for complex gaussian random variables this bias vector field can be shown to equal where log and is the digamma function
the intrinsic bias of the sample covariance matrix equals exp and the scm is asymptotically unbiased as
similarly the intrinsic inefficiency of the sample covariance matrix depends upon the riemannian curvature of the space of positive definite matrices
shrinkage estimation if the sample size is small and the number of considered variables is large the above empirical estimators of covariance and correlation are very unstable
specifically it is possible to furnish estimators that improve considerably upon the maximum likelihood estimate in terms of mean squared error
moreover for the number of observations is less than the number of random variables the empirical estimate of the covariance matrix becomes singular
it cannot be inverted to compute the precision matrix
as an alternative many methods have been suggested to improve the estimation of the covariance matrix
all of these approaches rely on the concept of shrinkage
this is implicit in bayesian methods and in penalized maximum likelihood methods and explicit in the stein type shrinkage approach
simple version of shrinkage estimator of the covariance matrix is represented by the ledoit wolf shrinkage estimator
one considers convex combination of the empirical estimator with some suitable chosen target the diagonal matrix
subsequently the mixing parameter is selected to maximize the expected accuracy of the shrunken estimator
this can be done by cross validation or by using an analytic estimate of the shrinkage intensity
the resulting regularized estimator can be shown to outperform the maximum likelihood estimator for small samples
for large samples the shrinkage intensity will reduce to zero hence in this case the shrinkage estimator will be identical to the empirical estimator
apart from increased efficiency the shrinkage estimate has the additional advantage that it is always positive definite and well conditioned
various shrinkage targets have been proposed the identity matrix scaled by the average sample variance the single index model the constant correlation model where the sample variances are preserved but all pairwise correlation coefficients are assumed to be equal to one another the two parameter matrix where all variances are identical and all covariances are identical to one another although not identical to the variances the diagonal matrix containing sample variances on the diagonal and zeros everywhere else the identity matrix the shrinkage estimator can be generalized to multi target shrinkage estimator that utilizes several targets simultaneously
software for computing covariance shrinkage estimator is available in packages corpcor and shrinkcovmat in python library scikit learn and in matlab
see also propagation of uncertainty sample mean and sample covariance variance components references
in information theory low density parity check ldpc code is linear error correcting code method of transmitting message over noisy transmission channel
an ldpc code is constructed using sparse tanner graph subclass of the bipartite graph
ldpc codes are capacity approaching codes which means that practical constructions exist that allow the noise threshold to be set very close to the theoretical maximum the shannon limit for symmetric memoryless channel
the noise threshold defines an upper bound for the channel noise up to which the probability of lost information can be made as small as desired
using iterative belief propagation techniques ldpc codes can be decoded in time linear to their block length
ldpc codes are finding increasing use in applications requiring reliable and highly efficient information transfer over bandwidth constrained or return channel constrained links in the presence of corrupting noise
implementation of ldpc codes has lagged behind that of other codes notably turbo codes
the fundamental patent for turbo codes expired on august ldpc codes are also known as gallager codes in honor of robert gallager who developed the ldpc concept in his doctoral dissertation at the massachusetts institute of technology in ldpc codes have also been shown to have ideal combinatorial properties
in his dissertation gallager showed that ldpc codes achieve the gilbert varshamov bound for linear codes over binary fields with high probability
in it was shown that gallager ldpc codes achieve list decoding capacity and also achieve the gilbert varshamov bound for linear codes over general fields
history impractical to implement when first developed by gallager in ldpc codes were forgotten until his work was rediscovered in turbo codes another class of capacity approaching codes discovered in became the coding scheme of choice in the late used for applications such as the deep space network and satellite communications
however the advances in low density parity check codes have seen them surpass turbo codes in terms of error floor and performance in the higher code rate range leaving turbo codes better suited for the lower code rates only
applications in an irregular repeat accumulate ira style ldpc code beat six turbo codes to become the error correcting code in the new dvb standard for digital television
the dvb selection committee made decoder complexity estimates for the turbo code proposals using much less efficient serial decoder architecture rather than parallel decoder architecture
this forced the turbo code proposals to use frame sizes on the order of one half the frame size of the ldpc proposals
in ldpc beat convolutional turbo codes as the forward error correction fec system for the itu hn standard
hn chose ldpc codes over turbo codes because of their lower decoding complexity especially when operating at data rates close to gbit and because the proposed turbo codes exhibited significant error floor at the desired range of operation ldpc codes are also used for gbase ethernet which sends data at gigabits per second over twisted pair cables
as of ldpc codes are also part of the wi fi standard as an optional part of and ac in the high throughput ht phy specification
ldpc is mandatory part of ax wi fi some ofdm systems add an additional outer error correction that fixes the occasional errors the error floor that get past the ldpc correction inner code even at low bit error rates
for example the reed solomon code with ldpc coded modulation rs lcm uses reed solomon outer code
the dvb the dvb and the dvb standards all use bch code outer code to mop up residual errors after ldpc decoding nr uses polar code for the control channels and ldpc for the data channels although ldpc code has had its success in commercial hard disk drives to fully exploit its error correction capability in ssds demands unconventional fine grained flash memory sensing leading to an increased memory read latency
ldpc in ssd is an effective approach to deploy ldpc in ssd with very small latency increase which turns ldpc in ssd into reality
since then ldpc has been widely adopted in commercial ssds in both customer grades and enterprise grades by major storage venders
many tlc and later ssds are using ldpc codes
fast hard decode binary erasure is first attempted which can fall back into the slower but more powerful soft decoding
operational use ldpc codes functionally are defined by sparse parity check matrix
this sparse matrix is often randomly generated subject to the sparsity constraints ldpc code construction is discussed later
these codes were first designed by robert gallager in below is graph fragment of an example ldpc code using forney factor graph notation
in this graph variable nodes in the top of the graph are connected to constraint nodes in the bottom of the graph
this is popular way of graphically representing an ldpc code
the bits of valid message when placed on the at the top of the graph satisfy the graphical constraints
specifically all lines connecting to variable node box with an sign have the same value and all values connecting to factor node box with sign must sum modulo two to zero in other words they must sum to an even number or there must be an even number of odd values
ignoring any lines going out of the picture there are eight possible six bit strings corresponding to valid codewords
this ldpc code fragment represents three bit message encoded as six bits
redundancy is used here to increase the chance of recovering from channel errors
this is linear code with and again ignoring lines going out of the picture the parity check matrix representing this graph fragment is
in this matrix each row represents one of the three parity check constraints while each column represents one of the six bits in the received codeword
in this example the eight codewords can be obtained by putting the parity check matrix into this form through basic row operations in gf step step row is added to row step row and are swapped
step row is added to row from this the generator matrix can be obtained as noting that in the special case of this being binary code or specifically
finally by multiplying all eight possible bit strings by all eight valid codewords are obtained
for example the codeword for the bit string is obtained by where is symbol of mod multiplication
as check the row space of is orthogonal to such that the bit string is found in as the first bits of the codeword
example encoder figure illustrates the functional components of most ldpc encoders
during the encoding of frame the input data bits are repeated and distributed to set of constituent encoders
the constituent encoders are typically accumulators and each accumulator is used to generate parity symbol
single copy of the original data is transmitted with the parity bits to make up the code symbols
the bits from each constituent encoder are discarded
the parity bit may be used within another constituent code
in an example using the dvb rate code the encoded block size is symbols with data bits and parity bits
each constituent code check node encodes data bits except for the first parity bit which encodes data bits
the first data bits are repeated times used in parity codes while the remaining data bits are used in parity codes irregular ldpc code
for comparison classic turbo codes typically use two constituent codes configured in parallel each of which encodes the entire input block of data bits
these constituent encoders are recursive convolutional codes rsc of moderate depth or states that are separated by code interleaver which interleaves one copy of the frame
the ldpc code in contrast uses many low depth constituent codes accumulators in parallel each of which encode only small portion of the input frame
the many constituent codes can be viewed as many low depth state convolutional codes that are connected via the repeat and distribute operations
the repeat and distribute operations perform the function of the interleaver in the turbo code
the ability to more precisely manage the connections of the various constituent codes and the level of redundancy for each input bit give more flexibility in the design of ldpc codes which can lead to better performance than turbo codes in some instances
turbo codes still seem to perform better than ldpcs at low code rates or at least the design of well performing low rate codes is easier for turbo codes
as practical matter the hardware that forms the accumulators is reused during the encoding process
that is once first set of parity bits are generated and the parity bits stored the same accumulator hardware is used to generate next set of parity bits
decoding as with other codes the maximum likelihood decoding of an ldpc code on the binary symmetric channel is an np complete problem
performing optimal decoding for np complete code of any useful size is not practical
however sub optimal techniques based on iterative belief propagation decoding give excellent results and can be practically implemented
the sub optimal decoding techniques view each parity check that makes up the ldpc as an independent single parity check spc code
each spc code is decoded separately using soft in soft out siso techniques such as sova bcjr map and other derivates thereof
the soft decision information from each siso decoding is cross checked and updated with other redundant spc decodings of the same information bit
each spc code is then decoded again using the updated soft decision information
this process is iterated until valid codeword is achieved or decoding is exhausted
this type of decoding is often referred to as sum product decoding
the decoding of the spc codes is often referred to as the check node processing and the cross checking of the variables is often referred to as the variable node processing
in practical ldpc decoder implementation sets of spc codes are decoded in parallel to increase throughput
in contrast belief propagation on the binary erasure channel is particularly simple where it consists of iterative constraint satisfaction
for example consider that the valid codeword from the example above is transmitted across binary erasure channel and received with the first and fourth bit erased to yield
since the transmitted message must have satisfied the code constraints the message can be represented by writing the received message on the top of the factor graph
in this example the first bit cannot yet be recovered because all of the constraints connected to it have more than one unknown bit
in order to proceed with decoding the message constraints connecting to only one of the erased bits must be identified
in this example only the second constraint suffices
examining the second constraint the fourth bit must have been zero since only zero in that position would satisfy the constraint
this procedure is then iterated
the new value for the fourth bit can now be used in conjunction with the first constraint to recover the first bit as seen below
this means that the first bit must be one to satisfy the leftmost constraint
thus the message can be decoded iteratively
for other channel models the messages passed between the variable nodes and check nodes are real numbers which express probabilities and likelihoods of belief
this result can be validated by multiplying the corrected codeword by the parity check matrix
because the outcome the syndrome of this operation is the three one zero vector the resulting codeword is successfully validated
after the decoding is completed the original message bits can be extracted by looking at the first bits of the codeword
while illustrative this erasure example does not show the use of soft decision decoding or soft decision message passing which is used in virtually all commercial ldpc decoders
updating node information in recent years there has also been great deal of work spent studying the effects of alternative schedules for variable node and constraint node update
the original technique that was used for decoding ldpc codes was known as flooding
this type of update required that before updating variable node all constraint nodes needed to be updated and vice versa
in later work by vila casado et al alternative update techniques were studied in which variable nodes are updated with the newest available check node information
the intuition behind these algorithms is that variable nodes whose values vary the most are the ones that need to be updated first
highly reliable nodes whose log likelihood ratio llr magnitude is large and does not change significantly from one update to the next do not require updates with the same frequency as other nodes whose sign and magnitude fluctuate more widely
these scheduling algorithms show greater speed of convergence and lower error floors than those that use flooding
these lower error floors are achieved by the ability of the informed dynamic scheduling ids algorithm to overcome trapping sets of near codewords when nonflooding scheduling algorithms are used an alternative definition of iteration is used
for an ldpc code of rate full iteration occurs when variable and constraint nodes have been updated no matter the order in which they were updated
code construction for large block sizes ldpc codes are commonly constructed by first studying the behaviour of decoders
as the block size tends to infinity ldpc decoders can be shown to have noise threshold below which decoding is reliably achieved and above which decoding is not achieved colloquially referred to as the cliff effect
this threshold can be optimised by finding the best proportion of arcs from check nodes and arcs from variable nodes
an approximate graphical approach to visualising this threshold is an exit chart
the construction of specific ldpc code after this optimization falls into two main types of techniques pseudorandom approaches combinatorial approachesconstruction by pseudo random approach builds on theoretical results that for large block size random construction gives good decoding performance
in general pseudorandom codes have complex encoders but pseudorandom codes with the best decoders can have simple encoders
various constraints are often applied to help ensure that the desired properties expected at the theoretical limit of infinite block size occur at finite block size
combinatorial approaches can be used to optimize the properties of small block size ldpc codes or to create codes with simple encoders
some ldpc codes are based on reed solomon codes such as the rs ldpc code used in the gigabit ethernet standard
compared to randomly generated ldpc codes structured ldpc codes such as the ldpc code used in the dvb standard can have simpler and therefore lower cost hardware in particular codes constructed such that the matrix is circulant matrix yet another way of constructing ldpc codes is to use finite geometries
this method was proposed by kou et al
ldpc codes vs turbo codes ldpc codes can be compared with other powerful coding schemes
in one hand ber performance of turbo codes is influenced by low codes limitations
ldpc codes have no limitations of minimum distance that indirectly means that ldpc codes may be more efficient on relatively large code rates
however ldpc codes are not the complete replacement turbo codes are the best solution at the lower code rates
see also people robert gallager richard hamming claude shannon david mackay irving reed michael luby theory belief propagation graph theory hamming code linear code sparse graph code expander code applications hn itu standard for networking over power lines phone lines and coaxial cable an or gbase gigabit ethernet over twisted pair cmmb china multimedia mobile broadcasting dvb dvb dvb digital video broadcasting nd generation dmb digital video broadcasting wimax ieee standard for microwave communications ieee wi fi standard docsis atsc next generation north america digital terrestrial broadcasting gpp nr data channel other capacity approaching codes turbo codes serial concatenated convolutional codes online codes fountain codes lt codes raptor codes repeat accumulate codes class of simple turbo codes tornado codes ldpc codes designed for erasure decoding polar codes references external links introducing low density parity check codes by sarah johnson ldpc codes brief tutorial by bernhard leiner ldpc codes tu wien archived february at the wayback machine the on line textbook information theory inference and learning algorithms by david mackay discusses ldpc codes in chapter iterative decoding of low density parity check codes by venkatesan guruswami ldpc codes an introduction by amin shokrollahi belief propagation decoding of ldpc codes by amir bennatan princeton university turbo and ldpc codes implementation simulation and standardization west virginia university information theory and coding marko hennh fer tu ilmenau discusses ldpc codes at pages
ldpc codes and performance results dvb link including ldpc coding matlab source code for encoding decoding and simulating ldpc codes is available from variety of locations binary ldpc codes in binary ldpc codes for python core algorithm in ldpc encoder and ldpc decoder in matlab fast forward error correction toolbox aff ct in for fast ldpc simulations
the sample mean or empirical mean and the sample covariance are statistics computed from sample of data on one or more random variables
the sample mean is the average value or mean value of sample of numbers taken from larger population of numbers where population indicates not number of people but the entirety of relevant data whether collected or not
sample of companies sales from the fortune might be used for convenience instead of looking at the population all companies sales
the sample mean is used as an estimator for the population mean the average value in the entire population where the estimate is more likely to be close to the population mean if the sample is large and representative
the reliability of the sample mean is estimated using the standard error which in turn is calculated using the variance of the sample
if the sample is random the standard error falls with the size of the sample and the sample mean distribution approaches the normal distribution as the sample size increases
the term sample mean can also be used to refer to vector of average values when the statistician is looking at the values of several variables in the sample
the sales profits and employees of sample of fortune companies
in this case there is not just sample variance for each variable but sample variance covariance matrix or simply covariance matrix showing also the relationship between each pair of variables
this would be matrix when variables are being considered
the sample covariance is useful in judging the reliability of the sample means as estimators and is also useful as an estimate of the population covariance matrix
due to their ease of calculation and other desirable characteristics the sample mean and sample covariance are widely used in statistics to represent the location and dispersion of the distribution of values in the sample and to estimate the values for the population
definition of the sample mean the sample mean is the average of the values of variable in sample which is the sum of those values divided by the number of values
using mathematical notation if sample of observations on variable is taken from the population the sample mean is under this definition if the sample is taken from the population then the sample mean is as compared to the population mean of even if sample is random it is rarely perfectly representative and other samples would have other sample means even if the samples were all from the same population
the sample for example would have sample mean of if the statistician is interested in variables rather than one each observation having value for each of those variables the overall sample mean consists of sample means for individual variables
let be the ith independently drawn observation on the jth random variable
these observations can be arranged into column vectors each with entries with the column vector giving the th observations of all variables being denoted
the sample mean vector is column vector whose th element is the average value of the observations of the jth variable thus the sample mean vector contains the average of the observations for each variable and is written definition of sample covariance the sample covariance matrix is by matrix with entries where is an estimate of the covariance between the jth variable and the kth variable of the population underlying the data
in terms of the observation vectors the sample covariance is
alternatively arranging the observation vectors as the columns of matrix so that which is matrix of rows and columns
here the sample covariance matrix can be computed as where is an by vector of ones
if the observations are arranged as rows instead of columns so is now row vector and is an matrix whose column is the vector of observations on variable then applying transposes in the appropriate places yields
like covariance matrices for random vector sample covariance matrices are positive semi definite
to prove it note that for any matrix the matrix is positive semi definite
furthermore covariance matrix is positive definite if and only if the rank of the
vectors is unbiasedness the sample mean and the sample covariance matrix are unbiased estimates of the mean and the covariance matrix of the random vector row vector whose jth element is one of the random variables
the sample covariance matrix has in the denominator rather than due to variant of bessel correction in short the sample covariance relies on the difference between each observation and the sample mean but the sample mean is slightly correlated with each observation since it is defined in terms of all observations
if the population mean is known the analogous unbiased estimate using the population mean has in the denominator
this is an example of why in probability and statistics it is essential to distinguish between random variables upper case letters and realizations of the random variables lower case letters
the maximum likelihood estimate of the covariance for the gaussian distribution case has in the denominator as well
the ratio of to approaches for large so the maximum likelihood estimate approximately equals the unbiased estimate when the sample is large
distribution of the sample mean for each random variable the sample mean is good estimator of the population mean where good estimator is defined as being efficient and unbiased
of course the estimator will likely not be the true value of the population mean since different samples drawn from the same distribution will give different sample means and hence different estimates of the true mean
thus the sample mean is random variable not constant and consequently has its own distribution
for random sample of observations on the jth random variable the sample mean distribution itself has mean equal to the population mean and variance equal to where is the population variance
the arithmetic mean of population or population mean is often denoted the sample mean the arithmetic mean of sample of values drawn from the population makes good estimator of the population mean as its expected value is equal to the population mean that is it is an unbiased estimator
the sample mean is random variable not constant since its calculated value will randomly differ depending on which members of the population are sampled and consequently it will have its own distribution
for random sample of independent observations the expected value of the sample mean is and the variance of the sample mean is var if the samples are not independent but correlated then special care has to be taken in order to avoid the problem of pseudoreplication
if the population is normally distributed then the sample mean is normally distributed as follows
if the population is not normally distributed the sample mean is nonetheless approximately normally distributed if is large and
this is consequence of the central limit theorem
weighted samples in weighted sample each vector each set of single observations on each of the random variables is assigned weight without loss of generality assume that the weights are normalized
if they are not divide the weights by their sum
then the weighted mean vector is given by and the elements of the weighted covariance matrix are
if all weights are the same the weighted mean and covariance reduce to the biased sample mean and covariance mentioned above
criticism the sample mean and sample covariance are not robust statistics meaning that they are sensitive to outliers
as robustness is often desired trait particularly in real world applications robust alternatives may prove desirable notably quantile based statistics such as the sample median for location and interquartile range iqr for dispersion
other alternatives include trimming and winsorising as in the trimmed mean and the winsorized mean
see also estimation of covariance matrices scatter matrix unbiased estimation of standard deviation references
in computer science and telecommunication hamming codes are family of linear error correcting codes
hamming codes can detect one bit and two bit errors or correct one bit errors without detection of uncorrected errors
by contrast the simple parity code cannot correct errors and can detect only an odd number of bits in error
hamming codes are perfect codes that is they achieve the highest possible rate for codes with their block length and minimum distance of three richard hamming invented hamming codes in as way of automatically correcting errors introduced by punched card readers
in his original paper hamming elaborated his general idea but specifically focused on the hamming code which adds three parity bits to four bits of data in mathematical terms hamming codes are class of binary linear code
for each integer there is code word with block length and message length hence the rate of hamming codes is which is the highest possible for codes with minimum distance of three the minimal number of bit changes needed to go from any code word to any other code word is three and block length the parity check matrix of hamming code is constructed by listing all columns of length that are non zero which means that the dual code of the hamming code is the shortened hadamard code
the parity check matrix has the property that any two columns are pairwise linearly independent
due to the limited redundancy that hamming codes add to the data they can only detect and correct errors when the error rate is low
this is the case in computer memory usually ram where bit errors are extremely rare and hamming codes are widely used and ram with this correction system is ecc ram ecc memory
in this context an extended hamming code having one extra parity bit is often used
extended hamming codes achieve hamming distance of four which allows the decoder to distinguish between when at most one one bit error occurs and when any two bit errors occur
in this sense extended hamming codes are single error correcting and double error detecting abbreviated as secded
history richard hamming the inventor of hamming codes worked at bell labs in the late on the bell model computer an electromechanical relay based machine with cycle times in seconds
input was fed in on punched paper tape seven eighths of an inch wide which had up to six holes per row
during weekdays when errors in the relays were detected the machine would stop and flash lights so that the operators could correct the problem
during after hours periods and on weekends when there were no operators the machine simply moved on to the next job
hamming worked on weekends and grew increasingly frustrated with having to restart his programs from scratch due to detected errors
in taped interview hamming said and so said damn it if the machine can detect an error why can it locate the position of the error and correct it
over the next few years he worked on the problem of error correction developing an increasingly powerful array of algorithms
in he published what is now known as hamming code which remains in use today in applications such as ecc memory
codes predating hamming number of simple error detecting codes were used before hamming codes but none were as effective as hamming codes in the same overhead of space
parity parity adds single bit that indicates whether the number of ones bit positions with values of one in the preceding data was even or odd
if an odd number of bits is changed in transmission the message will change parity and the error can be detected at this point however the bit that changed may have been the parity bit itself
the most common convention is that parity value of one indicates that there is an odd number of ones in the data and parity value of zero indicates that there is an even number of ones
if the number of bits changed is even the check bit will be valid and the error will not be detected
moreover parity does not indicate which bit contained the error even when it can detect it
the data must be discarded entirely and re transmitted from scratch
on noisy transmission medium successful transmission could take long time or may never occur
however while the quality of parity checking is poor since it uses only single bit this method results in the least overhead
two out of five code two out of five code is an encoding scheme which uses five bits consisting of exactly three and two
this provides ten possible combinations enough to represent the digits
this scheme can detect all single bit errors all odd numbered bit errors and some even numbered bit errors for example the flipping of both bits
however it still cannot correct any of these errors
repetition another code in use at the time repeated every data bit multiple times in order to ensure that it was sent correctly
for instance if the data bit to be sent is an repetition code will send if the three bits received are not identical an error occurred during transmission
if the channel is clean enough most of the time only one bit will change in each triple
therefore and each correspond to bit while and correspond to bit with the greater quantity of digits that are the same or indicating what the data bit should be
code with this ability to reconstruct the original message in the presence of errors is known as an error correcting code
this triple repetition code is hamming code with since there are two parity bits and data bit
such codes cannot correctly repair all errors however
in our example if the channel flips two bits and the receiver gets the system will detect the error but conclude that the original bit is which is incorrect
if we increase the size of the bit string to four we can detect all two bit errors but cannot correct them the quantity of parity bits is even at five bits we can both detect and correct all two bit errors but not all three bit errors
moreover increasing the size of the parity bit string is inefficient reducing throughput by three times in our original case and the efficiency drops drastically as we increase the number of times each bit is duplicated in order to detect and correct more errors
description if more error correcting bits are included with message and if those bits can be arranged such that different incorrect bits produce different error results then bad bits could be identified
in seven bit message there are seven possible single bit errors so three error control bits could potentially specify not only that an error occurred but also which bit caused the error
hamming studied the existing coding schemes including two of five and generalized their concepts
to start with he developed nomenclature to describe the system including the number of data bits and error correction bits in block
for instance parity includes single bit for any data word so assuming ascii words with seven bits hamming described this as an code with eight bits in total of which seven are data
the repetition example would be following the same logic
the code rate is the second number divided by the first for our repetition example
hamming also noticed the problems with flipping two or more bits and described this as the distance it is now called the hamming distance after him
parity has distance of so one bit flip can be detected but not corrected and any two bit flips will be invisible
the repetition has distance of as three bits need to be flipped in the same triple to obtain another code word with no visible errors
it can correct one bit errors or it can detect but not correct two bit errors
repetition each bit is repeated four times has distance of so flipping three bits can be detected but not corrected
when three bits flip in the same group there can be situations where attempting to correct will produce the wrong code word
in general code with distance can detect but not correct errors
hamming was interested in two problems at once increasing the distance as much as possible while at the same time increasing the code rate as much as possible
during the he developed several encoding schemes that were dramatic improvements on existing codes
the key to all of his systems was to have the parity bits overlap such that they managed to check each other as well as the data
general algorithm the following general algorithm generates single error correcting sec code for any number of bits
the main idea is to choose the error correcting bits such that the index xor the xor of all the bit positions containing is we use positions etc
in binary as the error correcting bits which guarantees it is possible to set the error correcting bits so that the index xor of the whole message is if the receiver receives string with index xor they can conclude there were no corruptions and otherwise the index xor indicates the index of the corrupted bit
an algorithm can be deduced from the following description number the bits starting from bit etc
write the bit numbers in binary etc
all bit positions that are powers of two have single bit in the binary form of their position are parity bits etc
all other bit positions with two or more bits in the binary form of their position are data bits
each data bit is included in unique set of or more parity bits as determined by the binary form of its bit position
parity bit covers all bit positions which have the least significant bit set bit the parity bit itself etc
parity bit covers all bit positions which have the second least significant bit set bits etc
parity bit covers all bit positions which have the third least significant bit set bits etc
parity bit covers all bit positions which have the fourth least significant bit set bits etc
in general each parity bit covers all bits where the bitwise and of the parity position and the bit position is non zero if byte of data to be encoded is then the data word using to represent the parity bits would be and the code word is the choice of the parity even or odd is irrelevant but the same choice must be used for both encoding and decoding
this general rule can be shown visually shown are only encoded bits parity data but the pattern continues indefinitely
the key thing about hamming codes that can be seen from visual inspection is that any given bit is included in unique set of parity bits
to check for errors check all of the parity bits
the pattern of errors called the error syndrome identifies the bit in error
if all parity bits are correct there is no error
otherwise the sum of the positions of the erroneous parity bits identifies the erroneous bit
for example if the parity bits in positions and indicate an error then bit is in error
if only one parity bit indicates an error the parity bit itself is in error
with parity bits bits from up to can be covered
after discounting the parity bits bits remain for use as data
as varies we get all the possible hamming codes hamming codes with additional parity secded hamming codes have minimum distance of which means that the decoder can detect and correct single error but it cannot distinguish double bit error of some codeword from single bit error of different codeword
thus some double bit errors will be incorrectly decoded as if they were single bit errors and therefore go undetected unless no correction is attempted
to remedy this shortcoming hamming codes can be extended by an extra parity bit
this way it is possible to increase the minimum distance of the hamming code to which allows the decoder to distinguish between single bit errors and two bit errors
thus the decoder can detect and correct single error and at the same time detect but not correct double error
if the decoder does not attempt to correct errors it can reliably detect triple bit errors
if the decoder does correct errors some triple errors will be mistaken for single errors and corrected to the wrong value
error correction is therefore trade off between certainty the ability to reliably detect triple bit errors and resiliency the ability to keep functioning in the face of single bit errors
this extended hamming code is popular in computer memory systems where it is known as secded abbreviated from single error correction double error detection
particularly popular is the code truncated hamming code plus an additional parity bit which has the same space overhead as parity code
hamming code in hamming introduced the hamming code
it encodes four data bits into seven bits by adding three parity bits
it can detect and correct single bit errors
with the addition of an overall parity bit it can also detect but not correct double bit errors
construction of and the matrix is called canonical generator matrix of linear code and is called parity check matrix
this is the construction of and in standard or systematic form
regardless of form and for linear block codes must satisfy an all zeros matrix since
the parity check matrix of hamming code is constructed by listing all columns of length that are pair wise independent
thus is matrix whose left side is all of the nonzero tuples where order of the tuples in the columns of matrix does not matter
the right hand side is just the identity matrix
so can be obtained from by taking the transpose of the left hand side of with the identity identity matrix on the left hand side of the code generator matrix and the parity check matrix are and finally these matrices can be mutated into equivalent non systematic codes by the following operations column permutations swapping columns elementary row operations replacing row with linear combination of rows encoding examplefrom the above matrix we have codewords
let be row vector of binary data bits
the codeword for any of the possible data vectors is given by the standard matrix product where the summing operation is done modulo
using the generator matrix from above we have after applying modulo to the sum hamming code with an additional parity bit the hamming code can easily be extended to an code by adding an extra parity bit on top of the encoded word see hamming
this can be summed up with the revised matrices and note that is not in standard form
to obtain elementary row operations can be used to obtain an equivalent matrix to in systematic form for example the first row in this matrix is the sum of the second and third rows of in non systematic form
using the systematic construction for hamming codes from above the matrix is apparent and the systematic form of is written as the non systematic form of can be row reduced using elementary row operations to match this matrix
the addition of the fourth row effectively computes the sum of all the codeword bits data and parity as the fourth parity bit
for example is encoded using the non systematic form of at the start of this section into where blue digits are data red digits are parity bits from the hamming code and the green digit is the parity bit added by the code
the green digit makes the parity of the codewords even
finally it can be shown that the minimum distance has increased from in the code to in the code
therefore the code can be defined as hamming code
to decode the hamming code first check the parity bit
if the parity bit indicates an error single error correction the hamming code will indicate the error location with no error indicating the parity bit
if the parity bit is correct then single error correction will indicate the bitwise exclusive or of two error locations
if the locations are equal no error then double bit error either has not occurred or has cancelled itself out
otherwise double bit error has occurred
see also notes references external links visual explanation of hamming codes cgi script for calculating hamming distances from tervo unb canada tool for calculating hamming code